{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FhGU4GF2qR0B",
    "outputId": "9fed5e71-f562-45cc-b97e-6ff7b28f2310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in ./venv/lib/python3.7/site-packages (0.46.3)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.7/site-packages (from simpletransformers) (0.8.1rc1)\n",
      "Requirement already satisfied: tensorboardx in ./venv/lib/python3.7/site-packages (from simpletransformers) (2.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.7/site-packages (from simpletransformers) (1.19.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.7/site-packages (from simpletransformers) (0.23.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.7/site-packages (from simpletransformers) (2.24.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.7/site-packages (from simpletransformers) (1.5.2)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.7/site-packages (from simpletransformers) (1.1.0)\n",
      "Requirement already satisfied: transformers>=3.0.2 in ./venv/lib/python3.7/site-packages (from simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in ./venv/lib/python3.7/site-packages (from simpletransformers) (4.48.2)\n",
      "Requirement already satisfied: seqeval in ./venv/lib/python3.7/site-packages (from simpletransformers) (0.0.12)\n",
      "Requirement already satisfied: wandb in ./venv/lib/python3.7/site-packages (from simpletransformers) (0.9.5)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.7/site-packages (from simpletransformers) (2020.7.14)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in ./venv/lib/python3.7/site-packages (from tensorboardx->simpletransformers) (3.12.4)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.7/site-packages (from tensorboardx->simpletransformers) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.7/site-packages (from scikit-learn->simpletransformers) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.7/site-packages (from scikit-learn->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests->simpletransformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests->simpletransformers) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.7/site-packages (from requests->simpletransformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests->simpletransformers) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.7/site-packages (from pandas->simpletransformers) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./venv/lib/python3.7/site-packages (from pandas->simpletransformers) (2020.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in ./venv/lib/python3.7/site-packages (from transformers>=3.0.2->simpletransformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in ./venv/lib/python3.7/site-packages (from transformers>=3.0.2->simpletransformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.7/site-packages (from transformers>=3.0.2->simpletransformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.7/site-packages (from transformers>=3.0.2->simpletransformers) (20.4)\n",
      "Requirement already satisfied: Keras>=2.2.4 in ./venv/lib/python3.7/site-packages (from seqeval->simpletransformers) (2.4.3)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (7.352.0)\n",
      "Requirement already satisfied: Click>=7.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (7.1.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (5.3.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (3.1.7)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (0.10.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (5.7.2)\n",
      "Requirement already satisfied: configparser>=3.8.1 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (0.16.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (1.0.1)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (3.5.4)\n",
      "Requirement already satisfied: gql==0.2.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./venv/lib/python3.7/site-packages (from wandb->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (49.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv/lib/python3.7/site-packages (from packaging->transformers>=3.0.2->simpletransformers) (2.4.7)\n",
      "Requirement already satisfied: h5py in ./venv/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb->simpletransformers) (4.0.5)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in ./venv/lib/python3.7/site-packages (from watchdog>=0.8.3->wandb->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: graphql-core<2,>=0.5.0 in ./venv/lib/python3.7/site-packages (from gql==0.2.0->wandb->simpletransformers) (1.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in ./venv/lib/python3.7/site-packages (from gql==0.2.0->wandb->simpletransformers) (2.3)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in ./venv/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->simpletransformers) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/Users/pablo/Desktop/resume_chatbot/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in ./venv/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.7/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.7/site-packages (from transformers) (4.48.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.7/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in ./venv/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in ./venv/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.7/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.2 is available.\r\n",
      "You should consider upgrading via the '/Users/pablo/Desktop/resume_chatbot/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYf4TtimxtHj"
   },
   "outputs": [],
   "source": [
    "personality= 'My name is Pablo Marino\\n'\\\n",
    "'I am a software and machine learning engineer\\n'\\\n",
    "'I am 25 years old\\n'\\\n",
    "'I live in Buenos Aires, Argentina\\n'\\\n",
    "'I studied software engineering, an AI specialization from Stanford university and currently study Computer Science at the university of London\\n'\\\n",
    "'I\\'m passionate about technology, AI, personal development and social dynamics.\\n'\\\n",
    "'In the future I see myself as a leader in the field of technology, and having a great work-life balance.\\n'\\\n",
    "'On my free time I like to read, exercise, travel with friends and new people.\\n'\\\n",
    "'I am working on a probability course\\n'\\\n",
    "'My biggest strength is that I love to improve all the time and see hard things as challenges\\n'\\\n",
    "'My biggest weakness is that in the past I have sometimes struggled with confidence, it has been helpful for me to keep a running document of the impact I have made on my team and at my organization to better understand why I should be confident about the skills and unique talents I bring to the table.'\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGmDkYNoQK2-"
   },
   "source": [
    "# QA using simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "aL1XiRttsih9",
    "outputId": "d56aaad5-d306-4e11-d417-cb22d152c9e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personality:  My name is Pablo Marino\n",
      "I am a software and machine learning engineer\n",
      "I am 25 years old\n",
      "I live in Buenos Aires, Argentina\n",
      "I studied software engineering, an AI specialization from Stanford university and currently study Computer Science at the university of London\n",
      "I'm passionate about technology, AI, personal development and social dynamics.\n",
      "In the future I see myself as a leader in the field of technology, and having a great work-life balance.\n",
      "On my free time I like to read, exercise, travel with friends and new people.\n",
      "I am working on a probability course\n",
      "My biggest strength is that I love to improve all the time and see hard things as challenges\n",
      "My biggest weakness is that in the past I have sometimes struggled with confidence, it has been helpful for me to keep a running document of the impact I have made on my team and at my organization to better understand why I should be confident about the skills and unique talents I bring to the table.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simpletransformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1c1510e24de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"personality: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersonality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimpletransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_answering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuestionAnsweringModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuestionAnsweringModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'twmkn9/bert-base-uncased-squad2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"max_answer_length\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_best_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"null_score_diff_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simpletransformers'"
     ]
    }
   ],
   "source": [
    "print(\"personality: \", personality)\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel\n",
    "\n",
    "model = QuestionAnsweringModel('bert', 'twmkn9/bert-base-uncased-squad2', use_cuda=False, args={\"max_answer_length\":200, \"n_best_size\":1,\"null_score_diff_threshold\": 0.4})\n",
    "\n",
    "def find_answers(questions=[]):\n",
    "    formatted_questions = list(map(lambda question: {'question': question[1], 'id': question[0]}, enumerate(questions)))\n",
    "    to_predict = [{'context':  personality,\n",
    "                   'qas': formatted_questions}]\n",
    "    predictions = model.predict(to_predict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600,
     "referenced_widgets": [
      "ec56803e768744638155cd635c7a7796",
      "b073304eee0848c7859c7687575f8fd5",
      "18a64b69f76c492a8627cf58afe133e5",
      "2cecb4cd0d1c4f7680f6e8a3f84db04e",
      "5eadd28c8ef146bba6f7210511f4927f",
      "312a194a1ea24da496e9a7eb43a3ea8e",
      "f718f4879dcd4ea193a3d6587c0f0405",
      "111eb32a733a4b70ac5387baf23ccce6"
     ]
    },
    "colab_type": "code",
    "id": "Y6prUm9PxRWZ",
    "outputId": "3c6e3bbe-b3e8-45a3-ef12-ceffb0ce21ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 14/14 [00:00<00:00, 110.59it/s]\n",
      "add example index and unique id: 100%|██████████| 14/14 [00:00<00:00, 21620.12it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec56803e768744638155cd635c7a7796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Prediction', max=2.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'answer': ['25'], 'id': 0},\n",
       "  {'answer': ['working on a probability course'], 'id': 1},\n",
       "  {'answer': ['empty'], 'id': 2},\n",
       "  {'answer': ['technology'], 'id': 3},\n",
       "  {'answer': ['read, exercise, travel with friends and new people'], 'id': 4},\n",
       "  {'answer': ['I'], 'id': 5},\n",
       "  {'answer': ['empty'], 'id': 6},\n",
       "  {'answer': ['biggest strength is'], 'id': 7},\n",
       "  {'answer': ['empty'], 'id': 8},\n",
       "  {'answer': ['empty'], 'id': 9},\n",
       "  {'answer': ['empty'], 'id': 10},\n",
       "  {'answer': ['empty'], 'id': 11},\n",
       "  {'answer': ['empty'], 'id': 12},\n",
       "  {'answer': ['I am working on a probability course'], 'id': 13}],\n",
       " [{'id': 0, 'probability': [0.9999998968502151]},\n",
       "  {'id': 1, 'probability': [0.9998231446316195]},\n",
       "  {'id': 2, 'probability': [3.1040746283127235e-07]},\n",
       "  {'id': 3, 'probability': [0.8445467504915382]},\n",
       "  {'id': 4, 'probability': [0.9991615644493881]},\n",
       "  {'id': 5, 'probability': [0.9938013402438216]},\n",
       "  {'id': 6, 'probability': [0.0069676281427095785]},\n",
       "  {'id': 7, 'probability': [0.9990206422908099]},\n",
       "  {'id': 8, 'probability': [0.9821128803004743]},\n",
       "  {'id': 9, 'probability': [0.7768635656451651]},\n",
       "  {'id': 10, 'probability': [0.42233553734947304]},\n",
       "  {'id': 11, 'probability': [7.17668960673443e-08]},\n",
       "  {'id': 12, 'probability': [0.00012192826551237611]},\n",
       "  {'id': 13, 'probability': [0.9310988813691834]}])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\"How old are you?\", \"what do you do?\", \"how do you see yourself in 5 years\",\n",
    "              \"Tell me about yourself\", \"what do you like?\", \"What are your biggest strengths?\",\n",
    "             \"why should we hire you?\", \"What do you consider to be your biggest professional achievement?\",\n",
    "             \"Describe your dream job.\", \"What kind of work environment do you like best?\",\n",
    "             \"Tell me how you think other people would describe you\", \"what is my biggest weakness?\",\n",
    "             \"What do you like to do outside of work?\",\n",
    "             \"What questions do you have for me?\"]\n",
    "result = find_answers(questions)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833,
     "referenced_widgets": [
      "f7bb2959734b42538ded1e272eeab331",
      "b31720de8e0147249d8b062f7f14cb7a",
      "fe557a2d8a7c4a678497293eb41291d4",
      "9340fec0fcc94e85b8d52013555ec888",
      "5f34d2a84ebb46a5a4d8b92db480e4d1",
      "52eb30b8e8264e89a18053023b619075",
      "46abf6cfdf6b4b0889b39deff11c2a74",
      "efc69b7d75f847048c479e83a28122d2"
     ]
    },
    "colab_type": "code",
    "id": "crRHskJJyANP",
    "outputId": "16f4ce5e-7f3a-4135-c709-d62fad12cf8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 88.46it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 1600.88it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bb2959734b42538ded1e272eeab331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Prediction', max=1.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'answer': ['in the past',\n",
       "    'that in the past',\n",
       "    'the past',\n",
       "    'in the past I',\n",
       "    'past',\n",
       "    'that in the past I',\n",
       "    'the past I',\n",
       "    'past I',\n",
       "    'biggest weakness is that in the past',\n",
       "    '',\n",
       "    'I am working on a probability course My biggest weakness is that in the past',\n",
       "    'is that in the past',\n",
       "    'in',\n",
       "    'biggest weakness is that in the past I',\n",
       "    'technology, AI, personal development and social dynamics. In the future I see myself as a leader in the field of technology, and having a great work-life balance. On my free time I like to read, exercise, travel with friends and new people. I am working on a probability course My biggest weakness is that in the past',\n",
       "    \"Pablo Marino I am a software and machine learning engineer I am 25 years old I live in Buenos Aires, Argentina I studied software engineering, an AI specialization from Stanford university and currently study Computer Science at the university of London I'm passionate about technology, AI, personal development and social dynamics. In the future I see myself as a leader in the field of technology, and having a great work-life balance. On my free time I like to read, exercise, travel with friends and new people. I am working on a probability course My biggest weakness is that in the past\",\n",
       "    'I',\n",
       "    \"software and machine learning engineer I am 25 years old I live in Buenos Aires, Argentina I studied software engineering, an AI specialization from Stanford university and currently study Computer Science at the university of London I'm passionate about technology, AI, personal development and social dynamics. In the future I see myself as a leader in the field of technology, and having a great work-life balance. On my free time I like to read, exercise, travel with friends and new people. I am working on a probability course My biggest weakness is that in the past\",\n",
       "    'My biggest weakness is that in the past'],\n",
       "   'id': 0}],\n",
       " [{'id': 0,\n",
       "   'probability': [0.6041763617913505,\n",
       "    0.21564596392613075,\n",
       "    0.07114226965614012,\n",
       "    0.05578586202814547,\n",
       "    0.0215085348912835,\n",
       "    0.019911397981280287,\n",
       "    0.00656883170278207,\n",
       "    0.001985963430140063,\n",
       "    0.0015809333731078795,\n",
       "    0.0003849574255110488,\n",
       "    0.000230535450766178,\n",
       "    0.00022254868352702484,\n",
       "    0.0002131277990705924,\n",
       "    0.0001459734882152574,\n",
       "    9.378012226418615e-05,\n",
       "    8.488393827878231e-05,\n",
       "    8.130356770381003e-05,\n",
       "    8.046416223462452e-05,\n",
       "    8.023583091560067e-05]}])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_answers([\"what is your greatest weakness?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ypL5y8CJ3Sf"
   },
   "source": [
    "# QA Using transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YX13sQ29J64m"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer, BertTokenizer, BertForQuestionAnswering\n",
    "import transformers\n",
    "BERT_MODEL = \"twmkn9/bert-base-uncased-squad2\"\n",
    "qa_bert_pipeline = pipeline('question-answering', model=BERT_MODEL, tokenizer=BERT_MODEL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "NaXnLop3X_bo",
    "outputId": "54627c47-ad65-43de-96ef-9512e3172e8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'I have sometimes struggled with confidence,',\n",
       " 'end': 647,\n",
       " 'score': 0.6880888952426,\n",
       " 'start': 604}"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"what is your greatest weakness?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "z70YBEixTorT",
    "outputId": "62dae0c0-00aa-4398-9148-110f2ddeae6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'a probability course',\n",
       " 'end': 563,\n",
       " 'score': 0.6277866026731317,\n",
       " 'start': 543}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"what are you working on?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Co1elYgdTtly",
    "outputId": "afcdf218-dd21-4b92-8eec-5921670734cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'I love to improve all the time and see hard things',\n",
       " 'end': 642,\n",
       " 'score': 0.0302563564158057,\n",
       " 'start': 592}"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"What's your biggest strength?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z7LzEbrEZW2g",
    "outputId": "d482c68a-8c59-4f2c-af0e-1833ec4c7ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '25 years old', 'end': 87, 'score': 0.3976427498947909, 'start': 75}"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"age?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "76z2QFiNjooo",
    "outputId": "3c73a641-60cb-42e7-b463-114bbcb9a0db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'read, exercise, travel with friends and new people.',\n",
       " 'end': 526,\n",
       " 'score': 0.48635351486282885,\n",
       " 'start': 475}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"What do you like to do outside of work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "KRutwvO_jyqw",
    "outputId": "ecc7ae05-7ab3-44ea-fdad-7339bdbe6c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'I like to read, exercise, travel with friends and new people.',\n",
       " 'end': 526,\n",
       " 'score': 0.009527206870379458,\n",
       " 'start': 465}"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"what questions do oyu have for me?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "-ej3eAlEiB2n",
    "outputId": "5be49807-6823-43e3-fda8-fe8363d1c9e9"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0b0dd9ae0213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqa_bert_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpersonality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         ),\n\u001b[1;32m   1315\u001b[0m                     }\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m                 ]\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         ),\n\u001b[1;32m   1315\u001b[0m                     }\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m                 ]\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGgBqJk5SToJ"
   },
   "source": [
    "in comparison w simpletransformers, vanilla transformers QA pipeline works much  better, and returns probability of answer, will use this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTWpB8DtiV9q"
   },
   "source": [
    "when there is an exception OR score <.1: fallback to dialogAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6N3k8Ur1U8l9"
   },
   "source": [
    "## Answers sometimes are too short, try w other models or parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "2J3yqu3BVFN_",
    "outputId": "70d59812-31fe-44e6-bce2-079e24066e88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'I love to improve all the time and see hard things as challenges',\n",
       " 'end': 656,\n",
       " 'score': 0.03640293981385039,\n",
       " 'start': 592}"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_MODEL = \"deepset/roberta-base-squad2\"\n",
    "qa_bert_pipeline = pipeline('question-answering', model=BERT_MODEL, tokenizer=BERT_MODEL)\n",
    "qa_bert_pipeline(context=personality, question=\"What's your biggest strength?\", max_length=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Zak5Nvv5bRfV",
    "outputId": "c2fe5e1c-b52b-4872-d505-2f1d3fe8c643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'confidence,',\n",
       " 'end': 740,\n",
       " 'score': 0.4154019417819221,\n",
       " 'start': 729}"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bert_pipeline(context=personality, question=\"What's your greatest weakness?\", max_length=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7qnHmwKjDXk"
   },
   "source": [
    "# DialoGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_mQ7s59qzEI"
   },
   "source": [
    "### interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "id": "AmmlV3YDjIcZ",
    "outputId": "bf6037ba-10d4-44e7-bbab-7d2e10928874"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4be6e91d6d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"microsoft/DialoGPT-medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"microsoft/DialoGPT-medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 50 lines\n",
    "for step in range(50):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZD44Y9GjrglO"
   },
   "source": [
    "w default hyper parameters model has the following problem: https://github.com/microsoft/DialoGPT/issues/45\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YUojyoOsjr4"
   },
   "source": [
    "### try same as above but with no history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ux5pKs-xsoZp"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 50 lines\n",
    "for step in range(50):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = new_user_input_ids#torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOyzNZXEy0yw"
   },
   "source": [
    "Having no history works perfectly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orUe-SH3rhse"
   },
   "source": [
    " this model has a seriuos bug w history, it gets screwed as soon as there is history, will move on and expriment w other models, and maybe come back to this later to debug it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZ1q67kJrkAK"
   },
   "source": [
    "### tinkering w model temperature and repetition_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VhR_TJtJC1V"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# Let's chat for 50 lines\n",
    "for step in range(50):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, temperature=0.6, repetition_penalty=1.3)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCskUmVHJfgS"
   },
   "source": [
    "Tweaking hyper paramters dramaticaly improved the responses.  \n",
    "I noticed that as chat history grows bot becomes more stupid, for implementation cap chat history at 5 last messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mJxOz3Y9Ds_"
   },
   "source": [
    "# Building and debugging the helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dTvX36ZB9HFd",
    "outputId": "21339f46-db4a-4b89-e989-2034b0582449"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at microsoft/DialoGPT-medium and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:how old are you?\n",
      "'chat_history_ids.shape:  tensor([[ 4919,  1468,   389,   345,    30, 50256,    40,  1101,  2310,   764,\n",
      "           314,  1053,   587,  2712,  1201,   262,  3726,   286,   262,   614,\n",
      "           837,   290,   423,   257,  1256,   517,   640,   284,   711,   783,\n",
      "           326,   314,  1101,   287,  4152,   764, 50256]])\n",
      "text:  how old are you?<|endoftext|>I'm 21. I've been playing since the beginning of the year, and have a lot more time to play now that I'm in college.<|endoftext|>\n",
      "DialoGPT: I'm 21. I've been playing since the beginning of the year, and have a lot more time to play now that I'm in college.\n",
      ">> User:how old are you?\n",
      "'chat_history_ids.shape:  tensor([[ 4919,  1468,   389,   345,    30, 50256,    40,  1101,  2310,   764,\n",
      "           314,  1053,   587,  2712,  1201,   262,  3726,   286,   262,   614,\n",
      "           837,   290,   423,   257,  1256,   517,   640,   284,   711,   783,\n",
      "           326,   314,  1101,   287,  4152,   764, 50256,  4919,  1468,   389,\n",
      "           345,    30, 50256,  1238,   812,  1862,  5145, 50256]])\n",
      "text:  how old are you?<|endoftext|>I'm 21. I've been playing since the beginning of the year, and have a lot more time to play now that I'm in college.<|endoftext|>how old are you?<|endoftext|>20 years young!<|endoftext|>\n",
      "DialoGPT: 20 years young!\n",
      ">> User:you are 25 years old\n",
      "'chat_history_ids.shape:  tensor([[ 4919,  1468,   389,   345,    30, 50256,    40,  1101,  2310,   764,\n",
      "           314,  1053,   587,  2712,  1201,   262,  3726,   286,   262,   614,\n",
      "           837,   290,   423,   257,  1256,   517,   640,   284,   711,   783,\n",
      "           326,   314,  1101,   287,  4152,   764, 50256,  4919,  1468,   389,\n",
      "           345,    30, 50256,  1238,   812,  1862,  5145, 50256,  5832,   389,\n",
      "          1679,   812,  1468, 50256,  1639,   821,   826,  2644,   616,  2089,\n",
      "         19462, 50256]])\n",
      "text:  how old are you?<|endoftext|>I'm 21. I've been playing since the beginning of the year, and have a lot more time to play now that I'm in college.<|endoftext|>how old are you?<|endoftext|>20 years young!<|endoftext|>you are 25 years old<|endoftext|>You're right... my bad lol<|endoftext|>\n",
      "DialoGPT: You're right... my bad lol\n",
      ">> User:how old are you?\n",
      "'chat_history_ids.shape:  tensor([[ 4919,  1468,   389,   345,    30, 50256,    40,  1101,  2310,   764,\n",
      "           314,  1053,   587,  2712,  1201,   262,  3726,   286,   262,   614,\n",
      "           837,   290,   423,   257,  1256,   517,   640,   284,   711,   783,\n",
      "           326,   314,  1101,   287,  4152,   764, 50256,  4919,  1468,   389,\n",
      "           345,    30, 50256,  1238,   812,  1862,  5145, 50256,  5832,   389,\n",
      "          1679,   812,  1468, 50256,  1639,   821,   826,  2644,   616,  2089,\n",
      "         19462, 50256,  4919,  1468,   389,   345,    30, 50256,  1495,   812,\n",
      "          1468, 50256]])\n",
      "text:  how old are you?<|endoftext|>I'm 21. I've been playing since the beginning of the year, and have a lot more time to play now that I'm in college.<|endoftext|>how old are you?<|endoftext|>20 years young!<|endoftext|>you are 25 years old<|endoftext|>You're right... my bad lol<|endoftext|>how old are you?<|endoftext|>25 years old<|endoftext|>\n",
      "DialoGPT: 25 years old\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-33f16082f642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# encode the new user input, add the eos_token and return a tensor in Pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_user_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">> User:\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# append the new user input tokens to the chat history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Let's chat for 50 lines\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "for step in range(50):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, temperature=0.6, repetition_penalty=1.3)\n",
    "    print(\"'chat_history_ids.shape: \", chat_history_ids)\n",
    "    text = tokenizer.decode(chat_history_ids[:, :][0])\n",
    "    print(\"text: \", text)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lE_sNtsDRSZw"
   },
   "outputs": [],
   "source": [
    "def chat(text, history=[]):\n",
    "    chat_history_ids = None;\n",
    "    for chat_text in history:\n",
    "        # add chat tokens to chat_history_ids\n",
    "        input_ids = tokenizer.encode(chat_text + tokenizer.eos_token, return_tensors='pt')\n",
    "        if chat_history_ids == None:\n",
    "            chat_history_ids = input_ids\n",
    "        else:\n",
    "            chat_history_ids = torch.cat([chat_history_ids, input_ids], dim=-1)\n",
    "\n",
    "    chat_history_text = tokenizer.decode(chat_history_ids[:, :][0])\n",
    "    print(\"chat_history_text: \", chat_history_text)\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    user_input = tokenizer.decode(new_user_input_ids[:, :][0])\n",
    "    print(\"user_input: \", user_input)\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if len(history) > 0 else new_user_input_ids\n",
    "    bot_input_text = tokenizer.decode(bot_input_ids[:, :][0])\n",
    "    print(\"bot_input_text: \", bot_input_text)\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id, temperature=0.6, repetition_penalty=1.3)\n",
    "\n",
    "    #return last output tokens from bot\n",
    "    return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "zWvRhf8QavNj",
    "outputId": "47b47a44-16bd-419e-bc4e-6d75040d443b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>\n",
      "user_input:  how old are you?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>how old are you?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I'm a teenager. I was born in 2000, so that's about 3 years ago now.\""
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\"]\n",
    "chat(\"how old are you?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "FyUkbiIXa4tw",
    "outputId": "382a09c1-b9f4-47a2-cbbe-1144581b517f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  I hate red<|endoftext|>\n",
      "user_input:  whats my favorite color?<|endoftext|>\n",
      "bot_input_text:  I hate red<|endoftext|>whats my favorite color?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Blue, duh.'"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"I hate red\"]\n",
    "chat(\"whats my favorite color?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "iChxelk0cWR8",
    "outputId": "0db7ca63-31ba-474f-b306-c31f0e8eb50a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>you live in argentina<|endoftext|>\n",
      "user_input:  whats your name?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>you live in argentina<|endoftext|>whats your name?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Pablo, you're a troll.\""
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\", \"you live in argentina\"]\n",
    "chat(\"whats your name?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "L5xVKQesccco",
    "outputId": "c5cd0e92-0a02-44d3-9b74-102d53d6ef7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>you live in argentina<|endoftext|>\n",
      "user_input:  where do you live?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>you live in argentina<|endoftext|>where do you live?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I'm not sure, but I think it's somewhere near the south of Spain.\""
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\", \"you live in argentina\"]\n",
    "chat(\"where do you live?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "kWRgvaGNcgtH",
    "outputId": "63bf143b-ead2-44ab-9d29-1f566cd1a8d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>you live in argentina<|endoftext|>\n",
      "user_input:  where do I live?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>you live in argentina<|endoftext|>where do I live?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"in the middle of nowhere, where you can't even get a job.\""
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\", \"you live in argentina\"]\n",
    "chat(\"where do I live?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "2AosQMFKclr7",
    "outputId": "52a7111c-d3be-4f61-d168-fb9ee79dbc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>\n",
      "user_input:  who is the president of the USA?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>who is the president of the USA?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"I am not a citizen. I have no power to vote on anything, but you can't deny that he's a good guy and has done some great things for our country! s\""
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\"]\n",
    "chat(\"who is the president of the USA?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "MD0mxeTXcqE3",
    "outputId": "d4c51944-fccd-4647-9281-9978eecaa4bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>I live in USA<|endoftext|>\n",
      "user_input:  where do I live?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>I live in USA<|endoftext|>where do I live?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'New Zealand.'"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\", \"I live in Usa\"]\n",
    "chat(\"where do I live?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "TJeAkCD6c4E_",
    "outputId": "9c1f96b5-4364-42d4-baa9-e8b59c3bf79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_history_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>I live in europe<|endoftext|>\n",
      "user_input:  where do you live?<|endoftext|>\n",
      "bot_input_text:  you are 1 year old<|endoftext|>your name is pablo<|endoftext|>I live in europe<|endoftext|>where do you live?<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"In the US, but I'm from Spain.\""
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\", \"I live in europe\"]\n",
    "chat(\"where do you live?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "gPsDFQ-Oc6uR",
    "outputId": "33c6e2cc-19a6-4df5-eb3f-8b822406c561"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b97856fe1e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"you are 1 year old\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"your name is pablo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I live in europe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"whats your name?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'chat' is not defined"
     ]
    }
   ],
   "source": [
    "history = [\"you are 1 year old\", \"your name is pablo\", \"I live in europe\"]\n",
    "chat(\"whats your name?\", history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-p + top-k parameters: https://github.com/microsoft/DialoGPT/issues/45#issuecomment-680338798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: sacremoses in ./venv/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.7/site-packages (from transformers) (4.48.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.7/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.7/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in ./venv/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in ./venv/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./venv/lib/python3.7/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./venv/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/Users/pablo/Desktop/resume_chatbot/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c310ed66fb12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's chat for 50 lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "for step in range(50):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id,\n",
    "                                        top_k=50, \n",
    "                                        top_p=0.95,\n",
    "                                        do_sample=True, \n",
    ")\n",
    "    print(\"'chat_history_ids.shape: \", chat_history_ids)\n",
    "    text = tokenizer.decode(chat_history_ids[:, :][0])\n",
    "    print(\"text: \", text)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVuD8ePbdFCX"
   },
   "source": [
    "# requests to local API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests;\n",
    "URL = \"http://127.0.0.1:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have sometimes struggled with confidence,\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": 'what are your greatest weakneses?'}\n",
    "response = requests.request(\"POST\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": 'None'}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pablo Marino\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"what's my name?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have sometimes struggled with confidence,\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"what's my biggest weakness?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to improve all the time and see hard things as challenges.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"what's my biggest strength?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software engineering, data science and Machine learning\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"what skills do you have?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to read, exercise, travel with friends and meet new people.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"what do you do?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pablo Marino\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"who are you?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buenos Aires, Argentina\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"where do you live?\"}\n",
    "response = requests.request(\"GET\", URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buenos Aires, Argentina\n"
     ]
    }
   ],
   "source": [
    "URL_PROD = \"https://api.pablomarino.me\"\n",
    "payload = {\"msg\": \"where do you live?\"}\n",
    "response = requests.request(\"POST\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're not a real Argentinian.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"I live in Argentina?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the mountains.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"where do I live?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a good guy.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"My name is pablo\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pablo Marino\n"
     ]
    }
   ],
   "source": [
    "payload = {\"msg\": \"what is my name?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to read, exercise, travel with friends and meet new people.\n",
      "CPU times: user 5.76 ms, sys: 3.2 ms, total: 8.96 ms\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\"msg\": \"what do I do?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have sometimes struggled with confidence,\n",
      "CPU times: user 6.06 ms, sys: 2.76 ms, total: 8.82 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\"msg\": \"what are your greatest weakneses?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, not at all.\n",
      "CPU times: user 4.25 ms, sys: 1.94 ms, total: 6.19 ms\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\"msg\": \"u like wine?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not really, no.\n",
      "CPU times: user 4.36 ms, sys: 1.74 ms, total: 6.1 ms\n",
      "Wall time: 2.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\"msg\": \"do you like wine?\"}\n",
    "response = requests.request(\"GET\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'URL_PROD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'URL_PROD' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\"msg\": \"what brand?\"}\n",
    "response = requests.request(\"POST\", URL_PROD, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "111eb32a733a4b70ac5387baf23ccce6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18a64b69f76c492a8627cf58afe133e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Running Prediction: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_312a194a1ea24da496e9a7eb43a3ea8e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5eadd28c8ef146bba6f7210511f4927f",
      "value": 2
     }
    },
    "2cecb4cd0d1c4f7680f6e8a3f84db04e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_111eb32a733a4b70ac5387baf23ccce6",
      "placeholder": "​",
      "style": "IPY_MODEL_f718f4879dcd4ea193a3d6587c0f0405",
      "value": " 2/2 [00:05&lt;00:00,  2.80s/it]"
     }
    },
    "312a194a1ea24da496e9a7eb43a3ea8e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46abf6cfdf6b4b0889b39deff11c2a74": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52eb30b8e8264e89a18053023b619075": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eadd28c8ef146bba6f7210511f4927f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5f34d2a84ebb46a5a4d8b92db480e4d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9340fec0fcc94e85b8d52013555ec888": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efc69b7d75f847048c479e83a28122d2",
      "placeholder": "​",
      "style": "IPY_MODEL_46abf6cfdf6b4b0889b39deff11c2a74",
      "value": " 1/1 [00:13&lt;00:00, 13.72s/it]"
     }
    },
    "b073304eee0848c7859c7687575f8fd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b31720de8e0147249d8b062f7f14cb7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec56803e768744638155cd635c7a7796": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_18a64b69f76c492a8627cf58afe133e5",
       "IPY_MODEL_2cecb4cd0d1c4f7680f6e8a3f84db04e"
      ],
      "layout": "IPY_MODEL_b073304eee0848c7859c7687575f8fd5"
     }
    },
    "efc69b7d75f847048c479e83a28122d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f718f4879dcd4ea193a3d6587c0f0405": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7bb2959734b42538ded1e272eeab331": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe557a2d8a7c4a678497293eb41291d4",
       "IPY_MODEL_9340fec0fcc94e85b8d52013555ec888"
      ],
      "layout": "IPY_MODEL_b31720de8e0147249d8b062f7f14cb7a"
     }
    },
    "fe557a2d8a7c4a678497293eb41291d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Running Prediction: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52eb30b8e8264e89a18053023b619075",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5f34d2a84ebb46a5a4d8b92db480e4d1",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
