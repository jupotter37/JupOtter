{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# llamafile setup\n",
    "\n",
    "# Step 1: Download a llamafile. The download may take several minutes.\n",
    "# wget https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/resolve/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile\n",
    "\n",
    "# Step 2: Make the llamafile executable. Note: if you're on Windows, just append '.exe' to the filename.\n",
    "# chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile\n",
    "\n",
    "# Step 3: Start llamafile server in background. All the server logs will be written to 'tinyllama.log'.\n",
    "# Alternatively, you can just open a separate terminal outside this notebook and run: \n",
    "#   ./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile --server --nobrowser --embedding\n",
    "# ./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile --server --nobrowser --embedding > tinyllama.log 2>&1 &\n",
    "./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding > tinyllama.log 2>&1 &\n",
    "pid=$!\n",
    "echo \"${pid}\" > .llamafile_pid  # write the process pid to a file so we can terminate the server later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from time import perf_counter\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from langchain_community.embeddings import LlamafileEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms.llamafile import Llamafile\n",
    "from utils.text import get_pdf_text, get_text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"chroma\"\n",
    "statistics = [f\"save_{experiment}\", f\"search_{experiment}\", f\"execute_{experiment}\"]\n",
    "dict_json = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = chromadb.HttpClient(settings=Settings(allow_reset=True))\n",
    "client.reset()  # resets the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = LlamafileEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llamafile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_text_chunks(get_pdf_text(\"resume.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record start time\n",
    "time_start = perf_counter()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs, embedder, client=client, collection_name=\"test_collection\"\n",
    ")\n",
    "time_duration = perf_counter() - time_start\n",
    "dict_json.setdefault(experiment, {})[\"save\"] = time_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 0, 'source': 'resume.pdf'}, page_content='DataOrchestrationandDataOpsEngineeringTemplate(03/2023-Present)-Developedanopen-sourcerepositoryandprojectprovidingacomprehensiveframeworkandasuiteoftoolsforDataOrchestrationandDataOps.-Designedtosimplifytheend-to-endmanagementofdataworkflows,theprojectincludestoolscapableofperformingdataextraction,transformation,andloading(ETL),datavalidation,andmonitoring.-Aimedatstreamliningdataoperationsandenhancingdatareliability,thetemplatefacilitatesseamlessorchestrationofdatapipelines,ensuringefficientdataflowandtimelyprocessing.-Committedtofosteringacollaborativeandthrivingdatacommunity,theprojectembracesopen-sourceprinciples,enablingdataengineerstoleveragethetemplatefordiversedata-drivenprojectswithease.'), Document(metadata={'page': 0, 'source': 'resume.pdf'}, page_content=\"0 3 / 2 0 1 0-1 2 / 2 0 1 5,8 0 . 5 %Master's,AppliedInformaticsUniversidadeFederalRuraldePernambuc o\\n0 3 / 2 0 1 8-0 2 / 2 0 2 0,H i g h e s tD i s t i n c t i o n-1 0 0 %SKILLS\\nPERSONALPROJECTSGravitationalWaveDetectionusingArtificialNeuralNetworks(02/2019-02/2020)-DevelopedacomputationalprocedurebasedonArtificialNeuralNetworks(ANN)todetectblackhole-blackholegravitationalwaveeventsfromLIGOdata.-Achievedaccurateidentificationwithahigh-scoresystemindicatinggravitationalwaveobservationandlowvaluesindicatingnoise.-ReceivedtopmarksandearnedmaximumscoreduringthethesisdefensepresentationformyMaster'sproject,demonstratingtheefficacyandsignificanceoftheANN-basedapproachindetectinggravitationalwavesignals.-PresentedresearchfindingsinanarticleshowcasingtheeffectivenessoftheANN-basedapproachindetectinggravitationalwavesignals.\"), Document(metadata={'page': 0, 'source': 'resume.pdf'}, page_content=\"-Mentoredandguidedstudentsinacademicprojects,fosteringcriticalthinkingandproblem-solvingskills.\\nSubstituteProfessoratIFPBInstitutoFederaldaParaíba\\n0 5 / 2 0 1 7-0 4 / 2 0 1 8,O n - s i t e\\nA c h i e v e m e n t s / T a s k s,M o t e i r o ,P a r a í b a ,B r a z i l\\n-Conduct edlecturesandpracticalsessionsinvariouscourses,includingAdminis trationofProprietaryandOpenOperatingSystems,IntroductiontoProgrammingandProjectManagement.\\n-Developedanddeliveredengagingteachingmaterials,integratinginnovativetoolslikeScratch,AppInventor,andunpluggedactivitiestoenhancestudentlearningexperiences.-Evaluatedstudentperformanceandprovidedvaluablefeedbacktosupporttheiracademicprogressandfostercontinuousimprovement.\\nEDUCATIONBachelor's,InformationSystemsUniversidadeFederalRuraldePernambuc o\\n0 3 / 2 0 1 0-1 2 / 2 0 1 5,8 0 . 5 %Master's,AppliedInformaticsUniversidadeFederalRuraldePernambuc o\\n0 3 / 2 0 1 8-0 2 / 2 0 2 0,H i g h e s tD i s t i n c t i o n-1 0 0 %SKILLS\"), Document(metadata={'page': 0, 'source': 'resume.pdf'}, page_content='GersonSantos\\nDataScientist|SoftwareDeveloperProficientinPython/MachineLearningbasedmicroservicesdevelopmen tandanabilitytotranslatebusinessrequirementsintotechnicalsolutions.Ihaveapassionforconsistentlearningandinnovating.\\nWORKEXPERIENCEDataScientistatCESARCentrodeEstudoseSistemasAvançadosdoRecife\\n0 2 / 2 0 2 0-P r e s e n t,R e m o t e\\nA c h i e v e m e n t s / T a s k s,R e c i f e ,P e r n a m b u c o ,B r a z i l\\n-WorkinginanR&DteamfocusedresearchinthefieldofSoftwareEngineering ,DataScienceandAIdevelopingsolutionstosolveavarietyofproblemsforaglobalclient;\\n-ResponsiblefordevelopingmicroservicesinPythoninvolvingpipelines,datastreaming(Spark),messagingsystems(Kafka,RabbitMQ )withtechnologiessuchasDocker,Kubernetes,HelmChart,RESTAPI,gRPC,MongoDB,SQL,OracleDB,CI/CD\\n-Planned,trained,evaluated,deployed,andmaintainedMachinelearning /Deeplearningmodelsusingtools/frameworkssuchasPyTorch,Scikit-Learn,Feast,ApacheAirflow,MLflow,Pandas,Numpy;')]\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"What professions did Gerson have?\"\n",
    "time_start = perf_counter()\n",
    "results = vectorstore.similarity_search(query)\n",
    "time_duration = perf_counter() - time_start\n",
    "dict_json.setdefault(experiment, {})[\"search\"] = time_duration\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gerson has experience working as a Data Scientist and Software Developer at different companies such as CESAR, in the field of Computer Science, Research & Development and Software Engineering, Software Development using Python in microservices involving pipelines, data streamings (Spark), message systems (Kafka, RabbitMQ) with technologies like Docker, Kubernetes, Helm Chart, REST API, GRPC, MongoDB, SQL, OracleDB, CI/CD. He has planned, trained, evaluated, deployed and maintained Machine Learning /Deep Learning models using tools such as PyTorch, Scikit-Learn, Feast, Apache Airflow, Mlflow, Pandas, Numpy.</s>\n"
     ]
    }
   ],
   "source": [
    "query = \"What professions did Gerson have?\"\n",
    "time_start = perf_counter()\n",
    "response = qa.run(query)\n",
    "time_duration = perf_counter() - time_start\n",
    "dict_json.setdefault(experiment, {})[\"execute\"] = time_duration\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cleanup: kill the llamafile server process\n",
    "kill $(cat .llamafile_pid)\n",
    "rm .llamafile_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "with open(f\"results/{experiment}/{timestr}.json\", \"w\") as f:\n",
    "    json.dump(dict_json, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
