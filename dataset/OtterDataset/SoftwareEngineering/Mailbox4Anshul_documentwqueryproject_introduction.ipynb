{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ETchGtMJ8Zo"
      },
      "source": [
        "# Introduction to AutoGen\n",
        "\n",
        "Welcome! AutoGen is an open-source framework that leverages multiple _agents_ to enable complex workflows. This tutorial introduces basic concepts and building blocks of AutoGen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7w5LmZ6J8Zr"
      },
      "source": [
        "## Why AutoGen?\n",
        "\n",
        "> _The whole is greater than the sum of its parts._<br/>\n",
        "> -**Aristotle**\n",
        "\n",
        "While there are many definitions of agents, in AutoGen, an agent is an entity that can send messages, receive messages and generate a reply using models, tools, human inputs or a mixture of them.\n",
        "This abstraction not only allows agents to model real-world and abstract entities, such as people and algorithms, but it also simplifies implementation of complex workflows as collaboration among agents.\n",
        "\n",
        "Further, AutoGen is extensible and composable: you can extend a simple agent with customizable components and create workflows that can combine these agents and power a more sophisticated agent, resulting in implementations that are modular and easy to maintain.\n",
        "\n",
        "Most importantly, AutoGen is developed by a vibrant community of researchers\n",
        "and engineers. It incorporates the latest research in multi-agent systems\n",
        "and has been used in many real-world applications, including agent platform,\n",
        "advertising, AI employees, blog/article writing, blockchain, calculate burned areas by wildfires,\n",
        "customer support, cybersecurity, data analytics, debate, education, finance, gaming, legal consultation,\n",
        "research, robotics, sales/marketing, social simulation, software engineering,\n",
        "software security, supply chain, t-shirt design, training data generation, Youtube service..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz7J7o6OJ8Zs"
      },
      "source": [
        "## Installation\n",
        "\n",
        "The simplest way to install AutoGen is from pip: `pip install pyautogen`. Find more options in [Installation](/docs/installation/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyautogen"
      ],
      "metadata": {
        "id": "_5m2Ya3JKSlq",
        "outputId": "1f9934c0-38ae-4d95-d7fa-513ed8f335da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.2.21-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.6/236.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker (from pyautogen)\n",
            "  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flaml (from pyautogen)\n",
            "  Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (1.25.2)\n",
            "Collecting openai>=1.3 (from pyautogen)\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.6.4)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen) (2.4.0)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->pyautogen) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.3->pyautogen)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->pyautogen) (2.16.3)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (24.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen) (3.3.2)\n",
            "Installing collected packages: python-dotenv, h11, flaml, diskcache, tiktoken, httpcore, docker, httpx, openai, pyautogen\n",
            "Successfully installed diskcache-5.6.3 docker-7.0.0 flaml-2.1.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.14.3 pyautogen-0.2.21 python-dotenv-1.0.1 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa63-EkQJ8Zs"
      },
      "source": [
        "## Agents\n",
        "\n",
        "In AutoGen, an agent is an entity that can send and receive messages to and from\n",
        "other agents in its environment. An agent can be powered by models (such as a large language model\n",
        "like GPT-4), code executors (such as an IPython kernel), human, or a combination of these\n",
        "and other pluggable and customizable components.\n",
        "\n",
        "```{=mdx}\n",
        "![ConversableAgent](./assets/conversable-agent.jpg)\n",
        "```\n",
        "\n",
        "An example of such agents is the built-in `ConversableAgent` which supports the following components:\n",
        "\n",
        "1. A list of LLMs\n",
        "2. A code executor\n",
        "3. A function and tool executor\n",
        "4. A component for keeping human-in-the-loop\n",
        "\n",
        "You can switch each component on or off and customize it to suit the need of\n",
        "your application. For advanced users, you can add additional components to the agent\n",
        "by using [`registered_reply`](../reference/agentchat/conversable_agent/#register_reply)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i6ozdzPJ8Zs"
      },
      "source": [
        "LLMs, for example, enable agents to converse in natural languages and transform between structured and unstructured text.\n",
        "The following example shows a `ConversableAgent` with a GPT-4 LLM switched on and other\n",
        "components switched off:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ngsTgwM5J8Zt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from autogen import ConversableAgent\n",
        "\n",
        "agent = ConversableAgent(\n",
        "    \"chatbot\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": \"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\"}]},\n",
        "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
        "    function_map=None,  # No registered functions, by default it is None.\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Zcf-pjJ8Zt"
      },
      "source": [
        "The `llm_config` argument contains a list of configurations for the LLMs.\n",
        "See [LLM Configuration](/docs/topics/llm_configuration) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6F8rRviJ8Zu"
      },
      "source": [
        "You can ask this agent to generate a response to a question using the `generate_reply` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "veiFkvPGJ8Zu",
        "outputId": "1c2658bc-6430-42c5-970b-a42c1a533887",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI, I don't have real-time news updates but the latest data that I have is about NASA's Mars rover Perseverance. The rover landed successfully on Mars in February 2020 to begin its search for signs of ancient life, and also to collect samples of rock and regolith (broken rock and soil) to bring back to Earth. Remember to check out the latest news on a reliable news website.\n"
          ]
        }
      ],
      "source": [
        "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a news.\", \"role\": \"user\"}])\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmMQkUxUJ8Zu"
      },
      "source": [
        "## Roles and Conversations\n",
        "\n",
        "In AutoGen, you can assign roles to agents and have them participate in conversations or chat with each other. A conversation is a sequence of messages exchanged between agents. You can then use these conversations to make progress on a task. For example, in the example below, we assign different roles to two agents by setting their\n",
        "`system_message`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zqAZGoDaJ8Zv"
      },
      "outputs": [],
      "source": [
        "cathy = ConversableAgent(\n",
        "    \"cathy\",\n",
        "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.9, \"api_key\": \"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\"}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")\n",
        "\n",
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.7, \"api_key\": \"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\"}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLyGlnAPJ8Zv"
      },
      "source": [
        "Now that we have two comedian agents, we can ask them to start a comedy show.\n",
        "This can be done using the `initiate_chat` method.\n",
        "We set the `max_turns` to 2 to keep the conversation short."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xzvoThPsJ8Zv",
        "outputId": "f7c4439a-02ef-44c4-9a88-d092c58482c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "Cathy, tell me a joke.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cathy (to joe):\n",
            "\n",
            "Sure, here you go:\n",
            "\n",
            "Why don't we ever trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "Haha, good one Cathy! Alright, here's mine:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they literally make up everything!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n",
            "WARNING:autogen.io.base:No default IOStream has been set, defaulting to IOConsole.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cathy (to joe):\n",
            "\n",
            "Haha, well we certainly are on the same wavelength! Here's another for you:\n",
            "\n",
            "Why was the math book sad?\n",
            "\n",
            "Because it had too many problems!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBOnF0mKJ8Zv"
      },
      "source": [
        "The comedians are bouncing off each other!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpzffY9XJ8Zv"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this chapter, we introduced the concept of agents, roles and conversations in AutoGen.\n",
        "For simplicity, we only used LLMs and created fully autonomous agents (`human_input_mode` was set to `NEVER`).\n",
        "In the next chapter,\n",
        "we will show how you can control when to _terminate_ a conversation between autonomous agents."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "autogen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}