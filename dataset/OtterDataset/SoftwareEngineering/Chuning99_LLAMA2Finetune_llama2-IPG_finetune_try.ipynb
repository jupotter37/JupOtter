{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SKSnF016yRgp"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "# base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "# # bnb_config = BitsAndBytesConfig(\n",
    "# #     load_in_4bit=True,\n",
    "# #     bnb_4bit_use_double_quant=True,\n",
    "# #     bnb_4bit_quant_type=\"nf4\",\n",
    "# #     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# # )\n",
    "\n",
    "\n",
    "# #model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload= True )\n",
    "\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id,  \n",
    "#     quantization_config=bnb_config,  # Same quantization config as before\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n",
    "\n",
    "# # import torch\n",
    "# # from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "# # # 加载基础模型和tokenizer\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# # base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuning/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chuning/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.14s/it]\n",
      "/home/chuning/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Define the base model ID\n",
    "base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "# Create a BitsAndBytesConfig object with the corrected settings\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Set as suggested in the error\n",
    ")\n",
    "\n",
    "# Load the base model with the updated quantization configuration\n",
    "# Adjust 'device_map' based on your system's GPU configuration\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  \n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a contract RTL design engineer, you will work as part of a GPU IP design team. This is an open-ended performance-driven position where the contractor will be tasked with supporting RTL design of various sub-blocks of the GPU graphics pipeline for a GPU targeted to the mobile market as well as other markets. Significant architectural, as well as RTL design experience. Key responsibilities include: •Micro-architecture and RTL definition •Partner with the physical design team to resolve implementation level details •Work closely with design verification to test plan and otherwise ensure proper functionality requirements •Deliver quality micro-architectural level documentation •Produce quality RTL on schedule meeting PPA goals. Optimize design for low power.\n",
      "\n",
      "\n",
      "Requirements\n",
      "Minimum requirements: •BSEE, Computer Engineer or comparable and 3 + years of experience •Experience driving the RTL design of various digital sub-blocks of GPU /CPU high-performance digital designs •Demonstrated experience of successful Architectural through RTL design experience on high-performance digital designs The preferred candidate will possess the following: •Good written and verbal communication skills •Pixel Pipe or GPU experience preferred •Low Power Design experience preferred\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "As a contract RTL design engineer, you will work as part of a GPU IP design team. This is an open-ended performance-driven position where the contractor will be tasked with supporting RTL design of various sub-blocks of the GPU graphics pipeline for a GPU targeted to the mobile market as well as other markets. Significant architectural, as well as RTL design experience. Key responsibilities include: •Micro-architecture and RTL definition •Partner with the physical design team to resolve implementation level details •Work closely with design verification to test plan and otherwise ensure proper functionality requirements •Deliver quality micro-architectural level documentation •Produce quality RTL on schedule meeting PPA goals. Optimize design for low power.\\n\\n\\nRequirements\\nMinimum requirements: •BSEE, Computer Engineer or comparable and 3 + years of experience •Experience driving the RTL design of various digital sub-blocks of GPU /CPU high-performance digital designs •Demonstrated experience of successful Architectural through RTL design experience on high-performance digital designs The preferred candidate will possess the following: •Good written and verbal communication skills •Pixel Pipe or GPU experience preferred •Low Power Design experience preferred\"\n",
    "\n",
    "\"\"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/home/chuning/Downloads/llama/llama2-13b-IPG-finetune/checkpoint-3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lMkVNEUvyRgp"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# ft_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "  \n",
    "eval_dataset = load_dataset('json', data_files='/home/chuning/Downloads/llama/new_test_data.json', split='train')\n",
    "\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### The job description: {example['text']}\\n ### The skills: \"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_finetune_model(model_id):\n",
    "\n",
    "    example = eval_dataset.filter(lambda x: x['id'] == model_id)[0]\n",
    "    formatted_text = formatting_func(example)\n",
    "    \n",
    "    #print(formatted_text)\n",
    "    model_input = tokenizer(formatted_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_tokens = ft_model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 222/222 [00:00<00:00, 37697.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The job description: As a web engineer, utilize your skills to help develop and maintain high-volume, critical FE web experiences in cloud environments.\n",
      " \n",
      "Requirements:\n",
      "Participate in the agile feature/product design process working with cross-functional teams including: Product Management, Experience Design, Analytics and Operations\n",
      "End-to-end design and implementation of new and existing features\n",
      "Improve, refactor and redesign existing services to improve performance.\n",
      "Collaborate with other engineers to share best practices and knowledge of emerging technologies\n",
      "Implement and maintain high-quality code and ship to production in a fast and secure manner.\n",
      "Drive and initiate proactive efforts to improve internal processes\n",
      "Engage with customers to get a first-hand understanding of their needs\n",
      "Mentor Junior Software Engineers (code reviews and technical sessions).\n",
      "Resolve defects/bugs during QA testing, pre-production, and in production\n",
      " \n",
      "Qualifications\n",
      "BS or MS in computer science or equivalent experience\n",
      "8-10 years of web development experience\n",
      "Solid grasp of software engineering fundamentals and their practical application\n",
      "Experience with React, HTML 5.0, CSS\n",
      "Experience with dynamic FE experience development a plus\n",
      "Solid design and coding skills\n",
      "Strong knowledge of Web design patterns and technologies\n",
      "\"Self-starter\"\" attitude and ability to make decisions independently\n",
      "Helpful, can-do attitude and a willingness to take ownership of problems.\n",
      "Strong desire to learn and grow\n",
      "Experience with the entire Software Development Life Cycle (SDLC)\n",
      "Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences\n",
      "Strong understanding of the Software design/architecture process\n",
      "Experience with unit testing & Test-Driven Development (TDD)\n",
      "Experience developing REST APIs and JSON using standard HTTP request format in Java or other OO language is a plus\n",
      " ### The skills:  ['coding', 'css', 'testing', 'sql', 'php', 'c++', 'git', 'unix', 'marketing', 'jquery', 'agile', 'css3', 'xml', 'bash', 'cloud', 'javascript', 'python', 'typescript', 'node.js', 'java', 'aws', 'system', 'html', 'apis', 'docker', 'machine learning', 'react', 'linux', 'html5', 'json'] ### The education: BS or MS in computer science or equivalent experience ### The internships: 10 years ### The special skills:\n",
      "\n",
      "\n",
      "\n",
      " ### The skills:coding\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_finetune_model(\"6066\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
