{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Welcome to the Google Colab notebook for Team 15 - ML Mavericks, part of the Data Science Group Project Module at the University of Birmingham for the 2023/2024 cohort. This notebook contains all the code implementations for our system, designed to automate the creation of \"See Also\" sections in Wikipedia articles.**"
      ],
      "metadata": {
        "id": "Br6chk8_0Qod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Collecting and Preparing the Data\n"
      ],
      "metadata": {
        "id": "O1Otw-iX0Ula"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Stage One: Selecting Articles' Titles"
      ],
      "metadata": {
        "id": "xRxCmh7k0gLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading the TSV file with custom column names for easier understanding. (The clickstream data we used is available on this link: https://dumps.wikimedia.org/other/clickstream/2024-01/)\n",
        "df = pd.read_csv(\"clickstream-enwiki-2024-01.tsv\", sep='\\t', names=['From', 'To', 'Type', 'Total Clicks'])\n",
        "\n",
        "# Just a quick check to see the first few rows and make sure our columns are correctly named.\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "CJDi4yHw0dzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouping by destination page, summing total clicks, and sorting.\n",
        "result_df = df.groupby('To')['Total Clicks'].sum().reset_index().sort_values(by='Total Clicks', ascending=False)\n",
        "\n",
        "# Renaming columns.\n",
        "result_df.columns = ['Title', 'Total clicks']\n",
        "\n",
        "# Saving the result as a CSV file with the top 60k titles.\n",
        "result_df.head(60000).to_csv('top_60k_titles.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Kh863Xow1VSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Stage Two: Delving into Article Features"
      ],
      "metadata": {
        "id": "a5tNc8Rx3fRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "\n",
        "def fetch_xtools_article_info(project, article, session):\n",
        "    article_formatted = article.replace(\" \", \"_\")\n",
        "    url = f\"https://xtools.wmcloud.org/api/page/articleinfo/{project}/{article_formatted}\"\n",
        "    response = session.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return {\n",
        "            \"revisions\": data.get(\"revisions\", 0),\n",
        "            \"quality\": data.get(\"assessment\", {}).get(\"value\", \"Unknown\")\n",
        "        }\n",
        "    else:\n",
        "        print(f\"Couldn't get data for {article}: HTTP {response.status_code}\")\n",
        "        return {}\n",
        "\n",
        "def fetch_article_details(title, session):\n",
        "    base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"prop\": \"extracts|info|categories\",\n",
        "        \"titles\": title,\n",
        "        \"exintro\": \"\",\n",
        "        \"explaintext\": \"\",\n",
        "        \"inprop\": \"url\",\n",
        "        \"cllimit\": \"max\",  # Getting max categories\n",
        "        \"clshow\": \"!hidden\"  # Excluding hidden categories\n",
        "    }\n",
        "    info_response = session.get(base_url, params=params)\n",
        "    if info_response.status_code == 200:\n",
        "        info_data = info_response.json()\n",
        "        page_id = next(iter(info_data['query']['pages']))\n",
        "        page_info = info_data['query']['pages'][page_id]\n",
        "\n",
        "        # Extracting categories, removing 'Category:' prefix\n",
        "        categories = [category['title'].replace('Category:', '').strip() for category in page_info.get('categories', []) if 'categories' in page_info]\n",
        "\n",
        "        total_views = 0\n",
        "        views_url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{title.replace(' ', '_')}/daily/20240101/20240131\"\n",
        "        views_response = session.get(views_url)\n",
        "        if views_response.status_code == 200:\n",
        "            views_data = views_response.json()\n",
        "            if 'items' in views_data:\n",
        "                total_views = sum(item['views'] for item in views_data['items'])\n",
        "            else:\n",
        "                print(f\"No view data for {title}\")\n",
        "        else:\n",
        "            print(f\"Couldn't fetch view data for {title}: HTTP {views_response.status_code}\")\n",
        "\n",
        "        xtools_info = fetch_xtools_article_info(\"en.wikipedia.org\", title, session)\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"size\": page_info.get('length', 0),\n",
        "            \"total_views\": total_views,\n",
        "            \"first_paragraph\": page_info.get('extract', \"\"),\n",
        "            \"article_quality\": xtools_info.get(\"quality\", \"Unknown\"),\n",
        "            \"article_categories\": categories  # Categories list without 'Category:' prefix, excluding hidden categories\n",
        "        }\n",
        "    else:\n",
        "        print(f\"Couldn't fetch article details for {title}: HTTP {info_response.status_code}\")\n",
        "        return {}\n",
        "\n",
        "def create_articles_df(titles, user_agent):\n",
        "    articles_data = []\n",
        "    with requests.Session() as session:\n",
        "        session.headers.update({'User-Agent': user_agent})\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            future_to_title = {executor.submit(fetch_article_details, title, session): title for title in titles}\n",
        "            for future in as_completed(future_to_title):\n",
        "                title = future_to_title[future]\n",
        "                try:\n",
        "                    article_details = future.result()\n",
        "                    articles_data.append(article_details)\n",
        "                except Exception as exc:\n",
        "                    print(f\"{title} encountered an issue: {exc}\")\n",
        "    return pd.DataFrame(articles_data)\n",
        "\n",
        "def process_batches(titles, user_agent, batch_size=100):\n",
        "    for i in range(0, len(titles), batch_size):\n",
        "        batch_titles = titles[i:i + batch_size]\n",
        "        start_time = time.time()\n",
        "        df = create_articles_df(batch_titles, user_agent)\n",
        "        csv_filename = f\"batch_{i//batch_size + 1}_articles.csv\"\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        end_time = time.time()\n",
        "        print(f\"Batch {i//batch_size + 1} done: {len(batch_titles)} articles saved to {csv_filename} in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# User agent information\n",
        "user_agent = \"mah338@student.bham.ac.uk / University of Birmingham - Data Science Project\"\n",
        "\n",
        "# Load the file as CSV\n",
        "top_60k_titles_csv = pd.read_csv(\"top_60k_titles.csv\")['Title'].tolist()\n",
        "\n",
        "# Process batches\n",
        "process_batches(top_60k_titles_csv, user_agent)\n"
      ],
      "metadata": {
        "id": "qO3zP6j23jNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. The Final Dataset: Combining Files"
      ],
      "metadata": {
        "id": "0_N0Q5Ug5LRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming all CSV files are in the current directory\n",
        "csv_files = [f\"batch_{i}_articles.csv\" for i in range(1, 603)]  # Generating list of CSV file names\n",
        "\n",
        "# Initializing an empty DataFrame\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Looping through each CSV file, reading it into a DataFrame, and appending it to combined_df\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(path_to_csv_files, file)\n",
        "    if os.path.exists(file_path):  # Checking if the file exists\n",
        "        temp_df = pd.read_csv(file_path)\n",
        "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
        "    else:\n",
        "        print(f\"File {file} doesn't exist.\")\n",
        "\n",
        "# Now combined_df contains all the data from the 250 CSV files\n",
        "print(f\"Combined DataFrame has {len(combined_df)} instances.\") #This is just to make sure\n",
        "\n",
        "# Saving the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv(\"600_field_combined.csv\", index=False)\n",
        "\n",
        "print(\"All files have been combined and saved to 250_field_combined.csv.\")\n"
      ],
      "metadata": {
        "id": "-XASJfb55VNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "bkcKorFYFifs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 An In-depth Examination of Article Size"
      ],
      "metadata": {
        "id": "cDsvUWVYHOHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('600_field_combined.csv')\n",
        "\n",
        "# Plot the histogram\n",
        "plt.hist(data['size'], bins=50, color='grey', edgecolor='black')\n",
        "plt.axvline(x=6000, color='black', linestyle='--', label='Threshold at 6K bytes')\n",
        "plt.title('Article Size Distribution')\n",
        "plt.xlabel('Size (bytes)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 350000)  # Limit x-axis to max value of 'size'\n",
        "plt.legend()\n",
        "\n",
        "# Remove the box around the graph\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['left'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_visible(False)\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('size.png', dpi=1000, bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Find the bins with the highest frequencies\n",
        "sorted_indices = counts.argsort()[::-1]  # Sort indices by count\n",
        "max_count_bins = [(bins[i], bins[i + 1]) for i in sorted_indices[:2]]  # Get top two bins\n",
        "max_count_values = [counts[i] for i in sorted_indices[:2]]  # Get counts of top two bins\n",
        "\n",
        "# Print info about top frequency bins\n",
        "for i in range(len(max_count_bins)):\n",
        "    print(f\"Bin {i+1}: Range {max_count_bins[i]} with a frequency of {max_count_values[i]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R-8I9b3_HU9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 An In-Depth Examination of Article Quality Distribution"
      ],
      "metadata": {
        "id": "h-9sY0o3aCBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group article qualities, combining less common categories into \"Others\"\n",
        "data['article_quality_grouped'] = data['article_quality'].apply(lambda x: x if x in ['FA', 'A', 'GA', 'B', 'C', 'Start', 'Stub'] else 'Others')\n",
        "quality_counts_grouped = data['article_quality_grouped'].value_counts()[['FA', 'A', 'GA', 'B', 'C', 'Start', 'Stub', 'Others']]\n",
        "\n",
        "# Create bar plot\n",
        "bars = plt.bar(quality_counts_grouped.index, quality_counts_grouped.values, color='grey', edgecolor='black')\n",
        "plt.title('Article Quality Distribution', pad=20)\n",
        "plt.xlabel('Article Quality')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels vertically\n",
        "\n",
        "# Add numbers on top of bars for better visibility\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05*yval, f'{int(yval)}',\n",
        "             ha='center', va='bottom', rotation=0, color='black', fontsize=8, zorder=3)  # Set zorder to bring text to front\n",
        "\n",
        "# Remove the box around the graph\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['left'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_visible(False)\n",
        "\n",
        "# Adjust axis limits to make space for text\n",
        "plt.ylim(0, max(quality_counts_grouped.values) * 1.03)\n",
        "plt.tight_layout()\n",
        "plt.savefig('quality.png', dpi=1000, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-ydEMkztaJb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 An In-Depth Examination of Article Category Distribution"
      ],
      "metadata": {
        "id": "svVzUUkUb3Sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "\n",
        "# Define a function to safely evaluate string literals\n",
        "def safe_literal_eval(x):\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            return literal_eval(x)\n",
        "        except Exception:\n",
        "            return []\n",
        "    return x\n",
        "\n",
        "# Convert string representations of lists into actual lists\n",
        "data['article_categories'] = data['article_categories'].apply(safe_literal_eval)\n",
        "\n",
        "# Adjusted category mapping using certain words\n",
        "category_mapping = {\n",
        "    'Sports': [\n",
        "        'olympic sport', 'football', 'basketball', 'sportspeople', 'athletic', 'soccer', 'tennis',\n",
        "        'athletics', 'baseball', 'rugby', 'cricket', 'volleyball', 'golf', 'swimming',\n",
        "        'track and field', 'hockey', 'table tennis', 'badminton', 'skiing', 'snowboarding',\n",
        "        'skating', 'cycling', 'boxing', 'mixed martial arts', 'wrestling', 'fencing', 'rowing',\n",
        "        'sailing', 'equestrian', 'gymnastics', 'weightlifting', 'biathlon', 'triathlon', 'marathon',\n",
        "        'sprint', 'judo', 'taekwondo', 'karate', 'archery', 'shooting sports', 'darts', 'bowling',\n",
        "        'billiards', 'snooker', 'bodybuilding', 'surfing', 'motorsport', 'racing', 'figure skating',\n",
        "        'doping in sports', 'sports nutrition', 'sports medicine', 'sports psychology',\n",
        "        'physical training', 'sports equipment', 'team sports', 'individual sports', 'extreme sports',\n",
        "        'water sports', 'winter sports', 'outdoor sports', 'indoor sports', 'professional sports',\n",
        "        'amateur sports', 'college sports', 'youth sports', 'master sports', 'olympic games', 'world cup',\n",
        "        'championships', 'sports leagues', 'sports teams', 'sportsmanship', 'coaching', 'sports strategy',\n",
        "        'sports analytics', 'sports history', 'sports culture', 'fan culture', 'sports broadcasting',\n",
        "        'sports journalism', 'sports awards', 'sports records', 'sports events', 'stadiums', 'arenas',\n",
        "        'sports fans', 'athlete training', 'sports science'\n",
        "    ],\n",
        "    'Technology': [\n",
        "        'software', 'hardware', 'internet', 'video game', 'computer', 'programming', 'ai',\n",
        "        'artificial intelligence', 'gadget', 'mobile device', 'smartphone', 'tablet', 'laptop',\n",
        "        'desktop', 'operating system', 'application', 'app development', 'user interface',\n",
        "        'user experience', 'data science', 'machine learning', 'robotics', 'automation', 'blockchain',\n",
        "        'cryptocurrency', 'cloud computing', 'big data', 'data analysis', 'networking',\n",
        "        'cybersecurity', 'information security', 'hacking', 'ethical hacking', 'virtual reality',\n",
        "        'augmented reality', 'drones', 'wearable technology', 'IoT', 'Internet of Things',\n",
        "        'semiconductors', 'silicon chips', 'quantum computing', 'database', 'data management',\n",
        "        'UI/UX design', 'web development', 'digital marketing', 'SEO', 'search engine optimization',\n",
        "        'social media', 'e-commerce', 'fintech', 'financial technology', 'tech startup', 'innovation',\n",
        "        'tech policy', 'privacy', 'tech ethics', 'software engineering', 'network infrastructure',\n",
        "        'wireless technology', '5G', 'telecommunications', 'nanotechnology', 'biotechnology',\n",
        "        'tech trends', 'gaming consoles', 'e-sports', 'tech reviews', 'tech tutorials',\n",
        "        'coding languages', 'software development kit', 'SDK', 'open source', 'API',\n",
        "        'application programming interface', 'tech integration', 'tech education', 'STEM',\n",
        "        'science technology engineering mathematics', 'tech entrepreneurship', 'tech investment',\n",
        "        'tech venture', 'tech gadgets', 'home automation', 'smart home', 'tech support',\n",
        "        'tech forums', 'tech community', 'tech events', 'tech conferences', 'tech exhibitions'\n",
        "    ],\n",
        "    'Literature': [\n",
        "        'novel', 'poetry', 'writer', 'book', 'literary genre', 'prose', 'drama', 'playwright',\n",
        "        'short story', 'biography', 'essay', 'anthology', 'classic literature', 'literary criticism',\n",
        "        'literary theory', 'non-fiction', 'fiction', 'science fiction', 'fantasy', 'mystery',\n",
        "        'horror', 'historical novel', 'romance', 'graphic novel', 'comic book', 'memoir',\n",
        "        'autobiography', 'epic', 'sonnet', 'haiku', 'limerick', 'ballad', 'literary device',\n",
        "        'narrative structure', 'plot', 'character development', 'theme', 'motif', 'symbolism',\n",
        "        'dialogue', 'rhetoric', 'satire', 'parody', 'allegory', 'critique', 'manuscript', 'publishing',\n",
        "        'e-book', 'audiobook', 'translation', 'book series', 'author', 'poet', 'novelist', 'editor',\n",
        "        'literary journal', 'book review', 'book club', 'reading', 'literature festival', 'literary award',\n",
        "        'bestseller', 'classic', 'literary canon', 'literary movement', 'literary period', 'poetic form',\n",
        "        'prose style', 'literary agent', 'book fair', 'public domain', 'copyright in literature',\n",
        "        'academic writing', 'scholarly publication', 'creative writing', 'young adult literature',\n",
        "        'childrenâ€™s literature', 'oral tradition', 'folklore', 'mythology', 'literary scholarship',\n",
        "        'text analysis', 'literature education', 'literary studies'\n",
        "    ],\n",
        "    'History': [\n",
        "        'historical event', 'ancient history', '20th century', 'historian', 'medieval history',\n",
        "        'world war', 'historical figure', 'modern history', 'renaissance', 'industrial revolution',\n",
        "        'civilization', 'empire', 'kingdom', 'monarchy', 'dynasty', 'archaeology', 'artifact',\n",
        "        'chronology', 'historiography', 'cultural heritage', 'historical research', 'military history',\n",
        "        'political history', 'social history', 'economic history', 'history of science',\n",
        "        'history of technology', 'art history', 'oral history', 'primary source', 'archive', 'documentary',\n",
        "        'biography', 'timeline', 'genealogy', 'prehistoric', 'classical antiquity', 'colonialism',\n",
        "        'revolution', 'exploration', 'historic site', 'museum', 'historical society', 'history education',\n",
        "        'public history', 'historical reenactment', 'historical fiction', 'national history',\n",
        "        'local history', 'global history', 'transnational history', 'history conference',\n",
        "        'history publication', 'history journal', 'historical methodology', 'chronicle', 'epic',\n",
        "        'folklore', 'mythology', 'ancient texts', 'inscription', 'paleography', 'numismatics',\n",
        "        'heraldry', 'philately', 'cartography', 'historical maps', 'historical narrative',\n",
        "        'historical analysis', 'historical period', 'historical drama', 'historical documentary',\n",
        "        'age of discovery', 'age of enlightenment', 'middle ages', 'renaissance history', 'baroque',\n",
        "        'classicism', 'romanticism', 'victorian era', 'modernism', 'postmodernism', 'contemporary history',\n",
        "        'digital history', 'historical simulation'\n",
        "    ],\n",
        "    'Politics': [\n",
        "        'politician', 'political party', 'election', 'government', 'democracy', 'political science',\n",
        "        'legislation', 'public policy', 'governance', 'diplomacy', 'international relations',\n",
        "        'political campaign', 'voting', 'civic engagement', 'civil rights', 'human rights',\n",
        "        'political ideology', 'conservatism', 'liberalism', 'socialism', 'communism', 'anarchism',\n",
        "        'federalism', 'parliamentary system', 'presidential system', 'monarchy', 'dictatorship',\n",
        "        'geopolitics', 'political theory', 'political economy', 'political history', 'political philosophy',\n",
        "        'political ethics', 'civic education', 'political analysis', 'political strategy', 'lobbying',\n",
        "        'advocacy', 'activism', 'public administration', 'bureaucracy', 'electoral system',\n",
        "        'political debate', 'political discourse', 'political leadership', 'nation-state', 'sovereignty',\n",
        "        'nationalism', 'patriotism', 'political culture', 'political reform', 'political crisis',\n",
        "        'campaign finance', 'voter turnout', 'political sociology', 'political psychology',\n",
        "        'political communication', 'political commentary', 'political journalism', 'political satire',\n",
        "        'political parties', 'independent politics', 'grassroots politics', 'party politics',\n",
        "        'political movement', 'political coalition', 'government institution', 'public office',\n",
        "        'political office', 'election law', 'political rights', 'political representation',\n",
        "        'political negotiation', 'political advocacy', 'municipal politics', 'regional politics',\n",
        "        'national politics', 'international politics', 'geopolitical conflict', 'political stability',\n",
        "        'political change', 'policy analysis', 'public affairs', 'political consulting',\n",
        "        'electoral politics', 'political management'\n",
        "    ],\n",
        "    'Science': [\n",
        "        'biology', 'physics', 'chemistry', 'space', 'astronomy', 'earth science', 'environment',\n",
        "        'genetics', 'botany', 'zoology', 'ecology', 'molecular biology', 'biochemistry',\n",
        "        'microbiology', 'neuroscience', 'evolution', 'immunology', 'cellular biology',\n",
        "        'quantum mechanics', 'thermodynamics', 'particle physics', 'nuclear physics',\n",
        "        'astrophysics', 'cosmology', 'planetary science', 'geochemistry', 'geophysics',\n",
        "        'meteorology', 'climatology', 'oceanography', 'paleontology', 'crystallography',\n",
        "        'inorganic chemistry', 'organic chemistry', 'analytical chemistry', 'physical chemistry',\n",
        "        'material science', 'science research', 'scientific method', 'experimental science',\n",
        "        'scientific theory', 'scientific discovery', 'natural science', 'applied science',\n",
        "        'interdisciplinary science', 'scientific community', 'scientific journal',\n",
        "        'peer-reviewed research', 'laboratory', 'scientific experiment', 'science education',\n",
        "        'science communication', 'science policy', 'science funding', 'science and technology',\n",
        "        'science history', 'science ethics', 'environmental science', 'conservation biology',\n",
        "        'wildlife science', 'earth systems', 'atmospheric science', 'space exploration',\n",
        "        'rocket science', 'satellite technology', 'science innovation', 'science awards',\n",
        "        'scientific breakthrough', 'scientific collaboration', 'science conference',\n",
        "        'science exhibition', 'science debate', 'science advocacy', 'citizen science',\n",
        "        'science literacy', 'science outreach', 'scientific literacy', 'science curriculum',\n",
        "        'STEM education', 'scientific investigation', 'science fair', 'science festival',\n",
        "        'science workshop', 'scientific inquiry', 'science news', 'science media'\n",
        "    ],\n",
        "    'Health': [\n",
        "        'medicine', 'medical science', 'healthcare', 'nutrition', 'disease', 'psychology',\n",
        "        'wellness', 'public health', 'epidemiology', 'pathology', 'pharmacology', 'anatomy',\n",
        "        'physiology', 'genetics', 'oncology', 'cardiology', 'neurology', 'dermatology',\n",
        "        'endocrinology', 'gastroenterology', 'immunology', 'ophthalmology', 'pediatrics',\n",
        "        'psychiatry', 'radiology', 'surgery', 'veterinary medicine', 'nursing', 'dentistry',\n",
        "        'mental health', 'preventive medicine', 'alternative medicine', 'holistic health',\n",
        "        'sports medicine', 'physical therapy', 'occupational therapy', 'personal health', 'fitness',\n",
        "        'exercise', 'weight management', 'diet', 'supplements', 'vitamins', 'mental wellness',\n",
        "        'stress management', 'self-care', 'sleep hygiene', 'hygiene', 'sexual health',\n",
        "        'reproductive health', 'women\"s health', 'men\"s health', 'pediatric health', 'geriatric health',\n",
        "        'chronic conditions', 'infectious diseases', 'vaccination', 'public health policy',\n",
        "        'health education', 'community health', 'health promotion', 'health insurance', 'medical research',\n",
        "        'clinical trials', 'medical diagnostics', 'health technology', 'health informatics', 'e-health',\n",
        "        'telemedicine', 'patient care', 'patient safety', 'medical ethics', 'health literacy',\n",
        "        'health communication', 'health facilities', 'emergency medicine', 'critical care',\n",
        "        'intensive care', 'health systems', 'global health', 'environmental health', 'health disparities',\n",
        "        'health economics', 'health laws', 'health regulations', 'medical devices', 'medical imaging',\n",
        "        'health data', 'medical records', 'health interventions', 'health outcomes', 'health risk factors',\n",
        "        'health services', 'healthcare quality', 'healthcare access', 'healthcare management'\n",
        "    ],\n",
        "    'Entertainment': [\n",
        "    'movies', 'television', 'music', 'theater', 'comedy', 'dance', 'pop culture', 'celebrities',\n",
        "    'film industry', 'TV shows', 'documentaries', 'streaming services', 'live concerts', 'festivals',\n",
        "    'awards shows', 'reality TV', 'animation', 'video games', 'board games', 'nightlife',\n",
        "    'performing arts', 'opera', 'ballet', 'musical theatre', 'drama', 'sitcoms', 'talk shows',\n",
        "    'radio', 'podcasts', 'audiobooks', 'music videos', 'songwriting', 'record labels',\n",
        "    'music production', 'concert tours', 'theatre productions', 'cinematography', 'directing',\n",
        "    'screenwriting', 'playwriting', 'acting', 'stand-up comedy', 'magic', 'circus',\n",
        "    'celebrity gossip', 'fan clubs', 'fan conventions', 'cosplay', 'gaming', 'e-sports',\n",
        "    'art exhibitions', 'museums', 'gallery shows', 'book readings', 'literary festivals',\n",
        "    'entertainment news', 'media criticism', 'film criticism', 'music criticism', 'theatre criticism',\n",
        "    'celebrity interviews', 'red carpet events', 'film festivals', 'entertainment technology',\n",
        "    'VR in entertainment', 'AR in entertainment', 'special effects', 'visual effects',\n",
        "    'production design', 'costume design', 'makeup artistry', 'choreography', 'talent shows',\n",
        "    'variety shows', 'game shows', 'sports entertainment', 'interactive entertainment',\n",
        "    'theme parks', 'amusement parks', 'carnivals', 'casinos', 'gambling', 'lotteries',\n",
        "    'entertainment law', 'media studies', 'entertainment industry', 'celebrity culture',\n",
        "    'entertainment history', 'cultural impact of entertainment', 'influencer culture', 'social media stars'\n",
        "    ]\n",
        "}\n",
        "\n",
        "def assign_general_category(specific_categories, mapping):\n",
        "    general_categories = set()\n",
        "    for specific in specific_categories:\n",
        "        specific_words = specific.lower().split()  # Split into words and convert to lowercase for matching\n",
        "        for general, specifics in mapping.items():\n",
        "            for specific_word in specific_words:\n",
        "                if any(specific_word == specific_mapped for specific_mapped in specifics):\n",
        "                    general_categories.add(general)\n",
        "    return list(general_categories)\n",
        "\n",
        "# Assign a general category to each article\n",
        "data['general_category'] = data['article_categories'].apply(lambda x: assign_general_category(x, category_mapping))\n",
        "\n",
        "# Flatten the list of general categories for all articles to count them\n",
        "all_general_categories = [category for sublist in data['general_category'] for category in sublist]\n",
        "\n",
        "# Count the occurrences of each general category\n",
        "general_category_counts = Counter(all_general_categories)\n",
        "\n",
        "# Convert to a sorted list of tuples for easier plotting or analysis\n",
        "sorted_general_category_counts = sorted(general_category_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Unpack the category names and counts for plotting\n",
        "categories, counts = zip(*sorted_general_category_counts)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(categories, counts, color='grey', edgecolor='black')\n",
        "plt.title('Article Counts by General Category')\n",
        "plt.xlabel('General Category')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "\n",
        "# Remove the box around the graph\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['left'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_visible(False)\n",
        "plt.savefig('Category.png', dpi=1000, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g8wrV3A7b4xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. An In-Depth Examination of Article Views"
      ],
      "metadata": {
        "id": "a9o0u9OTd8u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # This will be used for calculating histogram data\n",
        "\n",
        "# Calculate histogram data\n",
        "counts, bin_edges = np.histogram(data['total_views'], bins=1000, range=(0, 300000))\n",
        "\n",
        "# Find the largest bin\n",
        "largest_bin_index = np.argmax(counts)\n",
        "largest_bin_count = counts[largest_bin_index]\n",
        "largest_bin_range = (bin_edges[largest_bin_index], bin_edges[largest_bin_index + 1])\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.hist(data['total_views'], bins=1000, color='grey', edgecolor='black')\n",
        "plt.title('Distribution of Total Views in January 2024', loc='right', pad=20)\n",
        "plt.xlabel('Total Views')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 300000)\n",
        "\n",
        "# Remove the box around the graph\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['left'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_visible(False)\n",
        "\n",
        "# Annotate the largest bin\n",
        "vertical_offset = largest_bin_count * 19  # Applying a large offset\n",
        "annotation_y_position = largest_bin_count + vertical_offset\n",
        "\n",
        "plt.text(largest_bin_range[0] + (largest_bin_range[1]-largest_bin_range[0])/2, annotation_y_position,\n",
        "         f'{int(largest_bin_count)} articles\\n({int(largest_bin_range[0])}-{int(largest_bin_range[1])} views)',\n",
        "         ha='center', va='bottom')\n",
        "\n",
        "plt.savefig('views.png', dpi=1000, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OXcELrDyeBNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. NLP and Semantic Vectors Generation"
      ],
      "metadata": {
        "id": "eOayeg5jmwE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Semantic Vectors Generation Using BERT"
      ],
      "metadata": {
        "id": "f2lruTzHw3TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: We used Google Colab due to its computational efficiency.\n",
        "\n",
        "!pip install sentence-transformers\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "import ast  # For converting string representations of lists into actual lists\n",
        "\n",
        "# Path to the CSV file\n",
        "file_path = \"/content/drive/MyDrive/Data Science - UoB/Term 2/DS Project/Dataset and code file/21th Feb NLP/600_field_combined.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Selecting the 'title', 'first_paragraph', and 'article_categories' columns\n",
        "df = df[['title', 'first_paragraph', 'article_categories']].iloc[0:60000].copy()\n",
        "\n",
        "# Load the BERT model for generating embeddings\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Convert 'article_categories' from string representation of list to a single string\n",
        "def convert_categories_to_string(category_list_str):\n",
        "=    if isinstance(category_list_str, str):\n",
        "        categories = ast.literal_eval(category_list_str)\n",
        "    else:\n",
        "        categories = category_list_str\n",
        "    return ', '.join(categories)\n",
        "\n",
        "# Apply the function to convert categories\n",
        "df['article_categories'] = df['article_categories'].apply(convert_categories_to_string)\n",
        "\n",
        "# Concatenate title, first paragraph, and article categories into a single text column for embedding\n",
        "df['text'] = df['title'].astype(str) + ' ' + df['first_paragraph'].astype(str) + ' ' + df['article_categories'].astype(str)\n",
        "\n",
        "# Generate BERT embeddings for each text instance\n",
        "start_time = time.time()\n",
        "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True, convert_to_tensor=True)\n",
        "end_time = time.time()\n",
        "\n",
        "# Create a DataFrame to store the semantic vectors\n",
        "semantic_df = pd.DataFrame(embeddings.numpy(), columns=[f'bert_{i}' for i in range(len(embeddings[0]))])\n",
        "\n",
        "# Concatenate the semantic vectors DataFrame with the DataFrame containing the original features\n",
        "new_df = pd.concat([\n",
        "    df.reset_index(drop=True),\n",
        "    semantic_df\n",
        "], axis=1)\n",
        "\n",
        "# Calculate processing time\n",
        "processing_time = end_time - start_time\n",
        "print(f\"Processing time: {processing_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "RJcD3ROHm3-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. BERT Embeddings Validation through PCA"
      ],
      "metadata": {
        "id": "_ktZ_KGMyKaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('600_field_combined_with_vectors.csv')\n",
        "\n",
        "#List of specified indices for articles to include\n",
        "specified_indices = [521, 520, 221, 2290, 94, 186, 4541, 1759, 21329, 28794, 20170, 38807, 81, 165, 11, 49, 3152, 2254, 10369, 10817, 12882]\n",
        "\n",
        "# Select articles\n",
        "sampled_data = data.loc[specified_indices]\n",
        "\n",
        "# Extract BERT embeddings\n",
        "embeddings = sampled_data.loc[:, 'bert_0':'bert_767']\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i, txt in enumerate(sampled_data['title']):\n",
        "    if i not in [10, 2]:\n",
        "        # Adjust vertical position of labels based on index\n",
        "        vertical_position = reduced_embeddings[i, 1] + 0.01 if i % 2 == 0 else reduced_embeddings[i, 1] - 0.01\n",
        "        plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], color='grey', edgecolors='black', alpha=0.6)\n",
        "        plt.text(reduced_embeddings[i, 0] + 0.01, vertical_position, txt, fontsize=10, color='black', ha='left', va='bottom' if i % 2 == 0 else 'top')\n",
        "\n",
        "plt.title('PCA of Article Vector Embeddings')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
        "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# Remove the box around the graph\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['left'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_visible(False)\n",
        "plt.savefig('embeddings.png', dpi=1000, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YJJ7ci5dyOd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Workflow of the Automated \"See Also\" System"
      ],
      "metadata": {
        "id": "tI2eY3Ow0pid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Input Article, Cosine Similarity Calculation, and Size Exclusion"
      ],
      "metadata": {
        "id": "ca0qDiNI0_i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_name = 'full_dataset_with_vectors.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Extract these embeddings into a NumPy array\n",
        "embeddings = data.loc[:, 'bert_0':'bert_767'].values\n",
        "\n",
        "# Normalize the embeddings to have unit length\n",
        "norm_semantic_vectors = normalize(embeddings)\n",
        "\n",
        "def recommend_articles_with_info(current_article_index, data, top_n=20):\n",
        "    # Compute cosine similarity\n",
        "    similarities = cosine_similarity([norm_semantic_vectors[current_article_index]], norm_semantic_vectors)[0]\n",
        "\n",
        "    # Get indices of articles sorted by similarity (descending)\n",
        "    sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "    # Exclude the current article to prevent repetition\n",
        "    sorted_indices = sorted_indices[sorted_indices != current_article_index]\n",
        "\n",
        "    # Get top 20 articles based on similarity ratio, excluding the current article\n",
        "    top_20_indices = sorted_indices[:20]\n",
        "\n",
        "    # Filter out articles with less than 6K bytes from the top 20\n",
        "    filtered_indices = [i for i in top_20_indices if data.iloc[i]['size'] >= 6000]\n",
        "\n",
        "    # Select desired features for recommendations\n",
        "    selected_features = [\"title\", \"size\", \"total_views\", \"Introduction\", \"article_quality\", \"article_categories\"]\n",
        "\n",
        "    # Create DataFrame for recommendations including desired features and similarity scores\n",
        "    recommendations_df = data.iloc[filtered_indices][:top_n][selected_features].copy()\n",
        "    recommendations_df['Similarity Score'] = similarities[filtered_indices][:top_n]\n",
        "\n",
        "    # Adding current article info at the beginning\n",
        "    current_article_info = pd.DataFrame({\n",
        "        'title': [data.iloc[current_article_index]['title']],\n",
        "        'size': [data.iloc[current_article_index]['size']],\n",
        "        'total_views': [data.iloc[current_article_index]['total_views']],\n",
        "        'Introduction': [data.iloc[current_article_index]['Introduction']],\n",
        "        'article_quality': [data.iloc[current_article_index]['article_quality']],\n",
        "        'article_categories': [data.iloc[current_article_index]['article_categories']],\n",
        "        'Similarity Score': [np.nan]  # Current article won't have a similarity score with itself\n",
        "    })\n",
        "\n",
        "    return pd.concat([current_article_info, recommendations_df], ignore_index=True)\n",
        "\n",
        "# This is an example usage\n",
        "current_article_index = 301\n",
        "recommendations_df = recommend_articles_with_info(current_article_index, data, top_n)\n",
        "\n"
      ],
      "metadata": {
        "id": "MDY_JQqk1MW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Sorting by Number of Shared Categories"
      ],
      "metadata": {
        "id": "KA6A9arg2ZEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "original_categories = ast.literal_eval(data.iloc[current_article_index]['article_categories'])\n",
        "\n",
        "# Function to count shared categories\n",
        "def count_shared_categories(target_categories, original_categories):\n",
        "    # Convert target categories from string representation of list to actual list\n",
        "    target_categories_list = ast.literal_eval(target_categories)\n",
        "    # Use set intersection to find common elements\n",
        "    shared_categories = set(target_categories_list).intersection(set(original_categories))\n",
        "    return len(shared_categories)\n",
        "\n",
        "# Apply the function to each row in the DataFrame to calculate NSC\n",
        "recommendations_df['NSC'] = recommendations_df['article_categories'].apply(lambda x: count_shared_categories(x, original_categories))\n",
        "\n",
        "\n",
        "# Sort by 'NSC' in descending order, then by 'Similarity Score' in descending order\n",
        "# This ensures that articles with more shared categories come first\n",
        "# For articles with the same number of shared categories, they are then sorted by their similarity score\n",
        "recommendations_df = recommendations_df.sort_values(by=['NSC', 'Similarity Score'], ascending=[False, False])\n",
        "\n",
        "# Take the top 10 results after sorting\n",
        "top_10_recommendations = recommendations_df.head(11)\n"
      ],
      "metadata": {
        "id": "TV6Std6J2jxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Arranging by Views and Quality, and Final Selection"
      ],
      "metadata": {
        "id": "RH0mknd-2khg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the original article (assuming it is at index 0 of the DataFrame)\n",
        "original_article = top_10_recommendations.iloc[:1]\n",
        "\n",
        "# Exclude the original article from the sorting process\n",
        "rest_of_articles = top_10_recommendations.iloc[1:]\n",
        "\n",
        "# Sort the rest of the DataFrame by 'total_views' in descending order\n",
        "sorted_rest = rest_of_articles.sort_values(by='total_views', ascending=False)\n",
        "\n",
        "# Concatenate the original article back at the top\n",
        "sorted_df = pd.concat([original_article, sorted_rest])\n",
        "\n",
        "# Reset the index of the sorted DataFrame\n",
        "sorted_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "##Now for Quality:\n",
        "#mapping from quality categories to numerical scores\n",
        "quality_mapping = {\n",
        "    'FA': 1, 'A': 2, 'GA': 3, 'B': 4, 'C': 5, 'Start': 6, 'Stub': 7,\n",
        "    'FL': 8, 'AL': 9, 'BL': 10, 'CL': 11, 'NA': 12, '???': 13\n",
        "}\n",
        "\n",
        "#Map article qualities to scores, assigning 14 to unrecognized categories\n",
        "sorted_df['quality_score'] = sorted_df['article_quality'].apply(lambda x: quality_mapping.get(x, 14))\n",
        "\n",
        "#Exclude the original article for secondary sorting based on quality\n",
        "rest_of_articles_after_quality = sorted_df.iloc[1:].sort_values(by=['quality_score', 'total_views'], ascending=[True, False])\n",
        "\n",
        "#Re-add the original article to maintain its position at the top\n",
        "final_sorted_df = pd.concat([sorted_df.iloc[:1], rest_of_articles_after_quality])\n",
        "\n",
        "#Reset the index after sorting\n",
        "final_sorted_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Display the final sorted DataFrame, prioritized by quality and then views\n",
        "final_sorted_df.head(6)\n"
      ],
      "metadata": {
        "id": "P0XrGdY62pIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Steps Combined"
      ],
      "metadata": {
        "id": "4Mk2jyI83oRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'full_dataset_with_vectors.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Normalize the BERT embeddings\n",
        "embeddings = data.loc[:, 'bert_0':'bert_767'].values\n",
        "norm_embeddings = normalize(embeddings)\n",
        "\n",
        "def recommend_articles(article_index, data, top_n=5):\n",
        "    # Compute cosine similarity\n",
        "    similarities = cosine_similarity([norm_embeddings[article_index]], norm_embeddings)[0]\n",
        "\n",
        "    # Exclude the current article and sort others by similarity\n",
        "    sorted_indices = np.argsort(similarities)[::-1][1:]\n",
        "\n",
        "    # Select the top 20 based on similarity\n",
        "    top_20_indices = sorted_indices[:20]\n",
        "\n",
        "    # Filter out articles smaller than 6,000 bytes from the top 20\n",
        "    filtered_indices = [i for i in top_20_indices if data.iloc[i]['size'] >= 6000]\n",
        "\n",
        "    # Calculate the number of shared categories for the filtered articles\n",
        "    original_categories = set(ast.literal_eval(data.iloc[article_index]['article_categories']))\n",
        "    shared_counts = []\n",
        "    for i in filtered_indices:\n",
        "        article_categories = set(ast.literal_eval(data.iloc[i]['article_categories']))\n",
        "        shared_counts.append((i, len(original_categories.intersection(article_categories))))\n",
        "\n",
        "    # Sort by the number of shared categories, then by similarity, and take the top 10\n",
        "    shared_counts.sort(key=lambda x: (-x[1], -similarities[x[0]]))\n",
        "    top_10_indices = [i[0] for i in shared_counts][:10]\n",
        "\n",
        "    # Exclude the original article from the next steps\n",
        "    if article_index in top_10_indices:\n",
        "        top_10_indices.remove(article_index)\n",
        "\n",
        "    # Prepare the recommendations DataFrame\n",
        "    recommendations = data.iloc[top_10_indices].copy()\n",
        "    recommendations['similarity'] = similarities[top_10_indices]\n",
        "\n",
        "    # Sort by 'total_views' in descending order and select top 5\n",
        "    recommendations = recommendations.sort_values(by='total_views', ascending=False).head(5)\n",
        "\n",
        "    # Map article qualities to numerical scores and sort\n",
        "    quality_scores = {'FA': 1, 'A': 2, 'GA': 3, 'B': 4, 'C': 5, 'Start': 6, 'Stub': 7, 'List': 8}\n",
        "    recommendations['quality_score'] = recommendations['article_quality'].map(lambda x: quality_scores.get(x, 9))\n",
        "    final_recommendations = recommendations.sort_values(by='quality_score', ascending=True)\n",
        "\n",
        "    return final_recommendations\n",
        "\n",
        "# Example usage\n",
        "article_index = 53\n",
        "final_recommendations_df = recommend_articles(article_index, data, top_n=5)\n",
        "final_recommendations_df\n"
      ],
      "metadata": {
        "id": "gXPRC_3g3tLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Current See Also Lists Extraction\n"
      ],
      "metadata": {
        "id": "TPAj051K_97t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Extracting Current See Also Sections For All Articles Using Web Scraping"
      ],
      "metadata": {
        "id": "hEQMinTAEQ1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from bs4 import BeautifulSoup  # For parsing HTML\n",
        "import math\n",
        "import time\n",
        "\n",
        "def fetch_section_content(title, session, section_title=\"See also\"):\n",
        "    sections_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    sections_params = {\n",
        "        \"action\": \"parse\",\n",
        "        \"page\": title,\n",
        "        \"prop\": \"sections\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    sections_response = session.get(sections_url, params=sections_params)\n",
        "    see_also_section_index = None\n",
        "    if sections_response.status_code == 200:\n",
        "        sections_data = sections_response.json()\n",
        "        for section in sections_data[\"parse\"][\"sections\"]:\n",
        "            if section[\"line\"].lower() == section_title.lower():\n",
        "                see_also_section_index = section[\"index\"]\n",
        "                break\n",
        "\n",
        "    see_also_articles = []\n",
        "    if see_also_section_index:\n",
        "        content_params = {\n",
        "            \"action\": \"parse\",\n",
        "            \"page\": title,\n",
        "            \"section\": see_also_section_index,\n",
        "            \"format\": \"json\",\n",
        "            \"prop\": \"text\"\n",
        "        }\n",
        "        content_response = session.get(sections_url, params=content_params)\n",
        "        if content_response.status_code == 200:\n",
        "            content_data = content_response.json()\n",
        "            html_content = content_data[\"parse\"][\"text\"][\"*\"]\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "            links = soup.find_all('a')\n",
        "            for link in links:\n",
        "                link_text = link.get_text().strip()\n",
        "                if link_text and link_text.lower() != \"edit\":  # Filtering out 'edit' links and empty strings\n",
        "                    see_also_articles.append(link_text)\n",
        "\n",
        "    return see_also_articles\n",
        "\n",
        "def fetch_article_see_also(title, session):\n",
        "    see_also_articles = fetch_section_content(title, session)\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"see_also_articles\": see_also_articles,\n",
        "        \"see_also_count\": len(see_also_articles)  # Count of see also articles\n",
        "    }\n",
        "\n",
        "def create_articles_see_also_df(titles, user_agent):\n",
        "    articles_data = []\n",
        "    total_articles = len(titles)\n",
        "    articles_processed = 0\n",
        "    batch_start_time = time.time()\n",
        "\n",
        "    with requests.Session() as session:\n",
        "        session.headers.update({'User-Agent': user_agent})\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            future_to_title = {executor.submit(fetch_article_see_also, title, session): title for title in titles}\n",
        "\n",
        "            for future in as_completed(future_to_title):\n",
        "                articles_processed += 1\n",
        "                title = future_to_title[future]\n",
        "                try:\n",
        "                    article_see_also_details = future.result()\n",
        "                    articles_data.append(article_see_also_details)\n",
        "                except Exception as exc:\n",
        "                    print(f\"{title} generated an exception: {exc}\")\n",
        "\n",
        "                # Update the progress and timing after every 250 articles processed\n",
        "                if articles_processed % 250 == 0 or articles_processed == total_articles:\n",
        "                    batch_end_time = time.time()\n",
        "                    elapsed_time = batch_end_time - batch_start_time\n",
        "                    progress_percentage = (articles_processed / total_articles) * 100\n",
        "                    print(f\"Progress: {articles_processed}/{total_articles} articles processed ({math.floor(progress_percentage)}%). Time for batch: {elapsed_time:.2f} seconds.\")\n",
        "                    batch_start_time = time.time()\n",
        "\n",
        "                # Save every 1000 articles as a CSV\n",
        "                if articles_processed % 1000 == 0 or articles_processed == total_articles:\n",
        "                    batch_df = pd.DataFrame(articles_data)\n",
        "                    batch_file_name = f'articles_see_also_{articles_processed//1000}.csv'\n",
        "                    batch_df.to_csv(batch_file_name, index=False)\n",
        "                    print(f\"Saved {batch_file_name}\")\n",
        "\n",
        "    # The final DataFrame is saved after completing the loop\n",
        "    if articles_processed % 1000 != 0:\n",
        "        final_df = pd.DataFrame(articles_data)\n",
        "        final_batch_number = (articles_processed // 1000) + 1\n",
        "        final_file_name = f'articles_see_also_{final_batch_number}.csv'\n",
        "        final_df.to_csv(final_file_name, index=False)\n",
        "        print(f\"Saved {final_file_name}\")\n",
        "\n",
        "#usage\n",
        "user_agent = \"mah338@student.bham.ac.uk\"\n",
        "titles_list = data['title'].tolist()\n",
        "df = create_articles_see_also_df(titles_list, user_agent)\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "sENKYuJCEe3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}