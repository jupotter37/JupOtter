{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline, preprocessing\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn import model_selection\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from gensim import matutils\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def directory_list_generator(prime_directory):\n",
    "    \"\"\"Returns a list of all non-hidden directories\n",
    "    based on the path directory given, it will, return\n",
    "    only directories within the folder specified\"\"\"\n",
    "    directories=os.listdir(prime_directory)\n",
    "    dir_list = [x for x in directories if '.' not in x]\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_file_tabulator(dir_list):\n",
    "    \"\"\"Goes through all directories given as argument list\n",
    "    then picks up each text file and extracts text dumping it\n",
    "    to a column\"\"\"\n",
    "    paper_content = dict()\n",
    "    for txtDir in dir_list:\n",
    "        txtDir = prime_directory + txtDir\n",
    "        for txtfile in os.listdir(txtDir): #iterate through text files in directory\n",
    "            if txtfile[-3:] == 'txt':\n",
    "                document_path = txtDir + '/' + txtfile\n",
    "                with open(document_path) as fhand:\n",
    "                    content = fhand.read()\n",
    "                    paper_content[txtfile] = [content]\n",
    "    return paper_content              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_counter(compiled_documents):\n",
    "    \"\"\"Pipeline to convert list of documents with text\n",
    "    content from papers into a matrix using count\n",
    "    vectoriser.\"\"\"\n",
    "    # Create numpy array of text data from input dictionary\n",
    "    text_data = []\n",
    "    text_data.append([v for k,v in compiled_documents.items()])\n",
    "    text_data = np.array(text_data[0])\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid'])\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=my_stop_words, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    counts = count_vectorizer.fit_transform(text_data[:,0])\n",
    "    return counts, count_vectorizer,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter_fromCSV(df):\n",
    "    \"\"\"Pipeline to convert list of documents with text\n",
    "    content from papers into a sparse matrix using count\n",
    "    vectoriser.\"\"\"\n",
    "    # Create numpy array from inputted dataframe\n",
    "    text_data = df.values\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid','et','et al','al', 'yes', 'method',\n",
    "                                                   'results','citation','use','used','submitted','published'])\n",
    "    # Create a CountVectorizer for parsing/counting words\n",
    "    count_vectorizer = CountVectorizer(ngram_range=(2,3), \n",
    "                                       stop_words=my_stop_words, \n",
    "                                       token_pattern=\"\\\\b[a-z][a-z]{2,15}\\\\b\",\n",
    "                                       min_df=5,max_df=30)\n",
    "    counts = count_vectorizer.fit_transform(text_data[:,0])\n",
    "    return counts, count_vectorizer,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorizer_fromCSV(df):\n",
    "    \"\"\"Uses tfidf algorithm to return word frequency by\n",
    "    document.\"\"\"\n",
    "\n",
    "    # Create numpy array from inputted dataframe\n",
    "    text_data = df.values\n",
    "    # Create list of stop words\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union(['cid','et','et al','al', 'yes', 'method',\n",
    "                                                   'results','citation','use','used','submitted','published'])\n",
    "\n",
    "\n",
    "    # Vectorize the text using TFIDF\n",
    "    tfidf = TfidfVectorizer(ngram_range=(2,2), stop_words=my_stop_words, \n",
    "                            token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]{2,15}\\\\b\", \n",
    "                            min_df=5,max_df=30)\n",
    "    tfidf_vecs = tfidf.fit_transform(text_data[:,0])\n",
    "    return tfidf_vecs, tfidf,text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \", \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_grams_bydoc(result,model, feature_names,n_docs, n_top_words):\n",
    "    for i in range(n_docs):\n",
    "        doc_topic = np.argmax(result[i])\n",
    "        message = \"Document #%d, top topic #%d: \" % (i,doc_topic)\n",
    "        word_topic = model.components_[doc_topic]\n",
    "        message += \", \".join([feature_names[i]\n",
    "                     for i in word_topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_csv('text_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm_tf, tf_vectorizer,text_data = word_counter_fromCSV(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_dtm_tf = sparse.csr_matrix(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NMF_model = NMF(n_components=20,random_state=0)\n",
    "NMF_fitted = NMF_model.fit_transform(sp_dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0: multi task, multi task learning, different tasks, task feature, multi task feature\n",
      "Topic #1: metric learning, structured data, distance metric, learning feature, edit distance\n",
      "Topic #2: arxiv prints, value function, deep reinforcement, deep reinforcement learning, learning representations\n",
      "Topic #3: true true, performance measures, performance measure, cost sensitive, trained task\n",
      "Topic #4: hinge loss, mirror descent, step size, data distribution, shalev shwartz\n",
      "Topic #5: quantum machine, quantum machine learning, learning quantum, quantum computer, quantum algorithms\n",
      "Topic #6: function class, empirical risk, training points, function space, approximation error\n",
      "Topic #7: graphical models, exponential family, graphical model, family distributions, exp exp\n",
      "Topic #8: text categorization, acm international, acm international conference, conference research, sch utze\n",
      "Topic #9: adversarial examples, adversarial training, conference paper, adversarial example, org abs\n",
      "Topic #10: multi armed, armed bandit, armed bandits, exp exp, multi armed bandit\n",
      "Topic #11: new classes, new class, extreme learning, proposed algorithm, extreme learning machine\n",
      "Topic #12: uniform sampling, error bound, wang zhang, near optimal, rank rank\n",
      "Topic #13: hilbert space, reproducing kernel, kernel hilbert, hilbert spaces, valued functions\n",
      "Topic #14: internal representation, representation space, data sequence, universal learning, learning teaching\n",
      "Topic #15: knowledge base, speech language, conference ieee, speech signal, dictionary learning\n",
      "Topic #16: multi label, target labels, extreme learning, classification problems, streaming data\n",
      "Topic #17: feature set, feature sets, target function, cost sensitive, set feature\n",
      "Topic #18: curriculum learning, sequence learning, instances training, meta learning, training instances\n",
      "Topic #19: web pages, learning extract, genetic algorithm, search engine, number documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "countvec_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(NMF_model, countvec_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #0, top topic #10: empirical risk, risk minimization\n",
      "Document #1, top topic #0: decision support, recommender systems\n",
      "Document #2, top topic #13: adversarial examples, adversarial training\n",
      "Document #3, top topic #7: dictionary learning, hidden units\n",
      "Document #4, top topic #10: empirical risk, risk minimization\n",
      "Document #5, top topic #14: multi armed, armed bandits\n",
      "Document #6, top topic #12: learning extract, web pages\n",
      "Document #7, top topic #12: learning extract, web pages\n",
      "Document #8, top topic #3: contextual features, context sensitive\n",
      "Document #9, top topic #7: dictionary learning, hidden units\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_grams_bydoc(NMF_fitted,NMF_model, countvec_feature_names, 10,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TFIDF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tf, tf_vectorizer,text_data = tfidf_vectorizer_fromCSV(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_dtm_tf = sparse.csr_matrix(dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NMF_model = NMF(n_components=20,random_state=0)\n",
    "NMF_fitted = NMF_model.fit_transform(sp_dtm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0: decision support, recommender systems, health care, feature engineering, software engineering\n",
      "Topic #1: quantum machine, quantum learning, quantum algorithm, learning quantum, quantum algorithms\n",
      "Topic #2: scikit learn, ensemble learning, pedregosa varoquaux, learning python, learn machine\n",
      "Topic #3: contextual features, context sensitive, primary features, contextual information, sensitive features\n",
      "Topic #4: convergence rate, strongly convex, matrix factorization, step size, coordinate descent\n",
      "Topic #5: multi task, multitask learning, multiple tasks, sparse coding, lifelong learning\n",
      "Topic #6: state action, learning agents, value function, state space, learning agent\n",
      "Topic #7: dictionary learning, hidden units, deep belief, auto encoders, new classes\n",
      "Topic #8: meta learning, shot learning, meta features, meta data, instance level\n",
      "Topic #9: phys rev, rev lett, materials science, quantum machine, dimensional vectors\n",
      "Topic #10: empirical risk, risk minimization, training points, hilbert space, olkopf smola\n",
      "Topic #11: message passing, belief propagation, graphical models, exponential family, marginal likelihood\n",
      "Topic #12: learning extract, web pages, genetic algorithm, number documents, machine generated\n",
      "Topic #13: adversarial examples, adversarial training, adversarial example, adversarial samples, evasion attacks\n",
      "Topic #14: multi armed, armed bandits, uniform sampling, max min, armed bandit\n",
      "Topic #15: genetic programming, tree based, learning pipelines, based pipeline, data science\n",
      "Topic #16: metric learning, data representation, distance metric, low rank, task metric\n",
      "Topic #17: neural machine, translation model, beam search, rule based, policy gradient\n",
      "Topic #18: naive bayesian, anti spam, bayesian classifier, spam spam, text categorization\n",
      "Topic #19: theorem proving, higher order, proof steps, automated reasoning, learning proof\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "countvec_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(NMF_model, countvec_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #0, top topic #10: empirical risk, risk minimization\n",
      "Document #1, top topic #0: decision support, recommender systems\n",
      "Document #2, top topic #13: adversarial examples, adversarial training\n",
      "Document #3, top topic #7: dictionary learning, hidden units\n",
      "Document #4, top topic #10: empirical risk, risk minimization\n",
      "Document #5, top topic #14: multi armed, armed bandits\n",
      "Document #6, top topic #12: learning extract, web pages\n",
      "Document #7, top topic #12: learning extract, web pages\n",
      "Document #8, top topic #3: contextual features, context sensitive\n",
      "Document #9, top topic #7: dictionary learning, hidden units\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_grams_bydoc(NMF_fitted,NMF_model, countvec_feature_names, 10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
