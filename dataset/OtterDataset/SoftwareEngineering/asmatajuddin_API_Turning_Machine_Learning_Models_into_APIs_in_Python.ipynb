{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Turning Machine Learning Models into APIs in Python"
      ],
      "metadata": {
        "id": "_FhEjyR0_-kJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following situation:\n",
        "\n",
        "You have built a super cool machine learning model that can predict if a particular transaction is fraudulent or not. Now, a friend of yours is developing an android application for general banking activities and wants to integrate your machine learning model in their application for its super objective.\n",
        "\n",
        "But your friend found out that, you have coded your model in Python while your friend is building his application in Java. So? Won't it be possible to integrate your machine learning model into your friend's application?\n",
        "\n",
        "Fortunately enough, you have the power of APIs. And the above situation is one of the many where the need of turning your machine learning models into APIs is extremely important. Many of the industries are now looking for Data Scientists who can do this. Now, wrapping a machine learning model into an API is not very difficult, and that is precisely what you will be doing in this tutorial - Turn your machine learning model into an API.\n",
        "\n",
        "Options to implement Machine Learning models\n",
        "\n",
        "Most of the times, the real use of your machine learning model lies at the heart of an intelligent product – that may be a small component of a recommender system or an intelligent chat-bot. These are the times when the barriers seem very difficult to overcome.\n",
        "\n",
        "For example, the majority of the ML practitioners use R/Python for their experiments. But consumers of those ML models would be software engineers who use a completely different technology stack. There are two ways via which this problem can be solved:\n",
        "\n",
        "Rewriting the whole code in the language that the software engineering folks work. The above seems like a good idea, but the time & energy required to get those intricate models replicated would be utterly waste. Majority of languages like JavaScript, do not have great libraries to perform ML. One would be wise to stay away from it.\n",
        "API-first approach – Web APIs have made it easy for cross-language applications to work well. If a frontend developer needs to use your ML Model to create an ML powered web application, they would just need to get the URL Endpoint from where the API is being served.\n",
        "Now, before going any further let's study what really is an API.\n",
        "\n",
        "What are APIs?\n",
        "\n",
        "\"In simple words, an API is a (hypothetical) contract between 2 softwares saying if the user software provides input in a pre-defined format, the later with extend its functionality and provide the outcome to the user software.\" - Analytics Vidhya\n",
        "\n",
        "You can read the following articles to understand why APIs are a popular choice among developers:\n",
        "\n",
        "History of APIs\n",
        "Introduction to APIs\n",
        "Essentially, APIs are very much like web applications, but instead of giving you a nicely styled HTML page, APIs tend to return data in a standard data-exchange format such as JSON, XML, etc. Once you a developer has the desired output they can style it whatever the way they want. There are many popular ML APIs as well for example - IBM Watson's ML API which is capable of the following:\n",
        "\n",
        "Machine Translation - Helps translate text in different language pairs.\n",
        "Message Resonance – To find out the popularity of a phrase or word with a predetermined audience.\n",
        "Question and Answers - This service provides direct answers to the queries that are triggered by primary document sources.\n",
        "User Modelling – To make predictions about social characteristics of someone from a given text.\n",
        "Google Vision API is also an excellent example which provides dedicated services for Computer Vision tasks. Click here to get an idea of what can be done using Google Vision API.\n",
        "\n",
        "Basically what happens is a majority of the cloud providers, and smaller machine learning focused companies provide ready-to-use APIs. They cater to the needs of developers/businesses that do not have expertise in ML, who want to implement ML in their processes or product suites.\n",
        "\n",
        "Popular examples of machine learning APIs suited explicitly for web development stuff are DialogFlow, Microsoft's Cognitive Toolkit, TensorFlow.js, etc.\n",
        "\n",
        "Now that you have a fair idea of what APIs are, let's see how you can wrap a machine learning model (developed in Python) into an API in Python.\n",
        "\n",
        "Flask – A Web Services' Framework in Python:\n",
        "\n",
        "Now, you might think what is a web service? Web service is a form of API only that assumes that an API is hosted over a server and can be consumed. Web API, Web Service - these terms are generally used interchangeably.\n",
        "\n",
        "Coming to Flask, it is a web service development framework in Python. It is not the only one in Python, there couple others as well such as Django, Falcon, Hug, etc. But you will use Flask for this tutorial. For learning about Flask, you can refer to these tutorials.\n",
        "\n",
        "If you downloaded the Anaconda distribution, you already have Flask installed. Otherwise, you will have to install it yourself with:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iE7i075BAMOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask\n"
      ],
      "metadata": {
        "id": "JcgNy3CJAM6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "@app.route(\"\")\n",
        "def hello():\n",
        "    return \"Welcome to machine learning model APIs!\"\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "2QhbftSwAQDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "p9sg3epUAR-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset in a dataframe object and include only four features as mentioned\n",
        "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "include = ['Age', 'Sex', 'Embarked', 'Survived'] # Only four features\n",
        "df_ = df[include]\n"
      ],
      "metadata": {
        "id": "HVt9CwBMAWlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categoricals = []\n",
        "for col, col_type in df_.dtypes.iteritems():\n",
        "     if col_type == 'O':\n",
        "          categoricals.append(col)\n",
        "     else:\n",
        "          df_[col].fillna(0, inplace=True)\n"
      ],
      "metadata": {
        "id": "rAVtYxnNAYAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ohe = pd.get_dummies(df_, columns=categoricals, dummy_na=True)\n"
      ],
      "metadata": {
        "id": "Fbcg_Tw3AaEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "dependent_variable = 'Survived'\n",
        "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
        "y = df_ohe[dependent_variable]\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x, y)\n"
      ],
      "metadata": {
        "id": "pwyu4ueAAcdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
        "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
        "          verbose=0, warm_start=False)\n"
      ],
      "metadata": {
        "id": "YvgmoyxKAeSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.externals import joblib\n",
        "joblib.dump(lr, 'model.pkl')\n"
      ],
      "metadata": {
        "id": "ybcVGfchAfzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['model.pkl']\n"
      ],
      "metadata": {
        "id": "5mo3QfFZAhsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = joblib.load('model.pkl')\n"
      ],
      "metadata": {
        "id": "6JIKr6g-AjhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an API From a Machine Learning Model using Flask\n",
        "\n",
        "For serving your model with Flask, you will do the following two things:\n",
        "\n",
        "Load the already persisted model into memory when the application starts,\n",
        "Create an API endpoint that takes input variables, transforms them into the appropriate format, and returns predictions.\n",
        "More specifically, your sample input to the API will look like the following:"
      ],
      "metadata": {
        "id": "Cn1Qam9WAmnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "    {\"Age\": 85, \"Sex\": \"male\", \"Embarked\": \"S\"},\n",
        "    {\"Age\": 24, \"Sex\": '\"female\"', \"Embarked\": \"C\"},\n",
        "    {\"Age\": 3, \"Sex\": \"male\", \"Embarked\": \"C\"},\n",
        "    {\"Age\": 21, \"Sex\": \"male\", \"Embarked\": \"S\"}\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "yBw_jJC2Aq34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\"prediction\": [0, 1, 1, 0]}\n"
      ],
      "metadata": {
        "id": "kUZkJXvaAt5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, jsonify\n",
        "app = Flask(__name__)\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "     json_ = request.json\n",
        "     query_df = pd.DataFrame(json_)\n",
        "     query = pd.get_dummies(query_df)\n",
        "     prediction = lr.predict(query)\n",
        "     return jsonify({'prediction': list(prediction)})\n"
      ],
      "metadata": {
        "id": "JKBa4nvHAxjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_columns = list(x.columns)\n",
        "joblib.dump(model_columns, 'model_columns.pkl')\n"
      ],
      "metadata": {
        "id": "d-zwLpHxAzrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "['model_columns.pkl']\n"
      ],
      "metadata": {
        "id": "MM4W2O18A1zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/predict', methods=['POST']) # Your API endpoint URL would consist /predict\n",
        "def predict():\n",
        "    if lr:\n",
        "        try:\n",
        "            json_ = request.json\n",
        "            query = pd.get_dummies(pd.DataFrame(json_))\n",
        "            query = query.reindex(columns=model_columns, fill_value=0)\n",
        "\n",
        "            prediction = list(lr.predict(query))\n",
        "\n",
        "            return jsonify({'prediction': prediction})\n",
        "\n",
        "        except:\n",
        "\n",
        "            return jsonify({'trace': traceback.format_exc()})\n",
        "    else:\n",
        "        print ('Train the model first')\n",
        "        return ('No model here to use')\n"
      ],
      "metadata": {
        "id": "_xYWjy7-A3jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        port = int(sys.argv[1]) # This is for a command-line argument\n",
        "    except:\n",
        "        port = 12345 # If you don't provide any port then the port will be set to 12345\n",
        "    lr = joblib.load(model_file_name) # Load \"model.pkl\"\n",
        "    print ('Model loaded')\n",
        "    model_columns = joblib.load(model_columns_file_name) # Load \"model_columns.pkl\"\n",
        "    print ('Model columns loaded')\n",
        "    app.run(port=port, debug=True)\n"
      ],
      "metadata": {
        "id": "l1_CDNsJA8s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset in a dataframe object and include only four features as mentioned\n",
        "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "include = ['Age', 'Sex', 'Embarked', 'Survived'] # Only four features\n",
        "df_ = df[include]\n",
        "\n",
        "# Data Preprocessing\n",
        "categoricals = []\n",
        "for col, col_type in df_.dtypes.iteritems():\n",
        "     if col_type == 'O':\n",
        "          categoricals.append(col)\n",
        "     else:\n",
        "          df_[col].fillna(0, inplace=True)\n",
        "\n",
        "df_ohe = pd.get_dummies(df_, columns=categoricals, dummy_na=True)\n",
        "\n",
        "# Logistic Regression classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "dependent_variable = 'Survived'\n",
        "x = df_ohe[df_ohe.columns.difference([dependent_variable])]\n",
        "y = df_ohe[dependent_variable]\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x, y)\n",
        "\n",
        "# Save your model\n",
        "from sklearn.externals import joblib\n",
        "joblib.dump(lr, 'model.pkl')\n",
        "print(\"Model dumped!\")\n",
        "\n",
        "# Load the model that you just saved\n",
        "lr = joblib.load('model.pkl')\n",
        "\n",
        "# Saving the data columns from training\n",
        "model_columns = list(x.columns)\n",
        "joblib.dump(model_columns, 'model_columns.pkl')\n",
        "print(\"Models columns dumped!\")"
      ],
      "metadata": {
        "id": "MXlwebheBA7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "from flask import Flask, request, jsonify\n",
        "from sklearn.externals import joblib\n",
        "import traceback\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Your API definition\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if lr:\n",
        "        try:\n",
        "            json_ = request.json\n",
        "            print(json_)\n",
        "            query = pd.get_dummies(pd.DataFrame(json_))\n",
        "            query = query.reindex(columns=model_columns, fill_value=0)\n",
        "\n",
        "            prediction = list(lr.predict(query))\n",
        "\n",
        "            return jsonify({'prediction': str(prediction)})\n",
        "\n",
        "        except:\n",
        "\n",
        "            return jsonify({'trace': traceback.format_exc()})\n",
        "    else:\n",
        "        print ('Train the model first')\n",
        "        return ('No model here to use')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        port = int(sys.argv[1]) # This is for a command-line input\n",
        "    except:\n",
        "        port = 12345 # If you don't provide any port the port will be set to 12345\n",
        "\n",
        "    lr = joblib.load(\"model.pkl\") # Load \"model.pkl\"\n",
        "    print ('Model loaded')\n",
        "    model_columns = joblib.load(\"model_columns.pkl\") # Load \"model_columns.pkl\"\n",
        "    print ('Model columns loaded')\n",
        "\n",
        "    app.run(port=port, debug=True)"
      ],
      "metadata": {
        "id": "YcmW_2sFBCdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3pDKIC5BFEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}