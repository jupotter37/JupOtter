{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction and Loading Steps\n",
    "\n",
    "1. **Data Extraction**:\n",
    "   - **Connect to the Database**: Use appropriate libraries (e.g., `psycopg2` for PostgreSQL) to establish a connection.\n",
    "   - **Retrieve Data**: Write SQL queries to extract the necessary tables or data subsets.\n",
    "   - **Export Data**: Optionally, save the extracted data into CSV files for further processing.\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - **Load Data into DataFrames**: Use libraries like `pandas` to load the extracted CSV files or data directly from the database into DataFrames for manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: psycopg2 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.9)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\harshavardhanasadi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas  psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting table: _prisma_migrations\n",
      "Table _prisma_migrations exported to staging\\_prisma_migrations.csv\n",
      "Exporting table: Employee\n",
      "Table Employee exported to staging\\Employee.csv\n",
      "Exporting table: Course\n",
      "Table Course exported to staging\\Course.csv\n",
      "Exporting table: CourseEnrollment\n",
      "Table CourseEnrollment exported to staging\\CourseEnrollment.csv\n",
      "Exporting table: User\n",
      "Table User exported to staging\\User.csv\n",
      "Exporting table: QuestionBank\n",
      "Table QuestionBank exported to staging\\QuestionBank.csv\n",
      "Exporting table: Questions\n",
      "Table Questions exported to staging\\Questions.csv\n",
      "Exporting table: CourseEngageLogs\n",
      "Table CourseEngageLogs exported to staging\\CourseEngageLogs.csv\n",
      "Exporting table: Notifications\n",
      "Table Notifications exported to staging\\Notifications.csv\n",
      "Exporting table: LearningPathMap\n",
      "Table LearningPathMap exported to staging\\LearningPathMap.csv\n",
      "Exporting table: LearningPath\n",
      "Table LearningPath exported to staging\\LearningPath.csv\n",
      "Exporting table: Prerequisites\n",
      "Table Prerequisites exported to staging\\Prerequisites.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Temp\\ipykernel_25556\\3405956836.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n",
      "C:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Temp\\ipykernel_25556\\3405956836.py:39: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Define your database connection parameters\n",
    "db_params = {\n",
    "    'database': 'Emp_course_management',#'Emp_Course_Management_System', #'Emp_course_management',\n",
    "    'user': 'postgres',\n",
    "    'password': '965335',#'postgres', #'965335',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Fetch all table names from the public schema\n",
    "cur.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema='public';\n",
    "\"\"\")\n",
    "tables = cur.fetchall()\n",
    "\n",
    "# Define staging directory\n",
    "staging_dir = 'staging'\n",
    "os.makedirs(staging_dir, exist_ok=True)  # Create staging directory if it doesn't exist\n",
    "\n",
    "# Loop through each table and export to CSV\n",
    "for table in tables:\n",
    "    table_name = table[0]\n",
    "    print(f\"Exporting table: {table_name}\")\n",
    "    \n",
    "    # Read table into a DataFrame\n",
    "    df = pd.read_sql_query(f'SELECT * FROM public.\"{table_name}\";', conn)\n",
    "    \n",
    "    # Define the path for the CSV file\n",
    "    csv_file_path = os.path.join(staging_dir, f\"{table_name}.csv\")\n",
    "    \n",
    "    # Export DataFrame to CSV\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Table {table_name} exported to {csv_file_path}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Steps\n",
    "\n",
    "1. **Remove Duplicates**: \n",
    "   - Identify and remove duplicate records to ensure data integrity.\n",
    "\n",
    "2. **Handle Missing Values**:\n",
    "   - Decide on a strategy for missing data (e.g., imputation, removal, or using a placeholder).\n",
    "   - Implement the strategy based on your analysis needs.\n",
    "\n",
    "3. **Data Type Conversion**:\n",
    "   - Ensure all columns have the correct data types (e.g., integers, floats, dates).\n",
    "   - Convert categorical variables to a suitable format (e.g., using one-hot encoding).\n",
    "\n",
    "4. **Standardization and Normalization**:(data science)\n",
    "   - Standardize numerical columns to a common scale, if necessary.\n",
    "   - Normalize data for specific algorithms that require it.\n",
    "\n",
    "### Feature Engineering and Data Preparation Steps\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "   - Create new features that may be beneficial for prediction (e.g., extracting year from a date, combining features).\n",
    "   - Encode categorical variables using techniques like label encoding or one-hot encoding.(data science)\n",
    "\n",
    "2. **Aggregation and Grouping**:\n",
    "   - Aggregate data to a desired level (e.g., total sales per month).\n",
    "   - Group data based on relevant categories to simplify analysis.\n",
    "\n",
    "3. **Outlier Detection and Treatment**:\n",
    "   - Identify and handle outliers based on domain knowledge or statistical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   emp_id       103 non-null    string\n",
      " 1   email        103 non-null    string\n",
      " 2   emp_name     103 non-null    string\n",
      " 3   designation  103 non-null    string\n",
      "dtypes: string(4)\n",
      "memory usage: 3.3 KB\n",
      "None\n",
      "   emp_id                 email         emp_name  \\\n",
      "0  JMD128  JMD128@jmangroup.com    Perry Dibbert   \n",
      "1  JMD129  JMD129@jmangroup.com  Cristina Reilly   \n",
      "2  JMD130  JMD130@jmangroup.com  Cary Kerluke MD   \n",
      "3  JMD131  JMD131@jmangroup.com      Walter Veum   \n",
      "4  JMD132  JMD132@jmangroup.com      Rene Kirlin   \n",
      "\n",
      "                     designation  \n",
      "0               SOLUTION_ENABLER  \n",
      "1            SOLUTION_CONSULTANT  \n",
      "2  TECHNOLOGY_SOLUTION_ARCHITECT  \n",
      "3   PRINCIPAL_SOLUTION_ARCHITECT  \n",
      "4              SOFTWARE_ENGINEER  \n"
     ]
    }
   ],
   "source": [
    "# EMPLOYEE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file (assumed to be already extracted)\n",
    "employee_data = pd.read_csv('./staging/Employee.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_employee_data = employee_data[['emp_id', 'email', 'emp_name', 'designation']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_employee_data = cleaned_employee_data.drop_duplicates(subset='emp_id')\n",
    "\n",
    "# step 4: Change datatype\n",
    "cleaned_employee_data['emp_id'] = cleaned_employee_data['emp_id'].astype('string')\n",
    "cleaned_employee_data['email'] = cleaned_employee_data['email'].astype('string')\n",
    "cleaned_employee_data['emp_name'] = cleaned_employee_data['emp_name'].astype('string')\n",
    "cleaned_employee_data['designation'] = cleaned_employee_data['designation'].astype('string')\n",
    "\n",
    "# Step 5: Provide information about the cleaned table\n",
    "print(cleaned_employee_data.info())\n",
    "print(cleaned_employee_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_employee_data.to_csv('./prep/cleaned_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 5 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   course_id                 103 non-null    int64 \n",
      " 1   course_name               103 non-null    object\n",
      " 2   course_description        103 non-null    object\n",
      " 3   course_difficulty_level   103 non-null    object\n",
      " 4   course_duration_in_weeks  103 non-null    int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 4.2+ KB\n",
      "None\n",
      "   course_id                            course_name  \\\n",
      "0        100      Commissioning editor Fundamentals   \n",
      "1        101              Neurosurgeon Fundamentals   \n",
      "2        102      Merchandiser, retail Fundamentals   \n",
      "3        103  Arts development officer Fundamentals   \n",
      "4        104    Embryologist, clinical Fundamentals   \n",
      "\n",
      "                                  course_description course_difficulty_level  \\\n",
      "0  Loss give employee ball. Eye level popular app...            INTERMEDIATE   \n",
      "1  She change picture. Produce owner voice if. Di...                  EXPERT   \n",
      "2  Change finish production realize president. Op...                BEGINNER   \n",
      "3  Shoulder pressure subject notice determine sis...            INTERMEDIATE   \n",
      "4     Interview huge agree. They wish analysis city.                BEGINNER   \n",
      "\n",
      "   course_duration_in_weeks  \n",
      "0                        20  \n",
      "1                         4  \n",
      "2                         8  \n",
      "3                        12  \n",
      "4                        12  \n"
     ]
    }
   ],
   "source": [
    "# COURSE - TABLE\n",
    "\n",
    "# Step 1: Load the data from the CSV file\n",
    "courses_data = pd.read_csv('./staging/Course.csv')\n",
    "\n",
    "# Step 2: Extract relevant columns\n",
    "cleaned_courses_data = courses_data[['course_id', 'course_name', 'description', 'duration', 'difficulty_level']]\n",
    "\n",
    "# Step 3: Remove duplicates\n",
    "cleaned_courses_data = cleaned_courses_data.drop_duplicates(subset='course_id')\n",
    "\n",
    "# Step 3: Convert duration to weeks\n",
    "def duration_to_weeks(duration):\n",
    "    if 'months' in duration:\n",
    "        return int(duration.split()[0]) * 4  # Assuming 1 month = 4 weeks\n",
    "    elif 'years' in duration:\n",
    "        return int(duration.split()[0]) * 52  # Assuming 1 year = 52 weeks\n",
    "    elif 'weeks' in duration:\n",
    "        return int(duration.split()[0])\n",
    "    else:\n",
    "        return 1  # Handle any unexpected format\n",
    "\n",
    "cleaned_courses_data['duration_in_weeks'] = cleaned_courses_data['duration'].apply(duration_to_weeks)\n",
    "\n",
    "# Step 5: Clean the DataFrame by dropping the original duration column\n",
    "cleaned_courses_data = cleaned_courses_data.drop(columns=['duration'])\n",
    "\n",
    "# step 6: changing column name\n",
    "cleaned_courses_data.rename(columns={'description': 'course_description', 'difficulty_level' : 'course_difficulty_level', 'duration_in_weeks' : 'course_duration_in_weeks'}, inplace=True)\n",
    "\n",
    "# Step 7: Provide information about the cleaned table\n",
    "print(cleaned_courses_data.info())\n",
    "print(cleaned_courses_data.head())  # Show the first few rows of the cleaned data\n",
    "\n",
    "# Optionally, save the cleaned data to a new CSV file\n",
    "cleaned_courses_data.to_csv('./prep/cleaned_courses_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   learning_path_id           19 non-null     int64 \n",
      " 1   learning_path_description  19 non-null     object\n",
      " 2   learning_path_name         19 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 588.0+ bytes\n",
      "None\n",
      "   learning_path_id                          learning_path_description  \\\n",
      "0                 1  A machine learning (ML) learning path is a str...   \n",
      "1                 2  An Artificial Intelligence (AI) learning path ...   \n",
      "2                 3  The Full Stack Learning Path equips learners w...   \n",
      "3                 4  The Frontend Learning Path focuses on the desi...   \n",
      "4               100    Master the fundamentals of software development   \n",
      "\n",
      "        learning_path_name  \n",
      "0         Machine Learning  \n",
      "1  Artificial Intelligence  \n",
      "2               Full Stack  \n",
      "3                 Frontend  \n",
      "4     Software Engineering  \n"
     ]
    }
   ],
   "source": [
    "# LEARNING_PATH - TABLE\n",
    "learning_path_data = pd.read_csv('./staging/LearningPath.csv')\n",
    "\n",
    "cleaned_learningPath = learning_path_data[['learning_path_id', 'description', 'path_name']]\n",
    "\n",
    "cleaned_learningPath = cleaned_learningPath.drop_duplicates(subset='learning_path_id')\n",
    "\n",
    "cleaned_learningPath.rename(columns={'path_name' : 'learning_path_name', 'description' : 'learning_path_description'}, inplace=True)\n",
    "\n",
    "print(cleaned_learningPath.info())\n",
    "print(cleaned_learningPath.head())\n",
    "\n",
    "cleaned_learningPath.to_csv('./prep/cleaned_learning_paths.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 262 entries, 0 to 261\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   course_id         262 non-null    int64\n",
      " 1   learning_path_id  262 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.2 KB\n",
      "None\n",
      "   course_id  learning_path_id\n",
      "0         12                 1\n",
      "1         12                 2\n",
      "2         10                 3\n",
      "3         10                 4\n",
      "4          9                 3\n"
     ]
    }
   ],
   "source": [
    "# LearningPathMap - TABLE\n",
    "\n",
    "learning_path_map_data = pd.read_csv('./staging/LearningPathMap.csv')\n",
    "\n",
    "cleaned_learningPathMap = learning_path_map_data[['course_id', 'learning_path_id']]\n",
    "\n",
    "cleaned_learningPathMap = cleaned_learningPathMap.drop_duplicates(subset=['course_id', 'learning_path_id'], keep='first')\n",
    "\n",
    "print(cleaned_learningPathMap.info())\n",
    "print(cleaned_learningPathMap.head())\n",
    "\n",
    "cleaned_learningPathMap.to_csv('./prep/cleaned_learning_paths_map.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 296 entries, 0 to 295\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   enroll_id               296 non-null    int64  \n",
      " 1   emp_id                  296 non-null    object \n",
      " 2   course_id               296 non-null    int64  \n",
      " 3   current_page            294 non-null    float64\n",
      " 4   total_pages             294 non-null    float64\n",
      " 5   test_score              293 non-null    float64\n",
      " 6   course_certificate_url  195 non-null    object \n",
      " 7   createdAt               296 non-null    object \n",
      "dtypes: float64(3), int64(2), object(3)\n",
      "memory usage: 18.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 296 entries, 0 to 295\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   enroll_id                     296 non-null    int64  \n",
      " 1   emp_id                        296 non-null    object \n",
      " 2   course_id                     296 non-null    int64  \n",
      " 3   createdAt                     296 non-null    object \n",
      " 4   course_certificate_generated  296 non-null    bool   \n",
      " 5   completion_rate               296 non-null    float64\n",
      " 6   test_score_normalized         296 non-null    float64\n",
      "dtypes: bool(1), float64(2), int64(2), object(2)\n",
      "memory usage: 14.3+ KB\n",
      "None\n",
      "   enroll_id  emp_id  course_id                createdAt  \\\n",
      "0        100  JMD100        125  2024-08-19 14:25:07.025   \n",
      "1        101  JMD100         10  2024-08-15 08:12:56.833   \n",
      "2        102  JMD100        128  2024-04-29 19:10:54.473   \n",
      "3        103  JMD100        145  2023-12-21 02:48:19.687   \n",
      "4        104  JMD101        185  2024-10-06 14:53:40.859   \n",
      "\n",
      "   course_certificate_generated  completion_rate  test_score_normalized  \n",
      "0                          True             0.13                   0.13  \n",
      "1                          True             0.42                   0.15  \n",
      "2                          True             0.91                   0.13  \n",
      "3                          True             0.90                   0.66  \n",
      "4                          True             0.05                   0.37  \n"
     ]
    }
   ],
   "source": [
    "# CourseEnrollment - TABLE\n",
    "\n",
    "Course_Enrollment_data = pd.read_csv('./staging/CourseEnrollment.csv')\n",
    "\n",
    "cleaned_course_enrollment_data = Course_Enrollment_data[['enroll_id', 'emp_id', 'course_id', 'current_page', 'total_pages', 'test_score', 'course_certificate_url', 'createdAt']]\n",
    "\n",
    "cleaned_course_enrollment_data = cleaned_course_enrollment_data.drop_duplicates(subset=['enroll_id', 'course_id'], keep='first')\n",
    "\n",
    "print(cleaned_course_enrollment_data.info())\n",
    "# Replace missing values without using inplace\n",
    "cleaned_course_enrollment_data['current_page'] = cleaned_course_enrollment_data['current_page'].fillna(0)\n",
    "cleaned_course_enrollment_data['total_pages'] = cleaned_course_enrollment_data['total_pages'].fillna(100)\n",
    "cleaned_course_enrollment_data['test_score'] = cleaned_course_enrollment_data['test_score'].fillna(0)\n",
    "# Create a new boolean column 'course_certificate_generated'\n",
    "cleaned_course_enrollment_data['course_certificate_generated'] = cleaned_course_enrollment_data['course_certificate_url'].apply(lambda x: True if isinstance(x, str) and x.strip() else False)\n",
    "cleaned_course_enrollment_data = cleaned_course_enrollment_data.drop(columns=['course_certificate_url'])\n",
    "\n",
    "# Normalize current_page based on total_pages\n",
    "cleaned_course_enrollment_data['completion_rate'] = cleaned_course_enrollment_data['current_page'] / cleaned_course_enrollment_data['total_pages']\n",
    "# Normalize test_score (assuming the max score is 100)\n",
    "cleaned_course_enrollment_data['test_score_normalized'] = cleaned_course_enrollment_data['test_score'] / 100\n",
    "\n",
    "cleaned_course_enrollment_data.drop(columns=['current_page', 'total_pages', 'test_score'],axis=1, inplace=True)\n",
    "\n",
    "print(cleaned_course_enrollment_data.info())\n",
    "print(cleaned_course_enrollment_data.head())\n",
    "\n",
    "cleaned_course_enrollment_data.to_csv('./prep/cleaned_courseEnrollment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 791 entries, 0 to 790\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   enroll_id          791 non-null    int64 \n",
      " 1   start_time         791 non-null    object\n",
      " 2   time_spent_in_sec  791 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.7+ KB\n",
      "None\n",
      "   enroll_id               start_time  time_spent_in_sec\n",
      "0          2  2024-10-07 04:33:08.446                  8\n",
      "1          4  2024-10-07 04:33:22.656                  4\n",
      "2          3  2024-10-07 05:08:09.332                  4\n",
      "3          5  2024-10-08 09:14:53.354                 10\n",
      "4          3  2023-10-23 21:03:47.279               4191\n"
     ]
    }
   ],
   "source": [
    "# CourseEngageLogs - TABLE  \n",
    "\n",
    "CourseEngageLogs = pd.read_csv('./staging/CourseEngageLogs.csv')\n",
    "cleaned_course_engageLogs_data = CourseEngageLogs[['enroll_id', 'start_time', 'time_spent_in_sec']]\n",
    "\n",
    "cleaned_course_engageLogs_data = cleaned_course_engageLogs_data.drop_duplicates(subset=['enroll_id', 'start_time'], keep='first')\n",
    "\n",
    "print(cleaned_course_engageLogs_data.info())\n",
    "print(cleaned_course_engageLogs_data.head())\n",
    "\n",
    "cleaned_course_engageLogs_data.to_csv('./prep/cleaned_courseEngageLogs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1116 entries, 0 to 1115\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   notification_id  1116 non-null   int64 \n",
      " 1   enroll_id        1116 non-null   int64 \n",
      " 2   status           1115 non-null   object\n",
      " 3   user_viewed      1116 non-null   bool  \n",
      " 4   created_date     1116 non-null   object\n",
      "dtypes: bool(1), int64(2), object(2)\n",
      "memory usage: 36.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1116 entries, 0 to 1115\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   notification_id     1116 non-null   int64 \n",
      " 1   enroll_id           1116 non-null   int64 \n",
      " 2   certificate_status  1116 non-null   bool  \n",
      " 3   user_viewed         1116 non-null   bool  \n",
      " 4   created_date        1116 non-null   object\n",
      "dtypes: bool(2), int64(2), object(1)\n",
      "memory usage: 28.5+ KB\n",
      "None\n",
      "   notification_id  enroll_id  certificate_status  user_viewed  \\\n",
      "0              100        100               False        False   \n",
      "1              101        100               False        False   \n",
      "2              102        100               False        False   \n",
      "3              103        100                True        False   \n",
      "4              104        101               False        False   \n",
      "\n",
      "              created_date  \n",
      "0  2024-08-25 12:20:29.338  \n",
      "1  2024-07-30 22:55:12.561  \n",
      "2  2024-02-21 01:38:15.058  \n",
      "3  2024-10-13 09:06:38.309  \n",
      "4  2023-11-25 22:39:11.447  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HarshaVardhanAsadi\\AppData\\Local\\Temp\\ipykernel_25556\\3034790909.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned_notifications_data['status'] = cleaned_notifications_data['status'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "#  Notifications - TABLE\n",
    "\n",
    "notifications_data = pd.read_csv('./staging/Notifications.csv')\n",
    "\n",
    "cleaned_notifications_data = notifications_data[['notification_id', 'enroll_id', 'status', 'user_viewed', 'created_date']]\n",
    "\n",
    "print(cleaned_notifications_data.info())    # status contains null - admin to taken a desition (make it into false)\n",
    "\n",
    "cleaned_notifications_data['status'] = cleaned_notifications_data['status'].fillna(False)\n",
    "cleaned_notifications_data.rename(columns={'status': 'certificate_status'}, inplace=True)\n",
    "\n",
    "print(cleaned_notifications_data.info())    \n",
    "print(cleaned_notifications_data.head())\n",
    "\n",
    "cleaned_notifications_data.to_csv('./prep/cleaned_Notifications.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration & Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Integration and Storage Steps\n",
    "\n",
    "1. **Join Tables**:\n",
    "   - Merge or join different tables to create a unified dataset that includes all necessary features for analysis.\n",
    "   - Ensure that the join keys are appropriate and that the merging process retains the relevant data.\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - Create Final Tables:\n",
    "     - Organize the cleaned and transformed data into final tables that are structured for analysis and modeling.\n",
    "     - Save these final tables as CSV files or store them in a database for easy access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into DataFrames\n",
    "cleaned_employee_data = pd.read_csv('prep/cleaned_employee_data.csv')\n",
    "cleaned_course_enrollment_data = pd.read_csv('prep/cleaned_courseEnrollment.csv')\n",
    "cleaned_courses_data = pd.read_csv('prep/cleaned_courses_data.csv')\n",
    "cleaned_learning_paths_map = pd.read_csv('prep/cleaned_learning_paths_map.csv')\n",
    "cleaned_learning_paths_data = pd.read_csv('prep/cleaned_learning_paths.csv')\n",
    "cleaned_course_engage_logs = pd.read_csv('prep/cleaned_courseEngageLogs.csv')\n",
    "cleaned_notifications = pd.read_csv('prep/cleaned_Notifications.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  total_attempts  accepted_attempts  success_rate\n",
      "0            2              10                  0      0.000000\n",
      "1            3               9                  3      0.333333\n",
      "2            4              10                  3      0.300000\n",
      "3            5               7                  0      0.000000\n",
      "4          100               4                  1      0.250000\n",
      "..         ...             ...                ...           ...\n",
      "289        385               2                  1      0.500000\n",
      "290        386               2                  1      0.500000\n",
      "291        387               4                  1      0.250000\n",
      "292        388               5                  1      0.200000\n",
      "293        389               3                  0      0.000000\n",
      "\n",
      "[294 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "success_rate_df = cleaned_notifications.groupby('enroll_id').agg(\n",
    "    total_attempts=('certificate_status', 'size'),  # Total attempts\n",
    "    accepted_attempts=('certificate_status', lambda x: x.sum()),  # Count of accepted attempts\n",
    ").reset_index()\n",
    "\n",
    "# Calculate success rate\n",
    "success_rate_df['success_rate'] = success_rate_df['accepted_attempts'] / success_rate_df['total_attempts']\n",
    "\n",
    "# Display the results\n",
    "print(success_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  time_spent_in_sec\n",
      "0            2              23788\n",
      "1            3              25305\n",
      "2            4              15029\n",
      "3            5              21124\n",
      "4          100              10498\n",
      "..         ...                ...\n",
      "289        385               2746\n",
      "290        386               2697\n",
      "291        387               5573\n",
      "292        388               7283\n",
      "293        389               3064\n",
      "\n",
      "[294 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by enroll_id and calculate total time spent\n",
    "total_time_spent_df = cleaned_course_engage_logs.groupby('enroll_id')['time_spent_in_sec'].sum().reset_index()\n",
    "\n",
    "# Display the results\n",
    "print(total_time_spent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     enroll_id  emp_id  course_id                createdAt  \\\n",
      "0          100  JMD100        125  2024-08-19 14:25:07.025   \n",
      "1          100  JMD100        125  2024-08-19 14:25:07.025   \n",
      "2          101  JMD100         10  2024-08-15 08:12:56.833   \n",
      "3          101  JMD100         10  2024-08-15 08:12:56.833   \n",
      "4          102  JMD100        128  2024-04-29 19:10:54.473   \n",
      "..         ...     ...        ...                      ...   \n",
      "705        389  JMD199        171  2024-02-02 12:27:02.552   \n",
      "706        389  JMD199        171  2024-02-02 12:27:02.552   \n",
      "707          7  JMD101        107  2024-10-14 09:11:39.053   \n",
      "708          7  JMD101        107  2024-10-14 09:11:39.053   \n",
      "709          7  JMD101        107  2024-10-14 09:11:39.053   \n",
      "\n",
      "     course_certificate_generated  completion_rate  test_score_normalized  \\\n",
      "0                            True             0.13                   0.13   \n",
      "1                            True             0.13                   0.13   \n",
      "2                            True             0.42                   0.15   \n",
      "3                            True             0.42                   0.15   \n",
      "4                            True             0.91                   0.13   \n",
      "..                            ...              ...                    ...   \n",
      "705                         False             0.78                   0.84   \n",
      "706                         False             0.78                   0.84   \n",
      "707                         False             0.00                   0.00   \n",
      "708                         False             0.00                   0.00   \n",
      "709                         False             0.00                   0.00   \n",
      "\n",
      "                    email             emp_name                    designation  \\\n",
      "0    JMD100@jmangroup.com         Lynn Jenkins  TECHNOLOGY_SOLUTION_ARCHITECT   \n",
      "1    JMD100@jmangroup.com         Lynn Jenkins  TECHNOLOGY_SOLUTION_ARCHITECT   \n",
      "2    JMD100@jmangroup.com         Lynn Jenkins  TECHNOLOGY_SOLUTION_ARCHITECT   \n",
      "3    JMD100@jmangroup.com         Lynn Jenkins  TECHNOLOGY_SOLUTION_ARCHITECT   \n",
      "4    JMD100@jmangroup.com         Lynn Jenkins  TECHNOLOGY_SOLUTION_ARCHITECT   \n",
      "..                    ...                  ...                            ...   \n",
      "705  JMD199@jmangroup.com        Hilda Pfeffer           SR_SOFTWARE_ENGINEER   \n",
      "706  JMD199@jmangroup.com        Hilda Pfeffer           SR_SOFTWARE_ENGINEER   \n",
      "707  JMD101@jmangroup.com  Glenn Runolfsdottir   PRINCIPAL_SOLUTION_ARCHITECT   \n",
      "708  JMD101@jmangroup.com  Glenn Runolfsdottir   PRINCIPAL_SOLUTION_ARCHITECT   \n",
      "709  JMD101@jmangroup.com  Glenn Runolfsdottir   PRINCIPAL_SOLUTION_ARCHITECT   \n",
      "\n",
      "     ...                                 course_description  \\\n",
      "0    ...         Window PM enter check nothing hand always.   \n",
      "1    ...         Window PM enter check nothing hand always.   \n",
      "2    ...  A React.js course typically covers the fundame...   \n",
      "3    ...  A React.js course typically covers the fundame...   \n",
      "4    ...  Fish science blood determine ago great phone. ...   \n",
      "..   ...                                                ...   \n",
      "705  ...          Trade him thing former protect candidate.   \n",
      "706  ...          Trade him thing former protect candidate.   \n",
      "707  ...  Red true factor. Hot quality outside road each...   \n",
      "708  ...  Red true factor. Hot quality outside road each...   \n",
      "709  ...  Red true factor. Hot quality outside road each...   \n",
      "\n",
      "    course_difficulty_level course_duration_in_weeks  learning_path_id  \\\n",
      "0                    EXPERT                        4               104   \n",
      "1                    EXPERT                        4               112   \n",
      "2                  BEGINNER                        2                 3   \n",
      "3                  BEGINNER                        2                 4   \n",
      "4              INTERMEDIATE                       16               104   \n",
      "..                      ...                      ...               ...   \n",
      "705                BEGINNER                       20               109   \n",
      "706                BEGINNER                       20               112   \n",
      "707                  EXPERT                        4               114   \n",
      "708                  EXPERT                        4               109   \n",
      "709                  EXPERT                        4               100   \n",
      "\n",
      "                             learning_path_description  \\\n",
      "0                  Build websites and web applications   \n",
      "1              Ensure software quality through testing   \n",
      "2    The Full Stack Learning Path equips learners w...   \n",
      "3    The Frontend Learning Path focuses on the desi...   \n",
      "4                  Build websites and web applications   \n",
      "..                                                 ...   \n",
      "705     Connect and control devices with IoT solutions   \n",
      "706            Ensure software quality through testing   \n",
      "707  Design scalable and maintainable software systems   \n",
      "708     Connect and control devices with IoT solutions   \n",
      "709    Master the fundamentals of software development   \n",
      "\n",
      "           learning_path_name time_spent_in_sec  total_attempts  \\\n",
      "0             Web Development           10498.0             4.0   \n",
      "1           Quality Assurance           10498.0             4.0   \n",
      "2                  Full Stack            8395.0             4.0   \n",
      "3                    Frontend            8395.0             4.0   \n",
      "4             Web Development            7589.0             5.0   \n",
      "..                        ...               ...             ...   \n",
      "705  Internet of Things (IoT)            3064.0             3.0   \n",
      "706         Quality Assurance            3064.0             3.0   \n",
      "707     Software Architecture               NaN             NaN   \n",
      "708  Internet of Things (IoT)               NaN             NaN   \n",
      "709      Software Engineering               NaN             NaN   \n",
      "\n",
      "     accepted_attempts  success_rate  \n",
      "0                  1.0          0.25  \n",
      "1                  1.0          0.25  \n",
      "2                  1.0          0.25  \n",
      "3                  1.0          0.25  \n",
      "4                  1.0          0.20  \n",
      "..                 ...           ...  \n",
      "705                0.0          0.00  \n",
      "706                0.0          0.00  \n",
      "707                NaN           NaN  \n",
      "708                NaN           NaN  \n",
      "709                NaN           NaN  \n",
      "\n",
      "[710 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Merge the tables\n",
    "merged_data = (\n",
    "    cleaned_course_enrollment_data\n",
    "    .merge(cleaned_employee_data, on='emp_id', how='left')  # Join Employee Details with Course Enrollment\n",
    "    .merge(cleaned_courses_data, on='course_id', how='left')  # Join Course Enrollment with Course Details\n",
    "    .merge(cleaned_learning_paths_map, on='course_id', how='left')  # Join Course Enrollment with Learning Path Mapping\n",
    "    .merge(cleaned_learning_paths_data, on='learning_path_id', how='left')  # Join Course Enrollment with Course Details\n",
    "    .merge(total_time_spent_df, on='enroll_id', how='left')  # Join CourseEngageLogs\n",
    "    .merge(success_rate_df, on='enroll_id', how='left')  # Join Notifications\n",
    ")\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_data)\n",
    "\n",
    "merged_data.to_csv('./reporting/merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001           53846.0                 0.616928              0.5000   \n",
      "1  JMD002           25305.0                 1.000000              0.6000   \n",
      "2  JMD003           21124.0                 0.557994              0.0000   \n",
      "3  JMD100           65672.0                 0.590000              0.2675   \n",
      "4  JMD101           34973.0                 0.075000              0.3500   \n",
      "\n",
      "   total_certificates  \n",
      "0                   2  \n",
      "1                   1  \n",
      "2                   0  \n",
      "3                   8  \n",
      "4                   9  \n"
     ]
    }
   ],
   "source": [
    "# Group by emp_id to aggregate the features\n",
    "employee_performance = merged_data.groupby('emp_id').agg({\n",
    "    'time_spent_in_sec': 'sum',  # Total Time Spent\n",
    "    'completion_rate': 'mean',  # Average Course Completion\n",
    "    'test_score_normalized': 'mean',  # Average Test Score\n",
    "    'course_certificate_generated': lambda x: (x == True).sum(),  # Count of Generated Certificates\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "employee_performance.rename(columns={\n",
    "    'time_spent_in_sec': 'total_time_spent',\n",
    "    'completion_rate': 'average_completion_rate',\n",
    "    'test_score_normalized': 'average_test_score',\n",
    "    'course_certificate_generated' : 'total_certificates'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the employee performance data\n",
    "print(employee_performance.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id  total_time_spent  average_completion_rate  average_test_score  \\\n",
      "0  JMD001           53846.0                 0.616928              0.5000   \n",
      "1  JMD002           25305.0                 1.000000              0.6000   \n",
      "2  JMD003           21124.0                 0.557994              0.0000   \n",
      "3  JMD100           65672.0                 0.590000              0.2675   \n",
      "4  JMD101           34973.0                 0.075000              0.3500   \n",
      "\n",
      "   total_certificates  completed_courses_BEGINNER  completed_courses_EXPERT  \\\n",
      "0                   2                           1                         0   \n",
      "1                   1                           0                         0   \n",
      "2                   0                           0                         0   \n",
      "3                   8                           1                         1   \n",
      "4                   9                           1                         4   \n",
      "\n",
      "   completed_courses_INTERMEDIATE  \n",
      "0                               1  \n",
      "1                               1  \n",
      "2                               1  \n",
      "3                               2  \n",
      "4                               0  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates based on emp_id and course_id to ensure each course is counted once per employee\n",
    "unique_courses = merged_data.drop_duplicates(subset=['emp_id', 'course_id'])\n",
    "# Create a mapping of difficulty levels to counts, grouping by emp_id and course_difficulty_level\n",
    "difficulty_distribution = unique_courses.groupby(['emp_id', 'course_difficulty_level']).size().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns for clarity\n",
    "difficulty_distribution.columns = [f'completed_courses_{level}' for level in difficulty_distribution.columns]\n",
    "\n",
    "# Merge difficulty distribution with employee performance data\n",
    "employee_performance = employee_performance.merge(difficulty_distribution, on='emp_id', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for completed courses\n",
    "employee_performance.fillna(0, inplace=True)\n",
    "\n",
    "# Display the final employee performance data\n",
    "print(employee_performance.head())\n",
    "employee_performance.to_csv('./reporting/employee_performance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     emp_id  learning_path_id        learning_path_name  combined_score\n",
      "0    JMD001                 4                  Frontend        0.636564\n",
      "1    JMD002                 3                Full Stack        0.720000\n",
      "2    JMD003                 3                Full Stack        0.277651\n",
      "3    JMD100               113                  Big Data        0.573127\n",
      "4    JMD101               103             Cybersecurity        0.475254\n",
      "..      ...               ...                       ...             ...\n",
      "98   JMD195               101           Cloud Computing        0.394146\n",
      "99   JMD196               108                Blockchain        0.575945\n",
      "100  JMD197               107   AI and Machine Learning        0.739434\n",
      "101  JMD198               101           Cloud Computing        0.652786\n",
      "102  JMD199               109  Internet of Things (IoT)        0.553414\n",
      "\n",
      "[103 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the learning path predictions\n",
    "learning_path_performance = (\n",
    "    merged_data.groupby(['emp_id', 'learning_path_id'])\n",
    "    .agg({\n",
    "        'completion_rate': 'mean',  # Average completion percentage\n",
    "        'success_rate': 'mean',\n",
    "        'time_spent_in_sec': 'mean',\n",
    "        'test_score_normalized': 'mean'        \n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Min-Max normalization\n",
    "min_time_spent = learning_path_performance['time_spent_in_sec'].min()\n",
    "max_time_spent = learning_path_performance['time_spent_in_sec'].max()\n",
    "\n",
    "\n",
    "# Calculate the ratio or combined score\n",
    "learning_path_performance['combined_score'] = (\n",
    "    (learning_path_performance['completion_rate'] * 0.2) + \n",
    "    ((\n",
    "        (learning_path_performance['time_spent_in_sec'] - min_time_spent) /\n",
    "        (max_time_spent - min_time_spent)\n",
    "    )* 0.2) + \n",
    "    (learning_path_performance['success_rate'] * 0.15) + \n",
    "    (learning_path_performance['test_score_normalized'] * 0.45)\n",
    ")\n",
    "\n",
    "# Get the learning path with the highest combined score for each employee\n",
    "best_learning_paths = learning_path_performance.loc[learning_path_performance.groupby('emp_id')['combined_score'].idxmax()]\n",
    "\n",
    "# Merge with learning path details to get descriptions\n",
    "best_learning_paths = best_learning_paths.merge(cleaned_learning_paths_data[['learning_path_id', 'learning_path_name']], on='learning_path_id', how='left')\n",
    "\n",
    "# Display the final recommendations\n",
    "print(best_learning_paths[['emp_id', 'learning_path_id', 'learning_path_name', 'combined_score']])\n",
    "best_learning_paths.to_csv('./reporting/best_learning_paths.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
