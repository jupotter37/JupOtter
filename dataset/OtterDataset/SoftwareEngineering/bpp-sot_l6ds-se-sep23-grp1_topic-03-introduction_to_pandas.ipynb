{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas scikit-learn\n",
    "%pip freeze > ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header style=\"background: white; border-top: 8px solid #602663; padding: 1em;\">\n",
    "<div>\n",
    "<span style=\"color: black; font-size: medium; font-weight: 700; text-transform: uppercase;\">Level 6 Data Science / Software Engineering</span><br><span style=\"color: #602663; font-size: xxx-large; font-weight: 900;\">Topic 3 &mdash; Introduction to Pandas</span>\n",
    "</div>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Handling Complex Data\n",
    "\n",
    "In data science, we often encounter large, complex datasets that can be challenging to work with. These datasets may come from various sources and in different formats, such as:\n",
    "\n",
    "- CSV files\n",
    "- Excel spreadsheets\n",
    "- SQL databases\n",
    "- JSON files\n",
    "- Web APIs\n",
    "\n",
    "Working with this data efficiently and effectively presents several challenges:\n",
    "\n",
    "1. **Data cleaning**: Real-world data is often messy, containing missing values, duplicates, or inconsistencies.\n",
    "2. **Data manipulation**: Reshaping, merging, and transforming data to suit our analysis needs.\n",
    "3. **Data analysis**: Performing calculations, aggregations, and statistical operations on large datasets.\n",
    "4. **Performance**: Processing large amounts of data quickly and efficiently.\n",
    "\n",
    "## Pandas: Your Data Analysis Swiss Army Knife\n",
    "\n",
    "[Pandas](https://pandas.pydata.org) is a powerful, open-source library for Python that addresses these challenges and more. It provides high-performance, easy-to-use data structures and tools for data manipulation and analysis.\n",
    "\n",
    "### Key Features of Pandas:\n",
    "\n",
    "1. **DataFrame**: A two-dimensional labeled data structure with columns of potentially different types.\n",
    "2. **Series**: A one-dimensional labeled array that can hold data of any type.\n",
    "3. **Efficient data manipulation**: Tools for reading, writing, and transforming data.\n",
    "4. **Handling of missing data**: Built-in support for handling missing data.\n",
    "5. **Merging and joining datasets**: Combining multiple datasets easily.\n",
    "6. **Time series functionality**: Tools for working with date and time data.\n",
    "7. **Integration with other libraries**: Works well with [NumPy](https://numpy.org), [Matplotlib](https://matplotlib.org), and [scikit-learn](https://scikit-learn.org).\n",
    "\n",
    "### The Role of Pandas in Data Analysis\n",
    "\n",
    "Pandas serves as a crucial tool in the data analysis pipeline:\n",
    "\n",
    "1. **Data Loading**: Pandas can read data from various file formats and databases.\n",
    "2. **Data Cleaning**: It provides functions to handle missing values, remove duplicates, and format data.\n",
    "3. **Data Transformation**: Pandas allows you to reshape, merge, and pivot your data.\n",
    "4. **Data Analysis**: You can perform complex operations, grouping, and aggregations on your data.\n",
    "5. **Data Visualisation**: While not a visualisation library itself, Pandas integrates well with plotting libraries like Matplotlib.\n",
    "\n",
    "Let's explore these features hands-on and see how Pandas can make your data analysis tasks more efficient and enjoyable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's begin by importing Pandas and creating a simple `DataFrame` to explore its basic functionality. We'll use a dictionary to create a `DataFrame` with some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas - by convention it is aliased as `pd`\n",
    "import pandas as pd\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'City': ['New York', 'San Francisco', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore some basic operations that Pandas can perform on this `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a single column\n",
    "ages = df['Age']\n",
    "\n",
    "print(type(ages))\n",
    "\n",
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access multiple columns\n",
    "columns = df[['Name', 'City']]\n",
    "\n",
    "print(type(columns))\n",
    "\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific row by index\n",
    "row = df.iloc[1]\n",
    "\n",
    "print(type(row))\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific cell\n",
    "cell = df.loc[2, 'Age']\n",
    "\n",
    "print(type(cell))\n",
    "\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data\n",
    "df[df['Age'] >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column\n",
    "df['Country'] = 'USA'\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple data manipulation\n",
    "df['Age in 5 years'] = df['Age'] + 5\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These examples demonstrate some fundamental Pandas operations:\n",
    "\n",
    "1. Creating a `DataFrame` from a dictionary\n",
    "2. Accessing columns and rows\n",
    "3. Retrieving basic statistics\n",
    "4. Filtering data based on conditions\n",
    "5. Adding new columns\n",
    "6. Performing simple calculations on columns\n",
    "\n",
    "As you can see, Pandas makes it easy to work with structured data. In the next sections, we'll explore more advanced features like data loading from files, data cleaning, and more complex manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "One of the most common tasks in data analysis is loading data from various sources. Pandas provides functions to read data from CSV files, Excel spreadsheets, SQL databases, JSON files, and more. For a complete list of supported file formats, refer to the Pandas [Input/output documentation](https://pandas.pydata.org/docs/reference/io.html).\n",
    "\n",
    "Let's explore how to load data from a CSV file using Pandas. We'll use the `pd.read_csv()` function to read a CSV file into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Titanic dataset\n",
    "# path = os.path.join(os.getcwd(), '..','datasets', 'titanic.csv')\n",
    "url = \"https://raw.githubusercontent.com/bpp-sot/l6ds-se-sep23-grp1/refs/heads/main/datasets/titanic.csv\"\n",
    "\n",
    "titanic = pd.read_csv(url)\n",
    "\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once loaded, we can explore the data, check its structure, and perform various operations on it. This is the first step in the data analysis process: getting the data into a format that we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the DataFrame - a tuple: (rows, columns)\n",
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the DataFrame\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last few rows of the DataFrame\n",
    "titanic.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `groupby()` to group the data by a specific column and perform aggregations on it. This is a powerful feature that allows us to summarise and analyse data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of the DataFrame\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `pclass` and calculate the mean of the `fare` column\n",
    "titanic.groupby(['pclass'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `sex` and then `survived`, then count\n",
    "survivals = titanic.groupby(['sex', 'survived']).size()\n",
    "\n",
    "survivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Real-world data is often messy and requires cleaning before analysis. This involves handling missing values, removing duplicates, and fixing inconsistencies in the data. Pandas provides functions to help with these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "\n",
    "Missing data is a common issue in datasets and can affect the quality of our analysis. Pandas provides several functions to handle missing data, such as `isnull()`, `notnull()`, `dropna()`, and `fillna()`. These functions allow us to identify missing values, remove rows or columns with missing data, or fill in missing values with a specified value.\n",
    "\n",
    "Let's explore how to handle missing data in a `DataFrame` using Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of missing values in each column - `isna()` returns a DataFrame of booleans and `sum()` sums the columns\n",
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Python will coerce a boolean value into an integer value when needed, so `True` becomes `1` and `False` becomes `0`. This is why we can use the `sum()` function to count the number of missing values (`isna() => True`) in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values - Cabin\n",
    "\n",
    "There are 1,309 records in this dataset, and there are a significant number of missing values in the `cabin` column.\n",
    "\n",
    "We can use the `dropna()` function to remove rows with missing values in the `cabin` column, but this would result in losing a large portion of the dataset. Another approach might be to drop the `cabin` column entirely, but we should consider the impact of this decision on our analysis. What if the `cabin` column contains valuable information that we need for our analysis? \n",
    "\n",
    "Another option is to fill in the missing values with a placeholder value, such as `'None'`. This approach allows us to retain the information in the `cabin` column while handling the missing data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values in the `cabin` column with the string 'None'\n",
    "titanic['cabin'] = titanic['cabin'].fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values - Age\n",
    "\n",
    "The `age` column also has some missing values, but we can fill them in with the mean age of the passengers. This way, we retain the data while handling the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in `age` column with the mean\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].mean())\n",
    "\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values - Embarked\n",
    "\n",
    "The `embarked` column has only two missing values, which we can drop without losing much information, but it's also possible to fill them in with the most common value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in `embarked` column with the mode\n",
    "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])\n",
    "\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values - Fare\n",
    "\n",
    "The `fare` column only has one missing value, but we can't simply fill it with eith the mean or median value, as the fare is likely to be related to other factors like the passenger class or the port of embarkation. \n",
    "\n",
    "Let's have a look at the row in question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where `fare` is missing\n",
    "index = titanic[titanic['fare'].isna()].index\n",
    "\n",
    "titanic.iloc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can impute the median fare for the group of passengers that were in the same class and embarked from the same port, and use that value to fill in the missing fare value. We use median instead of mean as the fare distribution is unlikely to be continuous and may have outliers.\n",
    "\n",
    "To get the median fare for each group, we can use the `groupby()` function to group the data by passenger class and port of embarkation, and then calculate the median fare for each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by `pclass` and `embarked` and expose the `fare` column\n",
    "fare_by_pclass_and_embarked = titanic.groupby(['pclass', 'embarked'])['fare']\n",
    "fare_by_pclass_and_embarked.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the fare, we need to index the resulting `fare_by_pclass_and_embarked` object by the passenger class and port of embarkation. The resulting code is a little difficult to read, but here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean fare for 'pclass' = 3 and 'embarked' = 'S'\n",
    "pclass = 3\n",
    "embarked = 'S'\n",
    "\n",
    "fare_by_pclass_and_embarked.median()[pclass][embarked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do this manually, but we'll create a function to help automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_fare(row):\n",
    "    \"\"\"\n",
    "    Impute missing values in the `fare` column by taking the meadian of the fares of passengers with the same `pclass` and `embarked` values.\n",
    "\n",
    "    Parameters:\n",
    "    row: A row in the DataFrame\n",
    "\n",
    "    Returns:\n",
    "    The fare value if it is not missing, otherwise the median fare of passengers with the same `pclass` and `embarked` values\n",
    "    \"\"\"\n",
    "    if pd.isna(row['fare']):\n",
    "        pclass = row['pclass']\n",
    "        embarked = row['embarked']\n",
    "        return titanic.groupby(['pclass', 'embarked'])['fare'].median()[pclass][embarked]\n",
    "    else:\n",
    "        return row['fare']\n",
    "\n",
    "# Apply the `impute_fare` function to the DataFrame\n",
    "titanic['fare'] = titanic.apply(impute_fare, axis=1)\n",
    "\n",
    "titanic.iloc[index] # Check that the record with the missing fare has been updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there are no missing values\n",
    "assert titanic.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides other ways of handling missing data, such as [interpolation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html#pandas-dataframe-interpolate), [forward-fill](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html#pandas-dataframe-ffill), and [backward-fill](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html#pandas-dataframe-bfill), depending on the context of the data. For more information, refer to the Pandas [missing data documentation](https://pandas.pydata.org/docs/reference/frame.html#missing-data-handling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features\n",
    "\n",
    "In data analysis, we often need to create new features from existing data to extract more insights. Pandas makes it easy to add new columns to a `DataFrame` based on existing columns. We can perform calculations, transformations, or aggregations to create new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features - Title\n",
    "\n",
    "The `name` column in the Titanic dataset contains both the passenger's name and title. We can extract the title from the name and create a new column called `title` to store this information. This transformation can help us analyse the data based on the passenger's title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're using a regular expression to extract the title from the name. The pattern `([A-Za-z]+)\\.` captures one or more letters (`[A-Za-z]+`), followed by a period (`\\.`). This pattern extracts the title from the name, such as 'Mr.', 'Mrs.', 'Miss', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title from the `name` column\n",
    "titanic['title'] = titanic['name'].str.extract('([A-Za-z]+)\\\\.', expand=False)\n",
    "titanic['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace rare titles with 'Rare'\n",
    "rare_titles = ['Dr', 'Rev', 'Col', 'Major', 'Lady', 'Capt', 'Sir', 'Jonkheer', 'Dona', 'Don', 'Countess']\n",
    "titanic['title'] = titanic['title'].replace(rare_titles, 'Rare')\n",
    "\n",
    "# Replace 'Mlle' and 'Ms' with 'Miss', and 'Mme' with 'Mrs'\n",
    "titanic['title'] = titanic['title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "titanic['title'] = titanic['title'].replace('Mme', 'Mrs')\n",
    "\n",
    "titanic['title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features - One Hot Encoding\n",
    "\n",
    "Categorical variables like `sex` and `embarked` need to be converted into numerical form for machine learning algorithms. One common technique is one-hot encoding, which creates binary columns for each category in the original column. Pandas provides a function called `get_dummies()` to perform one-hot encoding on categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of `embarked` and `title` columns\n",
    "categorical_columns = ['embarked', 'title']\n",
    "\n",
    "titanic = pd.get_dummies(titanic, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features - Family Size and Alone\n",
    "\n",
    "We can create a new feature called `family_size` by combining the `sibsp` (number of siblings/spouses aboard) and `parch` (number of parents/children aboard) columns. This new feature represents the total number of family members aboard with each passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column `family_size` by summing the `sibsp` and `parch` columns\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch']\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a new feature called `alone` to indicate whether a passenger was traveling alone or with family. This binary feature can help us analyse the survival rates of passengers traveling alone versus those traveling with family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column `alone` that is `True` if `family_size` is 0, otherwise `False`\n",
    "titanic['alone'] = titanic['family_size'] == 0\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features - Deck\n",
    "\n",
    "We can use the `cabin` to determine the deck of the ship where the passenger's cabin was located. The deck information is encoded in the first character of the `cabin` value (e.g., `C85` corresponds to deck `C`). We can extract this information and create a new column called `deck` to store it.\n",
    "\n",
    "- a `cabin` value of `'None'` will map to deck 0\n",
    "- cabin `'A'` will map to deck 1, `'B'` to deck 2, and so on up to `'G'` (deck 7)\n",
    "- cabins `'T'`, `'U'`, `'W'`, `'X'`, `'Y'`, and `'Z'` were on the boat deck, so we will map them to deck 8 (only one record has a cabin value of `'T'`)\n",
    "- some records may contain multiple cabin values, separated by a space, in which case we will take the first cabin value to derive the deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the deck from the `cabin` column\n",
    "deck_mapping = {'N':0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8, 'U': 8, 'W': 8, 'X': 8, 'Y': 8, 'Z': 8}\n",
    "\n",
    "titanic['deck'] = titanic['cabin'].str[0].map(deck_mapping)\n",
    "\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Features\n",
    "\n",
    "Data transformation involves converting data into a suitable format for analysis. This may include scaling numerical features, encoding categorical variables, or normalising data. Pandas provides functions to help with these transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Features - Age\n",
    "\n",
    "The `age` column contains continuous numerical data that can be normalised to improve the performance of our model. We can use the `StandardScaler` from `scikit-learn` to scale the `age` column to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the `age` column\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "titanic['age'] = scaler.fit_transform(titanic[['age']])\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Features - Sex\n",
    "\n",
    "The `sex` column contains categorical data that can be converted into numerical form using one-hot encoding, but we can also map the values to integers directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map `sex` column to integer - male -> 0, female -> 1\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Unnecessary Columns\n",
    "\n",
    "After creating new features, we may want to remove unnecessary columns from the dataset to reduce complexity and improve performance. Pandas provides the `drop()` function to remove columns from a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `cabin`, `name`, `parch`, and `sibsp` columns - no longer needed\n",
    "titanic = titanic.drop(['cabin', 'name', 'parch', 'sibsp'], axis=1)\n",
    "\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `id``, and `ticket` columns - not useful\n",
    "titanic = titanic.drop(['id', 'ticket'], axis=1)\n",
    "\n",
    "titanic.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to a CSV file\n",
    "\n",
    "Now that we have cleaned and preprocessed the data, we can write it to a CSV file using the `to_csv()` method. It's a good practice to save the cleaned data to a file so that we can reuse it later for analysis or modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "titanic.to_csv('titanic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also split the data into training and testing sets before writing them to a file. This allows us to save the training and testing data separately for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = titanic.drop('survived', axis=1)\n",
    "y = titanic['survived']\n",
    "\n",
    "# Save the features and target to separate CSV files\n",
    "X.to_csv('titanic_features.csv', index=False)\n",
    "y.to_csv('titanic_target.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
