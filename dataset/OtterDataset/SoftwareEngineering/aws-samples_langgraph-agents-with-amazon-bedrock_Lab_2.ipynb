{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fa2e13-567d-4509-9023-c99fb230f31f",
   "metadata": {},
   "source": [
    "# Lab 2: LangGraph Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7ea45",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "# Set bedrock configs\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n",
    "\n",
    "# Create a bedrock client to check available models\n",
    "bedrock = boto3.client(\"bedrock\", region_name=aws_region, config=bedrock_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a89cc9",
   "metadata": {},
   "source": [
    "## LangGraph as a State Machine\n",
    "\n",
    "For solution architects familiar with system design, LangGraph can be thought of as a state machine for language models. Just as a state machine in software engineering defines a set of states and transitions between them, LangGraph allows us to define the states of our conversation (represented by nodes) and the transitions between them (represented by edges).\n",
    "\n",
    "**Analogy**: Think of LangGraph as a traffic control system for a smart city. Each intersection (node) represents a decision point, and the roads between them (edges) represent possible paths. The traffic lights (conditional edges) determine which path to take based on current conditions. In our case, the \"traffic\" is the flow of information and decisions in our AI agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=4)  # increased number of results\n",
    "print(type(tool))\n",
    "print(tool.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196c186-af55-4f2d-b569-b7d63a859304",
   "metadata": {},
   "source": [
    "> If you are not familiar with python typing annotation, you can refer to the [python documents](https://docs.python.org/3/library/typing.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40395e",
   "metadata": {},
   "source": [
    "## The Agent State Concept\n",
    "\n",
    "The AgentState class is crucial for maintaining context throughout the conversation. For data scientists, this can be compared to maintaining state in a recurrent neural network.\n",
    "\n",
    "**Analogy**: Think of the AgentState as a sophisticated notepad. As you brainstorm ideas (process queries), you jot down key points (messages). This notepad doesn't just record; it has a special property where new notes (messages) are seamlessly integrated with existing ones, maintaining a coherent flow of thought.\n",
    "At the same time, you can always go back in time and rewrite some parts of it - this is what we call \"time-travel\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7ba73-e603-453b-b06f-5db92c567b19",
   "metadata": {},
   "source": [
    "> Note: in `take_action` below, some logic was added to cover the case that the LLM returned a non-existent tool name.\n",
    "\n",
    "```python\n",
    "if not t[\"name\"] in self.tools:  # check for bad tool name from LLM\n",
    "    print(\"\\n ....bad tool name....\")\n",
    "    result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 727
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_bedrock)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_action, {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_bedrock(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t[\"name\"] in self.tools:  # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd50074",
   "metadata": {},
   "source": [
    "An often overlooked feature is the `??` to inspect the code of a function or object in python.\n",
    "\n",
    "Lets inspect the `bind_tools` method on the `ChatBedrockConverse` class.\n",
    "Can you spot if our tavily tool will be supported, and if there are any restrictions?\n",
    "\n",
    "If you are unsure, how would you check?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61313ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "??ChatBedrockConverse.bind_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence).\\\n",
    "Whenever you can, try to call multiple tools at once, to bring down inference time!\\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6f5f4-2392-41b9-ab96-7919840baa3e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# make sure to install pygraphviz if you haven't done so already using 'conda install --channel conda-forge pygraphviz'\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(abot.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a06a8c-fcd4-4ca6-98f0-36c5809813e6",
   "metadata": {
    "height": 30,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for message in result[\"messages\"]:\n",
    "    print(f\"{message}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF and LA?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f759e2d",
   "metadata": {},
   "source": [
    "## 4. Parallel vs. Sequential Tool Calling\n",
    "\n",
    "The ability of the agent to make both parallel and sequential tool calls is a powerful feature that solution architects should pay attention to.\n",
    "\n",
    "**Deep Dive**:\n",
    "\n",
    "- Parallel tool calling is like a multithreaded application, where multiple independent tasks can be executed simultaneously. This is efficient for queries that require multiple pieces of independent information.\n",
    "- Sequential tool calling is more like a pipeline, where the output of one operation becomes the input of the next. This is necessary for multi-step reasoning tasks.\n",
    "\n",
    "**Analogy**: Imagine a research team working on a complex project. Parallel tool calling is like assigning different team members to research different aspects simultaneously. Sequential tool calling is like a relay race, where each researcher builds on the findings of the previous one.\n",
    "\n",
    "Can you spot if we will have sequential or parallel tool calls and if we have parallel, would they really be executed in parallel?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Note, the query was modified to produce more consistent results.\n",
    "# Results may vary per run and over time as search information and models change.\n",
    "\n",
    "query = \"Who won the super bowl in 2024? In what state is the winning team headquarters located? \\\n",
    "What is the GDP of that state? Answer each question.\"\n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea5638",
   "metadata": {},
   "source": [
    "# Exercise: How would you have to change the tool definition, to allow for parallel calling of the tool?\n",
    "\n",
    "> Note: you can omit the parallel execution with async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833ba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-dev-env",
   "language": "python",
   "name": "agents-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
