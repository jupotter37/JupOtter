{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[adaptado de [Programa de cursos integrados Aprendizado de m√°quina](https://www.coursera.org/specializations/machine-learning-introduction) de [Andrew Ng](https://www.coursera.org/instructor/andrewng)  ([Stanford University](http://online.stanford.edu/), [DeepLearning.AI](https://www.deeplearning.ai/) ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar arquivos adicionais para o laborat√≥rio.\n",
    "!wget https://github.com/fabiobento/dnn-course-2024-1/raw/main/00_course_folder/ml_intro/class_03/9%20-%20Atividade%20Avaliativa%20-%20Regress%C3%A3o%20Log%C3%ADstica/lab_utils_ml_intro_assig_week_3.zip\n",
    "!unzip -n -q lab_utils_ml_intro_assig_week_3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar se estamos no Google Colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress√£o Log√≠stica\n",
    "\n",
    "Neste exerc√≠cio, voc√™ implementar√° a regress√£o log√≠stica e a aplicar√° a dois conjuntos de dados diferentes.\n",
    "\n",
    "\n",
    "# T√≥picos\n",
    "- [ 1 - Pacotes ](#1)\n",
    "- [ 2 - Regress√£o Log√≠stica](#2)\n",
    "  - [ 2.1 Defini√ß√£o do Problema](#2.1)\n",
    "  - [ 2.2 Carregando e visualizando os dados](#2.2)\n",
    "  - [ 2.3  Fun√ß√£o Sigmoid](#2.3)\n",
    "  - [ 2.4 Fun√ß√£o de custo para regress√£o log√≠stica](#2.4)\n",
    "  - [ 2.5 Gradiente para regress√£o log√≠stica](#2.5)\n",
    "  - [ 2.6 Par√¢metros de aprendizagem usando gradiente descendente ](#2.6)\n",
    "  - [ 2.7 Plotando a fronteira de decis√£o](#2.7)\n",
    "  - [ 2.8 Avaliando a regress√£o log√≠stica](#2.8)\n",
    "- [ 3 - Regress√£o L√≥g√≠stica Regularizada](#3)\n",
    "  - [ 3.1 Defini√ß√£o do Problema](#3.1)\n",
    "  - [ 3.2 Carregando e Visualizando os Dados](#3.2)\n",
    "  - [ 3.3 Mapeamento de Caracter√≠sticas](#3.3)\n",
    "  - [ 3.4 Fun√ß√£o de custo para regress√£o log√≠stica regularizada](#3.4)\n",
    "  - [ 3.5 Gradiente para regress√£o log√≠stica regularizada](#3.5)\n",
    "  - [ 3.6 Par√¢metros de aprendizagem usando gradiente descendente](#3.6)\n",
    "  - [ 3.7 Plotando a fronteira de decis√£o](#3.7)\n",
    "  - [ 3.8 Avaliando modelo de regress√£o log√≠stica regularizado](#3.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Pacotes \n",
    "\n",
    "Primeiro, vamos executar a c√©lula abaixo para importar todos os pacotes que voc√™ precisar√° durante esta tarefa.\n",
    "- [numpy](www.numpy.org) √© o pacote fundamental para computa√ß√£o cient√≠fica com Python.\n",
    "- [matplotlib](http://matplotlib.org) √© uma biblioteca famosa para plotar gr√°ficos em Python.\n",
    "- ``utils.py`` cont√©m fun√ß√µes auxiliares para esta tarefa. Voc√™ n√£o precisa modificar o c√≥digo neste arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Regress√£o Log√≠stica\n",
    "\n",
    "Nesta parte do exerc√≠cio, voc√™ construir√° um modelo de regress√£o log√≠stica para prever se um aluno ser√° admitido em uma universidade.\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Defini√ß√£o do problema\n",
    "\n",
    "Suponha que voc√™ seja o administrador de um departamento universit√°rio e queira determinar a chance de admiss√£o de cada candidato com base nos resultados de dois exames.\n",
    "* Voc√™ tem dados hist√≥ricos de candidatos anteriores que podem ser usados como conjunto de treinamento para regress√£o log√≠stica.\n",
    "* Para cada exemplo de treinamento, voc√™ tem as notas do candidato em dois exames e a decis√£o de admiss√£o.\n",
    "* Sua tarefa √© construir um modelo de classifica√ß√£o que estime a probabilidade de admiss√£o de um candidato com base nas notas desses dois exames.\n",
    "\n",
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Carregando e visualizando os dados\n",
    "\n",
    "Voc√™ come√ßar√° carregando o conjunto de dados para esta tarefa.\n",
    "- A fun√ß√£o `load_dataset()` mostrada abaixo carrega os dados nas vari√°veis `X_train` e `y_train`\n",
    "   - `X_train` cont√©m notas de exames em dois exames para um aluno\n",
    "   - `y_train` √© a decis√£o de admiss√£o\n",
    "       - `y_train = 1` se o aluno foi admitido\n",
    "       - `y_train = 0` se o aluno n√£o foi admitido\n",
    "   - Tanto `X_train` quanto `y_train` s√£o arrays numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# carregar o conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veja as vari√°veis\n",
    "Vamos nos familiarizar mais com seu conjunto de dados.\n",
    "- Um bom lugar para come√ßar √© simplesmente imprimir cada vari√°vel e ver o que ela cont√©m.\n",
    "\n",
    "O c√≥digo abaixo imprime os primeiros cinco valores de `X_train` e o tipo da vari√°vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"Os primeiros cinco elementos em X_train s√£o:\\n\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora imprima os primeiros cinco valores de `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"Os primeiros cinco elementos em y_train s√£o:\\n\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifique as dimens√µes de suas vari√°veis\n",
    "\n",
    "Outra forma √∫til de se familiarizar com seus dados √© visualizar suas dimens√µes. Vamos imprimir a forma de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print('A forma do X_train √©: ' + str(X_train.shape))\n",
    "print('A forma de y_train √©: ' + str(y_train.shape))\n",
    "print('Temos m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize seus dados\n",
    "\n",
    "Antes de come√ßar a implementar qualquer algoritmo de aprendizagem, √© sempre bom visualizar os dados, se poss√≠vel.\n",
    "- O c√≥digo abaixo exibe os dados em um gr√°fico 2D (conforme mostrado abaixo), onde os eixos s√£o as notas dos dois exames e os exemplos positivos e negativos s√£o mostrados com marcadores diferentes.\n",
    "- Usamos uma fun√ß√£o auxiliar no arquivo ``utils.py`` para gerar este gr√°fico.\n",
    "\n",
    "<img src=\"images/figure 1.png\" width=\"450\" height=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar os exmplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Aprovado\", neg_label=\"N√£o-aprovado \")\n",
    "\n",
    "# Definir o r√≥tulo do eixo y\n",
    "plt.ylabel('Pontua√ß√£o na prova 2') \n",
    "# Definir o r√≥tulo do eixo x\n",
    "plt.xlabel('Pontua√ß√£o na prova 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seu objetivo √© construir um modelo de regress√£o log√≠stica que se ajuste a esses dados.\n",
    "- Com este modelo, voc√™ pode prever se um novo aluno ser√° admitido com base nas notas dos dois exames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Fun√ß√£o sigm√≥ide\n",
    "\n",
    "Lembre-se que para regress√£o log√≠stica, o modelo √© representado como\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "onde a fun√ß√£o $g$ √© a fun√ß√£o sigm√≥ide. A fun√ß√£o sigm√≥ide √© definida como:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Vamos implementar a fun√ß√£o sigm√≥ide primeiro, para que ela possa ser usada no restante desta tarefa.\n",
    "\n",
    "<a name='ex-01'></a>\n",
    "### Exerc√≠cio 1\n",
    "Complete a fun√ß√£o `sigmoid` para calcular\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Observe que\n",
    "- `z` nem sempre √© um n√∫mero √∫nico, mas tamb√©m pode ser uma matriz de n√∫meros.\n",
    "- Se a entrada for uma matriz de n√∫meros, gostar√≠amos de aplicar a fun√ß√£o sigm√≥ide a cada valor da matriz de entrada.\n",
    "\n",
    "Se tiver d√∫vidas, voc√™ pode conferir as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcular o sigmoide de z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): vetor num√©rica de qualquer tamanho.\n",
    "\n",
    "    Retorns:\n",
    "        g (ndarray): sigmoid(z), com a mesma forma de z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### INICIE SEU C√ìDIGO AQUI ### \n",
    "    \n",
    "    ### TERMINE SEU C√ìDIGO AQUI ### \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "       \n",
    "   * O `numpy` tem uma fun√ß√£o chamada [`np.exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html), que oferece uma maneira conveniente de calcular o exponencial ( $e^{z}$) de todos os elementos da matriz de entrada (`z`).\n",
    "\n",
    " \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Clique para ainda mais dicas</b></font></summary>\n",
    "        \n",
    "  - Voc√™ pode traduzir $e^{-z}$ em c√≥digo como `np.exp(-z)` \n",
    "    \n",
    "  - Voc√™ pode traduzir $1/e^{-z}$ em c√≥digo como `1/np.exp(-z)` \n",
    "    \n",
    "    Se voc√™ ainda estiver com dificuldades, pode verificar as dicas apresentadas abaixo para descobrir como calcular `g` \n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular g</b></font></summary>\n",
    "        <code>g = 1 / (1 + np.exp(-z))</code>\n",
    "    </details>\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando terminar, tente testar alguns valores chamando `sigmoid(x)` na c√©lula abaixo. \n",
    "- Para valores positivos grandes de x, o sigmoide deve estar pr√≥ximo de 1, enquanto para valores negativos grandes, o sigmoide deve estar pr√≥ximo de 0. \n",
    "- A avalia√ß√£o de `sigmoid(0)` deve lhe dar exatamente 0,5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Observa√ß√£o: voc√™ pode editar esse valor\n",
    "value = 0\n",
    "\n",
    "print (f\"sigmoid({value}) = {sigmoid(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>sigmoid(0)<b></td>\n",
    "    <td> 0.5 </td> \n",
    "  </tr>\n",
    "</table>\n",
    "    \n",
    "- Como mencionado anteriormente, seu c√≥digo tamb√©m deve funcionar com vetores e matrizes. Para uma matriz, sua fun√ß√£o deve executar a fun√ß√£o sigmoide em cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))\n",
    "\n",
    "# TESTE DA UNIDADE\n",
    "from public_tests import *\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>sigmoid([-1, 0, 1, 2])<b></td> \n",
    "    <td>[0.26894142        0.5           0.73105858        0.88079708]</td> \n",
    "  </tr>    \n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### 2.4 Fun√ß√£o de custo para regress√£o log√≠stica\n",
    "\n",
    "Nesta se√ß√£o, voc√™ implementar√° a fun√ß√£o de custo para regress√£o log√≠stica.\n",
    "\n",
    "<a name='ex-02'></a>\n",
    "### Exerc√≠cio 2\n",
    "\n",
    "Complete a fun√ß√£o `compute_cost` usando as equa√ß√µes abaixo.\n",
    "\n",
    "Lembre-se de que, para a regress√£o log√≠stica, a fun√ß√£o de custo tem a forma \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "onde\n",
    "* m √© o n√∫mero de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ √© o custo de um √∫nico ponto de dados, que √© - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ √© a previs√£o do modelo, enquanto $y^{(i)}$, que √© o r√≥tulo real\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ m que a fun√ß√£o $g$ √© a fun√ß√£o sigmoide.\n",
    "  * Pode ser √∫til calcular primeiro uma vari√°vel intermedi√°ria $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ onde $n$ √© o n√∫mero de recursos, antes de calcular $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n",
    "\n",
    "Observa√ß√£o:\n",
    "* Enquanto estiver fazendo isso, lembre-se de que as vari√°veis `X_train` e `y_train` n√£o s√£o valores escalares, mas matrizes de forma ($m, n$) e ($ùëö$,1), respectivamente, em que $ùëõ$ √© o n√∫mero de recursos e $ùëö$ √© o n√∫mero de exemplos de treinamento.\n",
    "* Voc√™ pode usar a fun√ß√£o sigmoide que implementou acima para essa parte.\n",
    "\n",
    "Se tiver d√∫vidas, consulte as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Calcula o custo de todos os exemplos\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m examples by n features\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo      \n",
    "      b : (scalar)              valores dos par√¢metros do modelo      \n",
    "      *argv : n√£o utilizado, para compatibilidade com a vers√£o regularizada abaixo\n",
    "    Returns:\n",
    "      total_cost : (escalar) custo \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### INICIE SEU C√ìDIGO AQUI ### \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ### TERMINE SEU C√ìDIGO AQUI ###\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para Dicas</b></font></summary>\n",
    "    \n",
    "    * Voc√™ pode representar um operador de soma, por exemplo: $h = \\sum\\limits_{i = 0}^{m-1} 2i$ no c√≥digo da seguinte forma:   \n",
    "    ```python \n",
    "        h = 0\n",
    "        for i in range(m):\n",
    "            h = h + 2*i\n",
    "    ```\n",
    "   * Em seguida, voc√™ pode retornar o `total_cost` como `loss_sum` dividido por `m`.\n",
    "   * Se voc√™ for novato em Python, verifique se o c√≥digo est√° devidamente recuado com espa√ßos ou tabula√ß√µes consistentes. Caso contr√°rio, ele poder√° produzir uma sa√≠da diferente ou gerar um erro `IndentationError: unexpected indent`. Para obter detalhes, consulte [esse t√≥pico](https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398) no f√≥rum de DeepLearning.AI.\n",
    "     \n",
    "   <details>\n",
    "    <summary><font size=\"2\" color=\"darkblue\"><b> Clique para mai Dicas</b></font></summary>\n",
    "        \n",
    "    * Veja como voc√™ pode estruturar a implementa√ß√£o geral dessa fun√ß√£o\n",
    "        \n",
    "    ```python \n",
    "    def compute_cost(X, y, w, b, *argv):\n",
    "        m, n = X.shape\n",
    "    \n",
    "        ### INICIE SEU C√ìDIGO AQUI ###\n",
    "        loss_sum = 0 \n",
    "        \n",
    "        # Fazer um loop em cada exemplo de treinamento\n",
    "        for i in range(m): \n",
    "            \n",
    "            # Primeiro calcule z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b\n",
    "            z_wb = 0 \n",
    "            # Fazer um loop em cada recurso\n",
    "            for j in range(n): \n",
    "                # Adicione o termo correspondente a z_wb\n",
    "                z_wb_ij = # Seu c√≥digo para calcular w[j] * X[i][j]\n",
    "                z_wb += z_wb_ij # equivalente a z_wb = z_wb + z_wb_ij\n",
    "            # Adicione o termo de polariza√ß√£o a z_wb\n",
    "            z_wb += b #equivalente a z_wb = z_wb + b\n",
    "        \n",
    "            f_wb = # Seu c√≥digo aqui para calcular a previs√£o f_wb para um exemplo de treinamento\n",
    "            loss =  # Seu c√≥digo aqui para calcular a perda para um exemplo de treinamento\n",
    "            \n",
    "            loss_sum += loss # equivalent to loss_sum = loss_sum + loss\n",
    "        \n",
    "        total_cost = (1 / m) * loss_sum  \n",
    "        ### TERMINE SEU C√ìDIGO AQUI ### \n",
    "        \n",
    "        return total_cost\n",
    "    ```\n",
    "       Se ainda estiver com d√∫vidas, voc√™ pode consultar as dicas apresentadas abaixo para descobrir como calcular `z_wb_ij`, `f_wb` e `cost`.\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica pra calcular z_wb_ij</b></font></summary>\n",
    "           &emsp; &emsp; <code>z_wb_ij = w[j]*X[i][j] </code>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica pra calcular f_wb</b></font></summary>\n",
    "           &emsp; &emsp; $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$ onde $g$ √© a fun√ß√£o sigmoid. Voc√™ pode simplesmente chamar a fun√ß√£o `sigmoid` implementada acima.\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular f</b></font></summary>\n",
    "               &emsp; &emsp; Voc√™ pode calcular f_wb como <code>f_wb = sigmoid(z_wb) </code>\n",
    "           </details>\n",
    "    </details>\n",
    "\n",
    "     <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular a perda</b></font></summary>\n",
    "          &emsp; &emsp; Voc√™ pode usar fun√ß√£o <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log.html\">np.log</a> para calcular o log\n",
    "          <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular a perda</b></font></summary>\n",
    "              &emsp; &emsp; Voc√™ pode calcular a perda com <code>loss =  -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)</code>\n",
    "          </details>\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as c√©lulas abaixo para verificar sua implementa√ß√£o da fun√ß√£o `compute_cost` com duas inicializa√ß√µes diferentes dos par√¢metros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Calcular e exibir o custo com w e b inicializados como zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Custo em w e b iniciais (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo inicial w e b (zeros)<b></td>\n",
    "    <td> 0.693 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo com w e b diferentes de zero\n",
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Custo de w e b (n√£o-zeros) no conjunto de teste: {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "# TESTE DA UNIDADE\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo de w e b (n√£o-zeros) no conjunto de teste:<b></td>\n",
    "    <td> 0.218 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.5\"></a>\n",
    "### 2.5 Gradiente para regress√£o log√≠stica\n",
    "\n",
    "Nesta se√ß√£o, voc√™ implementar√° o gradiente para a regress√£o log√≠stica.\n",
    "\n",
    "Lembre-se de que o algoritmo de descida de gradiente √©:\n",
    "\n",
    "$$\\begin{align*}& \\text{repita at√© a converg√™ncia:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{para j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "onde os par√¢metros $b$, $w_j$ s√£o atualizados simultaneamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name='ex-03'></a>\n",
    "### Exerc√≠cio 3\n",
    "\n",
    "Complete a fun√ß√£o `compute_gradient` para calcular $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ das equa√ß√µes (2) e (3) abaixo.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m √© o n√∫mero de exemplos de treinamento no conjunto de dados\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ √© a predi√ß√£o do modelo, enquanto $y^{(i)}$ √© o r√≥tulo verdadeiro\n",
    "\n",
    "\n",
    "**Nota**: Embora esse gradiente pare√ßa id√™ntico ao gradiente da regress√£o linear, a f√≥rmula √©, na verdade, diferente porque a regress√£o linear e a regress√£o log√≠stica t√™m defini√ß√µes diferentes de $f_{\\mathbf{w},b}(x)$.\n",
    "\n",
    "Como antes, voc√™ pode usar a fun√ß√£o sigmoide que implementou acima e, se tiver d√∫vidas, pode consultar as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo com a implementa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regress√£o log√≠stica \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos n caracter√≠sticas\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo\n",
    "      b : (scalar)              valores dos par√¢metros do modelo\n",
    "      *argv : n√£o utilizado, para compatibilidade com a vers√£o regularizada abaixo\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em rela√ß√£o aos par√¢metros w. \n",
    "      dj_db : (scalar)             O gradiente do custo em rela√ß√£o aos par√¢metros b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### INICIE SEU C√ìDIGO AQUI ### \n",
    "    for i in range(m):\n",
    "        z_wb = None\n",
    "        for j in range(n): \n",
    "            z_wb += None\n",
    "        z_wb += None\n",
    "        f_wb = None\n",
    "        \n",
    "        dj_db_i = None\n",
    "        dj_db += None\n",
    "        \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = None\n",
    "            \n",
    "    dj_dw = None\n",
    "    dj_db = None\n",
    "    ### TERMINE SEU C√ìDIGO AQUI ### \n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para Dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como voc√™ pode estruturar a implementa√ß√£o geral dessa fun√ß√£o\n",
    "    ```python \n",
    "       def compute_gradient(X, y, w, b, *argv): \n",
    "            m, n = X.shape\n",
    "            dj_dw = np.zeros(w.shape)\n",
    "            dj_db = 0.\n",
    "        \n",
    "            ### INICIE SEU C√ìDIGO AQUI ### \n",
    "            for i in range(m):\n",
    "                # Calcule f_wb (exatamente como voc√™ fez na fun√ß√£o compute_cost acima)\n",
    "                f_wb = \n",
    "        \n",
    "                # Calcule o gradiente para b a partir deste exemplo\n",
    "                dj_db_i = # Seu c√≥digo aqui para calcular o erro\n",
    "        \n",
    "                # some a dj_db\n",
    "                dj_db += dj_db_i\n",
    "        \n",
    "                # obter dj_dw para cada atributo\n",
    "                for j in range(n):\n",
    "                    # Voc√™ codifica aqui para calcular o gradiente do i-√©simo exemplo para o j-√©simo atributo\n",
    "                    dj_dw_ij =  \n",
    "                    dj_dw[j] += dj_dw_ij\n",
    "        \n",
    "            # dividir dj_db e dj_dw pelo n√∫mero total de exemplos\n",
    "            dj_dw = dj_dw / m\n",
    "            dj_db = dj_db / m\n",
    "            ### TERMINE SEU C√ìDIGO AQUI ### \n",
    "       \n",
    "            return dj_db, dj_dw\n",
    "    ```\n",
    "   * Se voc√™ for novato em Python, verifique se o c√≥digo est√° devidamente recuado com espa√ßos ou tabula√ß√µes consistentes. Caso contr√°rio, ele poder√° produzir uma sa√≠da diferente ou gerar um erro `IndentationError: unexpected indent`. Para obter detalhes, consulte [esse t√≥pico](https://community.deeplearning.ai/t/indentation-in-python-indentationerror-unexpected-indent/159398) no f√≥rum de DeepLearning.AI.\n",
    "\n",
    "\n",
    "    * Se ainda tiver d√∫vidas, voc√™ pode consultar as dicas apresentadas abaixo para descobrir como calcular `f_wb`, `dj_db_i` e `dj_dw_ij` \n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to calculate f_wb</b></font></summary>\n",
    "           &emsp; &emsp; Lembre-se de que voc√™ calculou f_wb em <code>compute_cost</code> acima - para obter dicas detalhadas sobre como calcular cada termo intermedi√°rio, consulte a se√ß√£o de dicas abaixo desse exerc√≠cio\n",
    "           <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas pra calcular f_wb</b></font></summary>\n",
    "              &emsp; &emsp; Voc√™ pode calcular f_wb com\n",
    "               <pre>\n",
    "               for i in range(m):   \n",
    "                   # Calcule f_wb (exatamente como voc√™ fez na fun√ß√£o compute_cost acima)\n",
    "                   z_wb = 0\n",
    "                   # Fazer um loop em cada recurso\n",
    "                   for j in range(n): \n",
    "                       # Adicione o termo correspondente a z_wb\n",
    "                       z_wb_ij = X[i, j] * w[j]\n",
    "                       z_wb += z_wb_ij\n",
    "            \n",
    "                   # Adicionar termo de vi√©s \n",
    "                   z_wb += b\n",
    "        \n",
    "                   # Calcular a previs√£o do modelo\n",
    "                   f_wb = sigmoid(z_wb)\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular dj_db_i</b></font></summary>\n",
    "           &emsp; &emsp; Voc√™ pode calcular dj_db_i como <code>dj_db_i = f_wb - y[i]</code>\n",
    "    </details>\n",
    "        \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica pra calcular dj_dw_ij</b></font></summary>\n",
    "        &emsp; &emsp; Voc√™ pode calcular dj_dw_ij como <code>dj_dw_ij = (f_wb - y[i])* X[i][j]</code>\n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute as c√©lulas abaixo para verificar sua implementa√ß√£o da fun√ß√£o `compute_gradient` com duas inicializa√ß√µes diferentes dos par√¢metros $w$ e $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o gradiente com w e b inicializados como zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db em w and b (zeros) iniciais:{dj_db}' )\n",
    "print(f'dj_dw e, w and b (zeros) iniciais:{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w and b (zeros) iniciais:<b></td>\n",
    "    <td> -0.1 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw e, w and b (zeros) iniciais:<b></td>\n",
    "    <td> [-12.00921658929115, -11.262842205513591] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Calcular e exibir o custo e o gradiente com w e b diferentes de zero\n",
    "test_w = np.array([ 0.2, -0.5])\n",
    "test_b = -24\n",
    "dj_db, dj_dw  = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('dj_db em w and b de teste:', dj_db)\n",
    "print('dj_dw em w and b de teste:', dj_dw.tolist())\n",
    "\n",
    "# TESTE DA UNDADE    \n",
    "compute_gradient_test(compute_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db em w and b de teste:<b></td>\n",
    "    <td> -0.5999999999991071 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>dj_dw em w and b de teste:<b></td>\n",
    "    <td>  [-44.8313536178737957, -44.37384124953978] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\"></a>\n",
    "### 2.6 Par√¢metros de aprendizagem usando a descida de gradiente \n",
    "\n",
    "Da mesma forma que na tarefa anterior, agora voc√™ encontrar√° os par√¢metros ideais de um modelo de regress√£o log√≠stica usando a descida gradiente. \n",
    "- Voc√™ n√£o precisa implementar nada para esta parte. Basta executar as c√©lulas abaixo. \n",
    "\n",
    "- Uma boa maneira de verificar se a descida de gradiente est√° funcionando corretamente √© observar\n",
    "o valor de $J(\\mathbf{w},b)$ e verificar se ele est√° diminuindo a cada etapa. \n",
    "\n",
    "- Supondo que voc√™ tenha implementado o gradiente e calculado o custo corretamente, o valor de $J(\\mathbf{w},b)$ nunca deve aumentar e deve convergir para um valor est√°vel no final do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Executa a descida do gradiente em lote para aprender theta. Atualiza theta tomando \n",
    "    num_iters etapas de gradiente com taxa de aprendizado alfa\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) dados, m exemplos n caracter√≠sticas\n",
    "      y :    (ndarray Shape (m,))  valor alvo\n",
    "      w_in : (ndarray Shape (n,))  valores dos par√¢metros do modelo\n",
    "      b_in : (scalar)              valores dos par√¢metros do modelo\n",
    "      cost_function :              fun√ß√£o para calcular custo\n",
    "      gradient_function :          fun√ß√£o para calcular o gradiente\n",
    "      alpha : (float)              taxa de aprendizado\n",
    "      num_iters : (int)            n√∫mero de itera√ß√µes para executar a descida do gradiente\n",
    "      lambda_ : (scalar, float)    constante de regulariza√ß√£o\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Valores atualizados dos par√¢metros do modelo ap√≥s executar a descida do gradiente\n",
    "      b : (scalar)             Valor atualizado do par√¢metro do modelo ap√≥s a execu√ß√£o da descida do gradiente\n",
    "    \"\"\"\n",
    "    \n",
    "    # N√∫mero de exemplos de treinamento\n",
    "    m = len(X)\n",
    "    \n",
    "    # um vetor para armazenar os custos J e w em cada itera√ß√£o, principalmente para gr√°ficos posteriores\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calcular o gradiente e atualizar os par√¢metros\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Atualizar par√¢metros usando w, b, alfa e gradiente\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Salvar o custo J em cada itera√ß√£o\n",
    "        if i<100000:      # Evitar o esgotamento de recursos \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Imprimir o custo a cada intervalo de 10 vezes ou tantas itera√ß√µes se < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Itera√ß√£o {i:4}: Custo {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #retornar w e J,e hist√≥rico de w para gr√°ficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos executar o algoritmo de descida de gradiente acima para aprender os par√¢metros do nosso conjunto de dados.\n",
    "\n",
    "**Nota**\n",
    "O bloco de c√≥digo abaixo leva alguns minutos para ser executado, especialmente com uma vers√£o n√£o vetorizada. Voc√™ pode reduzir as `itera√ß√µes` para testar sua implementa√ß√£o e iterar mais rapidamente. Se voc√™ tiver tempo mais tarde, tente executar 100.000 itera√ß√µes para obter melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Algumas configura√ß√µes de descida de gradiente\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>Sa√≠da Esperada: Custo     0.30, (Clique para ver detalhes):</b>\n",
    "</summary>\n",
    "\n",
    "    # Com as seguintes configura√ß√µes\n",
    "    np.random.seed(1)\n",
    "    initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "    initial_b = -8\n",
    "    iterations = 10000\n",
    "    alpha = 0.001\n",
    "    #\n",
    "\n",
    "```\n",
    "Itera√ß√£o    0: Custo     0.96   \n",
    "Itera√ß√£o 1000: Custo     0.31   \n",
    "Itera√ß√£o 2000: Custo     0.30   \n",
    "Itera√ß√£o 3000: Custo     0.30   \n",
    "Itera√ß√£o 4000: Custo     0.30   \n",
    "Itera√ß√£o 5000: Custo     0.30   \n",
    "Itera√ß√£o 6000: Custo     0.30   \n",
    "Itera√ß√£o 7000: Custo     0.30   \n",
    "Itera√ß√£o 8000: Custo     0.30   \n",
    "Itera√ß√£o 9000: Custo     0.30   \n",
    "Itera√ß√£o 9999: Custo     0.30   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.7\"></a>\n",
    "### 2.7 Tra√ßando a fronteira  de decis√£o\n",
    "\n",
    "Agora usaremos os par√¢metros finais da descida de gradiente para plotar o ajuste linear. Se voc√™ implementou as partes anteriores corretamente, dever√° ver um gr√°fico semelhante ao seguinte:   \n",
    "<img src=\"images/figure 2.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "Usaremos uma fun√ß√£o auxiliar no arquivo `utils.py` para criar esse gr√°fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(w, b, X_train, y_train)\n",
    "# Definir o r√≥tulo do eixo y\n",
    "plt.ylabel('Pontua√ß√£o da prova 2') \n",
    "# Definir o r√≥tulo do eixo x\n",
    "plt.xlabel('Pontua√ß√£o da prova 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.8\"></a>\n",
    "### 2.8 Avalia√ß√£o da regress√£o log√≠stica\n",
    "\n",
    "Podemos avaliar a qualidade dos par√¢metros que encontramos verificando a qualidade da previs√£o do modelo aprendido em nosso conjunto de treinamento. \n",
    "\n",
    "Voc√™ implementar√° a fun√ß√£o `predict` abaixo para fazer isso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-04'></a>\n",
    "### Exerc√≠cio 4\n",
    "\n",
    "Complete a fun√ß√£o `predict` para produzir previs√µes `1` ou `0` com um conjunto de dados e um vetor de par√¢metros aprendidos $w$ e $b$.\n",
    "- Primeiro, voc√™ precisa calcular a previs√£o do modelo $f(x^{(i)}) = g(w \\cdot x^{(i)} + b)$ para cada exemplo \n",
    "    - Voc√™ j√° implementou isso antes nas partes acima\n",
    "- Interpretamos o resultado do modelo ($f(x^{(i)})$) como a probabilidade de que $y^{(i)}=1$ dado $x^{(i)}$ e parametrizado por $w$.\n",
    "- Portanto, para obter uma previs√£o final ($y^{(i)}=0$ ou $y^{(i)}=1$) do modelo de regress√£o log√≠stica, voc√™ pode usar a seguinte heur√≠stica -\n",
    "  if $f(x^{(i)}) >= 0.5$, predizer $y^{(i)}=1$\n",
    "  \n",
    "  if $f(x^{(i)}) < 0.5$, predizer $y^{(i)}=0$\n",
    "    \n",
    "Se tiver d√∫vidas, voc√™ pode consultar as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Prever se o r√≥tulo √© 0 ou 1 usando os par√¢metros de regress√£o log√≠stica aprendidos\n",
    "    de regress√£o log√≠stica aprendidos w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados,m exemplos, n recursos\n",
    "      w : (ndarray Shape (n,))  valores dos par√¢metros do modelo\n",
    "      b : (scalar)              valores dos par√¢metros do modelo\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) As previs√µes para X usando um limite de 0,5\n",
    "    \"\"\"\n",
    "    # N√∫mero de exemplos de treinamento\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### INICIE SEU C√ìDIGO AQUI ### \n",
    "    # Fazer um loop em cada exemplo\n",
    "    for i in range(m):   \n",
    "        z_wb = None\n",
    "        # Fazer um loop em cada recurso\n",
    "        for j in range(n): \n",
    "            # Adicione o termo correspondente a z_wb\n",
    "            z_wb += None\n",
    "        \n",
    "        # Adicionar termo de vi√©s \n",
    "        z_wb += None\n",
    "        \n",
    "        # Calcule a previs√£o para este exemplo\n",
    "        f_wb = None\n",
    "\n",
    "        # Aplicar o limiar (thershold)\n",
    "        p[i] = None\n",
    "        \n",
    "    ### TERMINE SEU C√ìDIGO AQUI ### \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para Dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como voc√™ pode estruturar a implementa√ß√£o geral dessa fun√ß√£o\n",
    "    ```python \n",
    "       def predict(X, w, b): \n",
    "            # N√∫mero de exemplos de treinamento\n",
    "            m, n = X.shape   \n",
    "            p = np.zeros(m)\n",
    "   \n",
    "            ### INICIE SEU C√ìDIGO AQUI ### \n",
    "            # Fazer um loop em cada exemplo\n",
    "            for i in range(m):   \n",
    "                \n",
    "                # Calcular f_wb (exatamente como voc√™ fez na fun√ß√£o compute_cost acima) \n",
    "                # usando algumas linhas de c√≥digo\n",
    "                f_wb = \n",
    "\n",
    "                # Calcule a previs√£o para esse exemplo de treinamento \n",
    "                p[i] = # Seu c√≥digo aqui para calcular a previs√£o com base em f_wb\n",
    "        \n",
    "            ### TERMINE SEU C√ìDIGO AQUI ### \n",
    "            return p\n",
    "    ```\n",
    "  \n",
    "        Se ainda estiver com d√∫vidas, voc√™ pode consultar as dicas apresentadas abaixo para descobrir como calcular `f_wb` e `p[i]` \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular f_wb</b></font></summary>\n",
    "           &emsp; &emsp; Lembre-se de que voc√™ calculou f_wb em <code>compute_cost</code> acima ‚Äî Para obter dicas detalhadas sobre como calcular cada termo intermedi√°rio, consulte a se√ß√£o de dicas abaixo desse exerc√≠cio\n",
    "           <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular f_wb</b></font></summary>\n",
    "              &emsp; &emsp; Voc√™ pode calcular f_wb como\n",
    "               <pre>\n",
    "               for i in range(m):   \n",
    "                    # Calcule f_wb (exatamente como voc√™ fez na fun√ß√£o compute_cost acima)\n",
    "                    z_wb = 0\n",
    "                    # Fazer um loop em cada recurso\n",
    "                    for j in range(n): \n",
    "                       # Adicionar o termo correspondente a z_wb\n",
    "                       z_wb_ij = X[i, j] * w[j]\n",
    "                       z_wb += z_wb_ij\n",
    "                    # Adicionar o termo de vi√©s\n",
    "                    z_wb += b\n",
    "                # Calcule a previs√£o do modelo\n",
    "                f_wb = sigmoid(z_wb)\n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular p[i]</b></font></summary>\n",
    "           &emsp; &emsp; Por exemplo, se voc√™ quiser dizer que x = 1 se y for menor que 3 e 0 caso contr√°rio, voc√™ pode expressar isso em c√≥digo como <code>x = y < 3 </code>. Agora fa√ßa o mesmo apra p[i] = 1 if f_wb >= 0.5 e 0 caso contr√°rio. \n",
    "           <details>\n",
    "              <summary><font size=\"2\" color=\"blue\"><b>&emsp; &emsp; Mais dicas para calcular p[i]</b></font></summary>\n",
    "              &emsp; &emsp; Voc√™ pode calcular p[i] como <code>p[i] = f_wb >= 0.5</code>\n",
    "          </details>\n",
    "    </details>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de concluir a fun√ß√£o `predict`, vamos executar o c√≥digo abaixo para informar a precis√£o do treinamento de seu classificador, calculando a porcentagem de exemplos corretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Teste seu c√≥digo de previs√£o\n",
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3    \n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Sa√≠da da previs√£o: formato {tmp_p.shape}, valor {tmp_p}')\n",
    "\n",
    "# TESTES DA UNIDADE\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Espereda** \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Sa√≠da da previs√£o: formato (4,), valor [0. 1. 1. 1.]<b></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos usar isso para calcular a precis√£o no conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Calcule a precis√£o em nosso conjunto de treinamento\n",
    "p = predict(X_train, w,b)\n",
    "print('Acur√°cia de Treino: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Acur√°cia de Treino (aprox):<b></td>\n",
    "    <td> 92.00 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Regress√£o log√≠stica regularizada\n",
    "\n",
    "Nesta parte do exerc√≠cio, voc√™ implementar√° a regress√£o log√≠stica regularizada para prever se os microchips de uma f√°brica ser√£o aprovados no controle de qualidade (QA). Durante o QA, cada microchip passa por v√°rios testes para garantir que esteja funcionando corretamente. \n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Defini√ß√£o do problema\n",
    "\n",
    "Suponha que voc√™ seja o gerente de produtos da f√°brica e tenha os resultados dos testes de alguns microchips em dois testes diferentes. \n",
    "- A partir desses dois testes, voc√™ gostaria de determinar se os microchips devem ser aceitos ou rejeitados. \n",
    "- Para ajud√°-lo a tomar a decis√£o, voc√™ tem um conjunto de dados de resultados de testes de microchips anteriores, a partir dos quais pode criar um modelo de regress√£o log√≠stica.\n",
    "\n",
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Carregando e visualizando os dados\n",
    "\n",
    "De forma semelhante √†s partes anteriores deste exerc√≠cio, vamos come√ßar carregando o conjunto de dados para esta tarefa e visualizando-o. \n",
    " \n",
    "- A fun√ß√£o `load_dataset()` mostrada abaixo carrega os dados nas vari√°veis `X_train` e `y_train`\n",
    "  - `X_train` cont√©m os resultados dos testes para os microchips de dois testes\n",
    "  - `y_train` cont√©m os resultados do controle de qualidade  \n",
    "      - `y_train = 1` se o microchip foi aceito \n",
    "      - `y_train = 0` se o microchip foi rejeitado \n",
    "  - Tanto `X_train` quanto `y_train` s√£o matrizes numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Carregar o conjunto de dados\n",
    "X_train, y_train = load_data(\"data/ex2data2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exibir as vari√°veis\n",
    "\n",
    "O c√≥digo abaixo imprime os cinco primeiros valores de `X_train` e `y_train` e o tipo das vari√°veis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# imprimir X_train\n",
    "print(\"X_train:\", X_train[:5])\n",
    "print(\"Tipo de X_train:\",type(X_train))\n",
    "\n",
    "# imprimir y_train\n",
    "print(\"y_train:\", y_train[:5])\n",
    "print(\"Tipo de y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifique as dimens√µes de suas vari√°veis\n",
    "\n",
    "Outra maneira √∫til de se familiarizar com seus dados √© visualizar suas dimens√µes. Vamos imprimir a forma de `X_train` e `y_train` e ver quantos exemplos de treinamento temos em nosso conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print ('O formaro de X_train √©: ' + str(X_train.shape))\n",
    "print ('O formato de y_train √©: ' + str(y_train.shape))\n",
    "print ('Temo m = %d exemplos de treinamento' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize seus dados\n",
    "\n",
    "A fun√ß√£o auxiliar `plot_data` (de `utils.py`) √© usada para gerar uma figura como a Figura 3, em que os eixos s√£o as duas pontua√ß√µes de teste, e os exemplos positivos (y = 1, aceito) e negativos (y = 0, rejeitado) s√£o mostrados com marcadores diferentes.\n",
    "\n",
    "<img src=\"images/figure 3.png\"  width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Plotar os exemplos\n",
    "plot_data(X_train, y_train[:], pos_label=\"Aceito\", neg_label=\"Rejeitado\")\n",
    "\n",
    "# Definir o r√≥tulo do eixo y\n",
    "plt.ylabel('Teste 2 de Microchip 2') \n",
    "# Definir o r√≥tulo do eixo x\n",
    "plt.xlabel('Teste 2 de Microchip 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Figura 3 mostra que nosso conjunto de dados n√£o pode ser separado em exemplos positivos e negativos por uma linha reta atrav√©s do gr√°fico. Portanto, uma aplica√ß√£o direta da regress√£o log√≠stica n√£o ter√° um bom desempenho nesse conjunto de dados, pois a regress√£o log√≠stica s√≥ conseguir√° encontrar um limite de decis√£o linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Mapeamento de recursos\n",
    "\n",
    "Uma maneira de ajustar melhor os dados √© criar mais recursos a partir de cada ponto de dados. Na fun√ß√£o fornecida `map_feature`, mapearemos os recursos em todos os termos polinomiais de $x_1$ e $x_2$ at√© a sexta pot√™ncia.\n",
    "\n",
    "$$\\mathrm{map\\_feature}(x) = \n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1 x_2\\\\\n",
    "x_2^2\\\\\n",
    "x_1^3\\\\\n",
    "\\vdots\\\\\n",
    "x_1 x_2^5\\\\\n",
    "x_2^6\\end{array}\\right]$$\n",
    "\n",
    "Como resultado desse mapeamento, nosso vetor de dois recursos (as pontua√ß√µes em dois testes de controle de qualidade) foi transformado em um vetor de 27 dimens√µes. \n",
    "\n",
    "- Um classificador de regress√£o log√≠stica treinado nesse vetor de recursos de dimens√£o mais alta ter√° um limite de decis√£o mais complexo e ser√° n√£o linear quando desenhado em nosso gr√°fico bidimensional. \n",
    "- Fornecemos a fun√ß√£o `map_feature` para voc√™ em utils.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"Formato original dos dados:\", X_train.shape)\n",
    "\n",
    "mapped_X =  map_feature(X_train[:, 0], X_train[:, 1])\n",
    "print(\"Forma ap√≥s o mapeamento de recursos:\", mapped_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tamb√©m imprimir os primeiros elementos de `X_train` e `mapped_X` para ver a transforma√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print(\"X_train[0]:\", X_train[0])\n",
    "print(\"X_train[0] mapeado:\", mapped_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora o mapeamento de recursos nos permita criar um classificador mais expressivo, ele tamb√©m √© mais suscet√≠vel ao ajuste excessivo. Nas pr√≥ximas partes do exerc√≠cio, voc√™ implementar√° a regress√£o log√≠stica regularizada para ajustar os dados e tamb√©m ver√° por si mesmo como a regulariza√ß√£o pode ajudar a combater o problema do ajuste excessivo.\n",
    "\n",
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Fun√ß√£o de custo para regress√£o log√≠stica regularizada\n",
    "\n",
    "Nesta parte, voc√™ implementar√° a fun√ß√£o de custo para a regress√£o log√≠stica regularizada.\n",
    "\n",
    "Lembre-se de que, para a regress√£o log√≠stica regularizada, a fun√ß√£o de custo tem a forma\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "Compare isso com a fun√ß√£o de custo sem regulariza√ß√£o (que voc√™ implementou acima), que tem o seguinte formato \n",
    "\n",
    "$$ J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$\n",
    "\n",
    "A diferen√ßa √© o termo de regulariza√ß√£o, que √© $$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$ \n",
    "Observe que o par√¢metro $b$ n√£o √© regularizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-05'></a>\n",
    "### Exerc√≠cio 5\n",
    "\n",
    "Complete a fun√ß√£o `compute_cost_reg` abaixo para calcular o seguinte termo para cada elemento em $w$ \n",
    "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$\n",
    "\n",
    "O c√≥digo inicial adiciona isso ao custo sem regulariza√ß√£o (que voc√™ calculou acima em `compute_cost`) para calcular o custo com regulatiza√ß√£o.\n",
    "\n",
    "Se tiver d√∫vidas, voc√™ pode consultar as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Calcula o custo de todos os exemplos\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados, m exemplos, n recursos\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores dos parametros do modelo\n",
    "      b : (escalar)              valores dos parametros do modelo\n",
    "      lambda_ : (escalar, float) Controla a quantidade de regulariza√ß√£o\n",
    "    Returns:\n",
    "      total_cost : (escalar)     custo \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Chama a fun√ß√£o compute_cost que voc√™ implementou acima\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "    # Voc√™ precisa calcular esse valor\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### INICIE SEU C√ìDIGO AQUI ###\n",
    "    \n",
    "        \n",
    "    \n",
    "    ### TERMINE SEU C√ìDIGO AQUI ###\n",
    "    \n",
    "    # Adicione o custo de regulariza√ß√£o para obter o custo total\n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como voc√™ pode estruturar a implementa√ß√£o geral dessa fun√ß√£o\n",
    "    ```python \n",
    "       def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "   \n",
    "           m, n = X.shape\n",
    "    \n",
    "            # Chama a fun√ß√£o compute_cost que voc√™ implementou acima\n",
    "            cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "            # Voc√™ precisa calcular esse valor\n",
    "            reg_cost = 0.\n",
    "    \n",
    "            ### INICIE SEU C√ìDIGO AQUI ###\n",
    "            for j in range(n):\n",
    "                reg_cost_j = # Seu c√≥digo aqui para calcular o custo de w[j]\n",
    "                reg_cost = reg_cost + reg_cost_j\n",
    "            reg_cost = (lambda_/(2 * m)) * reg_cost\n",
    "            ### TERMINE SEU C√ìDIGO AQUI ###\n",
    "    \n",
    "            # Adicione o custo de regulariza√ß√£o para obter o custo total\n",
    "            total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "        return total_cost\n",
    "    ```\n",
    "      Se ainda tiver d√∫vidas, voc√™ pode consultar as dicas apresentadas abaixo para descobrir como calcular o `reg_cost_j` \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dica para calcular reg_cost_j</b></font></summary>\n",
    "           &emsp; &emsp; Voc√™ pode calcular reg_cost_j como <code>reg_cost_j = w[j]**2 </code> \n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check your implementation of the `compute_cost_reg` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 0.5\n",
    "lambda_ = 0.5\n",
    "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(\"Custo Regularizado :\", cost)\n",
    "\n",
    "# TESTE DA UNIDADE\n",
    "compute_cost_reg_test(compute_cost_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Custo Regularizado : <b></td>\n",
    "    <td> 0.6618252552483948 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### 3.5 Gradiente para regress√£o log√≠stica regularizada\n",
    "\n",
    "Nesta se√ß√£o, voc√™ implementar√° o gradiente para regress√£o log√≠stica regularizada.\n",
    "\n",
    "\n",
    "O gradiente da fun√ß√£o de custo regularizado tem dois componentes. O primeiro, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ √© um escalar, o outro √© um vetor com a mesma forma dos par√¢metros $\\mathbf{w}$, em que o elemento $j^\\mathrm{th}$ √© definido da seguinte forma:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j  \\quad\\, \\text{para} \\, j=0...(n-1)$$\n",
    "\n",
    "Compare isso com o gradiente da fun√ß√£o de custo sem regulariza√ß√£o (que voc√™ implementou acima), que tem a forma \n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "\n",
    "\n",
    "Como voc√™ pode ver,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ √© o mesmo, a diferen√ßa √© o seguinte termo em $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, que √© $$\\frac{\\lambda}{m} w_j  \\quad\\, \\text{para}\\,  j=0...(n-1)$$ \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-06'></a>\n",
    "### Exerc√≠cio 6\n",
    "\n",
    "Preencha a fun√ß√£o `compute_gradient_reg` abaixo para modificar o c√≥digo abaixo e calcular o seguinte termo\n",
    "\n",
    "$$\\frac{\\lambda}{m} w_j \\quad\\, \\, \\text{for} \\, j=0...(n-1)$$\n",
    "\n",
    "O c√≥digo inicial adicionar√° esse termo ao $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$ retornado de `compute_gradient` acima para obter o gradiente da fun√ß√£o de custo regularizado.\n",
    "\n",
    "\n",
    "Se tiver d√∫vidas, voc√™ pode consultar as dicas apresentadas ap√≥s a c√©lula abaixo para ajud√°-lo na implementa√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Calcula o gradiente para regress√£o log√≠stica com regulariza√ß√£o\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) dados,m exemplos, n recursos\n",
    "      y : (ndarray Shape (m,))  valor alvo\n",
    "      w : (ndarray Shape (n,))  valores de par√¢metros do modelo\n",
    "      b : (scalar)                valores de par√¢metros do modelo\n",
    "      lambda_ : (scalar,float)  constante de regulariza√ß√£o\n",
    "    Returns\n",
    "      dj_db : (scalar)             O gradiente do custo em rela√ß√£o ao par√¢metro b. \n",
    "      dj_dw : (ndarray Shape (n,)) O gradiente do custo em rela√ß√£o ao par√¢metro w. \n",
    "\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "\n",
    "    ### INICIE SEU C√ìDIGO AQUI ###\n",
    "    \n",
    "        \n",
    "    ### TERMINE SEU C√ìDIGO AQUI ###\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Clique para dicas</b></font></summary>\n",
    "    \n",
    "    \n",
    "* Veja como voc√™ pode estruturar a implementa√ß√£o geral dessa fun√ß√£o\n",
    "    ```python \n",
    "    def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "        m, n = X.shape\n",
    "    \n",
    "        dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "\n",
    "        ### INICIE SEU C√ìDIGO AQUI ###\n",
    "        # Fazer um loop sobre os elementos de w\n",
    "        for j in range(n): \n",
    "            \n",
    "            dj_dw_j_reg = # Seu c√≥digo aqui para calcular o termo de regulariza√ß√£o para dj_dw[j]\n",
    "            \n",
    "            # Adicione o termo de regulariza√ß√£o ao elemento correspondente de dj_dw\n",
    "            dj_dw[j] = dj_dw[j] + dj_dw_j_reg\n",
    "        \n",
    "        ### TERMINE SEU C√ìDIGO AQUI ###\n",
    "        \n",
    "        return dj_db, dj_dw\n",
    "    ```\n",
    "  \n",
    "    Se ainda tiver d√∫vidas, consulte as dicas apresentadas abaixo para descobrir como calcular o `dj_dw_j_reg` \n",
    "    \n",
    "    <details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Dicar para calcular dj_dw_j_reg</b></font></summary>\n",
    "           &emsp; &emsp; Voc√™ pode calcular dj_dw_j_reg com <code>dj_dw_j_reg = (lambda_ / m) * w[j] </code> \n",
    "    </details>\n",
    "        \n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a c√©lula abaixo para verificar sua implementa√ß√£o da fun√ß√£o `compute_gradient_reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1) \n",
    "initial_w  = np.random.rand(X_mapped.shape[1]) - 0.5 \n",
    "initial_b = 0.5\n",
    " \n",
    "lambda_ = 0.5\n",
    "dj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(f\"dj_db: {dj_db}\", )\n",
    "print(f\"Primeiros elementos de regulariza√ß√£o dj_dw:\\n {dj_dw[:4].tolist()}\", )\n",
    "\n",
    "# TESTE DA UNIDADE    \n",
    "compute_gradient_reg_test(compute_gradient_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>dj_db:</b>0.07138288792343</td> </tr>\n",
    "  <tr>\n",
    "      <td> <b>Primeiros elementos de regulariza√ß√£o dj_dw:</b> </td> </tr>\n",
    "   <tr>\n",
    "   <td> [[-0.010386028450548], [0.011409852883280], [0.0536273463274], [0.003140278267313]] </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.6\"></a>\n",
    "### 3.6 Aprendendo par√¢metros usando a descida gradiente\n",
    "\n",
    "Da mesma forma que nas partes anteriores, voc√™ usar√° a fun√ß√£o de descida de gradiente implementada acima para aprender os par√¢metros ideais $w$,$b$. \n",
    "- Se voc√™ tiver conclu√≠do corretamente o custo e o gradiente da regress√£o log√≠stica regularizada, dever√° ser capaz de passar para a pr√≥xima c√©lula para aprender os par√¢metros $w$. \n",
    "- Depois de treinar nossos par√¢metros, n√≥s os usaremos para tra√ßar o limite de decis√£o. \n",
    "\n",
    "**Nota**\n",
    "\n",
    "O bloco de c√≥digo abaixo leva um bom tempo para ser executado, especialmente com uma vers√£o n√£o vetorizada. Voc√™ pode reduzir as `itera√ß√µes` para testar sua implementa√ß√£o e iterar mais rapidamente. Se voc√™ tiver tempo mais tarde, execute 100.000 itera√ß√µes para ver melhores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Inicializa√ß√£o dos par√¢metros de ajuste\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "initial_b = 1.\n",
    "\n",
    "# Defina o par√¢metro de regulariza√ß√£o lambda_ (voc√™ pode tentar variar isso)\n",
    "lambda_ = 0.01    \n",
    "\n",
    "# Algumas configura√ß√µes de descida de gradiente\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_mapped, y_train, initial_w, initial_b, \n",
    "                                    compute_cost_reg, compute_gradient_reg, \n",
    "                                    alpha, iterations, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>Sa√≠da Esperada: Custo < 0.5  (Clique para detalhes)</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "# Com as seguintes configura√ß√µes\n",
    "#np.random.seed(1)\n",
    "#initial_w = np.random.rand(X_mapped.shape[1])-0.5\n",
    "#initial_b = 1.\n",
    "#lambda_ = 0.01;                                          \n",
    "#iterations = 10000\n",
    "#alpha = 0.01\n",
    "Itera√ß√£o    0: Custo     0.72   \n",
    "Itera√ß√£o 1000: Custo     0.59   \n",
    "Itera√ß√£o 2000: Custo     0.56   \n",
    "Itera√ß√£o 3000: Custo     0.53   \n",
    "Itera√ß√£o 4000: Custo     0.51   \n",
    "Itera√ß√£o 5000: Custo     0.50   \n",
    "Itera√ß√£o 6000: Custo     0.48   \n",
    "Itera√ß√£o 7000: Custo     0.47   \n",
    "Itera√ß√£o 8000: Custo     0.46   \n",
    "Itera√ß√£o 9000: Custo     0.45   \n",
    "Itera√ß√£o 9999: Custo     0.45       \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.7\"></a>\n",
    "### 3.7 Plotar fronteira de decis√£o\n",
    "Para ajud√°-lo a visualizar o modelo aprendido por esse classificador, usaremos a nossa fun√ß√£o `plot_decision_boundary` que plota o limite de decis√£o (n√£o linear) que separa os exemplos positivos dos negativos. \n",
    "\n",
    "- Na fun√ß√£o, plotamos o limite de decis√£o n√£o linear calculando as previs√µes do classificador em uma grade com espa√ßamento uniforme e, em seguida, desenhamos um gr√°fico de contorno de onde as previs√µes mudam de y = 0 para y = 1.\n",
    "\n",
    "- Depois de aprender os par√¢metros $w$,$b$, a pr√≥xima etapa √© tra√ßar um limite de decis√£o semelhante ao da Figura 4.\n",
    "\n",
    "<img src=\"images/figure 4.png\"  width=\"450\" height=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Teste 2 de Microchip 2') \n",
    "# Set the x-axis label\n",
    "plt.xlabel('Teste 2 de Microchip 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.8\"></a>\n",
    "### 3.8 Avalia√ß√£o do modelo de regress√£o log√≠stica regularizada\n",
    "\n",
    "Voc√™ usar√° a fun√ß√£o `predict` que implementou acima para calcular a precis√£o do modelo de regress√£o log√≠stica regularizada no conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#Calcule a precis√£o no conjunto de treinamento\n",
    "p = predict(X_mapped, w, b)\n",
    "\n",
    "print('Acur√°cia de Treino: %f'%(np.mean(p == y_train) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sa√≠da Esperada**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>Acur√°cia de Treino:</b>~ 80%</td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parab√©ns por ter conclu√≠do esse laborat√≥rio! No pr√≥ximo t√≥pico da disciplina voc√™ usar√° algoritmos de aprendizado mais avan√ßados: redes neurais!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
