{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiegoGarciaFrade/DC3_Group8/blob/main/Pipeline_Group_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48461bd55d3d6f6a",
      "metadata": {
        "id": "48461bd55d3d6f6a"
      },
      "source": [
        "\n",
        "## Setup (+Installations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676d6b5dcac5c988",
      "metadata": {
        "id": "676d6b5dcac5c988"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# define a formatter to display the messages to console (standard output)\n",
        "console_formatter = logging.Formatter('%(message)s')\n",
        "# console_formatter = logging.Formatter('%(levelname)s:%(module)s:%(message)s')\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setFormatter(console_formatter)\n",
        "# define a logger for this notebook and attach the console handler\n",
        "logger = logging.getLogger('Label-Propagation')\n",
        "logger.handlers.clear()\n",
        "logger.propagate = False\n",
        "logger.addHandler(console_handler)\n",
        "# set an appropriate level of logging for this notebook\n",
        "logger.setLevel(logging.INFO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wcEJFiE4gn4l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcEJFiE4gn4l",
        "outputId": "d56355bf-03f2-4204-f5fe-369022130590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting supervision\n",
            "  Downloading supervision-0.24.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from supervision) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n",
            "Downloading supervision-0.24.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: supervision\n",
            "Successfully installed supervision-0.24.0\n",
            "Collecting jupyter_bbox_widget\n",
            "  Downloading jupyter_bbox_widget-0.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting anywidget>=0.9.0 (from jupyter_bbox_widget)\n",
            "  Downloading anywidget-0.9.13-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from anywidget>=0.9.0->jupyter_bbox_widget) (7.7.1)\n",
            "Collecting psygnal>=0.8.1 (from anywidget>=0.9.0->jupyter_bbox_widget)\n",
            "  Downloading psygnal-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from anywidget>=0.9.0->jupyter_bbox_widget) (4.12.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.6.9)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.2.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (24.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.16.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->anywidget>=0.9.0->jupyter_bbox_widget) (1.2.2)\n",
            "Downloading jupyter_bbox_widget-0.6.0-py3-none-any.whl (24 kB)\n",
            "Downloading anywidget-0.9.13-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.7/213.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psygnal-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (727 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.4/727.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psygnal, jedi, anywidget, jupyter_bbox_widget\n",
            "Successfully installed anywidget-0.9.13 jedi-0.19.1 jupyter_bbox_widget-0.6.0 psygnal-0.11.1\n",
            "Collecting segment-anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install supervision\n",
        "!pip install jupyter_bbox_widget\n",
        "!pip install segment-anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Eyp6IAmsGp9f",
      "metadata": {
        "id": "Eyp6IAmsGp9f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773ac710cf1d9880",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "773ac710cf1d9880",
        "outputId": "75096efb-e50a-41c3-bfad-2c0e62b42007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/pyg_lib-0.4.0%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_scatter-2.1.2%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_sparse-0.6.18%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_cluster-1.6.3%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (992 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m992.4/992.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt24cu124 torch_cluster-1.6.3+pt24cu124 torch_scatter-2.1.2+pt24cu124 torch_sparse-0.6.18+pt24cu124 torch_spline_conv-1.2.2+pt24cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "# workaround to overcome long duration needed for installing torch_scatter\n",
        "#!pip install pyg_lib torch_scatter -f https://data.pyg.org/whl/torch-{torch.2.4.1+cu124}.html\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100019e5a56d9c69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "100019e5a56d9c69",
        "outputId": "e6485362-e5b9-4c6d-d510-2cb476f4b9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0,>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.5.0-py3-none-any.whl (890 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.5/890.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.0\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49YdbOz_EcUp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49YdbOz_EcUp",
        "outputId": "ab7168bb-bcfb-4ef6-8fb9-7c2efa3b7b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.15)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.24.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore>=0.0.15 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.16)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.4.1)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0e351018b1c60b",
      "metadata": {
        "id": "6b0e351018b1c60b"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import albumentations\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ygF--KdyYA5Y",
      "metadata": {
        "id": "ygF--KdyYA5Y"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QuaKoYfHVdtk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuaKoYfHVdtk",
        "outputId": "9f85b43e-06b3-49bf-cb33-005e87b862d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cG5r3HDTVt5D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cG5r3HDTVt5D",
        "outputId": "73114e9f-9735-484c-9594-b40d68f9bb13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-sjr0geqs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-sjr0geqs\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# download segment anything\n",
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p5T9PxCqbhN2",
      "metadata": {
        "id": "p5T9PxCqbhN2"
      },
      "source": [
        "## Fileplaths setup\n",
        "TO DO:\n",
        "Change project root directory as needed for the DRIVE path where the project is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2WmytL2LYDxg",
      "metadata": {
        "id": "2WmytL2LYDxg"
      },
      "outputs": [],
      "source": [
        "PROJECT_ROOT_DIR = 'drive/MyDrive/Data Challenge 3/WorkingPipeline/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunvgoDAW7-J",
      "metadata": {
        "id": "sunvgoDAW7-J"
      },
      "outputs": [],
      "source": [
        "SAM_WEIGHTS_PATH = '/content/weights/'\n",
        "SAM_WEIGHTS_FILE = 'sam_vit_h_4b8939.pth'\n",
        "SAM_WEIGHTS_URL = f'https://dl.fbaipublicfiles.com/segment_anything/{SAM_WEIGHTS_FILE}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SJf6ZHZzXGTd",
      "metadata": {
        "id": "SJf6ZHZzXGTd"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT_DIR = os.path.join(PROJECT_ROOT_DIR,'jbg060_AI_for_Good_course_Shared',\n",
        "                             '01_Data', 'benthic_datasets')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ri9YzPluZt7x",
      "metadata": {
        "id": "Ri9YzPluZt7x"
      },
      "outputs": [],
      "source": [
        "\n",
        "regions_of_interest = ['PAC_AUS', 'PAC_USA', 'ATL', 'IDN_PHL']\n",
        "# REGION 1 #### assigned to?\n",
        "mask_folder = os.path.join(DATA_ROOT_DIR, 'mask_labels', 'reef_support', 'SEAVIEW_ATL', 'masks_stitched')\n",
        "img_folder = os.path.join(DATA_ROOT_DIR, 'point_labels', 'SEAVIEW', 'ATL')\n",
        "tabular_folder = os.path.join(DATA_ROOT_DIR, 'point_labels', 'SEAVIEW', 'tabular-data')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c768ad8b151d3717",
      "metadata": {
        "id": "c768ad8b151d3717"
      },
      "source": [
        "## Directory setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12920254db04a13e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12920254db04a13e",
        "outputId": "18297b96-19de-440c-ac5f-652385962caa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Root directory for project in Google Drive: drive/MyDrive/Data Challenge 3/WorkingPipeline/\n"
          ]
        }
      ],
      "source": [
        "# Root folder in Google Drive for this project\n",
        "import os\n",
        "logger.info(f\"Root directory for project in Google Drive: {PROJECT_ROOT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c955c112b55481",
      "metadata": {
        "id": "3c955c112b55481"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# add the path where point label aware superpixels and custom packags is located\n",
        "sys.path.append(os.path.join(PROJECT_ROOT_DIR, 'packages'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SSUYy_gbTVQU",
      "metadata": {
        "id": "SSUYy_gbTVQU"
      },
      "source": [
        "## Cuda and custom modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23993cb3e41e4f66",
      "metadata": {
        "id": "23993cb3e41e4f66"
      },
      "outputs": [],
      "source": [
        "custom_modules = ['labelmate.loader', 'labelmate.visualizer',\n",
        "                  'labelmate.evaluator', 'labelmate.propagator',\n",
        "                  'labelmate.hypertuner', 'labelmate.patchifier',\n",
        "                  ]\n",
        "for module_name in custom_modules:\n",
        "    logging.getLogger(module_name).setLevel(logging.INFO)\n",
        "custom_modules = ['torchmetrics',\n",
        "                  ]\n",
        "for module_name in custom_modules:\n",
        "    logging.getLogger(module_name).setLevel(logging.ERROR)\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ba8c623d872a0d",
      "metadata": {
        "id": "c7ba8c623d872a0d"
      },
      "source": [
        "## Import SAM pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4eef35a69f0334b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4eef35a69f0334b",
        "outputId": "339f745f-7f9a-41e8-9d0b-12057e26589a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights downloaded and saved to /content/weights/sam_vit_h_4b8939.pth\n"
          ]
        }
      ],
      "source": [
        "# dowload the pre-trained weights mentioned in GitHub and point to that file\n",
        "\n",
        "\n",
        "!wget {SAM_WEIGHTS_URL} -P {SAM_WEIGHTS_PATH}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63ca9bbbaa56985",
      "metadata": {
        "id": "b63ca9bbbaa56985"
      },
      "source": [
        "## DATA_ROOT_DIR Check + Results destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e5b85b9ca78d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26e5b85b9ca78d70",
        "outputId": "38f1f901-75d6-4bdc-a66e-b9166acb8966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dataset (images and masks) will be accessed from: drive/MyDrive/Data Challenge 3/WorkingPipeline/jbg060_AI_for_Good_course_Shared/01_Data/benthic_datasets\n"
          ]
        }
      ],
      "source": [
        "# Path to folder that contains images and dense masks\n",
        "logger.info(f\"Dataset (images and masks) will be accessed from: {DATA_ROOT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hGuDqGeNwfEN",
      "metadata": {
        "id": "hGuDqGeNwfEN"
      },
      "source": [
        "## Final Pipeline --- BASELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9078dbdb7bce90",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 845,
          "referenced_widgets": [
            "c43fe2e503474b54a96f9a9e129fa24f",
            "c93af692dd024493b00df581958ff546",
            "63cb7cd9b7b04637adaa7b388e54ea97",
            "8fd61bdc53e14adaa05f2001f2604536",
            "4294203710734b35b5d24ee743d3eaba",
            "d0aa74a0554b402eae1cbbbfdeab7b9a",
            "c73a07cf7b10420db2942fc35f246c85",
            "1925817e478840f18b8a411c3b88ee82",
            "9368408ef4e44e6a9b37db6ac7c6eb0c",
            "dd2fe9858c914a2390b1555f6f606c2f",
            "fcdedcb39841432fb45347ee2b22f8f9",
            "41c911da243a476390272aa42d199f93",
            "3f6a9768a26142059e801bed0ffeebc1",
            "e2218963a5344258a2973cd8816d4942",
            "971b3663cefe45a9999e65fc1b74a4aa",
            "e7d0129de86b4737acb5de07eba19340",
            "fadc2270a44945f495b1abe293ceaa3a",
            "7853f5c23f9344c1813f559ca8a8d608",
            "a273bdbd826e4f3da4d8e54bf5210932",
            "4a84abce446c4c22aee7efbe6b1439a0",
            "56746e56da174f0fb7c3edb304375cbc",
            "a9eed42cceee47ef8b79fe4e1eece0a0",
            "896eccdc94f34240b49a46293ef14d47",
            "601c418e2a664da0befea67187f64720",
            "4c8f21c0fe1144ccaf9a375bd519fe62",
            "4ff2d86a68f742db857bbd05831b5fab",
            "404ff73cd600453d9a9c7c1f7c4bebae",
            "0b0239257a0f45819d8aae495e3322cc",
            "378002232983485499b5d1e051e04540",
            "12accf86d1774dfeb9f3664b9fbce056",
            "b4ed2f12dfd5411db4dc90a2cccd6e5d",
            "e5a980873adb4e5090138b27fd61ce39",
            "08f2f7bbdbb44be28cbd8c00e0e2ecd1"
          ]
        },
        "id": "8f9078dbdb7bce90",
        "outputId": "87b5dabf-4166-44b7-fd34-1fe964ade90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image Names: ['17001652802.jpg', '17001669902.jpg', '17001737702.jpg', '17001738002.jpg', '17001738102.jpg', '17001738202.jpg', '17001738302.jpg', '17001738402.jpg', '17001738502.jpg', '17001738602.jpg', '17001738702.jpg', '17001738802.jpg', '17001738902.jpg', '17001739002.jpg', '17001739102.jpg', '17001739202.jpg', '17001739302.jpg', '17001739402.jpg', '17001739502.jpg', '17001739602.jpg', '17001739702.jpg', '17001739802.jpg', '17001739902.jpg', '17001740002.jpg', '17001740102.jpg', '17001740202.jpg', '17001740302.jpg', '17001740402.jpg', '17001740502.jpg', '17001740602.jpg', '17001740702.jpg', '17001740802.jpg', '17001740902.jpg', '17001745802.jpg', '17002336001.jpg', '17002369001.jpg', '17002409801.jpg', '17004348102.jpg', '17004348202.jpg', '17004348302.jpg', '17004348402.jpg', '17004348502.jpg', '17004348602.jpg', '17004348702.jpg', '17004348802.jpg', '17004348902.jpg', '17004349002.jpg', '17004349102.jpg', '17004349202.jpg', '17004349302.jpg', '17004349402.jpg', '17004349502.jpg', '17004349602.jpg', '17004349702.jpg', '17004349802.jpg', '17004349902.jpg', '17004350002.jpg', '17004350102.jpg', '17004350202.jpg', '17004350302.jpg', '17004350402.jpg', '17004350502.jpg', '17004350602.jpg', '17004350702.jpg', '17004350802.jpg', '17004350902.jpg', '17004351002.jpg', '17004366401.jpg', '17004415601.jpg', '17004426901.jpg', '17005029401.jpg', '17005056201.jpg', '17005057001.jpg', '17005057101.jpg', '17005057201.jpg', '17005057301.jpg', '17005057401.jpg', '17005057501.jpg', '17005057601.jpg', '17005057701.jpg', '17005057801.jpg', '17005057901.jpg', '17005058001.jpg', '17005058101.jpg', '17005058201.jpg', '17005058301.jpg', '17005058401.jpg', '17005058501.jpg', '17005058601.jpg', '17005058701.jpg', '17005058801.jpg', '17005058901.jpg', '17005059001.jpg', '17005059101.jpg', '17005059201.jpg', '17005059301.jpg', '17005059401.jpg', '17005059501.jpg', '17005059601.jpg', '17005059701.jpg', '17005059801.jpg', '17005059901.jpg', '17006011002.jpg', '17006011102.jpg', '17006011202.jpg', '17006011302.jpg', '17006011402.jpg', '17006011502.jpg', '17006011602.jpg', '17006011702.jpg', '17006011802.jpg', '17006011902.jpg', '17006012002.jpg', '17006012102.jpg', '17006012202.jpg', '17006012302.jpg', '17006012402.jpg', '17006012502.jpg', '17006012602.jpg', '17006012702.jpg', '17006012802.jpg', '17006012902.jpg', '17006013002.jpg', '17006013102.jpg', '17006013202.jpg', '17006013302.jpg', '17006013402.jpg', '17006013502.jpg', '17006013702.jpg', '17006013802.jpg', '17006013902.jpg', '17006014002.jpg', '17006058202.jpg', '17006078902.jpg', '17006100402.jpg', '17007661501.jpg', '17007739501.jpg', '17017815902.jpg', '17017820002.jpg', '17017820102.jpg', '17017820202.jpg', '17017820302.jpg', '17017820402.jpg', '17017820502.jpg', '17017820602.jpg', '17017820702.jpg', '17017820802.jpg', '17017820902.jpg', '17017821002.jpg', '17017821102.jpg', '17017821202.jpg', '17017821302.jpg', '17017821402.jpg', '17017821502.jpg', '17017821602.jpg', '17017821702.jpg', '17017821802.jpg', '17017821902.jpg', '17017822002.jpg', '17017822102.jpg', '17017822202.jpg', '17017822302.jpg', '17017822402.jpg', '17017822502.jpg', '17017822602.jpg', '17017822702.jpg', '17017822802.jpg', '17017822902.jpg', '17017839602.jpg', '17017852502.jpg', '17019870602.jpg', '17019871102.jpg', '17019872802.jpg', '17019874202.jpg', '17019878402.jpg', '17019880202.jpg', '17019885502.jpg', '17019935202.jpg', '17019936602.jpg', '17019937902.jpg', '17022848302.jpg', '17022850002.jpg', '17022941602.jpg', '17024674002.jpg', '17024717802.jpg', '17024735902.jpg', '17024748702.jpg', '17025758802.jpg', '17025785502.jpg', '17025809302.jpg', '17025816302.jpg', '17034030102.jpg', '17034030202.jpg', '17034030302.jpg', '17034030402.jpg', '17034030502.jpg', '17034030602.jpg', '17034030702.jpg', '17034030802.jpg', '17034030902.jpg', '17034031002.jpg', '17034031102.jpg', '17034031202.jpg', '17034031302.jpg', '17034031402.jpg', '17034031502.jpg', '17034031602.jpg', '17034031702.jpg', '17034031802.jpg', '17034031902.jpg', '17034032002.jpg', '17034032102.jpg', '17034032202.jpg', '17034032302.jpg', '17034032402.jpg', '17034032502.jpg', '17034032602.jpg', '17034032702.jpg', '17034032802.jpg', '17034032902.jpg', '17034033002.jpg', '17037005102.jpg', '17037005202.jpg', '17037005302.jpg', '17037005402.jpg', '17037005502.jpg', '17037005602.jpg', '17037005702.jpg', '17037005802.jpg', '17037005902.jpg', '17037006002.jpg', '17037006102.jpg', '17037006202.jpg', '17037006302.jpg', '17037006402.jpg', '17037006502.jpg', '17037006602.jpg', '17037006702.jpg', '17037006802.jpg', '17037006902.jpg', '17037007002.jpg', '17037007102.jpg', '17037007202.jpg', '17037007302.jpg', '17037007402.jpg', '17037007502.jpg', '17037007602.jpg', '17037007702.jpg', '17037007802.jpg', '17037007902.jpg', '17037008002.jpg', '17037019602.jpg', '17037020702.jpg', '17037023502.jpg', '17037025702.jpg', '17037066402.jpg', '17039196602.jpg', '17039216002.jpg', '17039236002.jpg', '17039236102.jpg', '17039236202.jpg', '17039236302.jpg', '17039236402.jpg', '17039236502.jpg', '17039236602.jpg', '17039236702.jpg', '17039236802.jpg', '17039236902.jpg', '17039237002.jpg', '17039237102.jpg', '17039237202.jpg', '17039237302.jpg', '17039237402.jpg', '17039237502.jpg', '17039237602.jpg', '17039237702.jpg', '17039237802.jpg', '17039237902.jpg', '17039238002.jpg', '17039238102.jpg', '17039238202.jpg', '17039238302.jpg', '17039238402.jpg', '17040024302.jpg', '17040039702.jpg', '17040085902.jpg', '17041122302.jpg', '17041190102.jpg', '17042201302.jpg', '17042217202.jpg', '17042217802.jpg', '17043062702.jpg', '17044150702.jpg', '17044194702.jpg', '17045302601.jpg', '17050217902.jpg', '17050224002.jpg', '17051036702.jpg', '17051099302.jpg', '20001049001.jpg', '20001049101.jpg', '20001049201.jpg', '20001049301.jpg', '20001049401.jpg', '20001049501.jpg', '20001049601.jpg', '20001049701.jpg', '20001049801.jpg', '20001049901.jpg', '20001050001.jpg', '20001050101.jpg', '20001050201.jpg', '20001050301.jpg', '20001050401.jpg', '20001050501.jpg', '20001050601.jpg', '20001050801.jpg', '20001050901.jpg', '20001051001.jpg', '20001051101.jpg', '20001051201.jpg', '20001051301.jpg', '20001051401.jpg', '20001051501.jpg', '20001051601.jpg', '20001051701.jpg', '20001051801.jpg', '20001051901.jpg', '20001052001.jpg', '20010147601.jpg', '20010147701.jpg', '20010147801.jpg', '20010147901.jpg', '20010148001.jpg', '20010148101.jpg', '20010148201.jpg', '20010148301.jpg', '20010148401.jpg', '20010148501.jpg', '20010148601.jpg', '20010148701.jpg', '20010148801.jpg', '20010148901.jpg', '20010149001.jpg', '20010149101.jpg', '20010149201.jpg', '20010149301.jpg', '20010149401.jpg', '20010149501.jpg', '20010149601.jpg', '20010149701.jpg', '20010149801.jpg', '20010149901.jpg', '20010150001.jpg', '20011030301.jpg', '20011045302.jpg', '20011049802.jpg', '20011051802.jpg', '20011053902.jpg', '20011054702.jpg', '20011055002.jpg', '20011055302.jpg', '20011058101.jpg', '20011059102.jpg', '20011070801.jpg', '20011070901.jpg', '20011071501.jpg', '20011074502.jpg', '20011074602.jpg', '20011076502.jpg', '20011077002.jpg', '20011080902.jpg', '20011081102.jpg', '20011083602.jpg', '20011087201.jpg', '20011093001.jpg', '20011096001.jpg', '20012174802.jpg', '20013208801.jpg', '20014015406.jpg', '20014055701.jpg', '20014072301.jpg', '20014076801.jpg', '20014076901.jpg', '20015127502.jpg', '20015151402.jpg', '20015173002.jpg', '20015173305.jpg', '20015175202.jpg', '20015175401.jpg', '20015176701.jpg', '20015178801.jpg', '20015179502.jpg', '20015183501.jpg', '20015184201.jpg', '20015187702.jpg', '20015188501.jpg', '20015190301.jpg', '20015195401.jpg', '20015204202.jpg', '20016207205.jpg', '20016207801.jpg', '20016208202.jpg', '20016209501.jpg', '20016211601.jpg', '20016211701.jpg', '20016214401.jpg', '20016216501.jpg', '20016217201.jpg', '20016218305.jpg', '20016218602.jpg', '20016225102.jpg', '20016227801.jpg', '20016229605.jpg', '20016230405.jpg', '20016233402.jpg', '20016233901.jpg', '20016234804.jpg', '20016235402.jpg', '20016236901.jpg', '20016238801.jpg', '20016241002.jpg', '20016241004.jpg', '20016241101.jpg', '20016244302.jpg', '20016248005.jpg', '20016248101.jpg', '20016248305.jpg', '20016263402.jpg', '20016272302.jpg', '20016295302.jpg', '20018115701.jpg', '20018115801.jpg', '20018115901.jpg', '20018116201.jpg', '20018116301.jpg', '20018116501.jpg', '20018116601.jpg', '20018116701.jpg', '20018116801.jpg', '20018116901.jpg', '20018117001.jpg', '20018117101.jpg', '20018117301.jpg', '20018117401.jpg', '20018117601.jpg', '20018117801.jpg', '20018117901.jpg', '20018118001.jpg', '20018118101.jpg', '20018118201.jpg', '20018118401.jpg', '20018118501.jpg', '20018119001.jpg', '20018119201.jpg', '20018119401.jpg', '20018119501.jpg', '20018119601.jpg', '20018120001.jpg', '20018120101.jpg', '20018120501.jpg', '20020007801.jpg', '20020013101.jpg', '20020013201.jpg', '20020017201.jpg', '20020018801.jpg', '20020024401.jpg', '20020025101.jpg', '20020025201.jpg', '20020025701.jpg', '20020027101.jpg', '20020027601.jpg', '20020027901.jpg', '20020028201.jpg', '20020028601.jpg', '20020032001.jpg', '20020033801.jpg', '20020035001.jpg', '20020035701.jpg', '20020041001.jpg', '20020041201.jpg', '20020043701.jpg', '20020044401.jpg', '20020045901.jpg', '20020051001.jpg', '20021100001.jpg', '20021100201.jpg', '20021100301.jpg', '20021100401.jpg', '20021100501.jpg', '20021100601.jpg', '20021100701.jpg', '20021100801.jpg', '20021100901.jpg', '20021101001.jpg', '20021101101.jpg', '20021101201.jpg', '20021101301.jpg', '20021101401.jpg', '20021101501.jpg', '20021101601.jpg', '20021101701.jpg', '20021101801.jpg', '20021101901.jpg', '20021102001.jpg', '20021102101.jpg', '20021102201.jpg', '20021102301.jpg', '20021102401.jpg', '20021102501.jpg', '20021102601.jpg', '20021102701.jpg', '20021102801.jpg', '20021102901.jpg', '20021103001.jpg', '20021146501.jpg', '20021149201.jpg', '20021149901.jpg', '20021151801.jpg', '20021152101.jpg', '20021152202.jpg', '20021164602.jpg', '20021173802.jpg', '20021183502.jpg', '20022281202.jpg', '20024002201.jpg', '20024002301.jpg', '20024002401.jpg', '20024002501.jpg', '20024002601.jpg', '20024002701.jpg', '20024002801.jpg', '20024002901.jpg', '20024003001.jpg', '20024003101.jpg', '20024003201.jpg', '20024003301.jpg', '20024003401.jpg', '20024003501.jpg', '20024003601.jpg', '20024003701.jpg', '20024003801.jpg', '20024004101.jpg', '20024004201.jpg', '20024004301.jpg', '20024004401.jpg', '20024004601.jpg', '20024004701.jpg', '20024004801.jpg', '20024005101.jpg', '20024005201.jpg', '20024005501.jpg', '20024005701.jpg', '20024005801.jpg', '20024005901.jpg', '20024036902.jpg', '20024058602.jpg', '20024071502.jpg', '20024078102.jpg', '20028199801.jpg', '20028201101.jpg', '20028206201.jpg', '20029004202.jpg', '20029004502.jpg', '20029008201.jpg', '20031013402.jpg', '20031021102.jpg', '20031048202.jpg', '20031062202.jpg', '20031098302.jpg', '20034046802.jpg', '20034061802.jpg', '20034067102.jpg', '20034084602.jpg', '20034088502.jpg', '20036101101.jpg', '20036110801.jpg', '20036140201.jpg', '20036143101.jpg', '20036157201.jpg', '20036158901.jpg', '20036170201.jpg', '20036170601.jpg', '20036186801.jpg', '20036188201.jpg', '20037196501.jpg', '20037216401.jpg', '20037236701.jpg', '20037245701.jpg', '20037274501.jpg', '20038011502.jpg', '20038021102.jpg', '20038082002.jpg', '20038082102.jpg', '20038082202.jpg', '20038082302.jpg', '20038082402.jpg', '20038082502.jpg', '20038082602.jpg', '20038082702.jpg', '20038082802.jpg', '20038082902.jpg', '20038083002.jpg', '20038083102.jpg', '20038083202.jpg', '20038083302.jpg', '20038083402.jpg', '20038083502.jpg', '20038083602.jpg', '20038083702.jpg', '20038083802.jpg', '20038083902.jpg', '20038084002.jpg', '20038084102.jpg', '20038084202.jpg', '20038084502.jpg', '20038084602.jpg', '20038084702.jpg', '20038084802.jpg', '20038084902.jpg', '20038085002.jpg', '20038085102.jpg', '20038087102.jpg', '20039102002.jpg', '20039104702.jpg', '20039106202.jpg', '20039136502.jpg', '20039159902.jpg', '20040184602.jpg', '20040191702.jpg', '20040194102.jpg', '20040196002.jpg', '20040196102.jpg', '20040196202.jpg', '20040196302.jpg', '20040196402.jpg', '20040196502.jpg', '20040196602.jpg', '20040196702.jpg', '20040196802.jpg', '20040196902.jpg', '20040197002.jpg', '20040197102.jpg', '20040197202.jpg', '20040197302.jpg', '20040197402.jpg', '20040197502.jpg', '20040197602.jpg', '20040197702.jpg', '20040197802.jpg', '20040197902.jpg', '20040198002.jpg', '20040198102.jpg', '20040198202.jpg', '20040198302.jpg', '20040198402.jpg', '20040214302.jpg', '20042047702.jpg', '20042054002.jpg', '20042060302.jpg', '20042103002.jpg', '20044097502.jpg', '20044101002.jpg', '20044141002.jpg', '20044178902.jpg', '20046221001.jpg', '20046223501.jpg', '20046223601.jpg', '20046223701.jpg', '20046223801.jpg', '20046223901.jpg', '20046224001.jpg', '20046224101.jpg', '20046224201.jpg', '20046224301.jpg', '20046224401.jpg', '20046224501.jpg', '20046224601.jpg', '20046224701.jpg', '20046224801.jpg', '20046224901.jpg', '20046225001.jpg', '20046225101.jpg', '20046225201.jpg', '20046225301.jpg', '20046225401.jpg', '20046225501.jpg', '20046225601.jpg', '20046225701.jpg', '20046225801.jpg', '20046225901.jpg', '20046226001.jpg', '20046226101.jpg', '20046226201.jpg', '20046226301.jpg', '20046226401.jpg', '20046237101.jpg', '20046248401.jpg', '20046250301.jpg', '20046281801.jpg', '21002112901.jpg', '21002116201.jpg', '21002118901.jpg', '21002135506.jpg', '21005043502.jpg', '21005043602.jpg', '21005043702.jpg', '21005043802.jpg', '21005043902.jpg', '21005044002.jpg', '21005044102.jpg', '21005044202.jpg', '21005044302.jpg', '21005044402.jpg', '21005044502.jpg', '21005044602.jpg', '21005044702.jpg', '21005044802.jpg', '21005044902.jpg', '21005045002.jpg', '21005045102.jpg', '21005045202.jpg', '21005045302.jpg', '21005045402.jpg', '21005045502.jpg', '21005045602.jpg', '21005045702.jpg', '21005045802.jpg', '21005045902.jpg', '21005046002.jpg', '21005046102.jpg', '21005046202.jpg', '21005046302.jpg', '21005046402.jpg', '21006118305.jpg', '21006150105.jpg', '21008092902.jpg', '21008095102.jpg', '21008105601.jpg', '21008108402.jpg', '21008145406.jpg', '21009171202.jpg', '21009180102.jpg', '21009180202.jpg', '21009180302.jpg', '21009180402.jpg', '21009180602.jpg', '21009180702.jpg', '21009180802.jpg', '21009180902.jpg', '21009181002.jpg', '21009181102.jpg', '21009181202.jpg', '21009181302.jpg', '21009181702.jpg', '21009181802.jpg', '21009181902.jpg', '21009182002.jpg', '21009182102.jpg', '21009182202.jpg', '21009182302.jpg', '21009182402.jpg', '21009182502.jpg', '21009182602.jpg', '21009182702.jpg', '21009182802.jpg', '21009182902.jpg', '21009183002.jpg', '21009183102.jpg', '21009183302.jpg', '21009183502.jpg', '21009183702.jpg', '21009189702.jpg', '21009208601.jpg', '21010021801.jpg', '21011126405.jpg', '21011130002.jpg', '22005132102.jpg', '22005132202.jpg', '22005132502.jpg', '22005132602.jpg', '22005132802.jpg', '22005132902.jpg', '22005133002.jpg', '22005133102.jpg', '22005133202.jpg', '22005133302.jpg', '22005133402.jpg', '22005133502.jpg', '22005133702.jpg', '22005134002.jpg', '22005134102.jpg', '22005134202.jpg', '22005134302.jpg', '22005134502.jpg', '22005134602.jpg', '22005134702.jpg', '22005134902.jpg', '22005135002.jpg', '22005135102.jpg', '22005135202.jpg', '22005135302.jpg', '22005135402.jpg', '22005135602.jpg', '22005135702.jpg', '22005135802.jpg', '22005135902.jpg', '22010004101.jpg', '22010004401.jpg', '22010010601.jpg', '22010039901.jpg', '22010043401.jpg', '22013006001.jpg', '22013006101.jpg', '22013006201.jpg', '22013006301.jpg', '22013006401.jpg', '22013006501.jpg', '22013006601.jpg', '22013006701.jpg', '22013006801.jpg', '22013006901.jpg', '22013007001.jpg', '22013007101.jpg', '22013007201.jpg', '22013007301.jpg', '22013007401.jpg', '22013007501.jpg', '22013007601.jpg', '22013007701.jpg', '22013007801.jpg', '22013007901.jpg', '22013008001.jpg', '22013008101.jpg', '22013008201.jpg', '22013008301.jpg', '22013008401.jpg', '22013008501.jpg', '22013008601.jpg', '22013008701.jpg', '22013008801.jpg', '22013008901.jpg', '22016006101.jpg', '22016009801.jpg', '22016011501.jpg', '22016011901.jpg', '22016014801.jpg', '22017024901.jpg', '22017027301.jpg', '22017030801.jpg', '22017035301.jpg', '22017039301.jpg', '22017046501.jpg', '22020062702.jpg', '22020066002.jpg', '22020074902.jpg', '22020079002.jpg', '22020085202.jpg', '22022198601.jpg', '22022204201.jpg', '22022205601.jpg', '22022222001.jpg', '22022227101.jpg', '22023002102.jpg', '22023003001.jpg', '22023004201.jpg', '22023018101.jpg', '22023019601.jpg', '22023020601.jpg', '22023022301.jpg', '22023027201.jpg', '22023028601.jpg', '22023029601.jpg', '22023038801.jpg', '22023039501.jpg', '22023040201.jpg', '22023044201.jpg', '22023051201.jpg', '22023052101.jpg', '22023055101.jpg', '22023076102.jpg', '22025180402.jpg', '22025195602.jpg', '22025198701.jpg', '22025205001.jpg', '22025205402.jpg', '22025214102.jpg', '22025220602.jpg', '22025225801.jpg', '22025229802.jpg', '22025231202.jpg', '24003176102.jpg', '24004288001.jpg', '24004288101.jpg', '24004288201.jpg', '24004288301.jpg', '24004288401.jpg', '24004288501.jpg', '24004288601.jpg', '24004288701.jpg', '24004288801.jpg', '24004288901.jpg', '24004289001.jpg', '24004289101.jpg', '24004289201.jpg', '24004289301.jpg', '24004289401.jpg', '24004289501.jpg', '24004289601.jpg', '24004289701.jpg', '24004289801.jpg', '24004289901.jpg', '24004290001.jpg', '24004290102.jpg', '24004290201.jpg', '24004290301.jpg', '24004290401.jpg', '24005020001.jpg', '24005070101.jpg', '24006085701.jpg', '24006090601.jpg', '24009004901.jpg', '24009007201.jpg', '24009012001.jpg', '24009012101.jpg', '24009012201.jpg', '24009012301.jpg', '24009012401.jpg', '24009012501.jpg', '24009012601.jpg', '24009012701.jpg', '24009012801.jpg', '24009012901.jpg', '24009013001.jpg', '24009013101.jpg', '24009013201.jpg', '24009013301.jpg', '24009013401.jpg', '24009013501.jpg', '24009013601.jpg', '24009013701.jpg', '24009013801.jpg', '24009013901.jpg', '24009014001.jpg', '24009014101.jpg', '24009014201.jpg', '24009014301.jpg', '24009014401.jpg', '24009017001.jpg', '24009029501.jpg', '24009045601.jpg', '24009054601.jpg', '24010097701.jpg', '24010114401.jpg', '24010125801.jpg', '24010151901.jpg', '24010162401.jpg', '24010170001.jpg', '24010170301.jpg', '24012138301.jpg', '24012141801.jpg', '24012149201.jpg', '24012172401.jpg', '24012185501.jpg', '24013229102.jpg', '24013232202.jpg', '24013240902.jpg', '24013258901.jpg', '24013312702.jpg', '24015106302.jpg', '24015107102.jpg', '24015111401.jpg', '24015145203.jpg', '24015148801.jpg', '24017103301.jpg', '24017108701.jpg', '24017134901.jpg', '24017160101.jpg', '24017175702.jpg', '24018011401.jpg', '24018014301.jpg', '24018029802.jpg', '24018064101.jpg', '24018093502.jpg', '24018099501.jpg', '24018100101.jpg', '24019002601.jpg', '24019095801.jpg', '24019097801.jpg', '24023141001.jpg', '24023158901.jpg', '24023181701.jpg', '24024263201.jpg', '24024268801.jpg', '24024269001.jpg', '24024269101.jpg', '24024269201.jpg', '24024269301.jpg', '24024269302.jpg', '24024269401.jpg', '24024269501.jpg', '24024269601.jpg', '24024269602.jpg', '24024269701.jpg', '24024269702.jpg', '24024269801.jpg', '24024269901.jpg', '24024270001.jpg', '24024270101.jpg', '24024270201.jpg', '24024270301.jpg', '24024270401.jpg', '24024270501.jpg', '24024270601.jpg', '24024270701.jpg', '24024270801.jpg', '24024270901.jpg', '24024271001.jpg', '24024273201.jpg', '24024281501.jpg', '24025027602.jpg', '24025044801.jpg', '24025068701.jpg', '24026161301.jpg', '24026177101.jpg', '24026179401.jpg', '24026179502.jpg', '24026179605.jpg', '24026179701.jpg', '24026179801.jpg', '24026179901.jpg', '24026180001.jpg', '24026180101.jpg', '24026180201.jpg', '24026180301.jpg', '24026180401.jpg', '24026180501.jpg', '24026180602.jpg', '24026180701.jpg', '24026180802.jpg', '24026181006.jpg', '24026181102.jpg', '24026181201.jpg', '24026181301.jpg', '24026181401.jpg', '24026181501.jpg', '24026181601.jpg', '24026181701.jpg', '24026181801.jpg', '24026181901.jpg', '24026197901.jpg', '24026202701.jpg', '24027220901.jpg', '24027301401.jpg', '24027301501.jpg', '24027301601.jpg', '24027301701.jpg', '24027301801.jpg', '24027301902.jpg', '24027302001.jpg', '24027302101.jpg', '24027302201.jpg', '24027302301.jpg', '24027302401.jpg', '24027302502.jpg', '24027302601.jpg', '24027302701.jpg', '24027302802.jpg', '24027302901.jpg', '24027303001.jpg', '24027303102.jpg', '24027303201.jpg', '24027303302.jpg', '24027303401.jpg', '24027303402.jpg', '24027303501.jpg', '24027303601.jpg', '24027303701.jpg', '24027319202.jpg', '24028005401.jpg', '24028012601.jpg', '24028014601.jpg', '24029131001.jpg', '24029167201.jpg', '24029178001.jpg', '24029178102.jpg', '24029178201.jpg', '24029178305.jpg', '24029178402.jpg', '24029178502.jpg', '24029178602.jpg', '24029178702.jpg', '24029178801.jpg', '24029178901.jpg', '24029179001.jpg', '24029179101.jpg', '24029179202.jpg', '24029179302.jpg', '24029179401.jpg', '24029179502.jpg', '24029179601.jpg', '24029179602.jpg', '24029179701.jpg', '24029179801.jpg', '24029179902.jpg', '24029180001.jpg', '24029180101.jpg', '24029180202.jpg', '24029180301.jpg', '24029180402.jpg', '24032035601.jpg', '24032040001.jpg', '24032092301.jpg', '24032104901.jpg', '24032124201.jpg', '24032127101.jpg', '24032127201.jpg', '24032127301.jpg', '24032127401.jpg', '24032127501.jpg', '24032127601.jpg', '24032127701.jpg', '24032127801.jpg', '24032127901.jpg', '24032128001.jpg', '24032128101.jpg', '24032128201.jpg', '24032128301.jpg', '24032128401.jpg', '24032128501.jpg', '24032128601.jpg', '24032128701.jpg', '24032128801.jpg', '24032128901.jpg', '24032129001.jpg', '24032129101.jpg', '24032129201.jpg', '24032129301.jpg', '24032129401.jpg', '24032129501.jpg', '24033227902.jpg', '24033230502.jpg', '24033231502.jpg', '24033234302.jpg', '24034278101.jpg', '24034306201.jpg', '24044080901.jpg', '24044092301.jpg', '24044095101.jpg', '24044095201.jpg', '24044095301.jpg', '24044095401.jpg', '24044095501.jpg', '24044095601.jpg', '24044095701.jpg', '24044095801.jpg', '24044095901.jpg', '24044096001.jpg', '24044096101.jpg', '24044096201.jpg', '24044096301.jpg', '24044096401.jpg', '24044096501.jpg', '24044096601.jpg', '24044096701.jpg', '24044096801.jpg', '24044096901.jpg', '24044097001.jpg', '24044097101.jpg', '24044097201.jpg', '24044097301.jpg', '24044097401.jpg', '24044097501.jpg', '24044098401.jpg', '24045035302.jpg', '24045052302.jpg', '24045068102.jpg', '24045090002.jpg', '24047210102.jpg', '24047242501.jpg', '24047242502.jpg', '24047242701.jpg', '24047256102.jpg', '24047269601.jpg', '25001007801.jpg', '25001007901.jpg', '25001008001.jpg', '25001008101.jpg', '25001008201.jpg', '25001008301.jpg', '25001008401.jpg', '25001008501.jpg', '25001008601.jpg', '25001008701.jpg', '25001008801.jpg', '25001008901.jpg', '25001009001.jpg', '25001009101.jpg', '25001009201.jpg', '25001009301.jpg', '25001009401.jpg', '25001009501.jpg', '25001009601.jpg', '25001009701.jpg', '25001009801.jpg', '25001009901.jpg', '25001010001.jpg', '25001010101.jpg', '25001010201.jpg', '25004137202.jpg', '25004144402.jpg', '25004148402.jpg', '25004149702.jpg', '25004176502.jpg', '25004184702.jpg', '25004187302.jpg', '25007084601.jpg', '25007084701.jpg', '25007084801.jpg', '25007084901.jpg', '25007085001.jpg', '25007085101.jpg', '25007085201.jpg', '25007085301.jpg', '25007085401.jpg', '25007085501.jpg', '25007085601.jpg', '25007085701.jpg', '25007085801.jpg', '25007085901.jpg', '25007086001.jpg', '25007086101.jpg', '25007086201.jpg', '25007086301.jpg', '25007086401.jpg', '25007086501.jpg', '25007086601.jpg', '25007086701.jpg', '25007086801.jpg', '25007086901.jpg', '25007087001.jpg', '25009010001.jpg', '25009010101.jpg', '25009010201.jpg', '25009010301.jpg', '25009010401.jpg', '25009010501.jpg', '25009010601.jpg', '25009010701.jpg', '25009010801.jpg', '25009010901.jpg', '25009011001.jpg', '25009011101.jpg', '25009011201.jpg', '25009011301.jpg', '25009011401.jpg', '25009011501.jpg', '25009011601.jpg', '25009011701.jpg', '25009011801.jpg', '25009011901.jpg', '25009012001.jpg', '25009012101.jpg', '25009012201.jpg', '25009012301.jpg', '25009012401.jpg', '25009028602.jpg', '25009031801.jpg', '25009041601.jpg', '25009044401.jpg', '25009051701.jpg', '25009052301.jpg', '25009054301.jpg', '25009056001.jpg', '25012087702.jpg', '25012093802.jpg', '25015006501.jpg', '25015032701.jpg', '25016061201.jpg', '25016075501.jpg', '25016076301.jpg', '25016076601.jpg', '25016136002.jpg', '25016136102.jpg', '25016136202.jpg', '25016136302.jpg', '25016136402.jpg', '25016136502.jpg', '25016136602.jpg', '25016136702.jpg', '25016136802.jpg', '25016136902.jpg', '25016137002.jpg', '25016137101.jpg', '25016137202.jpg', '25016137302.jpg', '25016137402.jpg', '25016137502.jpg', '25016137602.jpg', '25016137702.jpg', '25016137801.jpg', '25016137902.jpg', '25016138002.jpg', '25016138102.jpg', '25016138302.jpg', '25016138402.jpg', '25016138502.jpg', '25017147501.jpg', '25017148301.jpg', '25017185701.jpg', '25017189801.jpg', '25017192701.jpg', '25017195201.jpg', '25018038701.jpg', '25018041001.jpg', '25018041101.jpg', '25018041201.jpg', '25018041301.jpg', '25018041401.jpg', '25018041501.jpg', '25018041601.jpg', '25018041701.jpg', '25018041801.jpg', '25018041901.jpg', '25018042001.jpg', '25018042101.jpg', '25018042201.jpg', '25018042301.jpg', '25018042401.jpg', '25018042501.jpg', '25018042601.jpg', '25018042701.jpg', '25018042801.jpg', '25018042901.jpg', '25018043001.jpg', '25018043101.jpg', '25018043201.jpg', '25018043301.jpg', '25018043401.jpg', '25020214501.jpg', '25021011501.jpg', '25021024901.jpg', '25022171001.jpg', '25024028202.jpg', '25025071001.jpg', '25029083402.jpg', '25029084202.jpg', '25031081702.jpg', '25032120001.jpg', '25032120101.jpg', '25032120201.jpg', '25032120301.jpg', '25032120401.jpg', '25032120501.jpg', '25032120601.jpg', '25032120701.jpg', '25032120801.jpg', '25032120901.jpg', '25032121001.jpg', '25032121101.jpg', '25032121201.jpg', '25032121301.jpg', '25032121401.jpg', '25032121501.jpg', '25032121601.jpg', '25032121701.jpg', '25032121801.jpg', '25032121901.jpg', '25032122001.jpg', '25032122101.jpg', '25032122201.jpg', '25032122301.jpg', '25032122401.jpg', '25033018101.jpg', '25033029901.jpg', '25033032301.jpg', '25033037801.jpg', '25033075901.jpg', '25033076001.jpg', '25033076101.jpg', '25033076201.jpg', '25033076301.jpg', '25033076401.jpg', '25033076501.jpg', '25033076601.jpg', '25033076701.jpg', '25033076801.jpg', '25033076901.jpg', '25033077001.jpg', '25033077101.jpg', '25033077201.jpg', '25033077301.jpg', '25033077401.jpg', '25033077501.jpg', '25033077601.jpg', '25033077701.jpg', '25033077801.jpg', '25033077901.jpg', '25033078001.jpg', '25033078101.jpg', '25033078201.jpg', '25033078301.jpg', '25034113801.jpg', '25034117401.jpg', '25035025401.jpg', '25035084401.jpg', '25036123901.jpg']\n",
            "Mask Names: ['17001652802_mask.png', '17001738102_mask.png', '17001738202_mask.png', '17001738302_mask.png', '17001738402_mask.png', '17001738502_mask.png', '17001738602_mask.png', '17001738702_mask.png', '17001738902_mask.png', '17001739002_mask.png', '17001739502_mask.png', '17001739702_mask.png', '17001739802_mask.png', '17001740102_mask.png', '17001740202_mask.png', '17001740402_mask.png', '17001740802_mask.png', '17001740902_mask.png', '17001745802_mask.png', '17002336001_mask.png', '17002409801_mask.png', '17004348102_mask.png', '17004348202_mask.png', '17004348402_mask.png', '17004348902_mask.png', '17004349002_mask.png', '17004349202_mask.png', '17004349502_mask.png', '17004349602_mask.png', '17004349702_mask.png', '17004349802_mask.png', '17004349902_mask.png', '17004350002_mask.png', '17004350202_mask.png', '17004350402_mask.png', '17004350502_mask.png', '17004350602_mask.png', '17004350702_mask.png', '17004350802_mask.png', '17004350902_mask.png', '17004351002_mask.png', '17004426901_mask.png', '17005029401_mask.png', '17005057001_mask.png', '17005057101_mask.png', '17005057201_mask.png', '17005057301_mask.png', '17005057401_mask.png', '17005057501_mask.png', '17005057601_mask.png', '17005057701_mask.png', '17005057801_mask.png', '17005057901_mask.png', '17005058201_mask.png', '17005058301_mask.png', '17005058401_mask.png', '17005058701_mask.png', '17005058801_mask.png', '17005059001_mask.png', '17005059201_mask.png', '17005059401_mask.png', '17005059501_mask.png', '17005059601_mask.png', '17006011002_mask.png', '17006011102_mask.png', '17006011202_mask.png', '17006011302_mask.png', '17006011402_mask.png', '17006011502_mask.png', '17006011602_mask.png', '17006011702_mask.png', '17006011802_mask.png', '17006011902_mask.png', '17006012002_mask.png', '17006012102_mask.png', '17006012202_mask.png', '17006012302_mask.png', '17006012402_mask.png', '17006012502_mask.png', '17006012602_mask.png', '17006012702_mask.png', '17006012802_mask.png', '17006012902_mask.png', '17006013102_mask.png', '17006013302_mask.png', '17006013402_mask.png', '17006013502_mask.png', '17006013702_mask.png', '17006013802_mask.png', '17006013902_mask.png', '17006014002_mask.png', '17006058202_mask.png', '17006078902_mask.png', '17006100402_mask.png', '17007661501_mask.png', '17017815902_mask.png', '17017820102_mask.png', '17017820202_mask.png', '17017820302_mask.png', '17017820502_mask.png', '17017820602_mask.png', '17017821002_mask.png', '17017821102_mask.png', '17017821302_mask.png', '17017821902_mask.png', '17017822002_mask.png', '17017822102_mask.png', '17017822202_mask.png', '17017822302_mask.png', '17017822502_mask.png', '17017822602_mask.png', '17017822702_mask.png', '17019870602_mask.png', '17019871102_mask.png', '17019872802_mask.png', '17019874202_mask.png', '17019878402_mask.png', '17019885502_mask.png', '17019935202_mask.png', '17019937902_mask.png', '17022848302_mask.png', '17024674002_mask.png', '17024717802_mask.png', '17025816302_mask.png', '17034030202_mask.png', '17034030302_mask.png', '17034030602_mask.png', '17034030702_mask.png', '17034030902_mask.png', '17034031202_mask.png', '17034031302_mask.png', '17034031402_mask.png', '17034032002_mask.png', '17034032402_mask.png', '17034032502_mask.png', '17034032602_mask.png', '17034032902_mask.png', '17034033002_mask.png', '17037005102_mask.png', '17037005202_mask.png', '17037005402_mask.png', '17037005502_mask.png', '17037005602_mask.png', '17037005702_mask.png', '17037005802_mask.png', '17037006002_mask.png', '17037006102_mask.png', '17037006202_mask.png', '17037006302_mask.png', '17037006502_mask.png', '17037006602_mask.png', '17037006702_mask.png', '17037006802_mask.png', '17037007402_mask.png', '17037007702_mask.png', '17037007802_mask.png', '17039196602_mask.png', '17039236602_mask.png', '17039236802_mask.png', '17039237002_mask.png', '17039237102_mask.png', '17039237202_mask.png', '17039237402_mask.png', '17039237702_mask.png', '17039237902_mask.png', '17039238102_mask.png', '17039238402_mask.png', '20001049701_mask.png', '20010147901_mask.png', '20010148001_mask.png', '20010148101_mask.png', '20010148201_mask.png', '20010148401_mask.png', '20010148501_mask.png', '20010148701_mask.png', '20010148801_mask.png', '20010149001_mask.png', '20010149101_mask.png', '20010149201_mask.png', '20010149301_mask.png', '20010149401_mask.png', '20010149501_mask.png', '20010149601_mask.png', '20010149701_mask.png', '20010149801_mask.png', '20010149901_mask.png', '20010150001_mask.png', '20011030301_mask.png', '20011045302_mask.png', '20011049802_mask.png', '20011059102_mask.png', '20011070801_mask.png', '20011070901_mask.png', '20011071501_mask.png', '20011074502_mask.png', '20011077002_mask.png', '20011081102_mask.png', '20011083602_mask.png', '20011087201_mask.png', '20011093001_mask.png', '20011096001_mask.png', '20012174802_mask.png', '20014055701_mask.png', '20014072301_mask.png', '20014076801_mask.png', '20015151402_mask.png', '20015173002_mask.png', '20015173305_mask.png', '20015175202_mask.png', '20015178801_mask.png', '20015179502_mask.png', '20015183501_mask.png', '20015188501_mask.png', '20015190301_mask.png', '20015204202_mask.png', '20016207205_mask.png', '20016211701_mask.png', '20016214401_mask.png', '20016216501_mask.png', '20016218602_mask.png', '20016225102_mask.png', '20016230405_mask.png', '20016233402_mask.png', '20016241002_mask.png', '20016241004_mask.png', '20016241101_mask.png', '20016248005_mask.png', '20016248101_mask.png', '20016248305_mask.png', '20016263402_mask.png', '20016272302_mask.png', '20016295302_mask.png', '20018115701_mask.png', '20018116301_mask.png', '20018116501_mask.png', '20018117101_mask.png', '20018117801_mask.png', '20018118501_mask.png', '20018119001_mask.png', '20018119201_mask.png', '20018119401_mask.png', '20018119501_mask.png', '20018119601_mask.png', '20018120001_mask.png', '20018120101_mask.png', '20018120501_mask.png', '20020013101_mask.png', '20020013201_mask.png', '20020017201_mask.png', '20020018801_mask.png', '20020024401_mask.png', '20020025101_mask.png', '20020025201_mask.png', '20020025701_mask.png', '20020027101_mask.png', '20020027601_mask.png', '20020027901_mask.png', '20020028201_mask.png', '20020028601_mask.png', '20020032001_mask.png', '20020033801_mask.png', '20020035001_mask.png', '20020035701_mask.png', '20020041001_mask.png', '20020041201_mask.png', '20020043701_mask.png', '20020044401_mask.png', '20020045901_mask.png', '20020051001_mask.png', '20021100201_mask.png', '20021100301_mask.png', '20021100401_mask.png', '20021100501_mask.png', '20021100601_mask.png', '20021100701_mask.png', '20021100801_mask.png', '20021100901_mask.png', '20021101001_mask.png', '20021101101_mask.png', '20021101201_mask.png', '20021101301_mask.png', '20021101401_mask.png', '20021101501_mask.png', '20021101601_mask.png', '20021101901_mask.png', '20021102201_mask.png', '20021164602_mask.png', '20021183502_mask.png', '20022281202_mask.png', '20024002201_mask.png', '20024002301_mask.png', '20024002401_mask.png', '20024002501_mask.png', '20024002601_mask.png', '20024002701_mask.png', '20024002801_mask.png', '20024002901_mask.png', '20024003001_mask.png', '20024003101_mask.png', '20024003201_mask.png', '20024003301_mask.png', '20024003401_mask.png', '20024003501_mask.png', '20024003601_mask.png', '20024003701_mask.png', '20024003801_mask.png', '20024004101_mask.png', '20024004201_mask.png', '20024004301_mask.png', '20024004401_mask.png', '20024004601_mask.png', '20024004701_mask.png', '20024004801_mask.png', '20024005101_mask.png', '20024005201_mask.png', '20024005501_mask.png', '20024005701_mask.png', '20024005801_mask.png', '20024005901_mask.png', '20024036902_mask.png', '20024058602_mask.png', '20024071502_mask.png', '20024078102_mask.png', '20028199801_mask.png', '20028201101_mask.png', '20028206201_mask.png', '20029004202_mask.png', '20029004502_mask.png', '20029008201_mask.png', '20031013402_mask.png', '20031021102_mask.png', '20031048202_mask.png', '20031062202_mask.png', '20031098302_mask.png', '20034046802_mask.png', '20034061802_mask.png', '20034067102_mask.png', '20034084602_mask.png', '20034088502_mask.png', '20036101101_mask.png', '20036110801_mask.png', '20036140201_mask.png', '20040214302_mask.png', '20042060302_mask.png', '20042103002_mask.png', '20044141002_mask.png', '20046221001_mask.png', '20046223501_mask.png', '20046223601_mask.png', '20046223701_mask.png', '20046223801_mask.png', '20046223901_mask.png', '20046224001_mask.png', '20046224101_mask.png', '20046224201_mask.png', '20046224301_mask.png', '20046224401_mask.png', '20046224501_mask.png', '20046224601_mask.png', '20046224701_mask.png', '20046224801_mask.png', '20046224901_mask.png', '20046225001_mask.png', '20046225101_mask.png', '20046225201_mask.png', '20046225301_mask.png', '20046225401_mask.png', '20046225501_mask.png', '20046225601_mask.png', '20046225701_mask.png', '20046225801_mask.png', '20046225901_mask.png', '20046226001_mask.png', '20046226101_mask.png', '20046226201_mask.png', '20046226301_mask.png', '20046226401_mask.png', '20046237101_mask.png', '20046248401_mask.png', '20046250301_mask.png', '20046281801_mask.png', '21002112901_mask.png', '21002116201_mask.png', '21002118901_mask.png', '21002135506_mask.png', '21005043502_mask.png', '21005043602_mask.png', '21005043702_mask.png', '21005043802_mask.png', '21005043902_mask.png', '21005044002_mask.png', '21005044102_mask.png', '21005044202_mask.png', '21005044302_mask.png', '21005044402_mask.png', '21005044502_mask.png', '24010170001_mask.png', '24024269602_mask.png', '24024269701_mask.png', '24024269702_mask.png', '24024269801_mask.png', '24024270001_mask.png', '24024270101_mask.png', '24024270201_mask.png', '24024270301_mask.png', '24024270501_mask.png', '24024270701_mask.png', '24024270801_mask.png', '24024270901_mask.png', '24024271001_mask.png', '24024273201_mask.png', '24024281501_mask.png', '24025044801_mask.png', '24026161301_mask.png', '24026177101_mask.png', '24026179502_mask.png', '24026179605_mask.png', '24026179701_mask.png', '24026179801_mask.png', '24026179901_mask.png', '24026180001_mask.png', '24026180101_mask.png', '24026180201_mask.png', '24026180301_mask.png', '24026180401_mask.png', '24026180501_mask.png', '24026180602_mask.png', '24026180701_mask.png', '24026180802_mask.png', '24026181006_mask.png', '24026181102_mask.png', '24026181201_mask.png', '24026181301_mask.png', '24026181401_mask.png', '24026181501_mask.png', '24026181601_mask.png', '24026181701_mask.png', '24026181801_mask.png', '24026181901_mask.png', '24026197901_mask.png', '24026202701_mask.png', '24027220901_mask.png', '24027301401_mask.png', '24027301501_mask.png', '24027301601_mask.png', '24027301701_mask.png', '24027301801_mask.png', '24027301902_mask.png', '24027302001_mask.png', '24027302201_mask.png', '24027302401_mask.png', '24027302502_mask.png', '24027302601_mask.png', '24027302701_mask.png', '24027302802_mask.png', '24027302901_mask.png', '24027303001_mask.png', '24027303102_mask.png', '24027303201_mask.png', '24027303302_mask.png', '24027303401_mask.png', '24027303402_mask.png', '24027303501_mask.png', '24027303601_mask.png', '24027303701_mask.png', '24027319202_mask.png', '24028012601_mask.png', '24028014601_mask.png', '24029131001_mask.png', '24029178001_mask.png', '24029178102_mask.png', '24029178201_mask.png', '24029178305_mask.png', '24029178402_mask.png', '24029178602_mask.png', '24029178702_mask.png', '24029178801_mask.png', '24029179001_mask.png', '24029179101_mask.png', '24029179302_mask.png', '24029179401_mask.png', '24029179502_mask.png', '24029179601_mask.png', '24029179902_mask.png', '24029180001_mask.png', '24029180101_mask.png', '24029180202_mask.png', '24029180301_mask.png', '24029180402_mask.png', '24032035601_mask.png', '24032040001_mask.png', '24032092301_mask.png', '24032104901_mask.png', '24032124201_mask.png', '24032127201_mask.png', '24032127301_mask.png', '24032127401_mask.png', '24032127501_mask.png', '24032127601_mask.png', '24032127701_mask.png', '24032127801_mask.png', '24032128001_mask.png', '24032128101_mask.png', '24032128201_mask.png', '24032128301_mask.png', '24032128401_mask.png', '24032128501_mask.png', '24032128601_mask.png', '24032128701_mask.png', '24032128801_mask.png', '24032129001_mask.png', '24032129101_mask.png', '24032129201_mask.png', '24032129301_mask.png', '24032129401_mask.png', '24032129501_mask.png', '24033227902_mask.png', '24033230502_mask.png', '24033231502_mask.png', '24033234302_mask.png', '24044080901_mask.png', '24044092301_mask.png', '24044095401_mask.png', '24044095501_mask.png', '24044095801_mask.png', '24044095901_mask.png', '24044096001_mask.png', '24044096101_mask.png', '24044096301_mask.png', '24044096601_mask.png', '24044096801_mask.png', '24044096901_mask.png', '24044097001_mask.png', '24044097101_mask.png', '24044097201_mask.png', '24044097301_mask.png', '24044097501_mask.png', '24044098401_mask.png', '24045035302_mask.png', '24045068102_mask.png', '24047242502_mask.png', '24047242701_mask.png', '25001008201_mask.png', '25001009501_mask.png', '25001009601_mask.png', '25007085601_mask.png', '25007085701_mask.png', '25007086901_mask.png', '25007087001_mask.png', '25009010201_mask.png', '25009010301_mask.png', '25009010501_mask.png', '25009010601_mask.png', '25009010801_mask.png', '25009010901_mask.png', '25009011001_mask.png', '25009011201_mask.png', '25009011401_mask.png', '25009011501_mask.png', '25009011601_mask.png', '25009011701_mask.png', '25009011801_mask.png', '25009011901_mask.png', '25009012001_mask.png', '25009012101_mask.png', '25009012201_mask.png', '25009012301_mask.png', '25009012401_mask.png', '25009056001_mask.png', '25012093802_mask.png', '25015006501_mask.png', '25015032701_mask.png', '25016061201_mask.png', '25016075501_mask.png', '25016076301_mask.png', '25016076601_mask.png', '25016136002_mask.png', '25016136102_mask.png', '25016136202_mask.png', '25016136302_mask.png', '25016136402_mask.png', '25016136502_mask.png', '25016136602_mask.png', '25016136702_mask.png', '25016136802_mask.png', '25016137202_mask.png', '25016137302_mask.png', '25016137602_mask.png', '25016137702_mask.png', '25016137801_mask.png', '25016137902_mask.png', '25016138002_mask.png', '25016138102_mask.png', '25016138302_mask.png', '25016138502_mask.png', '25017147501_mask.png', '25017148301_mask.png', '25017185701_mask.png', '25018038701_mask.png', '25018041001_mask.png', '25018041101_mask.png', '25018041201_mask.png', '25018041301_mask.png', '25018041401_mask.png', '25018041501_mask.png', '25018041601_mask.png', '25018041701_mask.png', '25018041801_mask.png', '25018041901_mask.png', '25018042001_mask.png', '25018042101_mask.png', '25018042201_mask.png', '25018042301_mask.png', '25018042401_mask.png', '25018042501_mask.png', '25018042601_mask.png', '25018042701_mask.png', '25018042801_mask.png', '25018042901_mask.png', '25020214501_mask.png', '25032120201_mask.png', '25032120501_mask.png', '25032120601_mask.png', '25032120801_mask.png', '25032120901_mask.png', '25032121001_mask.png', '25032121101_mask.png', '25032121301_mask.png', '25032121401_mask.png', '25032121501_mask.png', '25032121601_mask.png', '25032121701_mask.png', '25032122101_mask.png', '25032122201_mask.png', '25032122301_mask.png', '25033018101_mask.png', '25033029901_mask.png', '25033032301_mask.png', '25033037801_mask.png', '25033075901_mask.png', '25033076001_mask.png', '25033076101_mask.png', '25033076201_mask.png', '25033076301_mask.png', '25033076401_mask.png', '25033076501_mask.png', '25033076701_mask.png', '25033076801_mask.png', '25033076901_mask.png', '25033077001_mask.png', '25033077101_mask.png', '25033077201_mask.png', '25033077301_mask.png', '25033077401_mask.png', '25033077501_mask.png', '25033077601_mask.png', '25033077701_mask.png', '25033077801_mask.png', '25033077901_mask.png']\n",
            "Filtered Image Names: ['17001652802.jpg', '17001738102.jpg', '17001738202.jpg', '17001738302.jpg', '17001738402.jpg', '17001738502.jpg']\n",
            "Filtered Mask Names: ['17001652802_mask.png', '17001738102_mask.png', '17001738202_mask.png', '17001738302_mask.png', '17001738402_mask.png', '17001738502_mask.png']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c43fe2e503474b54a96f9a9e129fa24f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading images and masks:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41c911da243a476390272aa42d199f93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading tabular data:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns in annotations_ATL.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_IND_CHA.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_PAC_SLB.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_PAC_TWN.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_PAC_USA.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_PAC_TLS.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_PAC_IDN_PHL.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_IND_MDV.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n",
            "Columns in annotations_PAC_AUS.csv: Index(['quadratid', 'y', 'x', 'label_name', 'label', 'func_group', 'method',\n",
            "       'data_set'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "896eccdc94f34240b49a46293ef14d47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing images:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image 0: type(img)=<class 'numpy.ndarray'>, shape(img)=(1024, 1024, 3), type(mask)=<class 'numpy.ndarray'>, shape(mask)=(1024, 1024)\n",
            "Predicted masks shape: (1024, 1024)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]\n",
            " [1. 1. 1. ... 0. 0. 0.]]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2907a882033f>\u001b[0m in \u001b[0;36m<cell line: 451>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;31m# Run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtabular_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msam_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;31m# Offload GPU memory after processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-2907a882033f>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m(img_folder, mask_folder, tabular_folder, sam_weights_path)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmask_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Use SAM to predict masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m           \u001b[0msam_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m           \u001b[0minput_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m           \u001b[0minput_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mm_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/predictor.py\u001b[0m in \u001b[0;36mset_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0minput_image_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_image_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_torch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/predictor.py\u001b[0m in \u001b[0;36mset_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_image_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_hw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Reverse window partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# qkv with shape (3, B, nHead, H * W, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;31m# q, k, v with shape (B * nHead, H * W, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import jaccard_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from pathlib import Path\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"\n",
        "    DataLoader class for loading, processing, and transforming image, mask, and tabular data.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_folder, mask_folder, tabular_folder, transform=None, load_fraction=0.01):\n",
        "        self.img_folder = img_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.tabular_folder = tabular_folder\n",
        "        self.transform = transform\n",
        "        self.load_fraction = load_fraction\n",
        "\n",
        "        # Load image and mask data\n",
        "        self.images = sorted([f for f in os.listdir(self.img_folder) if f.endswith(('.jpg', '.png'))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_folder) if f.endswith(('.jpg', '.png'))])\n",
        "\n",
        "        # Print image and mask names for debugging\n",
        "        print(f\"Image Names: {self.images}\")\n",
        "        print(f\"Mask Names: {self.masks}\")\n",
        "\n",
        "        # Create sets for comparison, stripping _mask suffix from masks\n",
        "        img_names_set = set([os.path.splitext(f)[0] for f in self.images])\n",
        "        mask_names_set = set([os.path.splitext(f)[0].replace('_mask', '') for f in self.masks])  # Ensure we handle '_mask'\n",
        "\n",
        "        # Find corresponding pairs\n",
        "        valid_img_names = img_names_set & mask_names_set\n",
        "        self.images = [f for f in self.images if os.path.splitext(f)[0] in valid_img_names]\n",
        "        self.masks = [f\"{os.path.splitext(f)[0]}_mask.png\" for f in self.images]  # Create the corresponding mask filenames\n",
        "\n",
        "        # Limit the number of images and masks loaded\n",
        "        num_images_to_load = int(len(self.images) * self.load_fraction)\n",
        "        self.images = self.images[:num_images_to_load]\n",
        "        self.masks = self.masks[:num_images_to_load]\n",
        "\n",
        "        # Print filtered names for debugging\n",
        "        print(f\"Filtered Image Names: {self.images}\")\n",
        "        print(f\"Filtered Mask Names: {self.masks}\")\n",
        "\n",
        "        # Create a dataset dictionary with image paths as keys and image, mask data\n",
        "        self.dataset = {}\n",
        "        for img_p in tqdm(self.images, desc=\"Loading images and masks\"):\n",
        "            img_name = os.path.splitext(img_p)[0]\n",
        "            img = cv2.imread(os.path.join(self.img_folder, img_p))\n",
        "            #################################################################################### image preprocess function goes here\n",
        "\n",
        "\n",
        "            mask_path = os.path.join(self.mask_folder, img_name + '_mask.png')  # Use .png as per your example\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Check if the mask is loaded correctly\n",
        "            if mask is None:\n",
        "                print(f\"Warning: Mask for {img_name} could not be loaded. Mask path: {mask_path}\")\n",
        "                continue\n",
        "\n",
        "            # Ensure images and masks have the same dimensions\n",
        "            if img is not None and img.shape[:2] != mask.shape[:2]:\n",
        "                print(f\"Image and mask sizes do not match for {img_p}. Resizing mask.\")\n",
        "                mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "\n",
        "            # Store the data\n",
        "            self.dataset[img_name] = {'image': img, 'mask': mask}\n",
        "\n",
        "        self.img_names = list(self.dataset.keys())\n",
        "\n",
        "        # Initialize the LabelEncoder\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        # Load tabular annotation data and add to the dataset\n",
        "        for annotation_df_path in tqdm(os.listdir(self.tabular_folder), desc=\"Loading tabular data\"):\n",
        "            # region = annotation_df_path.split('_', 1)[1].split('.')[0] ############################### to reduce annotation df size\n",
        "            if not annotation_df_path.startswith('annotation'):\n",
        "                continue\n",
        "\n",
        "            annotation_df = pd.read_csv(os.path.join(self.tabular_folder, annotation_df_path))\n",
        "            # Print column names for debugging\n",
        "            print(f\"Columns in {annotation_df_path}: {annotation_df.columns}\")\n",
        "\n",
        "            # Check if the expected 'quadratid' column exists, otherwise adjust\n",
        "            if 'quadratid' not in annotation_df.columns:\n",
        "                raise KeyError(f\"'quadratid' column not found in {annotation_df_path}. Available columns: {annotation_df.columns}\")\n",
        "\n",
        "            annotation_df['quadratid'] = annotation_df['quadratid'].astype(str)\n",
        "\n",
        "            # Fit the LabelEncoder on the func_group to convert them into integers\n",
        "            self.label_encoder.fit(annotation_df['func_group'].astype(str))  # Make sure to convert to string\n",
        "\n",
        "            # Match tabular data with image data in the dataset\n",
        "            for quadratid in annotation_df['quadratid'].unique():\n",
        "                if quadratid not in self.dataset:\n",
        "                    continue\n",
        "                # Ensure no duplicate quadratid with different data exists\n",
        "                if 'points' in self.dataset[quadratid]:\n",
        "                    raise Exception(\"Same quadrat, different datasets!\")\n",
        "\n",
        "                # Add points and point labels to the dataset\n",
        "                self.dataset[quadratid]['points'] = annotation_df[annotation_df['quadratid'] == quadratid][['x', 'y']].values\n",
        "                self.dataset[quadratid]['point_labels'] = annotation_df[annotation_df['quadratid'] == quadratid]['func_group'].map({'Hard Coral': 1, 'Soft Coral': 2}).fillna(0).astype(int)\n",
        "                point_labels = self.dataset[quadratid]['point_labels'].to_numpy()\n",
        "                # Transform labels to integers\n",
        "                self.dataset[quadratid]['point_labels'] =  point_labels\n",
        "\n",
        "          ##  added to return quadratid's for later analysis ###########################################################################################\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_dict = self.dataset[self.img_names[idx]]\n",
        "        img, mask, keypoints, point_labels = img_dict['image'], img_dict['mask'], img_dict.get('points', None), img_dict.get('point_labels', None)\n",
        "\n",
        "        if self.transform:\n",
        "            transform = self.get_resize_transform(image_size=1024)\n",
        "            transformed = transform(image=img, mask=mask)\n",
        "            img, mask = transformed['image'], transformed['mask']\n",
        "\n",
        "        return img, mask, keypoints, point_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_resize_transform(image_size=1024):\n",
        "        return albumentations.Compose([\n",
        "            albumentations.Resize(\n",
        "                height=image_size,\n",
        "                width=image_size,\n",
        "                interpolation=cv2.INTER_AREA,\n",
        "                p=1,\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "\n",
        "# Helper functions for SAM\n",
        "def load_sam_model(sam_weights_path):\n",
        "    sam = sam_model_registry['vit_h'](checkpoint=sam_weights_path)\n",
        "    predictor = SamPredictor(sam)\n",
        "    return predictor\n",
        "\n",
        "\n",
        "# Metric calculation functions\n",
        "def calculate_iou(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return jaccard_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_pixel_accuracy(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "def calculate_dice_coefficient(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice_score = (2 * intersection) / (np.sum(y_true) + np.sum(y_pred))\n",
        "    return dice_score\n",
        "\n",
        "def calculate_precision(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return precision_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_recall(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return recall_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "\n",
        "def calculate_iou(true_mask, pred_mask, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    iou_scores = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Flatten the masks\n",
        "        true_flat = (true_mask.flatten() == class_id).astype(int)\n",
        "        pred_flat = (pred_mask.flatten() == class_id).astype(int)\n",
        "\n",
        "        # Calculate IoU for each class\n",
        "        iou = jaccard_score(true_flat, pred_flat, zero_division=0)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "    return np.mean(iou_scores)  # Return average IoU\n",
        "\n",
        "def calculate_dice(true_mask, pred_mask, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate Dice Coefficient for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    dice_scores = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Flatten the masks\n",
        "        true_flat = (true_mask.flatten() == class_id).astype(int)\n",
        "        pred_flat = (pred_mask.flatten() == class_id).astype(int)\n",
        "\n",
        "        # Calculate Dice for each class\n",
        "        dice = f1_score(true_flat, pred_flat, zero_division=0)\n",
        "        dice_scores.append(dice)\n",
        "\n",
        "    return np.mean(dice_scores)  # Return average Dice\n",
        "\n",
        "def main_pipeline(img_folder, mask_folder, tabular_folder, sam_weights_path):\n",
        "    # Initialize the DataLoader with your dataset\n",
        "    data_loader = DataLoader(\n",
        "        img_folder=img_folder,\n",
        "        mask_folder=mask_folder,\n",
        "        tabular_folder=tabular_folder,\n",
        "        transform=DataLoader.get_resize_transform(image_size=1024)\n",
        "    )\n",
        "\n",
        "    # Load the SAM model\n",
        "    sam_predictor = load_sam_model(sam_weights_path)\n",
        "\n",
        "    # Progress bar for the main pipeline\n",
        "    progress_bar = tqdm(range(len(data_loader)), desc=\"Processing images\")\n",
        "\n",
        "    # Lists to store metrics\n",
        "    iou_scores = []\n",
        "    dice_scores = []\n",
        "    pixel_scores = []\n",
        "    precision_scores = []\n",
        "\n",
        "    num_classes = 3  # Set this to the number of classes in your dataset\n",
        "    mask_classes = [1, 2]\n",
        "\n",
        "    for idx in progress_bar:\n",
        "        img, mask, keypoints, point_labels = data_loader[idx]\n",
        "\n",
        "        # Check types of img and mask\n",
        "        print(f\"Processing image {idx}: type(img)={type(img)}, shape(img)={img.shape}, type(mask)={type(mask)}, shape(mask)={mask.shape}\")\n",
        "\n",
        "        # Check if keypoints and point_labels are not None\n",
        "        if keypoints is None or point_labels is None:\n",
        "            print(f\"Warning: Keypoints or point_labels are None for image {data_loader.img_names[idx]}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        pred_mask = np.zeros(img.shape[:2])\n",
        "        for m_class in mask_classes:\n",
        "        # Use SAM to predict masks\n",
        "          sam_predictor.set_image(img)\n",
        "          input_points = np.array(keypoints)\n",
        "          input_labels = np.where(point_labels == m_class, 1, 0)\n",
        "          masks, _, _ = sam_predictor.predict(point_coords=input_points, point_labels=input_labels, multimask_output=False)\n",
        "          pred_mask[masks[0] == 1] = m_class\n",
        "\n",
        "\n",
        "\n",
        "          # Check if predicted masks are empty or None\n",
        "          if pred_mask is None or pred_mask.size == 0:\n",
        "              print(f\"Warning: No masks predicted for image {data_loader.img_names[idx]}. Skipping.\")\n",
        "              continue  # Skip this iteration if there's no prediction\n",
        "\n",
        "          # Convert predicted masks to uint8 if they are boolean\n",
        "          if pred_mask.dtype == np.bool_:\n",
        "              pred_mask = pred_mask.astype(np.uint8) * 255\n",
        "          cv2.imwrite('/content/drive/MyDrive/please_god.jpg', pred_mask)\n",
        "          print(pred_mask)\n",
        "\n",
        "\n",
        "      # Update progress description\n",
        "    progress_bar.set_description(f\"Processing image {idx + 1}/{len(data_loader)}\")\n",
        "\n",
        "    # Print average IoU ########################################################### Added other accuracy metrics\n",
        "    avg_iou = np.mean(iou_scores) if iou_scores else 0\n",
        "    avg_dice = np.mean(dice_scores) if dice_scores else 0\n",
        "    avg_pixel = np.mean(pixel_scores) if pixel_scores else 0\n",
        "    avg_precision = np.mean(precision_scores) if precision_scores else 0\n",
        "\n",
        "    # Print average metrics\n",
        "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Average Dice: {avg_dice:.4f}\")\n",
        "    print(f\"Average Pixel Accuracy: {avg_pixel:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "\n",
        "    print(\"Pipeline completed!\")\n",
        "\n",
        "# Example usage\n",
        "img_folder = img_folder\n",
        "mask_folder = mask_folder\n",
        "tabular_folder = tabular_folder\n",
        "sam_weights_path = SAM_WEIGHTS_PATH+SAM_WEIGHTS_FILE\n",
        "\n",
        "# Run the pipeline\n",
        "main_pipeline(img_folder, mask_folder, tabular_folder, sam_weights_path)\n",
        "\n",
        "# Offload GPU memory after processing\n",
        "torch.cuda.empty_cache()\n",
        "# for TPU memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TIZSuCb318g2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIZSuCb318g2",
        "outputId": "39d783d2-cf27-4b74-ab48-e4ee7623de6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class SamPredictor in module segment_anything.predictor:\n",
            "\n",
            "class SamPredictor(builtins.object)\n",
            " |  SamPredictor(sam_model: segment_anything.modeling.sam.Sam) -> None\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, sam_model: segment_anything.modeling.sam.Sam) -> None\n",
            " |      Uses SAM to calculate the image embedding for an image, and then\n",
            " |      allow repeated, efficient mask prediction given prompts.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        sam_model (Sam): The model to use for mask prediction.\n",
            " |  \n",
            " |  get_image_embedding(self) -> torch.Tensor\n",
            " |      Returns the image embeddings for the currently set image, with\n",
            " |      shape 1xCxHxW, where C is the embedding dimension and (H,W) are\n",
            " |      the embedding spatial dimension of SAM (typically C=256, H=W=64).\n",
            " |  \n",
            " |  predict(self, point_coords: Optional[numpy.ndarray] = None, point_labels: Optional[numpy.ndarray] = None, box: Optional[numpy.ndarray] = None, mask_input: Optional[numpy.ndarray] = None, multimask_output: bool = True, return_logits: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
            " |      Predict masks for the given input prompts, using the currently set image.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        point_coords (np.ndarray or None): A Nx2 array of point prompts to the\n",
            " |          model. Each point is in (X,Y) in pixels.\n",
            " |        point_labels (np.ndarray or None): A length N array of labels for the\n",
            " |          point prompts. 1 indicates a foreground point and 0 indicates a\n",
            " |          background point.\n",
            " |        box (np.ndarray or None): A length 4 array given a box prompt to the\n",
            " |          model, in XYXY format.\n",
            " |        mask_input (np.ndarray): A low resolution mask input to the model, typically\n",
            " |          coming from a previous prediction iteration. Has form 1xHxW, where\n",
            " |          for SAM, H=W=256.\n",
            " |        multimask_output (bool): If true, the model will return three masks.\n",
            " |          For ambiguous input prompts (such as a single click), this will often\n",
            " |          produce better masks than a single prediction. If only a single\n",
            " |          mask is needed, the model's predicted quality score can be used\n",
            " |          to select the best mask. For non-ambiguous prompts, such as multiple\n",
            " |          input prompts, multimask_output=False can give better results.\n",
            " |        return_logits (bool): If true, returns un-thresholded masks logits\n",
            " |          instead of a binary mask.\n",
            " |      \n",
            " |      Returns:\n",
            " |        (np.ndarray): The output masks in CxHxW format, where C is the\n",
            " |          number of masks, and (H, W) is the original image size.\n",
            " |        (np.ndarray): An array of length C containing the model's\n",
            " |          predictions for the quality of each mask.\n",
            " |        (np.ndarray): An array of shape CxHxW, where C is the number\n",
            " |          of masks and H=W=256. These low resolution logits can be passed to\n",
            " |          a subsequent iteration as mask input.\n",
            " |  \n",
            " |  predict_torch(self, point_coords: Optional[torch.Tensor], point_labels: Optional[torch.Tensor], boxes: Optional[torch.Tensor] = None, mask_input: Optional[torch.Tensor] = None, multimask_output: bool = True, return_logits: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
            " |      Predict masks for the given input prompts, using the currently set image.\n",
            " |      Input prompts are batched torch tensors and are expected to already be\n",
            " |      transformed to the input frame using ResizeLongestSide.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        point_coords (torch.Tensor or None): A BxNx2 array of point prompts to the\n",
            " |          model. Each point is in (X,Y) in pixels.\n",
            " |        point_labels (torch.Tensor or None): A BxN array of labels for the\n",
            " |          point prompts. 1 indicates a foreground point and 0 indicates a\n",
            " |          background point.\n",
            " |        box (np.ndarray or None): A Bx4 array given a box prompt to the\n",
            " |          model, in XYXY format.\n",
            " |        mask_input (np.ndarray): A low resolution mask input to the model, typically\n",
            " |          coming from a previous prediction iteration. Has form Bx1xHxW, where\n",
            " |          for SAM, H=W=256. Masks returned by a previous iteration of the\n",
            " |          predict method do not need further transformation.\n",
            " |        multimask_output (bool): If true, the model will return three masks.\n",
            " |          For ambiguous input prompts (such as a single click), this will often\n",
            " |          produce better masks than a single prediction. If only a single\n",
            " |          mask is needed, the model's predicted quality score can be used\n",
            " |          to select the best mask. For non-ambiguous prompts, such as multiple\n",
            " |          input prompts, multimask_output=False can give better results.\n",
            " |        return_logits (bool): If true, returns un-thresholded masks logits\n",
            " |          instead of a binary mask.\n",
            " |      \n",
            " |      Returns:\n",
            " |        (torch.Tensor): The output masks in BxCxHxW format, where C is the\n",
            " |          number of masks, and (H, W) is the original image size.\n",
            " |        (torch.Tensor): An array of shape BxC containing the model's\n",
            " |          predictions for the quality of each mask.\n",
            " |        (torch.Tensor): An array of shape BxCxHxW, where C is the number\n",
            " |          of masks and H=W=256. These low res logits can be passed to\n",
            " |          a subsequent iteration as mask input.\n",
            " |  \n",
            " |  reset_image(self) -> None\n",
            " |      Resets the currently set image.\n",
            " |  \n",
            " |  set_image(self, image: numpy.ndarray, image_format: str = 'RGB') -> None\n",
            " |      Calculates the image embeddings for the provided image, allowing\n",
            " |      masks to be predicted with the 'predict' method.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        image (np.ndarray): The image for calculating masks. Expects an\n",
            " |          image in HWC uint8 format, with pixel values in [0, 255].\n",
            " |        image_format (str): The color format of the image, in ['RGB', 'BGR'].\n",
            " |  \n",
            " |  set_torch_image(self, transformed_image: torch.Tensor, original_image_size: Tuple[int, ...]) -> None\n",
            " |      Calculates the image embeddings for the provided image, allowing\n",
            " |      masks to be predicted with the 'predict' method. Expects the input\n",
            " |      image to be already transformed to the format expected by the model.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        transformed_image (torch.Tensor): The input image, with shape\n",
            " |          1x3xHxW, which has been transformed with ResizeLongestSide.\n",
            " |        original_image_size (tuple(int, int)): The size of the image\n",
            " |          before transformation, in (H, W) format.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  device\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(SamPredictor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RyZeF1HmR2iz",
      "metadata": {
        "id": "RyZeF1HmR2iz"
      },
      "source": [
        "# Added CLAHE preprocessing statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dkRfkaGOrmkg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "dkRfkaGOrmkg",
        "outputId": "f9694766-94fb-4c83-8054-99b6e17d052d"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-25-2805b31ac4f9>, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-2805b31ac4f9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "################ CLAHE PREPROCESSING STATEMENT\n",
        "            lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "            lab_planes = list(cv2.split(lab))\n",
        "            clahe = cv2.createCLAHE(clipLimit=20.0,tileGridSize=(124,124))\n",
        "            lab_planes[0] = clahe.apply(lab_planes[0])\n",
        "            lab = cv2.merge(lab_planes)\n",
        "            img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "            mask_path = os.path.join(self.mask_folder, img_name + '_mask.png')  # Use .png as per your example\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o49zMhQeR20F",
      "metadata": {
        "id": "o49zMhQeR20F"
      },
      "source": [
        "# Preprocessing two . . . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SFI4xm3oDT2i",
      "metadata": {
        "id": "SFI4xm3oDT2i"
      },
      "outputs": [],
      "source": [
        "def image_normalization (image):\n",
        "    \"\"\"\n",
        "    This definition will first grayscale images to keep the contrast in the image, whenever it is normalized.\n",
        "    It will normalize the image to a [0,1] scale.\n",
        "    :param image: the image that will be normalized\n",
        "    :return: a normalized image\n",
        "    \"\"\"\n",
        "\n",
        "    #greyscale the image\n",
        "    #first greyscale so that the contrast will be enhanced\n",
        "    greyscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    #normalize the image\n",
        "    normalized_image = cv2.normalize(greyscale_image, None , alpha=0, beta=255, norm_type= cv2.NORM_MINMAX)\n",
        "\n",
        "    #convert back to color to be used in the SAM models\n",
        "    normalized_image_color = cv2.cvtColor(normalized_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    #return the final output\n",
        "    return normalized_image_color\n",
        "\n",
        "\n",
        "\n",
        "def background_noise_removal (image):\n",
        "    \"\"\"\n",
        "    removes the background noise of the images. Returns the image without the background noise.\n",
        "    :param image: image, the path to the image\n",
        "    :return: the image without background noise.\n",
        "    \"\"\"\n",
        "    #denoising a coloured image\n",
        "    cleaned_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
        "    return cleaned_image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jxbgqBnHW5z6",
      "metadata": {
        "id": "jxbgqBnHW5z6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import jaccard_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from pathlib import Path\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"\n",
        "    DataLoader class for loading, processing, and transforming image, mask, and tabular data.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_folder, mask_folder, tabular_folder, transform=None, load_fraction=0.1):\n",
        "        self.img_folder = img_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.tabular_folder = tabular_folder\n",
        "        self.transform = transform\n",
        "        self.load_fraction = load_fraction\n",
        "\n",
        "        # Load image and mask data\n",
        "        self.images = sorted([f for f in os.listdir(self.img_folder) if f.endswith(('.jpg', '.png'))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_folder) if f.endswith(('.jpg', '.png'))])\n",
        "\n",
        "        # Print image and mask names for debugging\n",
        "        print(f\"Image Names: {self.images}\")\n",
        "        print(f\"Mask Names: {self.masks}\")\n",
        "\n",
        "        # Create sets for comparison, stripping _mask suffix from masks\n",
        "        img_names_set = set([os.path.splitext(f)[0] for f in self.images])\n",
        "        mask_names_set = set([os.path.splitext(f)[0].replace('_mask', '') for f in self.masks])  # Ensure we handle '_mask'\n",
        "\n",
        "        # Find corresponding pairs\n",
        "        valid_img_names = img_names_set & mask_names_set\n",
        "        self.images = [f for f in self.images if os.path.splitext(f)[0] in valid_img_names]\n",
        "        self.masks = [f\"{os.path.splitext(f)[0]}_mask.png\" for f in self.images]  # Create the corresponding mask filenames\n",
        "\n",
        "        # Limit the number of images and masks loaded\n",
        "        num_images_to_load = int(len(self.images) * self.load_fraction)\n",
        "        self.images = self.images[:num_images_to_load]\n",
        "        self.masks = self.masks[:num_images_to_load]\n",
        "\n",
        "        # Print filtered names for debugging\n",
        "        print(f\"Filtered Image Names: {self.images}\")\n",
        "        print(f\"Filtered Mask Names: {self.masks}\")\n",
        "\n",
        "        # Create a dataset dictionary with image paths as keys and image, mask data\n",
        "        self.dataset = {}\n",
        "        for img_p in tqdm(self.images, desc=\"Loading images and masks\"):\n",
        "            img_name = os.path.splitext(img_p)[0]\n",
        "            img = cv2.imread(os.path.join(self.img_folder, img_p))\n",
        "            #################################################################################### image preprocess function goes here\n",
        "            img =   image_normalization(background_noise_removal(img))\n",
        "            mask_path = os.path.join(self.mask_folder, img_name + '_mask.png')  # Use .png as per your example\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Check if the mask is loaded correctly\n",
        "            if mask is None:\n",
        "                print(f\"Warning: Mask for {img_name} could not be loaded. Mask path: {mask_path}\")\n",
        "                continue\n",
        "\n",
        "            # Ensure images and masks have the same dimensions\n",
        "            if img is not None and img.shape[:2] != mask.shape[:2]:\n",
        "                print(f\"Image and mask sizes do not match for {img_p}. Resizing mask.\")\n",
        "                mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "\n",
        "            # Store the data\n",
        "            self.dataset[img_name] = {'image': img, 'mask': mask}\n",
        "\n",
        "        self.img_names = list(self.dataset.keys())\n",
        "\n",
        "        # Initialize the LabelEncoder\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        # Load tabular annotation data and add to the dataset\n",
        "        for annotation_df_path in tqdm(os.listdir(self.tabular_folder), desc=\"Loading tabular data\"):\n",
        "            # region = annotation_df_path.split('_', 1)[1].split('.')[0] ############################### to reduce annotation df size\n",
        "            if not annotation_df_path.startswith('annotation'):\n",
        "                continue\n",
        "\n",
        "            annotation_df = pd.read_csv(os.path.join(self.tabular_folder, annotation_df_path))\n",
        "            # Print column names for debugging\n",
        "            print(f\"Columns in {annotation_df_path}: {annotation_df.columns}\")\n",
        "\n",
        "            # Check if the expected 'quadratid' column exists, otherwise adjust\n",
        "            if 'quadratid' not in annotation_df.columns:\n",
        "                raise KeyError(f\"'quadratid' column not found in {annotation_df_path}. Available columns: {annotation_df.columns}\")\n",
        "\n",
        "            annotation_df['quadratid'] = annotation_df['quadratid'].astype(str)\n",
        "\n",
        "            # Fit the LabelEncoder on the func_group to convert them into integers\n",
        "            self.label_encoder.fit(annotation_df['func_group'].astype(str))  # Make sure to convert to string\n",
        "\n",
        "            # Match tabular data with image data in the dataset\n",
        "            for quadratid in annotation_df['quadratid'].unique():\n",
        "                if quadratid not in self.dataset:\n",
        "                    continue\n",
        "\n",
        "                # Ensure no duplicate quadratid with different data exists\n",
        "                if 'points' in self.dataset[quadratid]:\n",
        "                    raise Exception(\"Same quadrat, different datasets!\")\n",
        "\n",
        "                # Add points and point labels to the dataset\n",
        "                self.dataset[quadratid]['points'] = annotation_df[annotation_df['quadratid'] == quadratid][['x', 'y']].values\n",
        "                point_labels = annotation_df[annotation_df['quadratid'] == quadratid]['func_group'].values\n",
        "\n",
        "                # Transform labels to integers\n",
        "                self.dataset[quadratid]['point_labels'] = self.label_encoder.transform(point_labels)\n",
        "\n",
        "          ##  added to return quadratid's for later analysis ###########################################################################################\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_dict = self.dataset[self.img_names[idx]]\n",
        "        img, mask, keypoints, point_labels = img_dict['image'], img_dict['mask'], img_dict.get('points', None), img_dict.get('point_labels', None)\n",
        "\n",
        "        if self.transform:\n",
        "            transform = self.get_resize_transform(image_size=1024)\n",
        "            transformed = transform(image=img, mask=mask)\n",
        "            img, mask = transformed['image'], transformed['mask']\n",
        "\n",
        "        return img, mask, keypoints, point_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_resize_transform(image_size=1024):\n",
        "        return albumentations.Compose([\n",
        "            albumentations.Resize(\n",
        "                height=image_size,\n",
        "                width=image_size,\n",
        "                interpolation=cv2.INTER_AREA,\n",
        "                p=1,\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "\n",
        "# Helper functions for SAM\n",
        "def load_sam_model(sam_weights_path):\n",
        "    sam = sam_model_registry['vit_h'](checkpoint=sam_weights_path)\n",
        "    predictor = SamPredictor(sam)\n",
        "    return predictor\n",
        "\n",
        "\n",
        "# Metric calculation functions\n",
        "def calculate_iou(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return jaccard_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_pixel_accuracy(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "def calculate_dice_coefficient(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice_score = (2 * intersection) / (np.sum(y_true) + np.sum(y_pred))\n",
        "    return dice_score\n",
        "\n",
        "def calculate_precision(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return precision_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_recall(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return recall_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "\n",
        "def calculate_iou(true_mask, pred_mask, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    iou_scores = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Flatten the masks\n",
        "        true_flat = (true_mask.flatten() == class_id).astype(int)\n",
        "        pred_flat = (pred_mask.flatten() == class_id).astype(int)\n",
        "\n",
        "        # Calculate IoU for each class\n",
        "        iou = jaccard_score(true_flat, pred_flat, zero_division=0)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "    return np.mean(iou_scores)  # Return average IoU\n",
        "\n",
        "def calculate_dice(true_mask, pred_mask, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate Dice Coefficient for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    dice_scores = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Flatten the masks\n",
        "        true_flat = (true_mask.flatten() == class_id).astype(int)\n",
        "        pred_flat = (pred_mask.flatten() == class_id).astype(int)\n",
        "\n",
        "        # Calculate Dice for each class\n",
        "        dice = f1_score(true_flat, pred_flat, zero_division=0)\n",
        "        dice_scores.append(dice)\n",
        "\n",
        "    return np.mean(dice_scores)  # Return average Dice\n",
        "\n",
        "def main_pipeline(img_folder, mask_folder, tabular_folder, sam_weights_path):\n",
        "    # Initialize the DataLoader with your dataset\n",
        "    data_loader = DataLoader(\n",
        "        img_folder=img_folder,\n",
        "        mask_folder=mask_folder,\n",
        "        tabular_folder=tabular_folder,\n",
        "        transform=DataLoader.get_resize_transform(image_size=1024)\n",
        "    )\n",
        "\n",
        "    # Load the SAM model\n",
        "    sam_predictor = load_sam_model(sam_weights_path)\n",
        "\n",
        "    # Progress bar for the main pipeline\n",
        "    progress_bar = tqdm(range(len(data_loader)), desc=\"Processing images\")\n",
        "\n",
        "    # Lists to store metrics\n",
        "    iou_scores = []\n",
        "    dice_scores = []\n",
        "    pixel_scores = []\n",
        "    precision_scores = []\n",
        "\n",
        "    num_classes = 3  # Set this to the number of classes in your dataset\n",
        "\n",
        "    for idx in progress_bar:\n",
        "        img, mask, keypoints, point_labels = data_loader[idx]\n",
        "\n",
        "        # Check types of img and mask\n",
        "        print(f\"Processing image {idx}: type(img)={type(img)}, shape(img)={img.shape}, type(mask)={type(mask)}, shape(mask)={mask.shape}\")\n",
        "\n",
        "        # Check if keypoints and point_labels are not None\n",
        "        if keypoints is None or point_labels is None:\n",
        "            print(f\"Warning: Keypoints or point_labels are None for image {data_loader.img_names[idx]}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Use SAM to predict masks\n",
        "        sam_predictor.set_image(img)\n",
        "\n",
        "        try:\n",
        "\n",
        "\n",
        "\n",
        "            # Unpack predicted masks and any additional output (if any)\n",
        "            predicted_masks, *additional_outputs = sam_predictor.predict(point_coords=keypoints, point_labels=point_labels)\n",
        "\n",
        "            # Print shape of predicted masks for debugging\n",
        "            if isinstance(predicted_masks, tuple):\n",
        "                predicted_masks = predicted_masks[0]  # Extract the first element if it's a tuple\n",
        "\n",
        "            print(f\"Predicted masks shape: {predicted_masks.shape if predicted_masks is not None else 'None'}\")\n",
        "\n",
        "            # Check if the predicted masks are empty or None\n",
        "            if predicted_masks is None or predicted_masks.size == 0:\n",
        "                print(f\"Warning: No masks predicted for image {data_loader.img_names[idx]}. Skipping.\")\n",
        "                continue  # Skip this iteration if there's no prediction\n",
        "\n",
        "            # Convert predicted masks to uint8 if they are boolean\n",
        "            if predicted_masks.dtype == np.bool_:\n",
        "                predicted_masks = predicted_masks.astype(np.uint8) * 255  # Convert to uint8 format\n",
        "\n",
        "            # Reduce to a single channel if necessary (e.g., take the maximum across channels)\n",
        "            if predicted_masks.ndim == 3:  # If there are multiple channels\n",
        "                predicted_masks = np.max(predicted_masks, axis=0)  # Or apply any other reduction logic (e.g., averaging)\n",
        "                                                                   ######################################################## Might be aff evaluation\n",
        "\n",
        "            # Ensure predicted_masks is uint8 after max operation\n",
        "            predicted_masks = (predicted_masks > 0).astype(np.uint8) * 255  # Thresholding to get binary mask\n",
        "\n",
        "            # Resize predicted mask if necessary\n",
        "            if predicted_masks.shape != mask.shape:\n",
        "                print(f\"Resizing predicted masks from {predicted_masks.shape} to {mask.shape}\")\n",
        "                predicted_masks = cv2.resize(predicted_masks, (mask.shape[1], mask.shape[0]))\n",
        "\n",
        "            # Calculate metrics\n",
        "            iou = calculate_iou(mask, predicted_masks, num_classes)\n",
        "            # dice = calculate_dice(mask, predicted_masks, num_classes)\n",
        "            # pixel = calculate_pixel_accuracy(mask, predicted_masks)\n",
        "            # precision = calculate_precision(mask, predicted_masks)\n",
        "            # Store metrics\n",
        "            iou_scores.append(iou)\n",
        "            # dice_scores.append(dice)\n",
        "            # pixel_scores.append(pixel)\n",
        "            # precision_scores.append(precision)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {idx}: {str(e)}\")\n",
        "            continue  # Skip this iteration if there's an error\n",
        "\n",
        "        # Update progress description\n",
        "        progress_bar.set_description(f\"Processing image {idx + 1}/{len(data_loader)}\")\n",
        "\n",
        "\n",
        "    # Print average IoU ########################################################### Added other accuracy metrics\n",
        "    avg_iou = np.mean(iou_scores) if iou_scores else 0\n",
        "    avg_dice = np.mean(dice_scores) if dice_scores else 0\n",
        "    avg_pixel = np.mean(pixel_scores) if pixel_scores else 0\n",
        "    avg_precision = np.mean(precision_scores) if precision_scores else 0\n",
        "\n",
        "    # Print average metrics\n",
        "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Average Dice: {avg_dice:.4f}\")\n",
        "    print(f\"Average Pixel Accuracy: {avg_pixel:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "\n",
        "    print(\"Pipeline completed!\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "img_folder = img_folder\n",
        "mask_folder = mask_folder\n",
        "tabular_folder = tabular_folder\n",
        "sam_weights_path = SAM_WEIGHTS_PATH+SAM_WEIGHTS_FILE\n",
        "\n",
        "# Run the pipeline\n",
        "main_pipeline(img_folder, mask_folder, tabular_folder, sam_weights_path)\n",
        "\n",
        "# Offload GPU memory after processing\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KDUhDc-2WoNu",
      "metadata": {
        "id": "KDUhDc-2WoNu"
      },
      "source": [
        "# Best model on 500 corals from everywhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aDcrbZuUW6Yw",
      "metadata": {
        "id": "aDcrbZuUW6Yw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import jaccard_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from segment_anything import SamPredictor, sam_model_registry\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"\n",
        "    DataLoader class for loading, processing, and transforming image, mask, and tabular data.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_folder, mask_folder, tabular_folder, transform=None, load_fraction=0.25):\n",
        "        self.img_folder = img_folder\n",
        "        self.mask_folder = mask_folder\n",
        "        self.tabular_folder = tabular_folder\n",
        "        self.transform = transform\n",
        "        self.load_fraction = load_fraction\n",
        "\n",
        "        # Load image and mask data\n",
        "        self.images = sorted([f for f in os.listdir(self.img_folder) if f.endswith(('.jpg', '.png'))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_folder) if f.endswith(('.jpg', '.png'))])\n",
        "\n",
        "        #preproces image data\n",
        "\n",
        "\n",
        "\n",
        "        # Print image and mask names for debugging\n",
        "        print(f\"Image Names: {self.images}\")\n",
        "        print(f\"Mask Names: {self.masks}\")\n",
        "\n",
        "        # Create sets for comparison, stripping _mask suffix from masks\n",
        "        img_names_set = set([os.path.splitext(f)[0] for f in self.images])\n",
        "        mask_names_set = set([os.path.splitext(f)[0].replace('_mask', '') for f in self.masks])  # Ensure we handle '_mask'\n",
        "\n",
        "        # Find corresponding pairs\n",
        "        valid_img_names = img_names_set & mask_names_set\n",
        "        self.images = [f for f in self.images if os.path.splitext(f)[0] in valid_img_names]\n",
        "        self.masks = [f\"{os.path.splitext(f)[0]}_mask.png\" for f in self.images]  # Create the corresponding mask filenames\n",
        "\n",
        "        # Limit the number of images and masks loaded\n",
        "        num_images_to_load = int(len(self.images) * self.load_fraction)\n",
        "        self.images = self.images[:num_images_to_load]\n",
        "        self.masks = self.masks[:num_images_to_load]\n",
        "\n",
        "        # Print filtered names for debugging\n",
        "        print(f\"Filtered Image Names: {self.images}\")\n",
        "        print(f\"Filtered Mask Names: {self.masks}\")\n",
        "\n",
        "        # Create a dataset dictionary with image paths as keys and image, mask data\n",
        "        self.dataset = {}\n",
        "        for img_p in tqdm(self.images, desc=\"Loading images and masks\"):\n",
        "            img_name = os.path.splitext(img_p)[0]\n",
        "            img = cv2.imread(os.path.join(self.img_folder, img_p))\n",
        "            #################################################################################### image preprocess function goes here\n",
        "\n",
        "\n",
        "            mask_path = os.path.join(self.mask_folder, img_name + '_mask.png')  # Use .png as per your example\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Check if the mask is loaded correctly\n",
        "            if mask is None:\n",
        "                print(f\"Warning: Mask for {img_name} could not be loaded. Mask path: {mask_path}\")\n",
        "                continue\n",
        "\n",
        "            # Ensure images and masks have the same dimensions\n",
        "            if img is not None and img.shape[:2] != mask.shape[:2]:\n",
        "                print(f\"Image and mask sizes do not match for {img_p}. Resizing mask.\")\n",
        "                mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "\n",
        "            # Store the data\n",
        "            self.dataset[img_name] = {'image': img, 'mask': mask}\n",
        "\n",
        "        self.img_names = list(self.dataset.keys())\n",
        "\n",
        "        # Initialize the LabelEncoder\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        # Load tabular annotation data and add to the dataset\n",
        "        for annotation_df_path in tqdm(os.listdir(self.tabular_folder), desc=\"Loading tabular data\"):\n",
        "            region = annotation_df_path.split('_', 1)[1].split('.')[0] ######################## added AND to reduce datasize to the region(s) selected\n",
        "            if (not annotation_df_path.startswith('annotation')) and (region not in regions_of_interest):\n",
        "                continue\n",
        "\n",
        "            annotation_df = pd.read_csv(os.path.join(self.tabular_folder, annotation_df_path))\n",
        "            # Print column names for debugging\n",
        "            print(f\"Columns in {annotation_df_path}: {annotation_df.columns}\")\n",
        "\n",
        "            # Check if the expected 'quadratid' column exists, otherwise adjust\n",
        "            if 'quadratid' not in annotation_df.columns:\n",
        "                raise KeyError(f\"'quadratid' column not found in {annotation_df_path}. Available columns: {annotation_df.columns}\")\n",
        "\n",
        "            annotation_df['quadratid'] = annotation_df['quadratid'].astype(str)\n",
        "\n",
        "            # Fit the LabelEncoder on the func_group to convert them into integers\n",
        "            self.label_encoder.fit(annotation_df['func_group'].astype(str))  # Make sure to convert to string\n",
        "\n",
        "            # Match tabular data with image data in the dataset\n",
        "            for quadratid in annotation_df['quadratid'].unique():\n",
        "                if quadratid not in self.dataset:\n",
        "                    continue\n",
        "\n",
        "                # Ensure no duplicate quadratid with different data exists\n",
        "                if 'points' in self.dataset[quadratid]:\n",
        "                    raise Exception(\"Same quadrat, different datasets!\")\n",
        "\n",
        "                # Add points and point labels to the dataset\n",
        "                self.dataset[quadratid]['points'] = annotation_df[annotation_df['quadratid'] == quadratid][['x', 'y']].values\n",
        "                point_labels = annotation_df[annotation_df['quadratid'] == quadratid]['func_group'].values\n",
        "\n",
        "                # Transform labels to integers\n",
        "                self.dataset[quadratid]['point_labels'] = self.label_encoder.transform(point_labels)\n",
        "          ##  added to return quadratid's for later analysis ###########################################################################################\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_dict = self.dataset[self.img_names[idx]]\n",
        "        img, mask, keypoints, point_labels = img_dict['image'], img_dict['mask'], img_dict.get('points', None), img_dict.get('point_labels', None)\n",
        "\n",
        "        if self.transform:\n",
        "            transform = self.get_resize_transform(image_size=1024)\n",
        "            transformed = transform(image=img, mask=mask)\n",
        "            img, mask = transformed['image'], transformed['mask']\n",
        "\n",
        "        return img, mask, keypoints, point_labels\n",
        "\n",
        "    @staticmethod\n",
        "    def get_resize_transform(image_size=1024):\n",
        "        return albumentations.Compose([\n",
        "            albumentations.Resize(\n",
        "                height=image_size,\n",
        "                width=image_size,\n",
        "                interpolation=cv2.INTER_AREA,\n",
        "                p=1,\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "\n",
        "# Helper functions for SAM\n",
        "def load_sam_model(sam_weights_path):\n",
        "    sam = sam_model_registry['vit_h'](checkpoint=sam_weights_path)\n",
        "    predictor = SamPredictor(sam)\n",
        "    return predictor\n",
        "\n",
        "\n",
        "# Metric calculation functions\n",
        "def calculate_iou(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return jaccard_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_pixel_accuracy(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "def calculate_dice_coefficient(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    dice_score = (2 * intersection) / (np.sum(y_true) + np.sum(y_pred))\n",
        "    return dice_score\n",
        "\n",
        "def calculate_precision(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return precision_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_recall(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return recall_score(y_true, y_pred, average='binary')\n",
        "\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    return f1_score(y_true, y_pred, average='binary')\n",
        "\n",
        "\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "\n",
        "def calculate_iou(true_mask, pred_mask, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    iou_scores = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Flatten the masks\n",
        "        true_flat = (true_mask.flatten() == class_id).astype(int)\n",
        "        pred_flat = (pred_mask.flatten() == class_id).astype(int)\n",
        "\n",
        "        # Calculate IoU for each class\n",
        "        iou = jaccard_score(true_flat, pred_flat, zero_division=0)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "    return np.mean(iou_scores)  # Return average IoU\n",
        "\n",
        "def calculate_dice(true_mask, pred_mask, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate Dice Coefficient for multi-class segmentation.\n",
        "    \"\"\"\n",
        "    dice_scores = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Flatten the masks\n",
        "        true_flat = (true_mask.flatten() == class_id).astype(int)\n",
        "        pred_flat = (pred_mask.flatten() == class_id).astype(int)\n",
        "\n",
        "        # Calculate Dice for each class\n",
        "        dice = f1_score(true_flat, pred_flat, zero_division=0)\n",
        "        dice_scores.append(dice)\n",
        "\n",
        "    return np.mean(dice_scores)  # Return average Dice\n",
        "\n",
        "def main_pipeline(img_folder, mask_folder, tabular_folder, sam_weights_path):\n",
        "    # Initialize the DataLoader with your dataset\n",
        "    data_loader = DataLoader(\n",
        "        img_folder=img_folder,\n",
        "        mask_folder=mask_folder,\n",
        "        tabular_folder=tabular_folder,\n",
        "        transform=DataLoader.get_resize_transform(image_size=1024)\n",
        "    )\n",
        "\n",
        "    # Load the SAM model\n",
        "    sam_predictor = load_sam_model(sam_weights_path)\n",
        "\n",
        "    # Progress bar for the main pipeline\n",
        "    progress_bar = tqdm(range(len(data_loader)), desc=\"Processing images\")\n",
        "\n",
        "    # Lists to store metrics\n",
        "    iou_scores = []\n",
        "    dice_scores = []\n",
        "\n",
        "    num_classes = 3  # Set this to the number of classes in your dataset\n",
        "\n",
        "    for idx in progress_bar:\n",
        "        img, mask, keypoints, point_labels = data_loader[idx]\n",
        "\n",
        "        # Check types of img and mask\n",
        "        print(f\"Processing image {idx}: type(img)={type(img)}, shape(img)={img.shape}, type(mask)={type(mask)}, shape(mask)={mask.shape}\")\n",
        "\n",
        "        # Check if keypoints and point_labels are not None\n",
        "        if keypoints is None or point_labels is None:\n",
        "            print(f\"Warning: Keypoints or point_labels are None for image {data_loader.img_names[idx]}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Use SAM to predict masks\n",
        "        sam_predictor.set_image(img)\n",
        "\n",
        "        try:\n",
        "            # Unpack predicted masks and any additional output (if any)\n",
        "            predicted_masks, *additional_outputs = sam_predictor.predict(point_coords=keypoints, point_labels=point_labels)\n",
        "\n",
        "            # Print shape of predicted masks for debugging\n",
        "            if isinstance(predicted_masks, tuple):\n",
        "                predicted_masks = predicted_masks[0]  # Extract the first element if it's a tuple\n",
        "\n",
        "            print(f\"Predicted masks shape: {predicted_masks.shape if predicted_masks is not None else 'None'}\")\n",
        "\n",
        "            # Check if the predicted masks are empty or None\n",
        "            if predicted_masks is None or predicted_masks.size == 0:\n",
        "                print(f\"Warning: No masks predicted for image {data_loader.img_names[idx]}. Skipping.\")\n",
        "                continue  # Skip this iteration if there's no prediction\n",
        "\n",
        "            # Convert predicted masks to uint8 if they are boolean\n",
        "            if predicted_masks.dtype == np.bool_:\n",
        "                predicted_masks = predicted_masks.astype(np.uint8) * 255  # Convert to uint8 format\n",
        "\n",
        "            # Reduce to a single channel if necessary (e.g., take the maximum across channels)\n",
        "            if predicted_masks.ndim == 3:  # If there are multiple channels\n",
        "                predicted_masks = np.max(predicted_masks, axis=0)  # Or apply any other reduction logic (e.g., averaging)\n",
        "                                                                   ######################################################## Might be aff evaluation\n",
        "\n",
        "            # Ensure predicted_masks is uint8 after max operation\n",
        "            predicted_masks = (predicted_masks > 0).astype(np.uint8) * 255  # Thresholding to get binary mask\n",
        "\n",
        "            # Resize predicted mask if necessary\n",
        "            if predicted_masks.shape != mask.shape:\n",
        "                print(f\"Resizing predicted masks from {predicted_masks.shape} to {mask.shape}\")\n",
        "                predicted_masks = cv2.resize(predicted_masks, (mask.shape[1], mask.shape[0]))\n",
        "\n",
        "            # Calculate metrics\n",
        "            iou = calculate_iou(mask, predicted_masks, num_classes)\n",
        "            dice = calculate_dice_coefficient(mask, predicted_masks, num_classes)\n",
        "            pixel = calculate_pixel_accuracy(mask, predicted_masks)\n",
        "            precision = calculate_precision(mask, predicted_masks)\n",
        "            # Store metrics\n",
        "            iou_scores.append(iou)\n",
        "            dice_scores.append(dice)\n",
        "            pixel_scores.append(pixel)\n",
        "            precision_scores.append(precision)\n",
        "\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {idx}: {str(e)}\")\n",
        "            continue  # Skip this iteration if there's an error\n",
        "\n",
        "        # Update progress description\n",
        "        progress_bar.set_description(f\"Processing image {idx + 1}/{len(data_loader)}\")\n",
        "\n",
        "        ####################################################################### Save predicted segmentation mask\n",
        "        output_folder = os.path.join(PROJECT_ROOT_DIR, 'mask_predictions')\n",
        "        quadratid = data_loader[idx]['quadratid']\n",
        "        pred_mask_name = quadratid+'_BASELINE.jpg'  # Each image gets a unique name\n",
        "        masks_stitched = predicted_masks # I think they get stitched by the averaging but I am not sure >>>> change this if it does not work\n",
        "        cv2.imwrite(os.path.join(output_folder, image_name), masks_stitched)\n",
        "        progress_bar.set_description(f\"mask saved!\")\n",
        "\n",
        "    # Print average IoU\n",
        "    avg_iou = np.mean(iou_scores) if iou_scores else 0\n",
        "    avg_dice = np.mean(dice_scores) if dice_scores else 0\n",
        "    avg_pixel = np.mean(pixel_scores) if pixel_scores else 0\n",
        "    avg_precision = np.mean(precision_scores) if precision_scores else 0\n",
        "\n",
        "    # Print average metrics\n",
        "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
        "    print(f\"Average Dice: {avg_dice:.4f}\")\n",
        "    print(f\"Average Pixel Accuracy: {avg_pixel:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "\n",
        "    print(\"Pipeline completed!\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "img_folder = img_folder\n",
        "mask_folder = mask_folder\n",
        "tabular_folder = tabular_folder\n",
        "sam_weights_path = SAM_WEIGHTS_PATH+SAM_WEIGHTS_FILE\n",
        "\n",
        "# Run the pipeline\n",
        "main_pipeline(img_folder, mask_folder, tabular_folder, sam_weights_path)\n",
        "\n",
        "# Offload GPU memory after processing\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08f2f7bbdbb44be28cbd8c00e0e2ecd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b0239257a0f45819d8aae495e3322cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12accf86d1774dfeb9f3664b9fbce056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1925817e478840f18b8a411c3b88ee82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378002232983485499b5d1e051e04540": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f6a9768a26142059e801bed0ffeebc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fadc2270a44945f495b1abe293ceaa3a",
            "placeholder": "​",
            "style": "IPY_MODEL_7853f5c23f9344c1813f559ca8a8d608",
            "value": "Loading tabular data: 100%"
          }
        },
        "404ff73cd600453d9a9c7c1f7c4bebae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c911da243a476390272aa42d199f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f6a9768a26142059e801bed0ffeebc1",
              "IPY_MODEL_e2218963a5344258a2973cd8816d4942",
              "IPY_MODEL_971b3663cefe45a9999e65fc1b74a4aa"
            ],
            "layout": "IPY_MODEL_e7d0129de86b4737acb5de07eba19340"
          }
        },
        "4294203710734b35b5d24ee743d3eaba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a84abce446c4c22aee7efbe6b1439a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c8f21c0fe1144ccaf9a375bd519fe62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12accf86d1774dfeb9f3664b9fbce056",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4ed2f12dfd5411db4dc90a2cccd6e5d",
            "value": 0
          }
        },
        "4ff2d86a68f742db857bbd05831b5fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a980873adb4e5090138b27fd61ce39",
            "placeholder": "​",
            "style": "IPY_MODEL_08f2f7bbdbb44be28cbd8c00e0e2ecd1",
            "value": " 0/6 [00:00&lt;?, ?it/s]"
          }
        },
        "56746e56da174f0fb7c3edb304375cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "601c418e2a664da0befea67187f64720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0239257a0f45819d8aae495e3322cc",
            "placeholder": "​",
            "style": "IPY_MODEL_378002232983485499b5d1e051e04540",
            "value": "Processing images:   0%"
          }
        },
        "63cb7cd9b7b04637adaa7b388e54ea97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1925817e478840f18b8a411c3b88ee82",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9368408ef4e44e6a9b37db6ac7c6eb0c",
            "value": 6
          }
        },
        "7853f5c23f9344c1813f559ca8a8d608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "896eccdc94f34240b49a46293ef14d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_601c418e2a664da0befea67187f64720",
              "IPY_MODEL_4c8f21c0fe1144ccaf9a375bd519fe62",
              "IPY_MODEL_4ff2d86a68f742db857bbd05831b5fab"
            ],
            "layout": "IPY_MODEL_404ff73cd600453d9a9c7c1f7c4bebae"
          }
        },
        "8fd61bdc53e14adaa05f2001f2604536": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd2fe9858c914a2390b1555f6f606c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_fcdedcb39841432fb45347ee2b22f8f9",
            "value": " 6/6 [00:30&lt;00:00,  5.01s/it]"
          }
        },
        "9368408ef4e44e6a9b37db6ac7c6eb0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "971b3663cefe45a9999e65fc1b74a4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56746e56da174f0fb7c3edb304375cbc",
            "placeholder": "​",
            "style": "IPY_MODEL_a9eed42cceee47ef8b79fe4e1eece0a0",
            "value": " 18/18 [00:21&lt;00:00,  2.27s/it]"
          }
        },
        "a273bdbd826e4f3da4d8e54bf5210932": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9eed42cceee47ef8b79fe4e1eece0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ed2f12dfd5411db4dc90a2cccd6e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c43fe2e503474b54a96f9a9e129fa24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c93af692dd024493b00df581958ff546",
              "IPY_MODEL_63cb7cd9b7b04637adaa7b388e54ea97",
              "IPY_MODEL_8fd61bdc53e14adaa05f2001f2604536"
            ],
            "layout": "IPY_MODEL_4294203710734b35b5d24ee743d3eaba"
          }
        },
        "c73a07cf7b10420db2942fc35f246c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c93af692dd024493b00df581958ff546": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0aa74a0554b402eae1cbbbfdeab7b9a",
            "placeholder": "​",
            "style": "IPY_MODEL_c73a07cf7b10420db2942fc35f246c85",
            "value": "Loading images and masks: 100%"
          }
        },
        "d0aa74a0554b402eae1cbbbfdeab7b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2fe9858c914a2390b1555f6f606c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2218963a5344258a2973cd8816d4942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a273bdbd826e4f3da4d8e54bf5210932",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a84abce446c4c22aee7efbe6b1439a0",
            "value": 18
          }
        },
        "e5a980873adb4e5090138b27fd61ce39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7d0129de86b4737acb5de07eba19340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fadc2270a44945f495b1abe293ceaa3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcdedcb39841432fb45347ee2b22f8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}