{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e23a0cb2f49f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier( r\"E:\\opencv-master\\data\\haarcascades\\haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(r\"E:\\opencv-master\\data\\haarcascades\\haarcascade_eye.xml\")\n",
    "smile_cascade = cv2.CascadeClassifier(r\"E:\\opencv-master\\data\\haarcascades\\haarcascade_smile.xml\")\n",
    "\n",
    "##img = cv2.imread(r'C:\\Users\\SUPRATIK\\Pictures\\2017-05\\two_friends.jpg')\n",
    "##print (type(image))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray  = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    if(len(faces) != 0):\n",
    "        for (x,y,w,h) in faces:\n",
    "                 cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                 #print(img)\n",
    "                 roi_gray = gray[y:y+h, x:x+w]\n",
    "                 roi_color = frame[y:y+h, x:x+w]\n",
    "                 eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "                 smiles = smile_cascade.detectMultiScale(roi_gray,scaleFactor=1.3,\n",
    "                                                     minNeighbors=22,\n",
    "                                                     minSize=(25,25))\n",
    "                 for (ex,ey,ew,eh) in eyes:\n",
    "                     cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "                 for (ex,ey,ew,eh) in smiles:\n",
    "                     print(\"found smile \",end=\"\")\n",
    "                     cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,0,250),2)\n",
    "                 cv2.imshow('Test image',frame)\n",
    "    else:\n",
    "        cv2.imshow('Test image',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') :\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'StringIO'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-466462b9adc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mStringIO\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefault_timer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'StringIO'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import base64\n",
    "import json\n",
    "import numpy as np\n",
    "from StringIO import StringIO\n",
    "from timeit import default_timer as timer\n",
    "from PIL import Image\n",
    "import datetime as dt\n",
    "from random import randint\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "class Spark_Object_Detector():\n",
    "\n",
    "    def __init__(self,\n",
    "                 interval=10,\n",
    "                 model_file='',\n",
    "                 labels_file='',\n",
    "                 number_classes=90,\n",
    "                 detect_treshold=.5,\n",
    "                 topic_to_consume='input_topic',\n",
    "                 topic_for_produce='result_topic',\n",
    "                 kafka_endpoint='127.0.0.1:9092'):\n",
    "        self.topic_to_consume = topic_to_consume\n",
    "        self.topic_for_produce = topic_for_produce\n",
    "        self.kafka_endpoint = kafka_endpoint\n",
    "        self.treshold = detect_treshold\n",
    "        self.v_sectors = ['top', 'middle', 'bottom']\n",
    "        self.h_sectors = ['left', 'center', 'right']\n",
    "\n",
    "        self.producer = KafkaProducer(bootstrap_servers=kafka_endpoint)\n",
    "\n",
    "        label_map = label_map_util.load_labelmap(labels_file)\n",
    "        categories = label_map_util.convert_label_map_to_categories(label_map,\n",
    "                                                                    max_num_classes=number_classes,\n",
    "                                                                    use_display_name=True\n",
    "                                                                    )\n",
    "        self.category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "        sc = SparkContext(appName='PyctureStream')\n",
    "        self.ssc = StreamingContext(sc, interval)\n",
    "\n",
    "        log4jLogger = sc._jvm.org.apache.log4j\n",
    "        log_level = log4jLogger.Level.ERROR\n",
    "        log4jLogger.LogManager.getLogger('org').setLevel(log_level)\n",
    "        log4jLogger.LogManager.getLogger('kafka').setLevel(log_level)\n",
    "        self.logger = log4jLogger.LogManager.getLogger(__name__)\n",
    "\n",
    "        with tf.gfile.FastGFile(model_file, 'rb') as f:\n",
    "            model_data = f.read()\n",
    "        self.model_data_bc = sc.broadcast(model_data)\n",
    "        self.graph_def = tf.GraphDef()\n",
    "        self.graph_def.ParseFromString(self.model_data_bc.value)\n",
    "\n",
    "    def start_processing(self):\n",
    "        kvs = KafkaUtils.createDirectStream(self.ssc,\n",
    "                                            [self.topic_to_consume],\n",
    "                                            {'metadata.broker.list': self.kafka_endpoint}\n",
    "                                            )\n",
    "        kvs.foreachRDD(self.handler)\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "\n",
    "    def load_image_into_numpy_array(self, image):\n",
    "        (im_width, im_height) = image.size\n",
    "        return np.array(image.getdata()).reshape(\n",
    "            (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "    def box_to_sector(self, box):\n",
    "        width = box[3] - box[1]\n",
    "        h_pos = box[1] + width / 2.0\n",
    "        height = box[2] - box[0]\n",
    "        v_pos = box[0] + height / 2.0\n",
    "        h_sector = min(int(h_pos * 3), 2)\n",
    "        v_sector = min(int(v_pos * 3), 2)\n",
    "        return (self.v_sectors[v_sector], self.h_sectors[h_sector])\n",
    "\n",
    "    def get_annotated_image_as_text(self, image_np, output):\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output['detection_boxes'],\n",
    "            output['detection_classes'],\n",
    "            output['detection_scores'],\n",
    "            self.category_index,\n",
    "            instance_masks=output.get('detection_masks'),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=3)\n",
    "\n",
    "        img = Image.fromarray(image_np)\n",
    "        text_stream = StringIO()\n",
    "        img.save(text_stream, 'JPEG')\n",
    "        contents = text_stream.getvalue()\n",
    "        text_stream.close()\n",
    "        img_as_text = base64.b64encode(contents).decode('utf-8')\n",
    "        return img_as_text\n",
    "\n",
    "    def format_object_desc(self, output):\n",
    "        objs = []\n",
    "        for i in range(len(output['detection_classes'])):\n",
    "            score = round(output['detection_scores'][i], 2)\n",
    "            if score > self.treshold:\n",
    "                cat_id = output['detection_classes'][i]\n",
    "                label = self.category_index[cat_id]['name']\n",
    "\n",
    "                box = output['detection_boxes'][i]\n",
    "\n",
    "                objs.append({\n",
    "                    'label': label,\n",
    "                    'score': score,\n",
    "                    'sector': self.box_to_sector(box)\n",
    "                })\n",
    "        return objs\n",
    "\n",
    "    def detect_objects(self, event):\n",
    "        decoded = base64.b64decode(event['image'])\n",
    "        stream = StringIO(decoded)\n",
    "        image = Image.open(stream)\n",
    "        image_np = self.load_image_into_numpy_array(image)\n",
    "        stream.close()\n",
    "\n",
    "        tf.import_graph_def(self.graph_def, name='')  # ~ 2.7 sec\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {\n",
    "                output.name for op in ops for output in op.outputs\n",
    "            }\n",
    "            tensor_dict = {}\n",
    "            for key in ['num_detections',\n",
    "                        'detection_boxes',\n",
    "                        'detection_scores',\n",
    "                        'detection_classes',\n",
    "                        'detection_masks'\n",
    "                        ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = (tf.get_default_graph()\n",
    "                                          .get_tensor_by_name(tensor_name)\n",
    "                                        )\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                detection_boxes = tf.squeeze(\n",
    "                    tensor_dict['detection_boxes'], [0]\n",
    "                )\n",
    "                detection_masks = tf.squeeze(\n",
    "                    tensor_dict['detection_masks'], [0]\n",
    "                )\n",
    "                real_num_detection = tf.cast(\n",
    "                    tensor_dict['num_detections'][0], tf.int32\n",
    "                )\n",
    "                detection_boxes = tf.slice(\n",
    "                    detection_boxes,\n",
    "                    [0, 0],\n",
    "                    [real_num_detection, -1]\n",
    "                )\n",
    "                detection_masks = tf.slice(\n",
    "                    detection_masks,\n",
    "                    [0, 0, 0],\n",
    "                    [real_num_detection, -1, -1]\n",
    "                )\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks,\n",
    "                    detection_boxes,\n",
    "                    image.shape[0],\n",
    "                    image.shape[1]\n",
    "                )\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5),\n",
    "                    tf.uint8\n",
    "                )\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "            image_tensor = (tf.get_default_graph()\n",
    "                            .get_tensor_by_name('image_tensor:0')\n",
    "                            )\n",
    "\n",
    "            output = sess.run(\n",
    "                tensor_dict,\n",
    "                feed_dict={image_tensor: np.expand_dims(image, 0)}\n",
    "            )\n",
    "\n",
    "            output['num_detections'] = int(output['num_detections'][0])\n",
    "            output['detection_classes'] = output['detection_classes'][0].astype(\n",
    "                np.uint8)\n",
    "            output['detection_boxes'] = output['detection_boxes'][0]\n",
    "            output['detection_scores'] = output['detection_scores'][0]\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        result = {'timestamp': event['timestamp'],\n",
    "                  'camera_id': event['camera_id'],\n",
    "                  'objects': self.format_object_desc(output),\n",
    "                  'image': self.get_annotated_image_as_text(image_np, output)\n",
    "                  }\n",
    "        return json.dumps(result)\n",
    "\n",
    "    def handler(self, timestamp, message):\n",
    "        records = message.collect()\n",
    "        to_process = {}\n",
    "        self.logger.info( '\\033[3' + str(randint(1, 7)) + ';1m' +  # Color\n",
    "            '-' * 25 +\n",
    "            '[ NEW MESSAGES: ' + str(len(records)) + ' ]'\n",
    "            + '-' * 25 +\n",
    "            '\\033[0m'\n",
    "            )\n",
    "        dt_now = dt.datetime.now()\n",
    "        for record in records:\n",
    "            event = json.loads(record[1])\n",
    "            self.logger.info('Received Message: ' +\n",
    "                             event['camera_id'] + ' - ' + event['timestamp'])\n",
    "            dt_event = dt.datetime.strptime(\n",
    "                event['timestamp'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "            delta = dt_now - dt_event\n",
    "            if delta.seconds > 5:\n",
    "                continue\n",
    "            to_process[event['camera_id']] = event\n",
    "\n",
    "        if len(to_process) == 0:\n",
    "            self.logger.info('Skipping processing...')\n",
    "\n",
    "        for key, event in to_process.items():\n",
    "            self.logger.info('Processing Message: ' +\n",
    "                             event['camera_id'] + ' - ' + event['timestamp'])\n",
    "            start = timer()\n",
    "            detection_result = self.detect_objects(event)\n",
    "            end = timer()\n",
    "            delta = end - start\n",
    "            self.logger.info('Done after ' + str(delta) + ' seconds.')\n",
    "            self.producer.send(self.topic_for_produce, detection_result)\n",
    "            self.producer.flush()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sod = Spark_Object_Detector(\n",
    "        interval=3,\n",
    "        model_file='/home/cloudera/smartcctv-master/frozen_inference_graph.pb',\n",
    "        labels_file='/home/cloudera/smartcctv-master/mscoco_label_map.pbtxt',\n",
    "        number_classes=90,\n",
    "        detect_treshold=.5,\n",
    "        topic_to_consume='input_topic',\n",
    "        topic_for_produce='result_topic',\n",
    "        kafka_endpoint='127.0.0.1:9092')\n",
    "    sod.start_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-af82488e6612>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-af82488e6612>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    face_cas = cv2.CascadeClassifier ('-File name.xml-')\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#import OpenCV module\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#function to detect face\n",
    "def detect_face (img):\n",
    "#convert the test image to gray image\n",
    "gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)\n",
    "#load OpenCV face detector\n",
    "face_cas = cv2.CascadeClassifier ('-File name.xml-')\n",
    "faces = face_cas.detectMultiScale (gray, scaleFactor=1.3, minNeighbors=4);\n",
    "#if no faces are detected then return image\n",
    "if (len (faces) == 0):\n",
    "return None, None\n",
    "#extract the face\n",
    "faces [0]=(x, y, w, h)\n",
    "#return only the face part\n",
    "return gray[y: y+w, x: x+h], faces [0]\n",
    "#this function will read all persons' training images, detect face #from each image\n",
    "#and will return two lists of exactly same size, one list\n",
    "def prepare_training_data(data_folder_path):\n",
    "#------STEP-1--------\n",
    "#get the directories (one directory for each subject) in data folder\n",
    "dirs = os.listdir(data_folder_path)\n",
    "faces = []\n",
    "labels = []\n",
    "for dir_name in dirs:\n",
    "#our subject directories start with letter 's' so\n",
    "#ignore any non-relevant directories if any\n",
    "if not dir_name.startswith(\"s\"):\n",
    "continue;\n",
    "#------STEP-2--------\n",
    "#extract label number of subject from dir_name\n",
    "#format of dir name = slabel\n",
    "#, so removing letter 's' from dir_name will give us label\n",
    "label = int(dir_name.replace(\"s\", \"\"))\n",
    "#build path of directory containin images for current subject subject\n",
    "#sample subject_dir_path = \"training-data/s1\"\n",
    "subject_dir_path = data_folder_path + \"/\" + dir_name\n",
    "#get the images names that are inside the given subject directory\n",
    "subject_images_names = os.listdir(subject_dir_path)\n",
    "#------STEP-3--------\n",
    "#go through each image name, read image,\n",
    "#detect face and add face to list of faces\n",
    "for image_name in subject_images_names:\n",
    "#ignore system files like .DS_Store\n",
    "if image_name.startswith(\".\"):\n",
    "continue;\n",
    "#build image path\n",
    "#sample image path = training-data/s1/1.pgm\n",
    "image_path = subject_dir_path + \"/\" + image_name\n",
    "#read image\n",
    "image = cv2.imread(image_path)\n",
    "#display an image window to show the image\n",
    "cv2.imshow(\"Training on image...\", image)\n",
    "cv2.waitKey(100)\n",
    "#detect face\n",
    "face, rect = detect_face(image)\n",
    "#------STEP-4--------\n",
    "#we will ignore faces that are not detected\n",
    "if face is not None:\n",
    "#add face to list of faces\n",
    "faces.append(face)\n",
    "#add label for this face\n",
    "labels.append(label)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "return faces, labels\n",
    "#let's first prepare our training data\n",
    "#data will be in two lists of same size\n",
    "#one list will contain all the faces\n",
    "#and other list will contain respective labels for each face\n",
    "print(\"Preparing data...\")\n",
    "faces, labels = prepare_training_data(\"training-data\")\n",
    "print(\"Data prepared\")\n",
    "#print total faces and labels\n",
    "print(\"Total faces: \", len(faces))\n",
    "print(\"Total labels: \", len(labels))\n",
    "#create our LBPH face recognizer\n",
    "face_recognizer = cv2.face.createLBPHFaceRecognizer()\n",
    "#train our face recognizer of our training faces\n",
    "face_recognizer.train(faces, np.array(labels))\n",
    "#function to draw rectangle on image\n",
    "#according to given (x, y) coordinates and\n",
    "#given width and heigh\n",
    "def draw_rectangle(img, rect):\n",
    "(x, y, w, h) = rect\n",
    "cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "#function to draw text on give image starting from\n",
    "#passed (x, y) coordinates.\n",
    "def draw_text(img, text, x, y):\n",
    "cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "#this function recognizes the person in image passed\n",
    "#and draws a rectangle around detected face with name of the subject\n",
    "def predict(test_img):\n",
    "#make a copy of the image as we don't want to chang original image\n",
    "img = test_img.copy()\n",
    "#detect face from the image\n",
    "face, rect = detect_face(img)\n",
    "#predict the image using our face recognizer\n",
    "label= face_recognizer.predict(face)\n",
    "#get name of respective label returned by face recognizer\n",
    "label_text = subjects[label]\n",
    "#draw a rectangle around face detected\n",
    "draw_rectangle(img, rect)\n",
    "#draw name of predicted person\n",
    "draw_text(img, label_text, rect[0], rect[1]-5)\n",
    "return img\n",
    "#load test images\n",
    "test_img1 = cv2.imread(\"test-data/test1.jpg\")\n",
    "test_img2 = cv2.imread(\"test-data/test2.jpg\")\n",
    "#perform a prediction\n",
    "predicted_img1 = predict(test_img1)\n",
    "predicted_img2 = predict(test_img2)\n",
    "print(\"Prediction complete\")\n",
    "#create a figure of 2 plots (one for each test image)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#display test image1 result\n",
    "ax1.imshow(cv2.cvtColor(predicted_img1, cv2.COLOR_BGR2RGB))\n",
    "#display test image2 result\n",
    "ax2.imshow(cv2.cvtColor(predicted_img2, cv2.COLOR_BGR2RGB))\n",
    "#display both images\n",
    "cv2.imshow(\"Tom cruise test\", predicted_img1)\n",
    "cv2.imshow(\"Shahrukh Khan test\", predicted_img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0f02e7e467ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import the libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# make a list of all the available images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import face_recognition\n",
    "\n",
    "# make a list of all the available images\n",
    "images = os.listdir('images')\n",
    "\n",
    "# load your image\n",
    "image_to_be_matched = face_recognition.load_image_file('my_image.jpg')\n",
    "\n",
    "# encoded the loaded image into a feature vector\n",
    "image_to_be_matched_encoded = face_recognition.face_encodings(\n",
    "    image_to_be_matched)[0]\n",
    "\n",
    "# iterate over each image\n",
    "for image in images:\n",
    "    # load the image\n",
    "    current_image = face_recognition.load_image_file(\"images/\" + image)\n",
    "    # encode the loaded image into a feature vector\n",
    "    current_image_encoded = face_recognition.face_encodings(current_image)[0]\n",
    "    # match your image with the image and check if it matches\n",
    "    result = face_recognition.compare_faces(\n",
    "        [image_to_be_matched_encoded], current_image_encoded)\n",
    "    # check if it was a match\n",
    "    if result[0] == True:\n",
    "        print (\"Matched: \" + image)\n",
    "    else:\n",
    "        print (\"Not matched: \" + image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
