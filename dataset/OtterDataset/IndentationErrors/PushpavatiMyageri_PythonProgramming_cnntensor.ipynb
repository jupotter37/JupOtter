{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m \n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexamples\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtutorials\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmnist\u001b[39;00m \u001b[39mimport\u001b[39;00m input_data\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow.examples.tutorials.mnist import input_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(): \n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) \n",
    "    learning_rate = 0.0001 \n",
    "    epochs = 10 \n",
    "    batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mInput(tf\u001b[39m.\u001b[39;49mfloat32, [\u001b[39mNone\u001b[39;49;00m, \u001b[39m784\u001b[39;49m]) \n\u001b[0;32m      2\u001b[0m x_shaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(x, [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m1\u001b[39m]) \n\u001b[0;32m      3\u001b[0m y \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mplaceholder(tf\u001b[39m.\u001b[39mfloat32, [\u001b[39mNone\u001b[39;00m, \u001b[39m10\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\User\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_layer.py:128\u001b[0m, in \u001b[0;36mInputLayer.__init__\u001b[1;34m(self, input_shape, batch_size, dtype, input_tensor, sparse, name, ragged, type_spec, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    124\u001b[0m     strategy\n\u001b[0;32m    125\u001b[0m     \u001b[39mand\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[39mand\u001b[39;00m distributed_training_utils\u001b[39m.\u001b[39mglobal_batch_size_supported(strategy)\n\u001b[0;32m    127\u001b[0m ):\n\u001b[1;32m--> 128\u001b[0m     \u001b[39mif\u001b[39;00m batch_size \u001b[39m%\u001b[39;49m strategy\u001b[39m.\u001b[39;49mnum_replicas_in_sync \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    129\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe `batch_size` argument (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be divisible by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe number of replicas (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    132\u001b[0m                 batch_size, strategy\u001b[39m.\u001b[39mnum_replicas_in_sync\n\u001b[0;32m    133\u001b[0m             )\n\u001b[0;32m    134\u001b[0m         )\n\u001b[0;32m    135\u001b[0m     batch_size \u001b[39m=\u001b[39m batch_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m strategy\u001b[39m.\u001b[39mnum_replicas_in_sync\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for %: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "x = tf.keras.Input(tf.float32, [None, 784]) \n",
    "x_shaped = tf.reshape(x, [-1, 28, 28, 1]) \n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_new_conv_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m layer1 \u001b[39m=\u001b[39m create_new_conv_layer(x_shaped, \u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m, [\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m], [\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m], name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlayer1\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m      2\u001b[0m layer2 \u001b[39m=\u001b[39m create_new_conv_layer(layer1, \u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, [\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m], [\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m], name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlayer2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_new_conv_layer' is not defined"
     ]
    }
   ],
   "source": [
    "layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1') \n",
    "layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (741579789.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [7], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64]) \n",
    " \n",
    "   wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1') \n",
    "   bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1') \n",
    "   dense_layer1 = tf.matmul(flattened, wd1) + bd1 \n",
    "   dense_layer1 = tf.nn.relu(dense_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2') \n",
    "bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2') \n",
    "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2 \n",
    "y_ = tf.nn.softmax(dense_layer2) \n",
    " \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y)) \n",
    " \n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy) \n",
    " \n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    " \n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('accuracy', accuracy) \n",
    " \n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('E:\\TensorFlowProject') \n",
    "with tf.Session() as sess: \n",
    " \n",
    "    sess.run(init_op) \n",
    "    total_batch = int(len(mnist.train.labels) / batch_size) \n",
    "    for epoch in range(epochs): \n",
    "            avg_cost = 0 \n",
    "        for i in range(total_batch): \n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size) \n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y}) \n",
    "            avg_cost += c / total_batch \n",
    "            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}) \n",
    "            summary = sess.run(merged, feed_dict={x: mnist.test.images, y: mnist.test.labels}) \n",
    "            writer.add_summary(summary, epoch) \n",
    " \n",
    "    print(\"\\nTraining complete!\") \n",
    "    writer.add_graph(sess.graph) \n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})) \n",
    " \n",
    "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name): \n",
    "  \n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters] \n",
    " \n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03), name=name+'_W') \n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b') \n",
    " \n",
    "#Out layer defines the output \n",
    "    out_layer =tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME') \n",
    " \n",
    "    out_layer += bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layer = tf.nn.relu(out_layer) \n",
    "ksize = [1, pool_shape[0], pool_shape[1], 1] \n",
    "strides = [1, 2, 2, 1] \n",
    "out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, padding='SAME') \n",
    "return out_layer \n",
    " \n",
    "if __name__ == \"__main__\": \n",
    "    run_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7bf67dc1677b68e4e57316c86db4d069ff1179e8cda4c0b936eefcdea412afe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
