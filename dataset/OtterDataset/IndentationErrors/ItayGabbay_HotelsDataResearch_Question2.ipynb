{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the spark path \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"hotels\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "hotels_df = spark.read.csv(\"../input/Hotels_data_Changed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the highest discount code for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------+--------------------+------------+-------+\n",
      "|CheckinDate|DayDiff|DiscountCode|           HotelName|SnapshotDate|WeekDay|\n",
      "+-----------+-------+------------+--------------------+------------+-------+\n",
      "| 2015-08-12|     26|           2|Best Western Plus...|  2015-07-17|    Wed|\n",
      "| 2015-08-19|     33|           2|Best Western Plus...|  2015-07-17|    Wed|\n",
      "| 2015-08-13|     27|           2|The Peninsula New...|  2015-07-17|    Thu|\n",
      "| 2015-07-26|      9|           1|Eventi Hotel a Ki...|  2015-07-17|    Sun|\n",
      "| 2015-08-12|     26|           2|Eventi Hotel a Ki...|  2015-07-17|    Wed|\n",
      "| 2015-08-07|     21|           1|Grand Hyatt New York|  2015-07-17|    Fri|\n",
      "| 2015-08-09|     23|           1|Grand Hyatt New York|  2015-07-17|    Sun|\n",
      "| 2015-08-12|     26|           1|Grand Hyatt New York|  2015-07-17|    Wed|\n",
      "| 2015-08-13|     27|           3|Grand Hyatt New York|  2015-07-17|    Thu|\n",
      "| 2015-07-22|      5|           2|Hilton New York F...|  2015-07-17|    Wed|\n",
      "| 2015-07-30|     13|           2|DoubleTree Suites...|  2015-07-17|    Thu|\n",
      "| 2015-07-31|     14|           1|DoubleTree Suites...|  2015-07-17|    Fri|\n",
      "| 2015-07-26|      9|           1|Hampton Inn Manha...|  2015-07-17|    Sun|\n",
      "| 2015-08-02|     16|           1|Hampton Inn Manha...|  2015-07-17|    Sun|\n",
      "| 2015-07-22|      5|           2| Park Hyatt New York|  2015-07-17|    Wed|\n",
      "| 2015-07-22|      5|           3| Park Hyatt New York|  2015-07-17|    Wed|\n",
      "| 2015-07-23|      6|           3| Park Hyatt New York|  2015-07-17|    Thu|\n",
      "| 2015-07-24|      7|           2| Park Hyatt New York|  2015-07-17|    Fri|\n",
      "| 2015-07-24|      7|           3| Park Hyatt New York|  2015-07-17|    Fri|\n",
      "| 2015-07-25|      8|           2| Park Hyatt New York|  2015-07-17|    Sat|\n",
      "+-----------+-------+------------+--------------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def rowToKeyValue(row):\n",
    "    key = (row['WeekDay'], row[\"Snapshot Date\"], row[\"Checkin Date\"], row[\"DayDiff\"], row[\"Hotel Name\"])\n",
    "    val = ([row[\"Discount Code\"]], row['DiscountPerc'])\n",
    "    return (key,val)\n",
    "\n",
    "def reduceToMaxDiscountPerKey(val1, val2):\n",
    "    codes1, discount1 = val1\n",
    "    codes2, discount2 = val2\n",
    "    if (discount1 > discount2):\n",
    "        return val1\n",
    "    elif(discount2 > discount1):\n",
    "        return val2\n",
    "    else: # In case the discounts are equals, merge the prices to same array\n",
    "        return (codes1+ codes2, discount1)\n",
    "\n",
    "def flatMapDiscountCodes(row):\n",
    "    key, val = row\n",
    "    codes = val[0]\n",
    "    # Return list of key & code\n",
    "    return [(key, code) for code in codes]\n",
    "\n",
    "def rddToRow(rddRow):\n",
    "    return Row(WeekDay=rddRow[0][0], SnapshotDate=rddRow[0][1], CheckinDate=rddRow[0][2],\\\n",
    "                DayDiff=rddRow[0][3], HotelName=rddRow[0][4], DiscountCode=rddRow[1])\n",
    "\n",
    "hotelsBestDiscountCode_df = hotels_df.rdd\\\n",
    "                .map(rowToKeyValue)\\\n",
    "                .reduceByKey(reduceToMaxDiscountPerKey)\\\n",
    "                .flatMap(flatMapDiscountCodes)\\\n",
    "                .map(rddToRow).toDF()\n",
    "hotelsBestDiscountCode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------+--------------+----------------+-----------------+---------------+---------------+----------------+--------------+\n",
      "|DayDiff|DiscountCode|WeekDayIndex|HotelNameIndex|SnapshotDateYear|SnapshotDateMonth|SnapshotDateDay|CheckinDateYear|CheckinDateMonth|CheckinDateDay|\n",
      "+-------+------------+------------+--------------+----------------+-----------------+---------------+---------------+----------------+--------------+\n",
      "|     26|           2|         0.0|         144.0|            2015|                7|             17|           2015|               8|            12|\n",
      "|     33|           2|         0.0|         144.0|            2015|                7|             17|           2015|               8|            19|\n",
      "|     27|           2|         1.0|          73.0|            2015|                7|             17|           2015|               8|            13|\n",
      "|      9|           1|         6.0|          83.0|            2015|                7|             17|           2015|               7|            26|\n",
      "|     26|           2|         0.0|          83.0|            2015|                7|             17|           2015|               8|            12|\n",
      "|     21|           1|         2.0|          17.0|            2015|                7|             17|           2015|               8|             7|\n",
      "|     23|           1|         6.0|          17.0|            2015|                7|             17|           2015|               8|             9|\n",
      "|     26|           1|         0.0|          17.0|            2015|                7|             17|           2015|               8|            12|\n",
      "|     27|           3|         1.0|          17.0|            2015|                7|             17|           2015|               8|            13|\n",
      "|      5|           2|         0.0|          61.0|            2015|                7|             17|           2015|               7|            22|\n",
      "|     13|           2|         1.0|          77.0|            2015|                7|             17|           2015|               7|            30|\n",
      "|     14|           1|         2.0|          77.0|            2015|                7|             17|           2015|               7|            31|\n",
      "|      9|           1|         6.0|          47.0|            2015|                7|             17|           2015|               7|            26|\n",
      "|     16|           1|         6.0|          47.0|            2015|                7|             17|           2015|               8|             2|\n",
      "|      5|           2|         0.0|          22.0|            2015|                7|             17|           2015|               7|            22|\n",
      "|      5|           3|         0.0|          22.0|            2015|                7|             17|           2015|               7|            22|\n",
      "|      6|           3|         1.0|          22.0|            2015|                7|             17|           2015|               7|            23|\n",
      "|      7|           2|         2.0|          22.0|            2015|                7|             17|           2015|               7|            24|\n",
      "|      7|           3|         2.0|          22.0|            2015|                7|             17|           2015|               7|            24|\n",
      "|      8|           2|         4.0|          22.0|            2015|                7|             17|           2015|               7|            25|\n",
      "+-------+------------+------------+--------------+----------------+-----------------+---------------+---------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "\n",
    "# Transform string values to numeric\n",
    "indexers = [StringIndexer(inputCol=\"WeekDay\", outputCol=\"WeekDayIndex\"),\n",
    "            StringIndexer(inputCol=\"HotelName\", outputCol=\"HotelNameIndex\"),]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "hotelsWithIndexedStrings_df = pipeline.fit(hotelsBestDiscountCode_df).transform(hotelsBestDiscountCode_df)\n",
    "\n",
    "# Extract date values\n",
    "dateYearValue = udf(lambda x: pd.to_datetime(x).year, IntegerType())\n",
    "dateDayValue = udf(lambda x: pd.to_datetime(x).day, IntegerType())\n",
    "dateMonthValue = udf(lambda x: pd.to_datetime(x).month, IntegerType())\n",
    "\n",
    "hotelsWithDateIndexed_df = hotelsWithIndexedStrings_df\\\n",
    "                     .withColumn('SnapshotDateYear', dateYearValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('SnapshotDateMonth', dateMonthValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('SnapshotDateDay', dateDayValue(col('SnapshotDate')))\\\n",
    "                     .withColumn('CheckinDateYear', dateYearValue(col('CheckinDate')))\\\n",
    "                     .withColumn('CheckinDateMonth', dateMonthValue(col('CheckinDate')))\\\n",
    "                     .withColumn('CheckinDateDay', dateDayValue(col('CheckinDate')))\n",
    "# Convert string column to int\n",
    "hotelsWithIntCoulmn_df = hotelsWithDateIndexed_df.withColumn(\"DayDiff\",\\\n",
    "                                   hotelsWithDateIndexed_df[\"DayDiff\"].cast(\"integer\"))\\\n",
    "                                .withColumn('DiscountCode', hotelsWithDateIndexed_df['DiscountCode'].cast('integer'))\n",
    "\n",
    "# Remove unneccesary columns\n",
    "hotelsWithoutColumns_df = hotelsWithIntCoulmn_df.drop('SnapshotDate').drop('CheckinDate')\\\n",
    "                         .drop('HotelName').drop('WeekDay')\n",
    "hotelsWithoutColumns_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|            index|  0|\n",
      "+-----------------+---+\n",
      "|          DayDiff| 34|\n",
      "|     DiscountCode|  4|\n",
      "|     WeekDayIndex|  7|\n",
      "|   HotelNameIndex|554|\n",
      "| SnapshotDateYear|  2|\n",
      "|SnapshotDateMonth|  7|\n",
      "|  SnapshotDateDay| 31|\n",
      "|  CheckinDateYear|  2|\n",
      "| CheckinDateMonth|  8|\n",
      "|   CheckinDateDay| 31|\n",
      "+-----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "def transposeDF(df):\n",
    "    pandas_df = df.toPandas().transpose().reset_index()\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "    \n",
    "# Show distinct values count\n",
    "distinctValuesDF = hotelsWithoutColumns_df.agg(*(countDistinct(col(c)).alias(c) for c in hotelsWithoutColumns_df.columns));\n",
    "transposeDF(distinctValuesDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns statsitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+------------------+------------------+----+-----+\n",
      "|            index|     0|                 1|                 2|   3|    4|\n",
      "+-----------------+------+------------------+------------------+----+-----+\n",
      "|          summary| count|              mean|            stddev| min|  max|\n",
      "|          DayDiff|115580|17.473083578473783|10.042081429292136|   1|   34|\n",
      "|     DiscountCode|115580| 2.410892888042914|1.0198765486288277|   1|    4|\n",
      "|     WeekDayIndex|115580| 2.696954490396262| 1.998808961886477| 0.0|  6.0|\n",
      "|   HotelNameIndex|115580| 65.97189825229279| 80.55926349229914| 0.0|553.0|\n",
      "| SnapshotDateYear|115580|2015.0051046893927|0.0712648263844711|2015| 2016|\n",
      "|SnapshotDateMonth|115580| 9.564544038761031|1.6544450517640836|   1|   12|\n",
      "|  SnapshotDateDay|115580| 16.81938051566015| 8.636772522445504|   1|   31|\n",
      "|  CheckinDateYear|115580|2015.0822028032533|0.2746746350709511|2015| 2016|\n",
      "| CheckinDateMonth|115580|  9.22885447309223|2.7938784591523005|   1|   12|\n",
      "|   CheckinDateDay|115580|16.211541789236893| 8.911702383047402|   1|   31|\n",
      "+-----------------+------+------------------+------------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data exploration\n",
    "describe_df = hotelsWithoutColumns_df.describe(hotelsWithoutColumns_df.columns);\n",
    "transposeDF(describe_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column DayDiff correlation: 0.03384978595429131\n",
      "Column DiscountCode correlation: 1.0\n",
      "Column WeekDayIndex correlation: -0.0444292123126588\n",
      "Column HotelNameIndex correlation: -0.05653999461890728\n",
      "Column SnapshotDateYear correlation: 0.0012586486536448303\n",
      "Column SnapshotDateMonth correlation: 0.010092504674174543\n",
      "Column SnapshotDateDay correlation: -0.0065184066646494674\n",
      "Column CheckinDateYear correlation: 0.0017019981329915415\n",
      "Column CheckinDateMonth correlation: 0.008843572569188144\n",
      "Column CheckinDateDay correlation: -0.010942946419650216\n"
     ]
    }
   ],
   "source": [
    "for column in hotelsWithoutColumns_df.columns:\n",
    "    corr = hotelsWithoutColumns_df.corr('DiscountCode', column)\n",
    "    print(\"Column %s correlation: %s\" % (column, corr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create vector of all features expect the label\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in hotelsWithoutColumns_df.columns if x != 'DiscountCode'],\n",
    "    outputCol='features')\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = hotelsWithoutColumns_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol='DiscountCode', featuresCol='features',\\\n",
    "                            impurity='entropy', maxDepth=20, maxBins=554)\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "tree_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|DiscountCode|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       4.0|           1|[1.0,0.0,1.0,2015...|\n",
      "|       2.0|           1|[1.0,0.0,2.0,2015...|\n",
      "|       1.0|           1|[1.0,0.0,2.0,2015...|\n",
      "|       3.0|           1|[1.0,0.0,2.0,2015...|\n",
      "|       1.0|           1|[1.0,0.0,3.0,2015...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Accuaracy = 0.654097 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions.\n",
    "predictions = tree_model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"DiscountCode\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DiscountCode\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuaracy = %g \" %  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree AUC using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-19-73b7ed4a1766>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-73b7ed4a1766>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    .withColumn('binaryPrediction', F.when(col('prediction')==label,1.0).otherwise(0.0))                .withColumn('binaryDiscountCode', F.when(col('DiscountCode')==label,1.0).otherwise(0.0))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Binary classifications\n",
    "for label in range(1,5):\n",
    "    # Filter only relevanot to class predictions\n",
    "    predictions.where(\"prediction == %s OR DiscountCode == %s\" % (label, label)).select('prediction', 'DiscountCode')\n",
    "                .withColumn('binaryPrediction', F.when(col('prediction')==label,1.0).otherwise(0.0))\\\n",
    "                .withColumn('binaryDiscountCode', F.when(col('DiscountCode')==label,1.0).otherwise(0.0))\n",
    "            \n",
    "    binaryPredictions.select('prediction', 'DiscountCode', 'binaryPrediction', 'binaryDiscountCode').show(5)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"binaryPrediction\", labelCol=\"binaryDiscountCode\")\n",
    "    auc = evaluator.evaluate(binaryPredictions)\n",
    "    \n",
    "    print(\"Class %s area under roc = %s\" % (label, auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for label in range(1, 5):\n",
    "    labelPredictions = predictions.where(\"prediction == %s OR DiscountCode == %s\" % (label, label))\\\n",
    "            .select('prediction', 'DiscountCode').collect()\n",
    "    \n",
    "    expected = labelPredictions.map(lambda x : x[0])\n",
    "    predicted = labelPredictions.map(lambda x : x[1])\n",
    "\n",
    "    fpr[i], tpr[i], _ = roc_curve(expected, predicted, pos_label=label)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(1, 5)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(1, 5):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= 4\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "lw=2\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "for i, color in zip(range(1,5), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i+1.0, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes - old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "model = NaiveBayes.train(training_data, 1.0)\n",
    "\n",
    "NaiveBayes_predictionAndLabel = test_data.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "naive_metrics = MulticlassMetrics(NaiveBayes_predictionAndLabel)\n",
    "\n",
    "print('Accuracy {}'.format(naive_metrics.accuracy))\n",
    "print('False positive rate {}'.format(naive_metrics.weightedFalsePositiveRate))\n",
    "print(naive_metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run naive bayes - new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = hotelsWithoutColumns_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Train a NaiveBayes model.\n",
    "dt = NaiveBayes(labelCol='DiscountCode', featuresCol='features')\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "naive_model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions.\n",
    "naive_predictions = naive_model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "naive_predictions.select(\"prediction\", \"DiscountCode\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"DiscountCode\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuaracy = %g \" %  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print naive bayes auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Binary classifications\n",
    "for label in range(1,5):\n",
    "    # Filter only relevanot to class predictions\n",
    "    binaryPredictions = naive_predictions.where(\"prediction == %s OR DiscountCode == %s\" % (label, label))\\\n",
    "                .withColumn('binaryPrediction', F.when(col('prediction')==label,1.0).otherwise(0.0))\\\n",
    "                .withColumn('binaryDiscountCode', F.when(col('DiscountCode')==label,1.0).otherwise(0.0))\n",
    "            \n",
    "    binaryPredictions.select('prediction', 'DiscountCode', 'binaryPrediction', 'binaryDiscountCode').show(5)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"binaryPrediction\", labelCol=\"binaryDiscountCode\")\n",
    "    auc = evaluator.evaluate(binaryPredictions)\n",
    "    \n",
    "    print(\"Class %s area under roc = %s\" % (label, auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baive bayes ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for label in range(1, 5):\n",
    "    labelPredictions = naive_predictions.where(\"prediction == %s OR DiscountCode == %s\" % (label, label))\\\n",
    "            .select('prediction', 'DiscountCode').collect()\n",
    "    \n",
    "    expected = labelPredictions.map(lambda x : x[0])\n",
    "    predicted = labelPredictions.map(lambda x : x[1])\n",
    "\n",
    "    fpr[i], tpr[i], _ = roc_curve(expected, predicted, pos_label=label)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(1, 5)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(1, 5):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= 4\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "lw=2\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "for i, color in zip(range(1,5), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i+1.0, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Decision Tree ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
