{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Converting sequences...\n",
      "Padding sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1579/1579 [00:00<00:00, 10716.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded successfully!\n",
      "X shape: torch.Size([1579, 1, 2948])\n",
      "y shape: torch.Size([1579, 2])\n",
      "X range: [0.00, 1.00]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create first cell for data loading\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert sequences\n",
    "    print(\"Converting sequences...\")\n",
    "    data['sequence'] = data['sequence'].apply(literal_eval)\n",
    "    \n",
    "    # Get max length\n",
    "    max_len = max(len(seq) for seq in data['sequence'])\n",
    "    \n",
    "    # Pad sequences\n",
    "    print(\"Padding sequences...\")\n",
    "    def pad_sequence(seq):\n",
    "        return np.pad([int(x) for x in seq], \n",
    "                     (0, max_len - len(seq)), \n",
    "                     'constant')\n",
    "    \n",
    "    X = np.array([pad_sequence(seq) for seq in tqdm(data['sequence'])])\n",
    "    \n",
    "    # Convert labels\n",
    "    y = pd.get_dummies(data['label']).values\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.FloatTensor(X).unsqueeze(1)\n",
    "    y_tensor = torch.FloatTensor(y)\n",
    "    \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Test the function\n",
    "X_tensor, y_tensor = load_and_preprocess_data('adfa_ld_processed.csv')\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"X shape: {X_tensor.shape}\")\n",
    "print(f\"y shape: {y_tensor.shape}\")\n",
    "print(f\"X range: [{X_tensor.min():.2f}, {X_tensor.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization Check:\n",
      "Minimum value: 0.000000\n",
      "Maximum value: 1.000000\n",
      "Mean value: 0.046142\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLD0lEQVR4nO3deViU9f7/8deALCLgkiIu5K65oKmVB9PUUnHJpE6ammtap5OWZtmJNjVLrVxPmdomqV/S9Kidb26QimbiKU1L+5a5pFQCtqiI5jgy9++PfsxxYrm5J2AGeT6ua66r+cznvu/3PfOGeHkvYzMMwxAAAAAAoEB+3i4AAAAAAHwdwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkA/r8pU6bIZrOVyra6du2qrl27up6npKTIZrNp9erVpbL9kSNHqn79+qWyLU9lZ2drzJgxioyMlM1m04QJE7xdklfk9kZKSoprzBuf3/Hjx2Wz2ZSQkFCq25Ukm82mKVOmlPp2AeBKBCcAV6WEhATZbDbXIzg4WLVr11ZsbKz++c9/6ty5c8WynZMnT2rKlCnav39/sayvOPlybUUxffp0JSQk6O9//7uWLVumYcOGFTi3fv36stlsevjhh/O8VtqhtDx75JFHZLPZdOTIkQLnPP3007LZbPryyy9LsTIA+PMITgCuas8//7yWLVumhQsXuv6onjBhgqKjo/P84fbMM8/ot99+s7T+kydPaurUqZbDSVJSkpKSkiwtY1Vhtb355ps6dOhQiW7/z9q6dav+8pe/aPLkyRo6dKjat29vusybb76pkydPlkJ13uWrn9+9994rSUpMTCxwznvvvafo6Gi1bt26tMoCgGJBcAJwVevdu7eGDh2qUaNGKT4+Xps3b9ZHH32kU6dO6Y477nALShUqVFBwcHCJ1nPhwgVJUmBgoAIDA0t0W4UJCAhQUFCQ17ZfFKdOnVKVKlWKPL9ly5bKycnRzJkzS64oSefPny/R9ReFr35+HTp0UOPGjfXee+/l+3pqaqq+++47V8ACgLKE4ASg3Ln11lv17LPP6sSJE1q+fLlrPL9rnJKTk9WpUydVqVJFoaGhatasmZ566ilJv58CduONN0qSRo0a5TotMPcakK5du6pVq1bau3evbrnlFoWEhLiW/eM1TrlycnL01FNPKTIyUpUqVdIdd9yh77//3m1O/fr1NXLkyDzLXrlOs9ryu0bm/PnzeuyxxxQVFaWgoCA1a9ZMs2bNkmEYbvNsNpvGjRundevWqVWrVgoKClLLli21adOm/N/wPzh16pRGjx6tmjVrKjg4WG3atNG7777rej331LrvvvtO69evd9V+/PjxQtdbv359DR8+vMhHnfbt26fevXsrPDxcoaGhuu2227R79263ObmnfG7fvl0PPfSQIiIiVLduXUn//Xy//PJLdenSRSEhIWrcuLHrlMDt27erQ4cOqlixopo1a6aPPvrIbd0nTpzQQw89pGbNmqlixYq65pprNGDAANP9lPJ+fl27dnU7NfXKx5XXJJ05c0YTJkxwfcaNGzfWSy+9JKfT6bb+M2fOaOTIkapcubKqVKmiESNG6MyZM6Z1Sb8fdfrmm2/0+eef53ktMTFRNptNgwcP1qVLl/Tcc8+pffv2qly5sipVqqTOnTtr27Ztlvc/V0HXKS5fvlzt27dXxYoVVa1aNQ0aNCjPz9Xhw4f117/+VZGRkQoODlbdunU1aNAgnT17tkj7DeDqV8HbBQCANwwbNkxPPfWUkpKSdP/99+c756uvvtLtt9+u1q1b6/nnn1dQUJCOHDmiTz75RJLUvHlzPf/883ruuef0wAMPqHPnzpKkjh07utbxyy+/qHfv3ho0aJCGDh2qmjVrFlrXiy++KJvNpn/84x86deqU5s2bp+7du2v//v2qWLFikfevKLVdyTAM3XHHHdq2bZtGjx6t66+/Xps3b9akSZP0448/au7cuW7zd+7cqTVr1uihhx5SWFiY/vnPf+qvf/2r0tLSdM011xRY12+//aauXbvqyJEjGjdunBo0aKBVq1Zp5MiROnPmjMaPH6/mzZtr2bJlevTRR1W3bl099thjkqQaNWqY7vfTTz+tpUuXaubMmfrnP/9Z4LyvvvpKnTt3Vnh4uJ544gkFBARo8eLF6tq1qyvwXOmhhx5SjRo19Nxzz7kdcTp9+rRuv/12DRo0SAMGDNDChQs1aNAg/c///I8mTJigBx98UEOGDNErr7yiu+++W99//73CwsIkSZ999pl27dqlQYMGqW7dujp+/LgWLlyorl276v/+7/8UEhJiur9X7veYMWPcxpYvX67NmzcrIiJC0u9HO7t06aIff/xRf/vb33Tttddq165dio+PV3p6uubNmyfp917o37+/du7cqQcffFDNmzfX2rVrNWLEiCLVcu+992rq1KlKTExUu3btXOM5OTl6//331blzZ1177bX6+eef9dZbb2nw4MG6//77de7cOb399tuKjY3Vp59+quuvv77I+1+YF198Uc8++6wGDhyoMWPG6KefftKrr76qW265Rfv27VOVKlV06dIlxcbGym636+GHH1ZkZKR+/PFHffjhhzpz5owqV65cLLUAKOMMALgKLVmyxJBkfPbZZwXOqVy5stG2bVvX88mTJxtX/lqcO3euIcn46aefClzHZ599ZkgylixZkue1Ll26GJKMRYsW5ftaly5dXM+3bdtmSDLq1KljZGVlucbff/99Q5Ixf/5811i9evWMESNGmK6zsNpGjBhh1KtXz/V83bp1hiTjhRdecJt39913GzabzThy5IhrTJIRGBjoNvbFF18YkoxXX301z7auNG/ePEOSsXz5ctfYpUuXjJiYGCM0NNRt3+vVq2f07du30PXlN3fUqFFGcHCwcfLkScMw/vverlq1yjU/Li7OCAwMNI4ePeoaO3nypBEWFmbccsstrrHcPurUqZNx+fJlt23mfr6JiYmusW+++caQZPj5+Rm7d+92jW/evDnPZ3HhwoU8+5GammpIMpYuXeoay61/27ZtrrE/fn5/9MknnxgBAQHGfffd5xqbNm2aUalSJePbb791m/vkk08a/v7+RlpammEY/+2Fl19+2TXn8uXLRufOnQvspz+68cYbjbp16xo5OTmusU2bNhmSjMWLF7vWabfb3ZY7ffq0UbNmTbe6DeP3nps8ebLp/v/xZ/j48eOGv7+/8eKLL7rNO3DggFGhQgXX+L59+/L0CAD8EafqASi3QkNDC727Xu71NR988EGeU5mKKigoSKNGjSry/OHDh7uOSEjS3XffrVq1amnDhg0ebb+oNmzYIH9/fz3yyCNu44899pgMw9DGjRvdxrt3765GjRq5nrdu3Vrh4eE6duyY6XYiIyM1ePBg11hAQIAeeeQRZWdna/v27X96X5555hldvny5wGudcnJylJSUpLi4ODVs2NA1XqtWLQ0ZMkQ7d+5UVlaW2zL333+//P3986wrNDRUgwYNcj1v1qyZqlSpoubNm7sdtcr97yvfnyuPIDocDv3yyy9q3LixqlSpku9pbkWVkZGhu+++W9dff71ef/111/iqVavUuXNnVa1aVT///LPr0b17d+Xk5GjHjh2Sfv+MKlSooL///e+uZf39/fO9Y2FBhg4dqh9++MG1Tun30/QCAwM1YMAA1zpzr/NzOp369ddfdfnyZd1www1/av+vtGbNGjmdTg0cONBtnyMjI9WkSRPXaYG5R5Q2b97sug4RAP6oXAenHTt2qF+/fqpdu7ZsNpvWrVtneR2GYWjWrFlq2rSpgoKCVKdOHb344ovFXyyAYpedne0WUv7onnvu0c0336wxY8aoZs2aGjRokN5//31LIapOnTqWbgLRpEkTt+c2m02NGzcu0nUvf8aJEydUu3btPO9H8+bNXa9f6dprr82zjqpVq+r06dOm22nSpIn8/Nz/91PQdjzRsGFDDRs2TG+88YbS09PzvP7TTz/pwoULatasWZ7XmjdvLqfTmef6lwYNGuS7rbp16+a5pqZy5cqKiorKMybJ7f357bff9Nxzz7muN6pevbpq1KihM2fOeHxdzeXLlzVw4EDl5ORozZo1bjeQOHz4sDZt2qQaNWq4Pbp37y7p92vPpN8/g1q1aik0NNRt3fm9XwUZNGiQ/P39XXfXu3jxotauXavevXuratWqrnnvvvuuWrdureDgYF1zzTWqUaOG1q9fX2zXFR0+fFiGYahJkyZ59vvrr7927XODBg00ceJEvfXWW6pevbpiY2O1YMECrm8C4KZcX+N0/vx5tWnTRvfdd5/uuusuj9Yxfvx4JSUladasWYqOjtavv/6qX3/9tZgrBVDcfvjhB509e1aNGzcucE7FihW1Y8cObdu2TevXr9emTZu0cuVK3XrrrUpKSsr3CER+6yhuBX1Jb05OTpFqKg4Fbcf4w40kvOXpp5/WsmXL9NJLLykuLu5Pr6+gz7Gg96Eo78/DDz+sJUuWaMKECYqJiVHlypVls9k0aNAgj49wTpo0Sampqfroo49cN7HI5XQ61aNHDz3xxBP5Ltu0aVOPtpmfiIgI9ejRQ//617+0YMEC/e///q/OnTvndje95cuXa+TIkYqLi9OkSZMUEREhf39/zZgxQ0ePHi10/YX9DFzJ6XTKZrNp48aNBR4xzDV79myNHDlSH3zwgZKSkvTII49oxowZ2r17d573EkD5VK6DU+/evdW7d+8CX7fb7Xr66af13nvv6cyZM2rVqpVeeukl112rvv76ay1cuFAHDx50/UtcQf8qCcC3LFu2TJIUGxtb6Dw/Pz/ddtttuu222zRnzhxNnz5dTz/9tLZt26bu3bsX+Aecpw4fPuz23DAMHTlyxO07b6pWrZrvHc5OnDjhduqZldrq1aunjz76SOfOnXM76vTNN9+4Xi8O9erV05dffimn0+l21Km4t9OoUSMNHTpUixcvznOjhxo1aigkJCTf70H65ptv5Ofnl+eIUUlYvXq1RowYodmzZ7vGLl68WOS71/3RihUrNG/ePM2bN09dunTJ83qjRo2UnZ3tOsJUkHr16mnLli3Kzs52CxZWvzfq3nvv1aZNm7Rx40YlJiYqPDxc/fr1c72+evVqNWzYUGvWrHHr1cmTJ5uuu7CfgSs1atRIhmGoQYMGRQqG0dHRio6O1jPPPKNdu3bp5ptv1qJFi/TCCy+YLgvg6leuT9UzM27cOKWmpmrFihX68ssvNWDAAPXq1cv1h83//u//qmHDhvrwww/VoEED1a9fX2PGjOGIE+Djtm7dqmnTpqlBgwaFfp9Mfj/LuXf6stvtkqRKlSpJksd/7P7R0qVL3a67Wr16tdLT093+kadRo0bavXu3Ll265Br78MMP85xeZqW2Pn36KCcnR6+99prb+Ny5c2Wz2Qr9RyYr+vTpo4yMDK1cudI1dvnyZb366qsKDQ3N9w9+Tz3zzDNyOBx6+eWX3cb9/f3Vs2dPffDBB26nQGZmZioxMVGdOnVSeHh4sdVREH9//zxH6F599dU8R02K4uDBgxozZoyGDh2q8ePH5ztn4MCBSk1N1ebNm/O8dubMGV2+fFnS75/R5cuXtXDhQtfrOTk5evXVVy3VFBcXp5CQEL3++uvauHGj7rrrLrfvScs9AnTle/Cf//xHqampputu1KiRzp496/Yl1unp6Vq7dq3bvLvuukv+/v6aOnVqnvfaMAz98ssvkqSsrCzX/ueKjo6Wn5+f62cdAMr1EafCpKWlacmSJUpLS1Pt2rUlSY8//rg2bdqkJUuWaPr06Tp27JhOnDihVatWaenSpcrJydGjjz6qu+++W1u3bvXyHgCQpI0bN+qbb77R5cuXlZmZqa1btyo5OVn16tXTv//970K/8Pb555/Xjh071LdvX9WrV0+nTp3S66+/rrp166pTp06Sfv8DrkqVKlq0aJHCwsJUqVIldejQweOjz9WqVVOnTp00atQoZWZmat68eWrcuLHbLdPHjBmj1atXq1evXho4cKCOHj2q5cuXu92swWpt/fr1U7du3fT000/r+PHjatOmjZKSkvTBBx9owoQJedbtqQceeECLFy/WyJEjtXfvXtWvX1+rV6/WJ598onnz5hV6zZlVuUedrvyOqFwvvPCC6zu6HnroIVWoUEGLFy+W3W7PE7RKyu23365ly5apcuXKatGihesUu8Ju516Q3BuQ3HLLLW7fTSb9fgv6hg0batKkSfr3v/+t22+/XSNHjlT79u11/vx5HThwQKtXr9bx48dVvXp19evXTzfffLOefPJJHT9+XC1atNCaNWssX+8TGhqquLg413VOf/xHittvv11r1qzRnXfeqb59++q7777TokWL1KJFC2VnZxe67kGDBukf//iH7rzzTj3yyCO6cOGCFi5cqKZNm7rdWKJRo0Z64YUXFB8fr+PHjysuLk5hYWH67rvvtHbtWj3wwAN6/PHHtXXrVo0bN04DBgxQ06ZNdfnyZS1btkz+/v7661//amm/AVzFvHQ3P58jyVi7dq3r+YcffmhIMipVquT2qFChgjFw4EDDMAzj/vvvNyQZhw4dci23d+9eQ5LxzTfflPYuALhC7m2kcx+BgYFGZGSk0aNHD2P+/Plut73O9cdbGW/ZssXo37+/Ubt2bSMwMNCoXbu2MXjw4Dy3c/7ggw+MFi1aGBUqVHC7XXOXLl2Mli1b5ltfQbcjf++994z4+HgjIiLCqFixotG3b1/jxIkTeZafPXu2UadOHSMoKMi4+eabjT179uRZZ2G15Xc753PnzhmPPvqoUbt2bSMgIMBo0qSJ8corrxhOp9NtniRj7NixeWoq6Dbpf5SZmWmMGjXKqF69uhEYGGhER0fne4trT29HfqXDhw8b/v7++d5q+vPPPzdiY2ON0NBQIyQkxOjWrZuxa9cutzmF3da+oM+3oFr++L6dPn3a9T6EhoYasbGxxjfffJPnfSzK7cjr1avn1u9XPq58b8+dO2fEx8cbjRs3NgIDA43q1asbHTt2NGbNmmVcunTJNe+XX34xhg0bZoSHhxuVK1c2hg0b5rpld1FuR55r/fr1hiSjVq1abrcmNwzDcDqdxvTp04169eoZQUFBRtu2bY0PP/ww397UH25HbhiGkZSUZLRq1coIDAw0mjVrZixfvjzPz3Cuf/3rX0anTp1c/y+/7rrrjLFjx7r+/33s2DHjvvvuMxo1amQEBwcb1apVM7p162Z89NFHRd5XAFc/m2H4yJW8Xmaz2bR27VrXRcQrV67Uvffeq6+++irPBaWhoaGKjIzU5MmTNX36dDkcDtdrv/32m0JCQpSUlKQePXqU5i4AAAAAKCGcqleAtm3bKicnR6dOnVLnzp3znXPzzTfr8uXLOnr0qOs0lm+//VZS8V3gDAAAAMD7yvURp+zsbB05ckTS70Fpzpw56tatm6pVq6Zrr71WQ4cO1SeffKLZs2erbdu2+umnn7Rlyxa1bt1affv2ldPp1I033qjQ0FDNmzdPTqdTY8eOVXh4uJKSkry8dwAAAACKS7kOTikpKerWrVue8REjRighIUEOh0MvvPCCli5dqh9//FHVq1fXX/7yF02dOlXR0dGSpJMnT+rhhx9WUlKSKlWqpN69e2v27NmqVq1aae8OAAAAgBJSroMTAAAAABQF3+MEAAAAACYITgAAAABgotzdVc/pdOrkyZMKCwuTzWbzdjkAAAAAvMQwDJ07d061a9eWn1/hx5TKXXA6efKkoqKivF0GAAAAAB/x/fffq27duoXOKXfBKSwsTNLvb054eLiXq5EcDoeSkpLUs2dPBQQEeLsc+Dj6BVbRM7CKnoFV9Ays8qWeycrKUlRUlCsjFKbcBafc0/PCw8N9JjiFhIQoPDzc640D30e/wCp6BlbRM7CKnoFVvtgzRbmEh5tDAAAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAIAJghMAAAAAmCA4AQAAAICJCt4uAL9rNWWz7Dk2S8scn9m3hKoBAAAAcCWOOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJjwmeA0c+ZM2Ww2TZgwodB5q1at0nXXXafg4GBFR0drw4YNpVMgAAAAgHLLJ4LTZ599psWLF6t169aFztu1a5cGDx6s0aNHa9++fYqLi1NcXJwOHjxYSpUCAAAAKI+8Hpyys7N177336s0331TVqlULnTt//nz16tVLkyZNUvPmzTVt2jS1a9dOr732WilVCwAAAKA8quDtAsaOHau+ffuqe/fueuGFFwqdm5qaqokTJ7qNxcbGat26dQUuY7fbZbfbXc+zsrIkSQ6HQw6Hw/PCi0luDUF+hsfLovzI/cz57FFU9AysomdgFT0Dq3ypZ6zU4NXgtGLFCn3++ef67LPPijQ/IyNDNWvWdBurWbOmMjIyClxmxowZmjp1ap7xpKQkhYSEWCu4BE27wWl5Ga7vKr+Sk5O9XQLKGHoGVtEzsIqegVW+0DMXLlwo8lyvBafvv/9e48ePV3JysoKDg0tsO/Hx8W5HqbKyshQVFaWePXsqPDy8xLZbVA6HQ8nJyXp2j5/sTpulZQ9OiS2hquCrcvulR48eCggI8HY5KAPoGVhFz8AqegZW+VLP5J6NVhReC0579+7VqVOn1K5dO9dYTk6OduzYoddee012u13+/v5uy0RGRiozM9NtLDMzU5GRkQVuJygoSEFBQXnGAwICvP5BXcnutMmeYy04+VL9KF2+1r/wffQMrKJnYBU9A6t8oWesbN9rN4e47bbbdODAAe3fv9/1uOGGG3Tvvfdq//79eUKTJMXExGjLli1uY8nJyYqJiSmtsgEAAACUQ1474hQWFqZWrVq5jVWqVEnXXHONa3z48OGqU6eOZsyYIUkaP368unTpotmzZ6tv375asWKF9uzZozfeeKPU6wcAAABQfnj9duSFSUtLU3p6uut5x44dlZiYqDfeeENt2rTR6tWrtW7dujwBDAAAAACKk9dvR36llJSUQp9L0oABAzRgwIDSKQgAAAAA5ONHnAAAAADAFxCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATHg1OC1cuFCtW7dWeHi4wsPDFRMTo40bNxY4PyEhQTabze0RHBxcihUDAAAAKI8qeHPjdevW1cyZM9WkSRMZhqF3331X/fv31759+9SyZct8lwkPD9ehQ4dcz202W2mVCwAAAKCc8mpw6tevn9vzF198UQsXLtTu3bsLDE42m02RkZGlUR4AAAAASPJycLpSTk6OVq1apfPnzysmJqbAednZ2apXr56cTqfatWun6dOnFxiyJMlut8tut7ueZ2VlSZIcDoccDkfx7YCHcmsI8jM8XhblR+5nzmePoqJnYBU9A6voGVjlSz1jpQabYRjW/2IvRgcOHFBMTIwuXryo0NBQJSYmqk+fPvnOTU1N1eHDh9W6dWudPXtWs2bN0o4dO/TVV1+pbt26+S4zZcoUTZ06Nc94YmKiQkJCinVfAAAAAJQdFy5c0JAhQ3T27FmFh4cXOtfrwenSpUtKS0vT2bNntXr1ar311lvavn27WrRoYbqsw+FQ8+bNNXjwYE2bNi3fOfkdcYqKitLPP/9s+uaUBofDoeTkZD27x092p7XrtQ5OiS2hquCrcvulR48eCggI8HY5KAPoGVhFz8AqegZW+VLPZGVlqXr16kUKTl4/VS8wMFCNGzeWJLVv316fffaZ5s+fr8WLF5suGxAQoLZt2+rIkSMFzgkKClJQUFC+y3r7g7qS3WmTPcdacPKl+lG6fK1/4fvoGVhFz8AqegZW+ULPWNm+z32Pk9PpdDtCVJicnBwdOHBAtWrVKuGqAAAAAJRnXj3iFB8fr969e+vaa6/VuXPnlJiYqJSUFG3evFmSNHz4cNWpU0czZsyQJD3//PP6y1/+osaNG+vMmTN65ZVXdOLECY0ZM8abuwEAAADgKufV4HTq1CkNHz5c6enpqly5slq3bq3NmzerR48ekqS0tDT5+f33oNjp06d1//33KyMjQ1WrVlX79u21a9euIl0PBQAAAACe8mpwevvttwt9PSUlxe353LlzNXfu3BKsCAAAAADy8rlrnAAAAADA1xCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMCEV4PTwoUL1bp1a4WHhys8PFwxMTHauHFjocusWrVK1113nYKDgxUdHa0NGzaUUrUAAAAAyiuvBqe6detq5syZ2rt3r/bs2aNbb71V/fv311dffZXv/F27dmnw4MEaPXq09u3bp7i4OMXFxengwYOlXDkAAACA8sSrwalfv37q06ePmjRpoqZNm+rFF19UaGiodu/ene/8+fPnq1evXpo0aZKaN2+uadOmqV27dnrttddKuXIAAAAA5UkFbxeQKycnR6tWrdL58+cVExOT75zU1FRNnDjRbSw2Nlbr1q0rcL12u112u931PCsrS5LkcDjkcDj+fOF/Um4NQX6Gx8ui/Mj9zPnsUVT0DKyiZ2AVPQOrfKlnrNTg9eB04MABxcTE6OLFiwoNDdXatWvVokWLfOdmZGSoZs2abmM1a9ZURkZGgeufMWOGpk6dmmc8KSlJISEhf674YjTtBqflZbi+q/xKTk72dgkoY+gZWEXPwCp6Blb5Qs9cuHChyHO9HpyaNWum/fv36+zZs1q9erVGjBih7du3FxierIqPj3c7SpWVlaWoqCj17NlT4eHhxbKNP8PhcCg5OVnP7vGT3WmztOzBKbElVBV8VW6/9OjRQwEBAd4uB2UAPQOr6BlYRc/AKl/qmdyz0YrC68EpMDBQjRs3liS1b99en332mebPn6/FixfnmRsZGanMzEy3sczMTEVGRha4/qCgIAUFBeUZDwgI8PoHdSW70yZ7jrXg5Ev1o3T5Wv/C99EzsIqegVX0DKzyhZ6xsn2f+x4np9Ppdk3SlWJiYrRlyxa3seTk5AKviQIAAACA4uDVI07x8fHq3bu3rr32Wp07d06JiYlKSUnR5s2bJUnDhw9XnTp1NGPGDEnS+PHj1aVLF82ePVt9+/bVihUrtGfPHr3xxhve3A0AAAAAVzmvBqdTp05p+PDhSk9PV+XKldW6dWtt3rxZPXr0kCSlpaXJz++/B8U6duyoxMREPfPMM3rqqafUpEkTrVu3Tq1atfLWLgAAAAAoB7wanN5+++1CX09JSckzNmDAAA0YMKCEKgIAAACAvHzuGicAAAAA8DUEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABMeBadjx44Vdx0AAAAA4LM8Ck6NGzdWt27dtHz5cl28eLG4awIAAAAAn+JRcPr888/VunVrTZw4UZGRkfrb3/6mTz/9tLhrAwAAAACf4FFwuv766zV//nydPHlS77zzjtLT09WpUye1atVKc+bM0U8//VTcdQIAAACA1/ypm0NUqFBBd911l1atWqWXXnpJR44c0eOPP66oqCgNHz5c6enphS4/Y8YM3XjjjQoLC1NERITi4uJ06NChQpdJSEiQzWZzewQHB/+Z3QAAAACAQv2p4LRnzx499NBDqlWrlubMmaPHH39cR48eVXJysk6ePKn+/fsXuvz27ds1duxY7d69W8nJyXI4HOrZs6fOnz9f6HLh4eFKT093PU6cOPFndgMAAAAAClXBk4XmzJmjJUuW6NChQ+rTp4+WLl2qPn36yM/v9xzWoEEDJSQkqH79+oWuZ9OmTW7PExISFBERob179+qWW24pcDmbzabIyEhPSgcAAAAAyzwKTgsXLtR9992nkSNHqlatWvnOiYiI0Ntvv21pvWfPnpUkVatWrdB52dnZqlevnpxOp9q1a6fp06erZcuW+c612+2y2+2u51lZWZIkh8Mhh8Nhqb6SkFtDkJ/h8bIoP3I/cz57FBU9A6voGVhFz8AqX+oZKzXYDMOw/hd7CXA6nbrjjjt05swZ7dy5s8B5qampOnz4sFq3bq2zZ89q1qxZ2rFjh7766ivVrVs3z/wpU6Zo6tSpecYTExMVEhJSrPsAAAAAoOy4cOGChgwZorNnzyo8PLzQuR4FpyVLlig0NFQDBgxwG1+1apUuXLigESNGWF2l/v73v2vjxo3auXNnvgGoIA6HQ82bN9fgwYM1bdq0PK/nd8QpKipKP//8s+mbUxocDoeSk5P17B4/2Z02S8senBJbQlXBV+X2S48ePRQQEODtclAG0DOwip6BVfQMrPKlnsnKylL16tWLFJw8OlVvxowZWrx4cZ7xiIgIPfDAA5aD07hx4/Thhx9qx44dlkKTJAUEBKht27Y6cuRIvq8HBQUpKCgo3+W8/UFdye60yZ5jLTj5Uv0oXb7Wv/B99AysomdgFT0Dq3yhZ6xs36O76qWlpalBgwZ5xuvVq6e0tLQir8cwDI0bN05r167V1q1b812nmZycHB04cKDAa60AAAAA4M/yKDhFREToyy+/zDP+xRdf6JprrinyesaOHavly5crMTFRYWFhysjIUEZGhn777TfXnOHDhys+Pt71/Pnnn1dSUpKOHTumzz//XEOHDtWJEyc0ZswYT3YFAAAAAEx5dKre4MGD9cgjjygsLMx12/Dt27dr/PjxGjRoUJHXs3DhQklS165d3caXLFmikSNHSvr96Fbubc4l6fTp07r//vuVkZGhqlWrqn379tq1a5datGjhya4AAAAAgCmPgtO0adN0/Phx3XbbbapQ4fdVOJ1ODR8+XNOnTy/yeopyX4qUlBS353PnztXcuXMt1QsAAAAAf4ZHwSkwMFArV67UtGnT9MUXX6hixYqKjo5WvXr1irs+AAAAAPA6j4JTrqZNm6pp06bFVQsAAAAA+CSPglNOTo4SEhK0ZcsWnTp1Sk6n0+31rVu3FktxAAAAAOALPApO48ePV0JCgvr27atWrVrJZrP2/UMAAAAAUJZ4FJxWrFih999/X3369CnuegAAAADA53j0PU6BgYFq3LhxcdcCAAAAAD7Jo+D02GOPaf78+UW6nTgAAAAAlHUenaq3c+dObdu2TRs3blTLli0VEBDg9vqaNWuKpTgAAAAA8AUeBacqVarozjvvLO5aAAAAAMAneRSclixZUtx1AAAAAIDP8ugaJ0m6fPmyPvroIy1evFjnzp2TJJ08eVLZ2dnFVhwAAAAA+AKPjjidOHFCvXr1Ulpamux2u3r06KGwsDC99NJLstvtWrRoUXHXCQAAAABe49ERp/Hjx+uGG27Q6dOnVbFiRdf4nXfeqS1bthRbcQAAAADgCzw64vTxxx9r165dCgwMdBuvX7++fvzxx2IpDAAAAAB8hUdHnJxOp3JycvKM//DDDwoLC/vTRQEAAACAL/EoOPXs2VPz5s1zPbfZbMrOztbkyZPVp0+f4qoNAAAAAHyCR6fqzZ49W7GxsWrRooUuXryoIUOG6PDhw6pevbree++94q4RAAAAALzKo+BUt25dffHFF1qxYoW+/PJLZWdna/To0br33nvdbhYBAAAAAFcDj4KTJFWoUEFDhw4tzloAAAAAwCd5FJyWLl1a6OvDhw/3qBgAAAAA8EUeBafx48e7PXc4HLpw4YICAwMVEhJCcAIAAABwVfHornqnT592e2RnZ+vQoUPq1KkTN4cAAAAAcNXxKDjlp0mTJpo5c2aeo1EAAAAAUNYVW3CSfr9hxMmTJ4tzlQAAAADgdR5d4/Tvf//b7blhGEpPT9drr72mm2++uVgKAwAAAABf4VFwiouLc3tus9lUo0YN3XrrrZo9e3Zx1AUAAAAAPsOj4OR0Oou7DgAAAADwWcV6jRMAAAAAXI08OuI0ceLEIs+dM2eOJ5sAAAAAAJ/hUXDat2+f9u3bJ4fDoWbNmkmSvv32W/n7+6tdu3aueTabrXiqBAAAAAAv8ig49evXT2FhYXr33XdVtWpVSb9/Ke6oUaPUuXNnPfbYY8VaJAAAAAB4k0fXOM2ePVszZsxwhSZJqlq1ql544QXuqgcAAADgquNRcMrKytJPP/2UZ/ynn37SuXPniryeGTNm6MYbb1RYWJgiIiIUFxenQ4cOmS63atUqXXfddQoODlZ0dLQ2bNhgqX4AAAAAsMKj4HTnnXdq1KhRWrNmjX744Qf98MMP+te//qXRo0frrrvuKvJ6tm/frrFjx2r37t1KTk6Ww+FQz549df78+QKX2bVrlwYPHqzRo0dr3759iouLU1xcnA4ePOjJrgAAAACAKY+ucVq0aJEef/xxDRkyRA6H4/cVVaig0aNH65VXXinyejZt2uT2PCEhQREREdq7d69uueWWfJeZP3++evXqpUmTJkmSpk2bpuTkZL322mtatGiRJ7sDAAAAAIXyKDiFhITo9ddf1yuvvKKjR49Kkho1aqRKlSr9qWLOnj0rSapWrVqBc1JTU/PcDj02Nlbr1q3Ld77dbpfdbnc9z8rKkiQ5HA5X6POm3BqC/AyPl0X5kfuZ89mjqOgZWEXPwCp6Blb5Us9YqcFmGIb1v9j/vyNHjujo0aO65ZZbVLFiRRmG4fEtyJ1Op+644w6dOXNGO3fuLHBeYGCg3n33XQ0ePNg19vrrr2vq1KnKzMzMM3/KlCmaOnVqnvHExESFhIR4VCsAAACAsu/ChQsaMmSIzp49q/Dw8ELnenTE6ZdfftHAgQO1bds22Ww2HT58WA0bNtTo0aNVtWpVj+6sN3bsWB08eLDQ0OSJ+Ph4tyNUWVlZioqKUs+ePU3fnNLgcDiUnJysZ/f4ye60FjoPToktoargq3L7pUePHgoICPB2OSgD6BlYRc/AKnoGVvlSz+SejVYUHgWnRx99VAEBAUpLS1Pz5s1d4/fcc48mTpxoOTiNGzdOH374oXbs2KG6desWOjcyMjLPkaXMzExFRkbmOz8oKEhBQUF5xgMCArz+QV3J7rTJnmMtOPlS/Shdvta/8H30DKyiZ2AVPQOrfKFnrGzfo7vqJSUl6aWXXsoTcpo0aaITJ04UeT2GYWjcuHFau3attm7dqgYNGpguExMToy1btriNJScnKyYmpsjbBQAAAAArPDridP78+XyvD/r111/zPbpTkLFjxyoxMVEffPCBwsLClJGRIUmqXLmyKlasKEkaPny46tSpoxkzZkiSxo8fry5dumj27Nnq27evVqxYoT179uiNN97wZFcAAAAAwJRHR5w6d+6spUuXup7bbDY5nU69/PLL6tatW5HXs3DhQp09e1Zdu3ZVrVq1XI+VK1e65qSlpSk9Pd31vGPHjkpMTNQbb7yhNm3aaPXq1Vq3bp1atWrlya4AAAAAgCmPjji9/PLLuu2227Rnzx5dunRJTzzxhL766iv9+uuv+uSTT4q8nqLc0C8lJSXP2IABAzRgwAArJQMAAACAxzw64tSqVSt9++236tSpk/r376/z58/rrrvu0r59+9SoUaPirhEAAAAAvMryESeHw6FevXpp0aJFevrpp0uiJgAAAADwKZaPOAUEBOjLL78siVoAAAAAwCd5dKre0KFD9fbbbxd3LQAAAADgkzy6OcTly5f1zjvv6KOPPlL79u1VqVIlt9fnzJlTLMUBAAAAgC+wFJyOHTum+vXr6+DBg2rXrp0k6dtvv3WbY7PZiq86AAAAAPABloJTkyZNlJ6erm3btkmS7rnnHv3zn/9UzZo1S6Q4AAAAAPAFlq5x+uP3Lm3cuFHnz58v1oIAAAAAwNd4dHOIXEX5AlsAAAAAKOssBSebzZbnGiauaQIAAABwtbN0jZNhGBo5cqSCgoIkSRcvXtSDDz6Y5656a9asKb4KAQAAAMDLLAWnESNGuD0fOnRosRYDAAAAAL7IUnBasmRJSdUBAAAAAD7rT90cAgAAAADKA4ITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJggOAEAAACACYITAAAAAJjwanDasWOH+vXrp9q1a8tms2ndunWFzk9JSZHNZsvzyMjIKJ2CAQAAAJRLXg1O58+fV5s2bbRgwQJLyx06dEjp6emuR0RERAlVCAAAAABSBW9uvHfv3urdu7fl5SIiIlSlSpUizbXb7bLb7a7nWVlZkiSHwyGHw2F528Utt4YgP8PjZVF+5H7mfPYoKnoGVtEzsIqegVW+1DNWarAZhmH9L/YSYLPZtHbtWsXFxRU4JyUlRd26dVO9evVkt9vVqlUrTZkyRTfffHOBy0yZMkVTp07NM56YmKiQkJDiKB0AAABAGXThwgUNGTJEZ8+eVXh4eKFzy1RwOnTokFJSUnTDDTfIbrfrrbfe0rJly/Sf//xH7dq1y3eZ/I44RUVF6eeffzZ9c0qDw+FQcnKynt3jJ7vTZmnZg1NiS6gq+KrcfunRo4cCAgK8XQ7KAHoGVtEzsIqegVW+1DNZWVmqXr16kYKTV0/Vs6pZs2Zq1qyZ63nHjh119OhRzZ07V8uWLct3maCgIAUFBeUZDwgI8PoHdSW70yZ7jrXg5Ev1o3T5Wv/C99EzsIqegVX0DKzyhZ6xsv0yfzvym266SUeOHPF2GQAAAACuYmU+OO3fv1+1atXydhkAAAAArmJePVUvOzvb7WjRd999p/3796tatWq69tprFR8frx9//FFLly6VJM2bN08NGjRQy5YtdfHiRb311lvaunWrkpKSvLULAAAAAMoBrwanPXv2qFu3bq7nEydOlCSNGDFCCQkJSk9PV1pamuv1S5cu6bHHHtOPP/6okJAQtW7dWh999JHbOgAAAACguHk1OHXt2lWF3dQvISHB7fkTTzyhJ554ooSrAgAAAAB3Zf4aJwAAAAAoaQQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADDh1eC0Y8cO9evXT7Vr15bNZtO6detMl0lJSVG7du0UFBSkxo0bKyEhocTrBAAAAFC+eTU4nT9/Xm3atNGCBQuKNP+7775T37591a1bN+3fv18TJkzQmDFjtHnz5hKuFAAAAEB5VsGbG+/du7d69+5d5PmLFi1SgwYNNHv2bElS8+bNtXPnTs2dO1exsbElVSYAAACAcs6rwcmq1NRUde/e3W0sNjZWEyZMKHAZu90uu93uep6VlSVJcjgccjgcJVKnFbk1BPkZHi+L8iP3M+ezR1HRM7CKnoFV9Ays8qWesVJDmQpOGRkZqlmzpttYzZo1lZWVpd9++00VK1bMs8yMGTM0derUPONJSUkKCQkpsVqtmnaD0/IyGzZsKIFKUBYkJyd7uwSUMfQMrKJnYBU9A6t8oWcuXLhQ5LllKjh5Ij4+XhMnTnQ9z8rKUlRUlHr27Knw8HAvVvY7h8Oh5ORkPbvHT3anzdKyB6dwemJ5k9svPXr0UEBAgLfLQRlAz8AqegZW0TOwypd6JvdstKIoU8EpMjJSmZmZbmOZmZkKDw/P92iTJAUFBSkoKCjPeEBAgNc/qCvZnTbZc6wFJ1+qH6XL1/oXvo+egVX0DKyiZ2CVL/SMle2Xqe9xiomJ0ZYtW9zGkpOTFRMT46WKAAAAAJQHXg1O2dnZ2r9/v/bv3y/p99uN79+/X2lpaZJ+P81u+PDhrvkPPvigjh07pieeeELffPONXn/9db3//vt69NFHvVE+AAAAgHLCq8Fpz549atu2rdq2bStJmjhxotq2bavnnntOkpSenu4KUZLUoEEDrV+/XsnJyWrTpo1mz56tt956i1uRAwAAAChRXr3GqWvXrjKMgm/DnZCQkO8y+/btK8GqAAAAAMBdmbrGCQAAAAC8geAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACZ8IjgtWLBA9evXV3BwsDp06KBPP/20wLkJCQmy2Wxuj+Dg4FKsFgAAAEB54/XgtHLlSk2cOFGTJ0/W559/rjZt2ig2NlanTp0qcJnw8HClp6e7HidOnCjFigEAAACUN14PTnPmzNH999+vUaNGqUWLFlq0aJFCQkL0zjvvFLiMzWZTZGSk61GzZs1SrBgAAABAeVPBmxu/dOmS9u7dq/j4eNeYn5+funfvrtTU1AKXy87OVr169eR0OtWuXTtNnz5dLVu2zHeu3W6X3W53Pc/KypIkORwOORyOYtoTz+XWEORneLwsyo/cz5zPHkVFz8AqegZW0TOwypd6xkoNNsMwrP/FXkxOnjypOnXqaNeuXYqJiXGNP/HEE9q+fbv+85//5FkmNTVVhw8fVuvWrXX27FnNmjVLO3bs0FdffaW6devmmT9lyhRNnTo1z3hiYqJCQkKKd4cAAAAAlBkXLlzQkCFDdPbsWYWHhxc616tHnDwRExPjFrI6duyo5s2ba/HixZo2bVqe+fHx8Zo4caLreVZWlqKiotSzZ0/TN6c0OBwOJScn69k9frI7bZaWPTgltoSqgq/K7ZcePXooICDA2+WgDKBnYBU9A6voGVjlSz2TezZaUXg1OFWvXl3+/v7KzMx0G8/MzFRkZGSR1hEQEKC2bdvqyJEj+b4eFBSkoKCgfJfz9gd1JbvTJnuOteDkS/WjdPla/8L30TOwip6BVfQMrPKFnrGyfa/eHCIwMFDt27fXli1bXGNOp1NbtmxxO6pUmJycHB04cEC1atUqqTIBAAAAlHNeP1Vv4sSJGjFihG644QbddNNNmjdvns6fP69Ro0ZJkoYPH646depoxowZkqTnn39ef/nLX9S4cWOdOXNGr7zyik6cOKExY8Z4czcAAAAAXMW8Hpzuuece/fTTT3ruueeUkZGh66+/Xps2bXLdYjwtLU1+fv89MHb69Gndf//9ysjIUNWqVdW+fXvt2rVLLVq08NYuAAAAALjKeT04SdK4ceM0bty4fF9LSUlxez537lzNnTu3FKoCAAAAgN95/QtwAQAAAMDXEZwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwATBCQAAAABMVPB2AQAA/Fn1n1zv0XLHZ/Yt5koAAFcrjjgBAAAAgAmOOAEAAFxlOAoLFD+CEwAAPo4/ggHA+3ziVL0FCxaofv36Cg4OVocOHfTpp58WOn/VqlW67rrrFBwcrOjoaG3YsKGUKgUAAABQHnn9iNPKlSs1ceJELVq0SB06dNC8efMUGxurQ4cOKSIiIs/8Xbt2afDgwZoxY4Zuv/12JSYmKi4uTp9//rlatWrlhT0Ayg7+1RoAAMAzXg9Oc+bM0f33369Ro0ZJkhYtWqT169frnXfe0ZNPPpln/vz589WrVy9NmjRJkjRt2jQlJyfrtdde06JFi0q1dgC+hWBYsFZTNsueY7O0THl4X+gZWEXPAOWXV4PTpUuXtHfvXsXHx7vG/Pz81L17d6Wmpua7TGpqqiZOnOg2Fhsbq3Xr1uU73263y263u56fPXtWkvTrr7/K4XD8yT348xwOhy5cuKAKDj/lOK39UdP48fdLqKr8/Sf+No+W6zBjS5nYnqc8rdMTuf3yyy+/KCAgwPLyFS6f92i7v/zyi0fLlfZn7+n+lfbPkqc8eV+89TumtH9+S/t/Zp6+N6Xd25787P7Z3zOe+DO/t0vzd7BU+r9HPXW190xZUtr/LywLfKlnzp07J0kyDMN8suFFP/74oyHJ2LVrl9v4pEmTjJtuuinfZQICAozExES3sQULFhgRERH5zp88ebIhiQcPHjx48ODBgwcPHjzyfXz//fem2cXrp+qVtPj4eLcjVE6nU7/++quuueYa2WzW/vW1JGRlZSkqKkrff/+9wsPDvV0OfBz9AqvoGVhFz8AqegZW+VLPGIahc+fOqXbt2qZzvRqcqlevLn9/f2VmZrqNZ2ZmKjIyMt9lIiMjLc0PCgpSUFCQ21iVKlU8L7qEhIeHe71xUHbQL7CKnoFV9Aysomdgla/0TOXKlYs0z6u3Iw8MDFT79u21Zct/z/10Op3asmWLYmJi8l0mJibGbb4kJScnFzgfAAAAAP4sr5+qN3HiRI0YMUI33HCDbrrpJs2bN0/nz5933WVv+PDhqlOnjmbMmCFJGj9+vLp06aLZs2erb9++WrFihfbs2aM33njDm7sBAAAA4Crm9eB0zz336KefftJzzz2njIwMXX/99dq0aZNq1qwpSUpLS5Of338PjHXs2FGJiYl65pln9NRTT6lJkyZat25dmf0Op6CgIE2ePDnP6YRAfugXWEXPwCp6BlbRM7CqrPaMzTCKcu89AAAAACi/vHqNEwAAAACUBQQnAAAAADBBcAIAAAAAEwQnAAAAADBBcCphCxYsUP369RUcHKwOHTro008/LXT+qlWrdN111yk4OFjR0dHasGFDKVUKX2GlZ95880117txZVatWVdWqVdW9e3fTHsPVx+rvmVwrVqyQzWZTXFxcyRYIn2O1Z86cOaOxY8eqVq1aCgoKUtOmTfn/UzljtWfmzZunZs2aqWLFioqKitKjjz6qixcvllK18LYdO3aoX79+ql27tmw2m9atW2e6TEpKitq1a6egoCA1btxYCQkJJV6nVQSnErRy5UpNnDhRkydP1ueff642bdooNjZWp06dynf+rl27NHjwYI0ePVr79u1TXFyc4uLidPDgwVKuHN5itWdSUlI0ePBgbdu2TampqYqKilLPnj31448/lnLl8BarPZPr+PHjevzxx9W5c+dSqhS+wmrPXLp0ST169NDx48e1evVqHTp0SG+++abq1KlTypXDW6z2TGJiop588klNnjxZX3/9td5++22tXLlSTz31VClXDm85f/682rRpowULFhRp/nfffae+ffuqW7du2r9/vyZMmKAxY8Zo8+bNJVypRQZKzE033WSMHTvW9TwnJ8eoXbu2MWPGjHznDxw40Ojbt6/bWIcOHYy//e1vJVonfIfVnvmjy5cvG2FhYca7775bUiXCx3jSM5cvXzY6duxovPXWW8aIESOM/v37l0Kl8BVWe2bhwoVGw4YNjUuXLpVWifAxVntm7Nixxq233uo2NnHiROPmm28u0TrhmyQZa9euLXTOE088YbRs2dJt7J577jFiY2NLsDLrOOJUQi5duqS9e/eqe/furjE/Pz91795dqamp+S6TmprqNl+SYmNjC5yPq4snPfNHFy5ckMPhULVq1UqqTPgQT3vm+eefV0REhEaPHl0aZcKHeNIz//73vxUTE6OxY8eqZs2aatWqlaZPn66cnJzSKhte5EnPdOzYUXv37nWdznfs2DFt2LBBffr0KZWaUfaUlb+BK3i7gKvVzz//rJycHNWsWdNtvGbNmvrmm2/yXSYjIyPf+RkZGSVWJ3yHJz3zR//4xz9Uu3btPL98cHXypGd27typt99+W/v37y+FCuFrPOmZY8eOaevWrbr33nu1YcMGHTlyRA899JAcDocmT55cGmXDizzpmSFDhujnn39Wp06dZBiGLl++rAcffJBT9VCggv4GzsrK0m+//aaKFSt6qTJ3HHECrhIzZ87UihUrtHbtWgUHB3u7HPigc+fOadiwYXrzzTdVvXp1b5eDMsLpdCoiIkJvvPGG2rdvr3vuuUdPP/20Fi1a5O3S4KNSUlI0ffp0vf766/r888+1Zs0arV+/XtOmTfN2acCfwhGnElK9enX5+/srMzPTbTwzM1ORkZH5LhMZGWlpPq4unvRMrlmzZmnmzJn66KOP1Lp165IsEz7Eas8cPXpUx48fV79+/VxjTqdTklShQgUdOnRIjRo1Ktmi4VWe/J6pVauWAgIC5O/v7xpr3ry5MjIydOnSJQUGBpZozfAuT3rm2Wef1bBhwzRmzBhJUnR0tM6fP68HHnhATz/9tPz8+Hd7uCvob+Dw8HCfOdokccSpxAQGBqp9+/basmWLa8zpdGrLli2KiYnJd5mYmBi3+ZKUnJxc4HxcXTzpGUl6+eWXNW3aNG3atEk33HBDaZQKH2G1Z6677jodOHBA+/fvdz3uuOMO112MoqKiSrN8eIEnv2duvvlmHTlyxBWyJenbb79VrVq1CE3lgCc9c+HChTzhKDd4G4ZRcsWizCozfwN7++4UV7MVK1YYQUFBRkJCgvF///d/xgMPPGBUqVLFyMjIMAzDMIYNG2Y8+eSTrvmffPKJUaFCBWPWrFnG119/bUyePNkICAgwDhw44K1dQCmz2jMzZ840AgMDjdWrVxvp6emux7lz57y1CyhlVnvmj7irXvljtWfS0tKMsLAwY9y4ccahQ4eMDz/80IiIiDBeeOEFb+0CSpnVnpk8ebIRFhZmvPfee8axY8eMpKQko1GjRsbAgQO9tQsoZefOnTP27dtn7Nu3z5BkzJkzx9i3b59x4sQJwzAM48knnzSGDRvmmn/s2DEjJCTEmDRpkvH1118bCxYsMPz9/Y1NmzZ5axfyRXAqYa+++qpx7bXXGoGBgcZNN91k7N692/Valy5djBEjRrjNf//9942mTZsagYGBRsuWLY3169eXcsXwNis9U69ePUNSnsfkyZNLv3B4jdXfM1ciOJVPVntm165dRocOHYygoCCjYcOGxosvvmhcvny5lKuGN1npGYfDYUyZMsVo1KiRERwcbERFRRkPPfSQcfr06dIvHF6xbdu2fP8+ye2TESNGGF26dMmzzPXXX28EBgYaDRs2NJYsWVLqdZuxGQbHTAEAAACgMFzjBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAod7p27aoJEyZ4uwwAQBlCcAIAlCn9+vVTr1698n3t448/ls1m05dfflnKVQEArnYEJwBAmTJ69GglJyfrhx9+yPPakiVLdMMNN6h169ZeqAwAcDUjOAEAypTbb79dNWrUUEJCgtt4dna2Vq1apbi4OA0ePFh16tRRSEiIoqOj9d577xW6TpvNpnXr1rmNValSxW0b33//vQYOHKgqVaqoWrVq6t+/v44fP148OwUA8HkEJwBAmVKhQgUNHz5cCQkJMgzDNb5q1Srl5ORo6NChat++vdavX6+DBw/qgQce0LBhw/Tpp596vE2Hw6HY2FiFhYXp448/1ieffKLQ0FD16tVLly5dKo7dAgD4OIITAKDMue+++3T06FFt377dNbZkyRL99a9/Vb169fT444/r+uuvV8OGDfXwww+rV69eev/99z3e3sqVK+V0OvXWW28pOjpazZs315IlS5SWlqaUlJRi2CMAgK8jOAEAypzrrrtOHTt21DvvvCNJOnLkiD7++GONHj1aOTk5mjZtmqKjo1WtWjWFhoZq8+bNSktL83h7X3zxhY4cOaKwsDCFhoYqNDRU1apV08WLF3X06NHi2i0AgA+r4O0CAADwxOjRo/Xwww9rwYIFWrJkiRo1aqQuXbropZde0vz58zVv3jxFR0erUqVKmjBhQqGn1NlsNrfT/qTfT8/LlZ2drfbt2+t//ud/8ixbo0aN4tspAIDPIjgBAMqkgQMHavz48UpMTNTSpUv197//XTabTZ988on69++voUOHSpKcTqe+/fZbtWjRosB11ahRQ+np6a7nhw8f1oULF1zP27Vrp5UrVyoiIkLh4eElt1MAAJ/FqXoAgDIpNDRU99xzj+Lj45Wenq6RI0dKkpo0aaLk5GTt2rVLX3/9tf72t78pMzOz0HXdeuuteu2117Rv3z7t2bNHDz74oAICAlyv33vvvapevbr69++vjz/+WN99951SUlL0yCOP5HtbdADA1YfgBAAos0aPHq3Tp08rNjZWtWvXliQ988wzateunWJjY9W1a1dFRkYqLi6u0PXMnj1bUVFR6ty5s4YMGaLHH39cISEhrtdDQkK0Y8cOXXvttbrrrrvUvHlzjR49WhcvXuQIFACUEzbjjyd1AwAAAADccMQJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEz8P3+5lQYTS/WlAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check normalization range\n",
    "print(\"Normalization Check:\")\n",
    "print(f\"Minimum value: {X_tensor.min().item():.6f}\")\n",
    "print(f\"Maximum value: {X_tensor.max().item():.6f}\")\n",
    "print(f\"Mean value: {X_tensor.mean().item():.6f}\")\n",
    "\n",
    "# Create histogram to visualize distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(X_tensor.numpy().flatten(), bins=50)\n",
    "plt.title('Distribution of Normalized Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Additional checks\n",
    "assert X_tensor.min() >= 0, \"Values below 0 found\"\n",
    "assert X_tensor.max() <= 1, \"Values above 1 found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models initialized successfully!\n",
      "Generator parameters: 3601796\n",
      "Discriminator parameters: 3545089\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Create DataLoader\n",
    "def create_dataloader(X_tensor, y_tensor, batch_size=32):\n",
    "    dataset = data.TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "# 2. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tensor.squeeze(1), y_tensor, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Create models\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_length):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, sequence_length),\n",
    "            nn.Sigmoid()  # Ensure output is [0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, sequence_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(sequence_length, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            ResidualBlock(2048),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            ResidualBlock(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            ResidualBlock(512),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# 4. Initialize models\n",
    "sequence_length = X_tensor.shape[2]\n",
    "latent_dim = 100\n",
    "\n",
    "generator = Generator(latent_dim, sequence_length)\n",
    "discriminator = Discriminator(sequence_length)\n",
    "\n",
    "# 5. Create dataloaders\n",
    "train_loader = create_dataloader(X_train, y_train)\n",
    "test_loader = create_dataloader(X_test, y_test)\n",
    "\n",
    "print(\"Models initialized successfully!\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters())}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training Configuration\n",
    "config = {\n",
    "    'n_epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.0002,\n",
    "    'beta1': 0.5,\n",
    "    'latent_dim': 100,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "# Move models to device\n",
    "generator = generator.to(config['device'])\n",
    "discriminator = discriminator.to(config['device'])\n",
    "\n",
    "# Setup optimizers\n",
    "g_optimizer = Adam(generator.parameters(), lr=config['lr'], betas=(config['beta1'], 0.999))\n",
    "d_optimizer = Adam(discriminator.parameters(), lr=config['lr'], betas=(config['beta1'], 0.999))\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Lists to store metrics\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "print(f\"Training setup complete on device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training progress\n",
    "def plot_losses(d_losses, g_losses):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(d_losses, label='Discriminator Loss', alpha=0.8)\n",
    "    plt.plot(g_losses, label='Generator Loss', alpha=0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('GAN Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Modify learning rate\n",
    "config['lr'] = 0.0001  # Reduce learning rate\n",
    "\n",
    "# Add gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "\n",
    "# Update optimizers\n",
    "g_optimizer = Adam(generator.parameters(), lr=config['lr'], betas=(0.5, 0.999))\n",
    "d_optimizer = Adam(discriminator.parameters(), lr=config['lr'], betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1216153583.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    d_losses.append(d_epoch_loss/len(train_loader))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Batch [30/40]\n",
      "Epoch [1/100] D_loss: 0.1648 G_loss: 19.5993\n",
      "Epoch [2/100] Batch [30/40]\n",
      "Epoch [2/100] D_loss: 0.1840 G_loss: 18.9878\n",
      "Epoch [3/100] Batch [30/40]\n",
      "Epoch [3/100] D_loss: 0.1666 G_loss: 18.5643\n",
      "Epoch [4/100] Batch [30/40]\n",
      "Epoch [4/100] D_loss: 0.1526 G_loss: 18.1249\n",
      "Epoch [5/100] Batch [30/40]\n",
      "Epoch [5/100] D_loss: 0.1523 G_loss: 18.7198\n",
      "Epoch [6/100] Batch [30/40]\n",
      "Epoch [6/100] D_loss: 0.1598 G_loss: 18.4655\n",
      "Epoch [7/100] Batch [30/40]\n",
      "Epoch [7/100] D_loss: 0.1476 G_loss: 18.4071\n",
      "Epoch [8/100] Batch [30/40]\n",
      "Epoch [8/100] D_loss: 0.1460 G_loss: 18.6833\n",
      "Epoch [9/100] Batch [30/40]\n",
      "Epoch [9/100] D_loss: 0.1554 G_loss: 17.7874\n",
      "Epoch [10/100] Batch [30/40]\n",
      "Epoch [10/100] D_loss: 0.1433 G_loss: 17.6819\n",
      "Epoch [11/100] Batch [30/40]\n",
      "Epoch [11/100] D_loss: 0.1391 G_loss: 16.5094\n",
      "Epoch [12/100] Batch [30/40]\n",
      "Epoch [12/100] D_loss: 0.1419 G_loss: 17.3514\n",
      "Epoch [13/100] Batch [30/40]\n",
      "Epoch [13/100] D_loss: 0.1410 G_loss: 17.4806\n",
      "Epoch [14/100] Batch [30/40]\n",
      "Epoch [14/100] D_loss: 0.1233 G_loss: 17.3191\n",
      "Epoch [15/100] Batch [30/40]\n",
      "Epoch [15/100] D_loss: 0.1327 G_loss: 16.6406\n",
      "Epoch [16/100] Batch [30/40]\n",
      "Epoch [16/100] D_loss: 0.1378 G_loss: 17.3968\n",
      "Epoch [17/100] Batch [30/40]\n",
      "Epoch [17/100] D_loss: 0.1235 G_loss: 15.1883\n",
      "Epoch [18/100] Batch [30/40]\n",
      "Epoch [18/100] D_loss: 0.1183 G_loss: 16.6019\n",
      "Epoch [19/100] Batch [30/40]\n",
      "Epoch [19/100] D_loss: 0.1108 G_loss: 17.2380\n",
      "Epoch [20/100] Batch [30/40]\n",
      "Epoch [20/100] D_loss: 0.1124 G_loss: 15.9937\n",
      "Epoch [21/100] Batch [30/40]\n",
      "Epoch [21/100] D_loss: 0.1056 G_loss: 17.4737\n",
      "Epoch [22/100] Batch [30/40]\n",
      "Epoch [22/100] D_loss: 0.1144 G_loss: 15.4446\n",
      "Epoch [23/100] Batch [30/40]\n",
      "Epoch [23/100] D_loss: 0.1132 G_loss: 15.6123\n",
      "Epoch [24/100] Batch [30/40]\n",
      "Epoch [24/100] D_loss: 0.0923 G_loss: 15.0819\n",
      "Epoch [25/100] Batch [30/40]\n",
      "Epoch [25/100] D_loss: 0.0988 G_loss: 14.9804\n",
      "Epoch [26/100] Batch [30/40]\n",
      "Epoch [26/100] D_loss: 0.1073 G_loss: 15.1612\n",
      "Epoch [27/100] Batch [30/40]\n",
      "Epoch [27/100] D_loss: 0.0977 G_loss: 15.1208\n",
      "Epoch [28/100] Batch [30/40]\n",
      "Epoch [28/100] D_loss: 0.0978 G_loss: 14.7773\n",
      "Epoch [29/100] Batch [30/40]\n",
      "Epoch [29/100] D_loss: 0.1004 G_loss: 14.5191\n",
      "Epoch [30/100] Batch [30/40]\n",
      "Epoch [30/100] D_loss: 0.1004 G_loss: 14.0043\n",
      "Epoch [31/100] Batch [30/40]\n",
      "Epoch [31/100] D_loss: 0.0927 G_loss: 13.6494\n",
      "Epoch [32/100] Batch [30/40]\n",
      "Epoch [32/100] D_loss: 0.0951 G_loss: 13.0095\n",
      "Epoch [33/100] Batch [30/40]\n",
      "Epoch [33/100] D_loss: 0.0851 G_loss: 14.2069\n",
      "Epoch [34/100] Batch [30/40]\n",
      "Epoch [34/100] D_loss: 0.0953 G_loss: 13.8962\n",
      "Epoch [35/100] Batch [30/40]\n",
      "Epoch [35/100] D_loss: 0.0883 G_loss: 14.3826\n",
      "Epoch [36/100] Batch [30/40]\n",
      "Epoch [36/100] D_loss: 0.0825 G_loss: 13.3385\n",
      "Epoch [37/100] Batch [30/40]\n",
      "Epoch [37/100] D_loss: 0.1025 G_loss: 12.8458\n",
      "Epoch [38/100] Batch [30/40]\n",
      "Epoch [38/100] D_loss: 0.0792 G_loss: 13.7037\n",
      "Epoch [39/100] Batch [30/40]\n",
      "Epoch [39/100] D_loss: 0.0972 G_loss: 13.1006\n",
      "Epoch [40/100] Batch [30/40]\n",
      "Epoch [40/100] D_loss: 0.0876 G_loss: 12.3560\n",
      "Epoch [41/100] Batch [30/40]\n",
      "Epoch [41/100] D_loss: 0.0803 G_loss: 12.4521\n",
      "Epoch [42/100] Batch [30/40]\n",
      "Epoch [42/100] D_loss: 0.0781 G_loss: 13.3280\n",
      "Epoch [43/100] Batch [30/40]\n",
      "Epoch [43/100] D_loss: 0.0829 G_loss: 11.7562\n",
      "Epoch [44/100] Batch [30/40]\n",
      "Epoch [44/100] D_loss: 0.0820 G_loss: 11.9186\n",
      "Epoch [45/100] Batch [30/40]\n",
      "Epoch [45/100] D_loss: 0.0766 G_loss: 11.6406\n",
      "Epoch [46/100] Batch [30/40]\n",
      "Epoch [46/100] D_loss: 0.0791 G_loss: 11.7376\n",
      "Epoch [47/100] Batch [30/40]\n",
      "Epoch [47/100] D_loss: 0.0754 G_loss: 11.0763\n",
      "Epoch [48/100] Batch [30/40]\n",
      "Epoch [48/100] D_loss: 0.0790 G_loss: 10.9231\n",
      "Epoch [49/100] Batch [30/40]\n",
      "Epoch [49/100] D_loss: 0.0888 G_loss: 10.5802\n",
      "Epoch [50/100] Batch [30/40]\n",
      "Epoch [50/100] D_loss: 0.0827 G_loss: 10.6076\n",
      "Epoch [51/100] Batch [30/40]\n",
      "Epoch [51/100] D_loss: 0.0727 G_loss: 10.2292\n",
      "Epoch [52/100] Batch [30/40]\n",
      "Epoch [52/100] D_loss: 0.0739 G_loss: 9.8642\n",
      "Epoch [53/100] Batch [30/40]\n",
      "Epoch [53/100] D_loss: 0.0838 G_loss: 10.4195\n",
      "Epoch [54/100] Batch [30/40]\n",
      "Epoch [54/100] D_loss: 0.0736 G_loss: 9.3265\n",
      "Epoch [55/100] Batch [30/40]\n",
      "Epoch [55/100] D_loss: 0.0794 G_loss: 9.3517\n",
      "Epoch [56/100] Batch [30/40]\n",
      "Epoch [56/100] D_loss: 0.0764 G_loss: 9.0556\n",
      "Epoch [57/100] Batch [30/40]\n",
      "Epoch [57/100] D_loss: 0.0792 G_loss: 8.3863\n",
      "Epoch [58/100] Batch [30/40]\n",
      "Epoch [58/100] D_loss: 0.0694 G_loss: 8.4364\n",
      "Epoch [59/100] Batch [30/40]\n",
      "Epoch [59/100] D_loss: 0.0772 G_loss: 8.5610\n",
      "Epoch [60/100] Batch [30/40]\n",
      "Epoch [60/100] D_loss: 0.0701 G_loss: 7.6130\n",
      "Epoch [61/100] Batch [30/40]\n",
      "Epoch [61/100] D_loss: 0.0701 G_loss: 8.0455\n",
      "Epoch [62/100] Batch [30/40]\n",
      "Epoch [62/100] D_loss: 0.0702 G_loss: 8.0439\n",
      "Epoch [63/100] Batch [30/40]\n",
      "Epoch [63/100] D_loss: 0.0736 G_loss: 7.7290\n",
      "Epoch [64/100] Batch [30/40]\n",
      "Epoch [64/100] D_loss: 0.0847 G_loss: 7.6963\n",
      "Epoch [65/100] Batch [30/40]\n",
      "Epoch [65/100] D_loss: 0.0591 G_loss: 7.5081\n",
      "Epoch [66/100] Batch [0/40]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d_losses, g_losses\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m d_losses, g_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     33\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(discriminator(generated_data), valid)\n\u001b[1;32m     36\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/project/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm  # Changed to tqdm.auto\n",
    "\n",
    "# Create folder for model checkpoints\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_dir = f'model_checkpoints_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "def train_gan():\n",
    "    # Initialize loss lists\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    for epoch in range(config['n_epochs']):\n",
    "        d_epoch_loss = 0\n",
    "        g_epoch_loss = 0\n",
    "        \n",
    "        # Modified progress bar\n",
    "        for batch_idx, (real_data, _) in enumerate(train_loader):\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{config[\"n_epochs\"]}] Batch [{batch_idx}/{len(train_loader)}]', end='\\r')\n",
    "                \n",
    "            batch_size = real_data.size(0)\n",
    "            real_data = real_data.to(config['device'])\n",
    "\n",
    "            # Ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(config['device'])\n",
    "            fake = torch.zeros(batch_size, 1).to(config['device'])\n",
    "\n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            z = torch.randn(batch_size, config['latent_dim']).to(config['device'])\n",
    "            generated_data = generator(z)\n",
    "            g_loss = adversarial_loss(discriminator(generated_data), valid)\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(generated_data.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Update losses\n",
    "            d_epoch_loss += d_loss.item()\n",
    "            g_epoch_loss += g_loss.item()\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'\\nEpoch [{epoch+1}/{config[\"n_epochs\"]}] D_loss: {d_epoch_loss/len(train_loader):.4f} G_loss: {g_epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "        # Save epoch losses\n",
    "        d_losses.append(d_epoch_loss/len(train_loader))\n",
    "        g_losses.append(g_epoch_loss/len(train_loader))\n",
    "        \n",
    "        # Save model checkpoint every 1000 epochs\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                'g_loss': g_epoch_loss/len(train_loader),\n",
    "                'd_loss': d_epoch_loss/len(train_loader)\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return d_losses, g_losses\n",
    "\n",
    "# Start training\n",
    "d_losses, g_losses = train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
