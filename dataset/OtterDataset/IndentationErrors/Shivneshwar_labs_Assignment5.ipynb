{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3adec658",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3161976162.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 35\u001b[0;36m\u001b[0m\n\u001b[0;31m    def ISTA(samples, xcoord, ycoord, alpha, xinit, step=0.99, iternum=100):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.fftpack as transform\n",
    "\n",
    "\n",
    "# Take a number sample_num of samples and generate their\n",
    "# horizontal and vertical coordinates (indicies) randomly in arrays xcoord, ycoord\n",
    "# Store these samples in an array called y.\n",
    "\n",
    "# The goal is to find the dct components x of the image from y and\n",
    "# then use inverse dct to reconstruct the image.\n",
    "\n",
    "def sq_loss_gradient(x,y, xcoord, ycoord):\n",
    "    # samp_approx is Ax\n",
    "    img_approx= transform.idct(transform.idct(x, axis=1, norm='ortho'),axis=0, norm='ortho')\n",
    "    samp_approx=np.zeros((samp))\n",
    "    for l in range(sample_num):\n",
    "        samp_approx[l]=img_approx[xcoord[l],ycoord[l]]\n",
    "\n",
    "    # The following calculates e=Ax-y\n",
    "    error=samp_approx-samples\n",
    "    img_err=np.zeros(bw_img.shape)\n",
    "    for l in range(samp):\n",
    "        img_err[xcoord[l],ycoord[l]]=error[l]\n",
    "    # This calculates A^Te which is the gradient \n",
    "    grad=transform.dct(transform.dct(img_err, axis=0, norm='ortho'),axis=1, norm='ortho')\n",
    "\n",
    "    return grad\n",
    "\n",
    "def shrinkage(x,alpha):\n",
    "    # Implement the shrinkage operator which is the proximal operator\n",
    "    # of the regularizer\n",
    "\n",
    "def ISTA(samples, xcoord, ycoord, alpha, xinit, step=0.99, iternum=100):\n",
    "    # Implement the ISTA algorithm as in the previous exercise\n",
    "    # In this case, it should work for any step size less than or equal 1\n",
    "    # Your require the gradient function and the proximal function (shrinkage)\n",
    "    # Initialization:\n",
    "    x=xinit\n",
    "    for iter in range(iternum):\n",
    "        # Implement the iteration and also calculate the objective value and\n",
    "        # the vaue of the regularization term at each iteration\n",
    "    return x\n",
    "\n",
    "def ISTA(samples, xcoord, ycoord, alpha, xinit, step=0.99, iternum=100):\n",
    "    # Implement the FISTA algorithm similar to ISTA\n",
    "    # It needs extra variables t and y\n",
    "\n",
    "\n",
    "# run ISTA/FISTA and generate the approximate DCT coefficients x\n",
    "# Then, generate the recovered image\n",
    "img_approx= transform.idct(transform.idct(x, axis=1),axis=0, , norm='ortho')\n",
    "\n",
    "# Plot the result\n",
    "fig = plt.figure()    \n",
    "plt.imshow(img_approx, cmap='gray')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1197a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2d03c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta = 0.1, Loss = 2.9564469942565876e-14\n",
      "Beta = 0.5, Loss = 4.023360010629817e-14\n",
      "Beta = 0.9, Loss = 8.341314754910837e-14\n"
     ]
    }
   ],
   "source": [
    "def proximal_gradient(X, B, R, beta, step_size, max_iter=10000):\n",
    "    m, n = X.shape\n",
    "    k = B.shape[1]\n",
    "    momentum_B = np.zeros_like(B)\n",
    "    momentum_R = np.zeros_like(R)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Compute gradient\n",
    "        gradient_B = -2 * np.dot(X - np.dot(B, R), R.T)\n",
    "        gradient_R = -2 * np.dot(B.T, X - np.dot(B, R))\n",
    "\n",
    "        # Update momentum\n",
    "        momentum_B = beta * momentum_B + step_size * gradient_B\n",
    "        momentum_R = beta * momentum_R + step_size * gradient_R\n",
    "        \n",
    "        # Update variables\n",
    "        B -= momentum_B\n",
    "        R -= momentum_R\n",
    "\n",
    "    return B, R\n",
    "\n",
    "np.random.seed(0)\n",
    "m, n, k = 10, 20, 100\n",
    "B_true = np.random.choice([0, 1], size=(m, k))\n",
    "R_true = np.random.uniform(0, 1, size=(k, n))\n",
    "X = np.dot(B_true, R_true)\n",
    "beta_values = [0.1, 0.5, 0.9]\n",
    "B_init = np.random.uniform(0, 1, size=(m, k))\n",
    "R_init = np.random.uniform(0, 1, size=(k, n))\n",
    "\n",
    "for beta in beta_values:\n",
    "    B, R = proximal_gradient(X, B_init, R_init, beta, step_size=1e-3)\n",
    "    loss = np.linalg.norm(X - np.dot(B, R))\n",
    "    print(f\"Beta = {beta}, Loss = {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
