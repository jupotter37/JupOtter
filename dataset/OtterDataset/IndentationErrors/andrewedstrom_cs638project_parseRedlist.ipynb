{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# - name\n",
    "# - taxonomic classification\n",
    "# - countries of occurrence, \n",
    "# - endangered status,\n",
    "# - ecology\n",
    "########- generation length (not listed often enough?)\n",
    "# - population trend (decreasing, increasing, unkown),\n",
    "########- subspecies list (not gonna work)\n",
    "# - listed on CITES or not (conservation actions)\n",
    "# - threats\n",
    "\n",
    "#     try:\n",
    "#         # code to process download here\n",
    "#     except Exception as e:     # most generic exception you can catch\n",
    "#         logf.write(\"Failed to download {0}: {1}\\n\".format(str(download), str(e)))\n",
    "#         # optional: delete local version of failed download\n",
    "#     finally:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getRedlistFeaturesDF( htmlSiteSoup ):\n",
    "\n",
    "    featureList = {}\n",
    "\n",
    "    ### animal name ###\n",
    "    try:\n",
    "        animalName = soup.title.string \n",
    "        animalName\n",
    "        # seems that some include nickname, see how many later after table is made\n",
    "        # process out nickname in cleaning stage, if desired\n",
    "        featureList['animalName'] = animalName\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['animalName'] = 'NA'\n",
    "\n",
    "\n",
    "    ### taxonomy ###\n",
    "    try:\n",
    "        taxonomyList = soup.body.find_all('table',{'class':\"tab_data\"})[0].find_all('td') # returns list of taxonomic categorization from kindgom to family (skips genus and species)\n",
    "        taxonomyStrList = []\n",
    "\n",
    "        for index, item in enumerate(taxonomyList):\n",
    "            taxonomyStrList.append(str(item)) #= str()\n",
    "        KINGDOM = taxonomyStrList[0][4:-5]\n",
    "        PHYLUM = taxonomyStrList[1][4:-5]\n",
    "        CLASS = taxonomyStrList[2][4:-5]\n",
    "        ORDER = taxonomyStrList[3][4:-5]\n",
    "        FAMILY = taxonomyStrList[4][4:-5]\n",
    "        featureList['KINGDOM'] = KINGDOM\n",
    "        featureList['PHYLUM'] = PHYLUM\n",
    "        featureList['CLASS'] = CLASS\n",
    "        featureList['ORDER'] = ORDER\n",
    "        featureList['FAMILY'] = FAMILY\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['KINGDOM'] = 'NA'\n",
    "        featureList['PHYLUM'] = 'NA'\n",
    "        featureList['CLASS'] = 'NA'\n",
    "        featureList['ORDER'] = 'NA'\n",
    "        featureList['FAMILY'] = 'NA'\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    ### countries of occurence ###\n",
    "    try:\n",
    "        rangeList = soup.body.find_all('div',{'class':\"group\"})\n",
    "\n",
    "        countriesPerGroupType = []\n",
    "\n",
    "        # loop through groups (e.g. Possibly exting, Regionally extinct, Native)\n",
    "        for index, item in enumerate(rangeList):\n",
    "            listCountries = item.contents # list with first item as group name, and rest as countries\n",
    "            countriesPerGroupType.append(listCountries)\n",
    "\n",
    "        # TO-DO: further country list parsing\n",
    "        fullListCountries = ''\n",
    "        for groupNum, group in enumerate(countriesPerGroupType):\n",
    "            groupName = group[0]\n",
    "            groupCountries = group[1]\n",
    "            fullListCountries = str(groupCountries) +  \"; \" + fullListCountries\n",
    "\n",
    "        COUNTRIES = fullListCountries\n",
    "        COUNTRIES\n",
    "        featureList['COUNTRIES'] = COUNTRIES\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['COUNTRIES'] = 'NA'\n",
    "        \n",
    "\n",
    "    ### current population trend ###\n",
    "    try:\n",
    "        popTrend = soup.body.find_all('td',{'id':\"popTrend\"})\n",
    "        POP_TREND = popTrend[0].find_all('span')[0].string\n",
    "        POP_TREND\n",
    "        featureList['POP_TREND'] = POP_TREND\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['POP_TREND'] = 'NA'\n",
    "\n",
    "\n",
    "    ### status ### \n",
    "    try:\n",
    "        statusInfo = soup.body.find_all('table',{'class':\"tab_data\"})\n",
    "        # again, need to clean this bit by grabbing first string between first two new lines\n",
    "        STATUS = statusInfo[2].find_all('td')[1].contents[0]\n",
    "        featureList['STATUS'] = STATUS\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['STATUS'] = 'NA'\n",
    "\n",
    "\n",
    "    ### ecology ###\n",
    "    try:\n",
    "        ecologyInfo = soup.body.find_all('strong')\n",
    "        for index, item in enumerate(ecologyInfo):\n",
    "            if item.contents == ['Systems:']:\n",
    "                ecology = str(item.parent.next_sibling)\n",
    "\n",
    "        ECOLOGY = ecology[4:-5]\n",
    "        ECOLOGY\n",
    "        featureList['ECOLOGY'] = ECOLOGY\n",
    "    except Exception as e:    \n",
    "        featureList['ECOLOGY']  = 'NA'\n",
    "\n",
    "\n",
    "    ### threat list ###\n",
    "    try:   \n",
    "        threatInfo = soup.body.find_all('strong')\n",
    "        for index, item in enumerate(threatInfo):\n",
    "            if item.contents == ['Major Threat(s):']:\n",
    "#                 threatText = item.parent.next_sibling.parent.find_all('p')\n",
    "                THREAT_PARAGRAPH = item.parent.parent.find_all('td')[1].contents\n",
    "#         THREAT_PARAGRAPH = threatText[1].contents[0]\n",
    "        THREAT_PARAGRAPH\n",
    "        featureList['THREAT_PARAGRAPH'] = THREAT_PARAGRAPH\n",
    "    \n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['THREAT_PARAGRAPH'] = 'NA'\n",
    "\n",
    "    \n",
    "#     threatInfo = soup.body.find_all('strong')\n",
    "#     for index, item in enumerate(threatInfo):\n",
    "#         if item.contents == ['Major Threat(s):']:\n",
    "#             threatText = item.parent.next_sibling.parent.find_all('p')\n",
    "\n",
    "#     THREAT_PARAGRAPH = threatText[1].contents[0]\n",
    "#     THREAT_PARAGRAPH\n",
    "#     featureList['THREAT_PARAGRAPH'] = THREAT_PARAGRAPH\n",
    "\n",
    "\n",
    "\n",
    "    ### listed on CITES or not (conservation actions) ###\n",
    "    try: \n",
    "        for index, item in enumerate(threatInfo):\n",
    "        #     print(item.contents)\n",
    "            if item.contents == ['Conservation Actions:']:\n",
    "    #                 print(item.parent.parent.find_all('td'))\n",
    "                CONSERVATION_PARAGRAPH = item.parent.parent.find_all('td')[1].contents\n",
    "\n",
    "    #             print(CONSERVATION_PARAGRAPH.contents)\n",
    "    #         fullString = ''\n",
    "    #         for index, item in enumerate(CONSERVATION_PARAGRAPH):\n",
    "    #             fullString = str(item.contents) + fullString\n",
    "\n",
    "    #         CONSERVATION_PARAGRAPH = fullString\n",
    "    #         print('CONSERVATION_PARAGRAPH' + CONSERVATION_PARAGRAPH) \n",
    "\n",
    "        featureList['CONSERVATION_PARAGRAPH'] = str(CONSERVATION_PARAGRAPH)\n",
    "\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        featureList['CONSERVATION_PARAGRAPH'] = 'NA'\n",
    "\n",
    "    \n",
    "    \n",
    "    # featureList = {}\n",
    "    # featureList = getRedlistFeatures(soup)\n",
    "#     print(featureList)\n",
    "#     print(featureList.keys())\n",
    "    columns =['htmlPage','animalName','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "#     print(len(columns))\n",
    "#     print(featureList.items())\n",
    "#     df = pd.DataFrame.from_items(featureList.items())#, columns=['Date', 'DateValue'])\n",
    "    df = pd.DataFrame(featureList,dtype=None,index=[0])\n",
    "#     print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Initialize df to save to csv    \n",
    "columns =['htmlPage','animalName','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "featureList = pd.DataFrame(columns=columns)\n",
    "with open('redlistTable.csv', 'w') as f:\n",
    "    newFeatureList.to_csv(f, header=True)\n",
    "\n",
    "## Get list of links\n",
    "import traceback\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"/Users/tammi/Desktop/cs638project/redlist-endangered-html/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# onlyfiles = onlyfiles[0:100]\n",
    "\n",
    "# Get ability to log errors\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('getFeatures')\n",
    "hdlr = logging.FileHandler('//Users/tammi/Desktop/errorLog_endangered.txt')\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.DEBUG)\n",
    "# logging.basicConfig( filename=,\n",
    "#                      filemode='w',\n",
    "#                      level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# For loop of links, run program\n",
    "for index, file in enumerate(onlyfiles):\n",
    "    try:\n",
    "#         print(index)\n",
    "        soup = BeautifulSoup(open(mypath + file),\"lxml\")\n",
    "        newFeatureList = getRedlistFeaturesDF(soup)\n",
    "        newFeatureList['htmlPage'] = str(file)\n",
    "        columns =['animalName','htmlPage','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "        newFeatureList = newFeatureList[columns]\n",
    "        \n",
    "        with open('redlistTable.csv', 'a') as f:\n",
    "            newFeatureList.to_csv(f, header=False)\n",
    "        \n",
    "#         featureList = featureList.append(newFeatureList, ignore_index=True)\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "#         traceback.print_exc()   \n",
    "#         logger.exception(\"oops!\")\n",
    "        x=1\n",
    "\n",
    "\n",
    "\n",
    "###### NEXT SET OF HTML FILES ######\n",
    "# # Initialize df to save to csv    \n",
    "# columns =['htmlPage','animalName','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "# featureList = pd.DataFrame(columns=columns)\n",
    "\n",
    "## Get list of links\n",
    "mypath = \"/Users/tammi/Desktop/cs638project/redlist-critically-endangered-html/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# onlyfiles = onlyfiles[0:100]\n",
    "\n",
    "\n",
    "# Get ability to log errors\n",
    "import logging\n",
    "logger = logging.getLogger('getFeatures')\n",
    "hdlr = logging.FileHandler('//Users/tammi/Desktop/errorLog_critEndangered.txt')\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# For loop of links, run program\n",
    "for index, file in enumerate(onlyfiles):\n",
    "    try:\n",
    "#         print(file)\n",
    "#         print(\"sec set: \" + str(index))\n",
    "        soup = BeautifulSoup(open(mypath + file),\"lxml\")\n",
    "#         soup = BeautifulSoup(open(\"/Users/tammi/Desktop/cs638project/redlist-endangered-html/3334.html\"),\"lxml\")\n",
    "        \n",
    "        newFeatureList = getRedlistFeaturesDF(soup)\n",
    "        newFeatureList['htmlPage'] = str(file)\n",
    "#         print(newFeatureList)\n",
    "#         print(newFeatureList)\n",
    "#         featureList = featureList.append(newFeatureList, ignore_index=True)\n",
    "        columns =['animalName','htmlPage','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "        newFeatureList = newFeatureList[columns]\n",
    "    \n",
    "        with open('redlistTable.csv', 'a') as f:\n",
    "            newFeatureList.to_csv(f, header=False)\n",
    "        \n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "#         traceback.print_exc()   \n",
    "#         logger.exception(\"oops!\")\n",
    "#         print(file)\n",
    "        x=1\n",
    "\n",
    "\n",
    "# columns =['animalName','htmlPage','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "# featureList = featureList[columns]\n",
    "# featureList.to_csv('redlistTable.csv')\n",
    "# featureList.to_csv('critEndangeredTable.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001.html\n",
      "Animalia\n",
      "Chordata\n",
      "Mammalia\n",
      "Afrosoricida\n",
      "Chrysochloridae\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "['Protected in the De Hoek, New Agatha and Woodbush Forest Reserves. Research needed to document most aspects of natural history, ecology, evolutionary relationships, phylogeography and population genetics of this species. In the former Transvaal Province (South Africa), its was given the highest regional priority score for mammals (Freitag and van Jaarsveld 1997). It currently ranks among the top 100 mammalian species (no. 73) of the EDGE of Existence Programme (Zoological Society of London), which aims to conserve the world’s Evolutionary Distinct and Globally Endangered species (', <span class=\"citation\"><em>Mammals on the EDGE: Conservation Priorities Based on Threat and Phylogeny, </em>Isaac<em> et al. </em>2007 and subsequent updates). The species is not receiving dedicated conservation attention at present.</span>]\n"
     ]
    }
   ],
   "source": [
    "# - name\n",
    "# - taxonomic classification\n",
    "# - countries of occurrence, \n",
    "# - endangered status,\n",
    "# - ecology\n",
    "########- generation length (not listed often enough?)\n",
    "# - population trend (decreasing, increasing, unkown),\n",
    "########- subspecies list (not gonna work)\n",
    "# - listed on CITES or not (conservation actions)\n",
    "# - threats\n",
    "\n",
    "#     try:\n",
    "#         # code to process download here\n",
    "#     except Exception as e:     # most generic exception you can catch\n",
    "#         logf.write(\"Failed to download {0}: {1}\\n\".format(str(download), str(e)))\n",
    "#         # optional: delete local version of failed download\n",
    "#     finally:\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize df to save to csv    \n",
    "columns =['animalName','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "featureList = pd.DataFrame(columns=columns)\n",
    "\n",
    "## Get list of links\n",
    "import traceback\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"/Users/tammi/Desktop/cs638project/redlist-endangered-html/\"\n",
    "onlyFiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# print(onlyfiles)\n",
    "# print(onlyFiles[1:5])\n",
    "\n",
    "onlyFiles = onlyFiles[1:2]\n",
    "# print(onlyFiles)\n",
    "\n",
    "\n",
    "# Get ability to log errors\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('getFeatures')\n",
    "hdlr = logging.FileHandler('//Users/tammi/Desktop/getFeatures_errorLog.txt')\n",
    "logger.addHandler(hdlr) \n",
    "logger.setLevel(logging.DEBUG)\n",
    "# logging.basicConfig( filename=,\n",
    "#                      filemode='w',\n",
    "#                      level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# For loop of links, run program\n",
    "for index, file in enumerate(onlyFiles):\n",
    "    try:\n",
    "        print(file)\n",
    "        file = '1087.html'\n",
    "        soup = BeautifulSoup(open(mypath + file),\"lxml\")\n",
    "#         soup = BeautifulSoup(open(\"/Users/tammi/Desktop/cs638project/redlist-endangered-html/3334.html\"),\"lxml\")\n",
    "\n",
    "        featureList = {}\n",
    "\n",
    "        ### animal name ###\n",
    "        animalName = soup.title.string \n",
    "        animalName\n",
    "        # seems that some include nickname, see how many later after table is made\n",
    "        # process out nickname in cleaning stage, if desired\n",
    "        featureList['animalName'] = animalName\n",
    "\n",
    "\n",
    "        ### taxonomy ###\n",
    "        taxonomyList = soup.body.find_all('table',{'class':\"tab_data\"})[0].find_all('td') # returns list of taxonomic categorization from kindgom to family (skips genus and species)\n",
    "        #print(taxonomyList)\n",
    "        taxonomyStrList = []\n",
    "\n",
    "        for index, item in enumerate(taxonomyList):\n",
    "            #print(index)\n",
    "            taxonomyStrList.append(str(item)) #= str()\n",
    "            #print(taxonomyStrList)\n",
    "            #print(item)\n",
    "\n",
    "        KINGDOM = taxonomyStrList[0][4:-5]\n",
    "        PHYLUM = taxonomyStrList[1][4:-5]\n",
    "        CLASS = taxonomyStrList[2][4:-5]\n",
    "        ORDER = taxonomyStrList[3][4:-5]\n",
    "        FAMILY = taxonomyStrList[4][4:-5]\n",
    "        featureList['KINGDOM'] = KINGDOM\n",
    "        featureList['PHYLUM'] = PHYLUM\n",
    "        featureList['CLASS'] = CLASS\n",
    "        featureList['ORDER'] = ORDER\n",
    "        featureList['FAMILY'] = FAMILY\n",
    "\n",
    "        print(KINGDOM)\n",
    "        print(PHYLUM)\n",
    "        print(CLASS)\n",
    "        print(ORDER)\n",
    "        print(FAMILY)\n",
    "\n",
    "        ### countries of occurence ###\n",
    "        rangeList = soup.body.find_all('div',{'class':\"group\"})\n",
    "\n",
    "        countriesPerGroupType = []\n",
    "\n",
    "        # loop through groups (e.g. Possibly exting, Regionally extinct, Native)\n",
    "        for index, item in enumerate(rangeList):\n",
    "            listCountries = item.contents # list with first item as group name, and rest as countries\n",
    "            countriesPerGroupType.append(listCountries)\n",
    "\n",
    "        # TO-DO: further country list parsing\n",
    "        print()\n",
    "        fullListCountries = ''\n",
    "        for groupNum, group in enumerate(countriesPerGroupType):\n",
    "            groupName = group[0]\n",
    "            groupCountries = group[1]\n",
    "            fullListCountries = groupCountries +  \"; \" + fullListCountries\n",
    "            #print(groupCountries)\n",
    "\n",
    "        COUNTRIES = fullListCountries\n",
    "        COUNTRIES\n",
    "        featureList['COUNTRIES'] = COUNTRIES\n",
    "\n",
    "\n",
    "\n",
    "        ### current population trend ###\n",
    "        threatInfo = soup.body.find_all('strong')\n",
    "        for index, item in enumerate(threatInfo):\n",
    "#             print(item)\n",
    "            if item.contents == ['Current Population Trend:']:\n",
    "#                 print(item)\n",
    "                threatText = item.parent.next_sibling.parent.find_all('p')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        popTrend = soup.body.find_all('td',{'id':\"popTrend\"})\n",
    "#         print(popTrend)\n",
    "        POP_TREND = popTrend[0].find_all('span')[0].string\n",
    "        POP_TREND\n",
    "        featureList['POP_TREND'] = POP_TREND\n",
    "        \n",
    "        ### threat list ###\n",
    "        threatInfo = soup.body.find_all('strong')\n",
    "        for index, item in enumerate(threatInfo):\n",
    "            if item.contents == ['Major Threat(s):']:\n",
    "#                 print(item)\n",
    "                threatText = item.parent.next_sibling\n",
    "                print(threatText)\n",
    "#                 print(item.parent.parent.find_all('td')[1].contents)\n",
    "\n",
    "\n",
    "#                 threatText = item.parent.next_sibling.parent.find_all('p')\n",
    "        print(threatText)\n",
    "#         THREAT_PARAGRAPH = threatText[1].contents[0]\n",
    "#         THREAT_PARAGRAPH\n",
    "#         featureList['THREAT_PARAGRAPH'] = THREAT_PARAGRAPH\n",
    "\n",
    "\n",
    "        ### listed on CITES or not (conservation actions) ###\n",
    "        for index, item in enumerate(threatInfo):\n",
    "        #     print(item.contents)\n",
    "            if item.contents == ['Conservation Actions:']:\n",
    "                CONSERVATION_PARAGRAPH = item.parent.parent.find_all('td')[1].find_all('p')\n",
    "\n",
    "        fullString = ''\n",
    "\n",
    "        for index, item in enumerate(CONSERVATION_PARAGRAPH):\n",
    "            fullString = str(item.contents) + fullString\n",
    "\n",
    "        CONSERVATION_PARAGRAPH = fullString\n",
    "        \n",
    "        featureList['CONSERVATION_PARAGRAPH'] = CONSERVATION_PARAGRAPH\n",
    "\n",
    "\n",
    "    except Exception as e:     # most generic exception you can catch\n",
    "        traceback.print_exc()   \n",
    "        logger.exception(\"oops!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-fe57ed1e4e1a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-fe57ed1e4e1a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    statusInfo = soup.body.find_all('table',{'class':\"tab_data\"})\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        ### status ### \n",
    "        statusInfo = soup.body.find_all('table',{'class':\"tab_data\"})\n",
    "        # again, need to clean this bit by grabbing first string between first two new lines\n",
    "        STATUS = statusInfo[2].find_all('td')[1].contents[0]\n",
    "        print(STATUS)\n",
    "        featureList['STATUS'] = STATUS\n",
    "\n",
    "\n",
    "\n",
    "        ### ecology ###\n",
    "        ecologyInfo = soup.body.find_all('strong')\n",
    "        for index, item in enumerate(ecologyInfo):\n",
    "            if item.contents == ['Systems:']:\n",
    "                ecology = str(item.parent.next_sibling)\n",
    "\n",
    "        ECOLOGY = ecology[4:-5]\n",
    "        ECOLOGY\n",
    "        featureList['ECOLOGY'] = ECOLOGY\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        ### listed on CITES or not (conservation actions) ###\n",
    "        for index, item in enumerate(threatInfo):\n",
    "        #     print(item.contents)\n",
    "            if item.contents == ['Conservation Actions:']:\n",
    "        #         print(item)\n",
    "                CONSERVATION_PARAGRAPH = item.parent.parent.find_all('strong')[1].contents\n",
    "        #         print(threatText.contents)\n",
    "        fullString = ''\n",
    "\n",
    "        for index, item in enumerate(CONSERVATION_PARAGRAPH):\n",
    "    #         print(item)\n",
    "        #     print()\n",
    "            fullString = str(item) + fullString\n",
    "\n",
    "        CONSERVATION_PARAGRAPH = fullString\n",
    "    #     print(CONSERVATION_PARAGRAPH)\n",
    "        featureList['CONSERVATION_PARAGRAPH'] = CONSERVATION_PARAGRAPH\n",
    "\n",
    "        # featureList = {}\n",
    "        # featureList = getRedlistFeatures(soup)\n",
    "    #     print(featureList)\n",
    "    #     print(featureList.keys())\n",
    "        columns =['animalName','KINGDOM','PHYLUM','CLASS','ORDER','FAMILY','ECOLOGY','COUNTRIES','THREAT_PARAGRAPH','CONSERVATION_PARAGRAPH','POP_TREND','STATUS']\n",
    "    #     print(len(columns))\n",
    "    #     print(featureList.items())\n",
    "    #     df = pd.DataFrame.from_items(featureList.items())#, columns=['Date', 'DateValue'])\n",
    "        df = pd.DataFrame(featureList,dtype=None,index=[0])\n",
    "    #     print(df)\n",
    "\n",
    "        newFeatureList = df\n",
    "        featureList = featureList.append(newFeatureList, ignore_index=True)\n",
    "        featureList.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
