{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 261)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m261\u001b[0m\n\u001b[1;33m    X.drop(columns = ['matchId',\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Import necessary everyday os libs\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Import the usual suspects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def df_footprint_reduce(df, skip_obj=False, skip_int=False, skip_float=False, print_comparison=True):\n",
    "    '''\n",
    "    :param df              : Pandas Dataframe to shrink in memory footprint size\n",
    "    :param skip_obj        : If not desired string columns can be skipped during shrink operation\n",
    "    :param skip_int        : If not desired integer columns can be skipped during shrink operation\n",
    "    :param skip_float      : If not desired float columns can be skipped during shrink operation\n",
    "    :param print_comparison: Beware! Printing comparison needs calculation of each columns datasize\n",
    "                             so if you need speed turn this off. It's just here to show you info                            \n",
    "    :return                : Pandas Dataframe of exactly the same data and dtypes but in less memory footprint    \n",
    "    '''\n",
    "    if print_comparison:\n",
    "        print(f\"Dataframe size before shrinking column types into smallest possible: {round((sys.getsizeof(df)/1024/1024),4)} MB\")\n",
    "    for column in df.columns:\n",
    "        if (skip_obj is False) and (str(df[column].dtype)[:6] == 'object'):\n",
    "            num_unique_values = len(df[column].unique())\n",
    "            num_total_values = len(df[column])\n",
    "            if num_unique_values / num_total_values < 0.5:\n",
    "                df.loc[:,column] = df[column].astype('category')\n",
    "            else:\n",
    "                df.loc[:,column] = df[column]\n",
    "        elif (skip_int is False) and (str(df[column].dtype)[:3] == 'int'):\n",
    "            if df[column].min() > np.iinfo(np.int8).min and df[column].max() < np.iinfo(np.int8).max:\n",
    "                df[column] = df[column].astype(np.int8)\n",
    "            elif df[column].min() > np.iinfo(np.int16).min and df[column].max() < np.iinfo(np.int16).max:\n",
    "                df[column] = df[column].astype(np.int16)\n",
    "            elif df[column].min() > np.iinfo(np.int32).min and df[column].max() < np.iinfo(np.int32).max:\n",
    "                df[column] = df[column].astype(np.int32)\n",
    "        elif (skip_float is False) and (str(df[column].dtype)[:5] == 'float'):\n",
    "            if df[column].min() > np.finfo(np.float16).min and df[column].max() < np.finfo(np.float16).max:\n",
    "                df[column] = df[column].astype(np.float16)\n",
    "            elif df[column].min() > np.finfo(np.float32).min and df[column].max() < np.finfo(np.float32).max:\n",
    "                df[column] = df[column].astype(np.float32)\n",
    "    if print_comparison:\n",
    "        print(f\"Dataframe size after shrinking column types into smallest possible: {round((sys.getsizeof(df)/1024/1024),4)} MB\")\n",
    "    return df\n",
    "def df_null_cleaner(df, fill_with=None, drop_na=False, axis=0):\n",
    "    df[(df == np.NINF)] = np.NaN\n",
    "    df[(df == np.Inf)] = np.NaN\n",
    "    if drop_na:\n",
    "        df.dropna(axis=axis,inplace=True)\n",
    "    if ~fill_with:\n",
    "        df.fillna(fill_with, inplace=True)\n",
    "    return df\n",
    "def feature_engineering(df,is_train=True):\n",
    "    if is_train:          \n",
    "        df = df[df['maxPlace'] > 1].copy()\n",
    "\n",
    "    target = 'winPlacePerc'\n",
    "    print('Grouping similar match types together')\n",
    "    df.loc[(df['matchType'] == 'solo'), 'matchType'] = 1\n",
    "    df.loc[(df['matchType'] == 'normal-solo'), 'matchType'] = 1\n",
    "    df.loc[(df['matchType'] == 'solo-fpp'), 'matchType'] = 1\n",
    "    df.loc[(df['matchType'] == 'normal-solo-fpp'), 'matchType'] = 1\n",
    "\n",
    "    df.loc[(df['matchType'] == 'duo'), 'matchType'] = 2\n",
    "    df.loc[(df['matchType'] == 'normal-duo'), 'matchType'] = 2\n",
    "    df.loc[(df['matchType'] == 'duo-fpp'), 'matchType'] = 2    \n",
    "    df.loc[(df['matchType'] == 'normal-duo-fpp'), 'matchType'] = 2\n",
    "\n",
    "    df.loc[(df['matchType'] == 'squad'), 'matchType'] = 3\n",
    "    df.loc[(df['matchType'] == 'normal-squad'), 'matchType'] = 3    \n",
    "    df.loc[(df['matchType'] == 'squad-fpp'), 'matchType'] = 3\n",
    "    df.loc[(df['matchType'] == 'normal-squad-fpp'), 'matchType'] = 3\n",
    "    \n",
    "    df.loc[(df['matchType'] == 'flaretpp'), 'matchType'] = 0\n",
    "    df.loc[(df['matchType'] == 'flarefpp'), 'matchType'] = 0\n",
    "    df.loc[(df['matchType'] == 'crashtpp'), 'matchType'] = 0\n",
    "    df.loc[(df['matchType'] == 'crashfpp'), 'matchType'] = 0\n",
    "    df.loc[(df['rankPoints'] < 0), 'rankPoints'] = 0\n",
    "     print('Adding new features using existing ones')\n",
    "    df['headshotrate'] = df['kills']/df['headshotKills']\n",
    "    df['killStreakrate'] = df['killStreaks']/df['kills']\n",
    "    df['healthitems'] = df['heals'] + df['boosts']\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n",
    "    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired']\n",
    "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
    "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n",
    "    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n",
    "    df['skill'] = df['headshotKills'] + df['roadKills']\n",
    "    \n",
    "    print('Adding normalized features')\n",
    "    df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n",
    "    gc.collect()\n",
    "    df['killsNorm'] = df['kills']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['damageDealtNorm'] = df['damageDealt']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['maxPlaceNorm'] = df['maxPlace']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['matchDurationNorm'] = df['matchDuration']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['headshotKillsNorm'] = df['headshotKills']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['killPlaceNorm'] = df['killPlace']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['killPointsNorm'] = df['killPoints']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['killStreaksNorm'] = df['killStreaks']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['longestKillNorm'] = df['longestKill']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['roadKillsNorm'] = df['roadKills']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['teamKillsNorm'] = df['teamKills']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['damageDealtNorm'] = df['damageDealt']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['DBNOsNorm'] = df['DBNOs']*((100-df['playersJoined'])/100 + 1)\n",
    "    df['revivesNorm'] = df['revives']*((100-df['playersJoined'])/100 + 1)    \n",
    "    \n",
    "    # Clean null values from dataframe\n",
    "    df = df_null_cleaner(df,fill_with=0)\n",
    "\n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchType\")  \n",
    "     y = pd.DataFrame()\n",
    "    if is_train: \n",
    "        print('Preparing target variable')\n",
    "        y = df.groupby(['matchId','groupId'])[target].agg('mean')\n",
    "        gc.collect()\n",
    "        features.remove(target)\n",
    "        \n",
    "    print('Aggregating means')\n",
    "    means_features = list(df.columns)\n",
    "    means_features.remove(\"Id\")\n",
    "    means_features.remove(\"matchId\")\n",
    "    means_features.remove(\"groupId\")\n",
    "    means_features.remove(\"matchType\")  \n",
    "    \n",
    "    if is_train:\n",
    "        means_features.remove(target)\n",
    "    \n",
    "    agg = df.groupby(['matchId','groupId'])[means_features].agg('mean')\n",
    "    gc.collect()\n",
    "    agg_rank = agg.groupby('matchId')[means_features].rank(pct=True).reset_index()\n",
    "    gc.collect()\n",
    "    \n",
    "    if is_train: \n",
    "        X = agg.reset_index()[['matchId','groupId']]\n",
    "    else: \n",
    "        X = df[['matchId','groupId']]\n",
    "\n",
    "    X = X.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    X = X.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del agg, agg_rank\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Aggregating maxes')\n",
    "    maxes_features = list(df.columns) \n",
    "    maxes_features.remove(\"Id\")\n",
    "    maxes_features.remove(\"matchId\")\n",
    "    maxes_features.remove(\"groupId\")\n",
    "    maxes_features.remove(\"matchType\")  \n",
    "\n",
    "    if is_train:\n",
    "        maxes_features.remove(target)\n",
    "    \n",
    "    agg = df.groupby(['matchId','groupId'])[maxes_features].agg('max')\n",
    "    gc.collect()\n",
    "    agg_rank = agg.groupby('matchId')[maxes_features].rank(pct=True).reset_index()\n",
    "    gc.collect()\n",
    "    X = X.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    X = X.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del agg, agg_rank\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Aggregating mins')\n",
    "    mins_features = list(df.columns) \n",
    "    mins_features.remove(\"Id\")\n",
    "    mins_features.remove(\"matchId\")\n",
    "    mins_features.remove(\"groupId\")\n",
    "    mins_features.remove(\"matchType\")  \n",
    "    \n",
    "    if is_train:\n",
    "        mins_features.remove(target)\n",
    "    \n",
    "    agg = df.groupby(['matchId','groupId'])[mins_features].agg('min')\n",
    "    gc.collect()\n",
    "    agg_rank = agg.groupby('matchId')[mins_features].rank(pct=True).reset_index()\n",
    "    gc.collect()\n",
    "    X = X.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    X = X.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    del agg, agg_rank\n",
    "    gc.collect()\n",
    "    \n",
    "    print('Aggregating group sizes')\n",
    "    grpsize_features = list(df.columns) \n",
    "    grpsize_features.remove(\"Id\")\n",
    "    grpsize_features.remove(\"matchId\")\n",
    "    grpsize_features.remove(\"groupId\")\n",
    "    grpsize_features.remove(\"matchType\")  \n",
    "    grpsize_features.remove(\"DBNOsNorm\")\n",
    "    grpsize_features.remove(\"damageDealtNorm\")\n",
    "    grpsize_features.remove(\"headshotKillsNorm\")\n",
    "    grpsize_features.remove(\"killPlaceNorm\")\n",
    "    grpsize_features.remove(\"killPlace_over_maxPlace\")\n",
    "    grpsize_features.remove(\"killPointsNorm\")\n",
    "    grpsize_features.remove(\"killStreaksNorm\")\n",
    "    grpsize_features.remove(\"killsNorm\")\n",
    "    grpsize_features.remove(\"longestKillNorm\")\n",
    "    grpsize_features.remove(\"matchDurationNorm\")\n",
    "    grpsize_features.remove(\"matchDuration\")\n",
    "    grpsize_features.remove(\"maxPlaceNorm\")\n",
    "    grpsize_features.remove(\"maxPlace\")\n",
    "    grpsize_features.remove(\"numGroups\")\n",
    "    grpsize_features.remove(\"playersJoined\")\n",
    "    grpsize_features.remove(\"revivesNorm\")\n",
    "    grpsize_features.remove(\"roadKillsNorm\")\n",
    "    grpsize_features.remove(\"teamKillsNorm\")    \n",
    "    agg = df.groupby(['matchId','groupId'])[grpsize_features].size().reset_index(name='group_size')\n",
    "    gc.collect()\n",
    "    X = X.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print('Aggregating match means')\n",
    "    mmeans_features = list(df.columns) \n",
    "    mmeans_features.remove(\"Id\")\n",
    "    mmeans_features.remove(\"matchId\")\n",
    "    mmeans_features.remove(\"groupId\")\n",
    "    mmeans_features.remove(\"DBNOsNorm\")\n",
    "    mmeans_features.remove(\"damageDealtNorm\")\n",
    "    mmeans_features.remove(\"headshotKillsNorm\")\n",
    "    mmeans_features.remove(\"killPlace_over_maxPlace\")\n",
    "    mmeans_features.remove(\"killPointsNorm\")\n",
    "    mmeans_features.remove(\"killStreaksNorm\")\n",
    "     mmeans_features.remove(\"longestKillNorm\")\n",
    "    mmeans_features.remove(\"matchDurationNorm\")\n",
    "    mmeans_features.remove(\"matchDuration\")\n",
    "    mmeans_features.remove(\"maxPlaceNorm\")\n",
    "    mmeans_features.remove(\"numGroups\")\n",
    "    mmeans_features.remove(\"revivesNorm\")\n",
    "    mmeans_features.remove(\"roadKillsNorm\")\n",
    "    mmeans_features.remove(\"teamKillsNorm\")      \n",
    "    agg = df.groupby(['matchId'])[mmeans_features].agg('mean').reset_index()\n",
    "    gc.collect()\n",
    "    X = X.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    print('Aggregating match sizes')\n",
    "    msizes_features = list(df.columns) \n",
    "    msizes_features.remove(\"Id\")\n",
    "    msizes_features.remove(\"matchId\")\n",
    "    msizes_features.remove(\"groupId\")\n",
    "    msizes_features.remove(\"DBNOsNorm\")\n",
    "    msizes_features.remove(\"damageDealtNorm\")\n",
    "    msizes_features.remove(\"headshotKillsNorm\")\n",
    "    msizes_features.remove(\"killPlace_over_maxPlace\")\n",
    "    msizes_features.remove(\"killPointsNorm\")\n",
    "    msizes_features.remove(\"killStreaksNorm\")\n",
    "    msizes_features.remove(\"longestKillNorm\")\n",
    "    msizes_features.remove(\"matchDurationNorm\")\n",
    "    msizes_features.remove(\"matchDuration\")\n",
    "    msizes_features.remove(\"maxPlaceNorm\")\n",
    "    msizes_features.remove(\"numGroups\")\n",
    "    msizes_features.remove(\"revivesNorm\")\n",
    "    msizes_features.remove(\"roadKillsNorm\")\n",
    "    msizes_features.remove(\"teamKillsNorm\")      \n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    gc.collect()\n",
    "    X = X.merge(agg, how='left', on=['matchId'])\n",
    "    del df, agg\n",
    "    gc.collect()\n",
    " X.drop(columns = ['matchId', \n",
    "                      'groupId'\n",
    "                     ], axis=1, inplace=True)  \n",
    "    gc.collect()\n",
    "    if is_train:\n",
    "        return X, y\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../input/train_V2.csv', engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_footprint_reduce(X_train, skip_obj=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(model)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean memory and load test set\n",
    "del X_train, X_validation, y_train, y_validation \n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('../input/test_V2.csv', engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = pred_test.reshape(-1)\n",
    "pred_test = (pred_test + 1) / 2\n",
    "for i in range(len(test_set)):\n",
    "    winPlacePerc = pred_test[i]\n",
    "    maxPlace = int(test_set.iloc[i]['maxPlace'])\n",
    "    if maxPlace == 0:\n",
    "        winPlacePerc = 0.0\n",
    "    elif maxPlace == 1:\n",
    "        winPlacePerc = 1.0\n",
    "    else:\n",
    "        gap = 1.0 / (maxPlace - 1)\n",
    "        winPlacePerc = round(winPlacePerc / gap) * gap\n",
    "    \n",
    "    if winPlacePerc < 0: winPlacePerc = 0.0\n",
    "    if winPlacePerc > 1: winPlacePerc = 1.0    \n",
    "    pred_test[i] = winPlacePerc\n",
    "\n",
    "    if (i + 1) % 100000 == 0:\n",
    "        print(i, flush=True, end=\" \")\n",
    "\n",
    "test_set['winPlacePerc'] = pred_test\n",
    "\n",
    "submission = test_set[['Id', 'winPlacePerc']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
