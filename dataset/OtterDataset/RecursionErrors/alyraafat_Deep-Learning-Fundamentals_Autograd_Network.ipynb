{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dbf533",
   "metadata": {},
   "source": [
    "[https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8f79f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708ddd0b-501f-42e4-a03e-fd788308df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 18:41:37.625029: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 18:41:37.645317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-24 18:41:37.645338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-24 18:41:37.645908: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-24 18:41:37.649744: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 18:41:38.121355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import keras\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "from optimizers.sgd import SGD\n",
    "from optimizers.adam import Adam\n",
    "# from network import Network\n",
    "# from layers.conv2d import Conv2D\n",
    "# from layers.dense import FCLayer\n",
    "# from layers.activation import ActivationLayer, SoftmaxLayer, tanh, tanh_prime, softmax, softmax_prime, relu, relu_prime, sigmoid, sigmoid_prime\n",
    "# from layers.flatten import FlattenLayer\n",
    "from losses import mse, categorical_crossentropy, binary_crossentropy, mae\n",
    "from layers import Conv2D, FCLayer, ActivationLayer, FlattenLayer, DropoutLayer, GlobalAveragePoolingLayer, \\\n",
    "      RNN, MaxPool2D, LSTM, GRU, Embedding, BatchNorm2D, LayerNorm\n",
    "from autograd import tanh, relu, sigmoid, softmax, Module, Tensor\n",
    "from utils import draw_computation_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9218fe3a-33d6-4120-9707-25d1eb7eeb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_test_samples(x_test,y_test,samples,network):\n",
    "  for test, true in zip(x_test[:samples], y_test[:samples]):\n",
    "    pred = network.predict([test])[0][0]\n",
    "    idx = np.argmax(pred)\n",
    "    idx_true = np.argmax(true)\n",
    "    plt.title('pred: %s, prob: %.2f, true: %d' % (idx, pred[idx], idx_true))\n",
    "    plt.imshow(test, cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e4645e-3125-4c92-883e-d40beba683c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_data(image,is_conv=False):\n",
    "  image = np.cast['float32'](image)\n",
    "  if is_conv:\n",
    "    image = np.expand_dims(image,axis=-1)\n",
    "  image/=255\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8820af65-b125-4121-a2f4-ac6c1c524acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a23092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = adjust_data(x_train,is_conv=True)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "x_test = adjust_data(x_test,is_conv=True)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dda0d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (60000, 10), (10000, 28, 28, 1), (10000, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab5b76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_batches(data, labels, batch_size):\n",
    "#     num_batches = int(len(data) / batch_size)\n",
    "#     data_batches = []\n",
    "#     label_batches = []\n",
    "    \n",
    "#     for i in range(num_batches):\n",
    "#         start = i * batch_size\n",
    "#         end = min((i + 1) * batch_size, len(data))\n",
    "#         data_batches.append(data[start:end])\n",
    "#         label_batches.append(labels[start:end])\n",
    "    \n",
    "#     return np.array(data_batches), np.array(label_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "000bf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# x_train_batches, y_train_batches = create_batches(x_train, y_train, batch_size)\n",
    "# x_test_batches, y_test_batches = create_batches(x_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf3e2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches.shape, y_train_batches.shape, x_test_batches.shape, y_test_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1a546a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches = adjust_data(x_train_batches,True)\n",
    "# y_train_batches = keras.utils.to_categorical(y_train_batches)\n",
    "# x_test_batches = adjust_data(x_test_batches,True)\n",
    "# y_test_batches = keras.utils.to_categorical(y_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d33dba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_batches.shape, y_train_batches.shape, x_test_batches.shape, y_test_batches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6a045-36d6-444b-bf5f-00cf08b1607a",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64864602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55.05290804531892\n",
      "1 46.3684569405066\n",
      "2 42.33992724926197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m inputs \u001b[38;5;241m=\u001b[39m x_train[start:end]\n\u001b[0;32m---> 42\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m actual \u001b[38;5;241m=\u001b[39m y_train[start:end]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# print(predicted.shape, actual.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/module.py:32\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm(x)\n\u001b[1;32m     13\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x0)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/layers/layer.py:21\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(inp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/layers/conv2d.py:70\u001b[0m, in \u001b[0;36mConv2D.forward_propagation\u001b[0;34m(self, inp, training)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_channels):\n\u001b[0;32m---> 70\u001b[0m         cross_corr_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_conv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m         intermediate_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv[b,f] \u001b[38;5;241m+\u001b[39m cross_corr_out\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_conv\u001b[38;5;241m.\u001b[39mset_item((b,f),intermediate_res)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/layers/conv2d.py:109\u001b[0m, in \u001b[0;36mConv2D.cross_correlation\u001b[0;34m(self, inp, kernel)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# print(f'mat : {mat.shape}, kernel: {kernel.shape}')\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# if mat.shape==kernel.shape:\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     convolve \u001b[38;5;241m=\u001b[39m (mat\u001b[38;5;241m*\u001b[39mkernel)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m--> 109\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconvolve\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     w\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m h\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/tensor.py:200\u001b[0m, in \u001b[0;36mTensor.set_item\u001b[0;34m(self, idxs, value)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs, value):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _set_item\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _set_item(\u001b[38;5;28mself\u001b[39m, ensure_tensor(value), idxs)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:408\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.conv = Conv2D(filters=3, kernel_size=5, padding='same', strides=2)\n",
    "        self.batchnorm = BatchNorm2D()\n",
    "        self.maxpool = MaxPool2D(pool_size=2, strides=1, padding='same')\n",
    "        self.flatten = FlattenLayer()\n",
    "        self.linear1 = FCLayer(output_dim=20)\n",
    "        self.linear2 = FCLayer(output_dim=10)\n",
    "\n",
    "    def forward(self, inputs: Tensor, training=True) -> Tensor:\n",
    "        x = self.conv(inputs, training=training)\n",
    "        x0 = self.batchnorm(x)\n",
    "        x1 = self.maxpool(x0)\n",
    "        x2 = self.flatten(x1, training=training)\n",
    "        x3 = self.linear1(x2, training=training)\n",
    "        x4 = tanh(x3)\n",
    "        x5 = self.linear2(x4, training=training)\n",
    "        x6 = tanh(x5)\n",
    "        x7 = softmax(x6)\n",
    "        return x7\n",
    "\n",
    "optimizer = SGD(learning_rate=0.001)\n",
    "batch_size = 32\n",
    "model = Model()\n",
    "x_train = Tensor(x_train)\n",
    "y_train = Tensor(y_train)\n",
    "i = 0\n",
    "# print(x_train.shape)\n",
    "# train_data = x_train.shape[0]\n",
    "train_data = 1000\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0.0\n",
    "    # print(f'before: {x_train.shape}')\n",
    "    for start in range(0, train_data, batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = x_train[start:end]\n",
    "\n",
    "        predicted = model(inputs)\n",
    "        actual = y_train[start:end]\n",
    "        # print(predicted.shape, actual.shape)\n",
    "        loss = categorical_crossentropy(y_true=actual, y_pred=predicted)\n",
    "        if i==0:\n",
    "            draw_computation_graph(loss)\n",
    "        i+=1\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.name, param.id)\n",
    "        # print(start, loss)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        optimizer.step(model)\n",
    "    # print(f'after: {x_train.shape}')\n",
    "    epoch_loss /= (train_data//batch_size + train_data % batch_size)\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de14286",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdfa810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3, 1) (800,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def create_dataset(num_sequences, sequence_length):\n",
    "    return np.array([np.arange(start, start + sequence_length) for start in range(num_sequences)])\n",
    "\n",
    "# Helper function to create inputs and targets from the sequences\n",
    "def create_inputs_targets(data):\n",
    "    X = data[:, :-1]  # all but the last item in each sequence as input\n",
    "    Y = data[:, -1]  # all but the first item in each sequence as target (for sequence prediction)\n",
    "    return X, Y\n",
    "\n",
    "# Helper function to create batches\n",
    "def create_batches(X, Y, batch_size):\n",
    "    num_batches = len(X) // batch_size\n",
    "    X_batches = np.array(np.array_split(X, num_batches))\n",
    "    Y_batches = np.array(np.array_split(Y, num_batches))\n",
    "    return X_batches, Y_batches\n",
    "\n",
    "# Generate dataset\n",
    "num_sequences = 1000  # The number of sequences you want\n",
    "sequence_length = 4  # The length of each sequence\n",
    "batch_size = 32  # The size of each batch\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_dataset(num_sequences, sequence_length)\n",
    "\n",
    "# Split dataset into training and testing sets (80-20 split)\n",
    "train_size = int(num_sequences * 0.8)\n",
    "train_set, test_set = dataset[:train_size], dataset[train_size:]\n",
    "\n",
    "# Create inputs (X) and targets (Y) for training and testing\n",
    "x_train, y_train = create_inputs_targets(train_set)\n",
    "x_test, y_test = create_inputs_targets(test_set)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "# Shuffle training data\n",
    "indices = np.arange(train_size)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Apply shuffled indices to create shuffled training data\n",
    "x_train_shuffled = x_train[indices]\n",
    "y_train_shuffled = y_train[indices]\n",
    "# Normalize inputs\n",
    "# x_train_max = np.max(x_train_shuffled)\n",
    "# x_train_shuffled = x_train_shuffled / x_train_max\n",
    "# x_test = x_test / x_train_max  # use the same scale as train set\n",
    "\n",
    "# print(x_train_shuffled.shape, y_train_shuffled.shape)\n",
    "# Create batches from the training and testing data\n",
    "# x_train_batches, y_train_batches = create_batches(x_train_shuffled, y_train_shuffled, batch_size)\n",
    "# x_test_batches, y_test_batches = create_batches(x_test, y_test, batch_size)\n",
    "\n",
    "\n",
    "# Example: Print the first training batch\n",
    "# print(\"First training batch (x_train, y_train):\")\n",
    "# print(x_train_batches.shape, y_train_batches.shape)\n",
    "# x_train_batches[0], y_train_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56272de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m m \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m     27\u001b[0m m(Tensor(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 28\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(summary)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(summary)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/module.py:58\u001b[0m, in \u001b[0;36mModule.summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module):\n\u001b[1;32m     52\u001b[0m     summary\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39minput_shape,\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39moutput_shape,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum params\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39mnum_params\n\u001b[1;32m     57\u001b[0m     })\n\u001b[0;32m---> 58\u001b[0m     nested_summary \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nested_summary:\n\u001b[1;32m     60\u001b[0m         summary\u001b[38;5;241m.\u001b[39mextend(nested_summary)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/module.py:58\u001b[0m, in \u001b[0;36mModule.summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module):\n\u001b[1;32m     52\u001b[0m     summary\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39minput_shape,\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39moutput_shape,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum params\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39mnum_params\n\u001b[1;32m     57\u001b[0m     })\n\u001b[0;32m---> 58\u001b[0m     nested_summary \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nested_summary:\n\u001b[1;32m     60\u001b[0m         summary\u001b[38;5;241m.\u001b[39mextend(nested_summary)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.summary at line 58 (2968 times)]\u001b[0m\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/module.py:58\u001b[0m, in \u001b[0;36mModule.summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module):\n\u001b[1;32m     52\u001b[0m     summary\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39minput_shape,\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39moutput_shape,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum params\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39mnum_params\n\u001b[1;32m     57\u001b[0m     })\n\u001b[0;32m---> 58\u001b[0m     nested_summary \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nested_summary:\n\u001b[1;32m     60\u001b[0m         summary\u001b[38;5;241m.\u001b[39mextend(nested_summary)\n",
      "File \u001b[0;32m~/ml/Deep-Learning-Fundamentals/with_autograd/autograd/module.py:51\u001b[0m, in \u001b[0;36mModule.summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# print(inspect.getmembers(self))\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# for name, value in inspect.getmembers(self):\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModule\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     52\u001b[0m         summary\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39minput_shape,\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39moutput_shape,\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum params\u001b[39m\u001b[38;5;124m'\u001b[39m: module\u001b[38;5;241m.\u001b[39mnum_params\n\u001b[1;32m     57\u001b[0m         })\n\u001b[1;32m     58\u001b[0m         nested_summary \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # self.rnn = RNN(hidden_size=20,output_size=30, return_sequences=False, bidirectional=False, init='glorot_uniform')\n",
    "        self.add_module('lstm', LSTM(hidden_size=20, return_sequences=True, bidirectional=False))\n",
    "        # self.lstm = LSTM(hidden_size=20, return_sequences=True, bidirectional=False)\n",
    "        self.add_module('layernorm', LayerNorm(D=1))\n",
    "        # self.layernorm = LayerNorm(D=1)\n",
    "        self.add_module('flatten', FlattenLayer())\n",
    "        # self.rnn2 = RNN(hidden_size=20,output_size=30, return_sequences=False, init='glorot_uniform')\n",
    "        # self.flatten = FlattenLayer()\n",
    "        # self.linear1 = FCLayer(output_dim=20)\n",
    "        self.add_module('linear2', FCLayer(output_dim=1))\n",
    "        # self.linear2 = FCLayer(output_dim=1)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        x = self.lstm(inputs)\n",
    "        # print(f'before layernorm: {x}')\n",
    "        x1 = self.layernorm(x)\n",
    "        # print(f'after layernorm: {x1}')\n",
    "        # x = self.rnn2(x)\n",
    "        x2 = self.flatten(x1)\n",
    "        # x3 = self.linear1(x2)\n",
    "        x4 = self.linear2(x2)\n",
    "        return x4\n",
    "    \n",
    "m = Model()\n",
    "m(Tensor(np.random.randn(32, 3, 1)))\n",
    "summary = m.summary()\n",
    "# print(summary)\n",
    "pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c8288a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 137619.5544946631\n",
      "1 137386.89973783083\n",
      "2 137135.88611734737\n",
      "3 136859.3391069833\n",
      "4 136550.53694213482\n",
      "5 136203.32031139964\n",
      "6 135813.6163885433\n",
      "7 135379.3827680809\n",
      "8 134899.6430867293\n",
      "9 134373.98859426836\n",
      "10 133802.4134634241\n",
      "11 133185.22015668644\n",
      "12 132522.94535706958\n",
      "13 131816.30552408777\n",
      "14 131066.15738187465\n",
      "15 130273.46866490059\n",
      "16 129439.29629158911\n",
      "17 128564.77004913658\n",
      "18 127651.08028387825\n",
      "19 126699.46844245035\n",
      "20 125711.21958827086\n",
      "21 124687.65628344375\n",
      "22 123630.13342538937\n",
      "23 122540.03375383464\n",
      "24 121418.76385155563\n",
      "25 120267.75051378376\n",
      "26 119088.4374084517\n",
      "27 117882.28196815592\n",
      "28 116650.75247987888\n",
      "29 115395.3253438052\n",
      "30 114117.48247538824\n",
      "31 112818.70885286172\n",
      "32 111500.4901721799\n",
      "33 110164.31061957963\n",
      "34 108811.65075183015\n",
      "35 107443.9854718018\n",
      "36 106062.78208730958\n",
      "37 104669.49845481224\n",
      "38 103265.5812194507\n",
      "39 101852.46411934953\n",
      "40 100431.56635648423\n",
      "41 99004.2910496821\n",
      "42 97572.0237462188\n",
      "43 96136.13099208778\n",
      "44 94697.95896755044\n",
      "45 93258.83217674737\n",
      "46 91820.05219426262\n",
      "47 90382.89645695417\n",
      "48 88948.61711539052\n",
      "49 87518.4399252373\n",
      "50 86093.56318695314\n",
      "51 84675.1567338684\n",
      "52 83264.36096184733\n",
      "53 81862.2859005629\n",
      "54 80470.01032628352\n",
      "55 79088.58091387572\n",
      "56 77719.01142481294\n",
      "57 76362.28194854215\n",
      "58 75019.33816056543\n",
      "59 73691.09064602104\n",
      "60 72378.41422397831\n",
      "61 71082.14734550276\n",
      "62 69803.09151272898\n",
      "63 68542.01074459373\n",
      "64 67299.63107023298\n",
      "65 66076.64007145578\n",
      "66 64873.68646784405\n",
      "67 63691.37974219892\n",
      "68 62530.289807028435\n",
      "69 61390.94672991818\n",
      "70 60273.84049406035\n",
      "71 59179.42081673671\n",
      "72 58108.09702491714\n",
      "73 57060.23798789712\n",
      "74 56036.172111596374\n",
      "75 55036.18739811446\n",
      "76 54060.531575030756\n",
      "77 53109.412304382924\n",
      "78 52182.99746307785\n",
      "79 51281.415503416436\n",
      "80 50404.755910704844\n",
      "81 49553.069735428566\n",
      "82 48726.37021787063\n",
      "83 47924.63349935117\n",
      "84 47147.7994178651\n",
      "85 46395.772387438425\n",
      "86 45668.42234866484\n",
      "87 44965.585792335965\n",
      "88 44287.06684069048\n",
      "89 43632.638380298566\n",
      "90 43002.043241556144\n",
      "91 42394.995418195664\n",
      "92 41811.1813012375\n",
      "93 41250.26095538229\n",
      "94 40711.86940464089\n",
      "95 40195.61794267358\n",
      "96 39701.09546014788\n",
      "97 39227.86979046755\n",
      "98 38775.489080021674\n",
      "99 38343.483182483855\n",
      "100 37931.365072869296\n",
      "101 37538.632296188756\n",
      "102 37164.76843623573\n",
      "103 36809.24461496945\n",
      "104 36471.52101171783\n",
      "105 36151.04840693473\n",
      "106 35847.26974039274\n",
      "107 35559.62168103375\n",
      "108 35287.536204507414\n",
      "109 35030.442165584085\n",
      "110 34787.76686361837\n",
      "111 34558.93759273644\n",
      "112 34343.383162869824\n",
      "113 34140.53539250021\n",
      "114 33949.830556253786\n",
      "115 33770.710787044605\n",
      "116 33602.62541825266\n",
      "117 33445.03226344601\n",
      "118 33297.3988239658\n",
      "119 33159.20342054813\n",
      "120 33029.936240785806\n",
      "121 32909.10029976106\n",
      "122 32796.21230870518\n",
      "123 32690.80344906682\n",
      "124 32592.42004991931\n",
      "125 32500.624165939706\n",
      "126 32414.994057396303\n",
      "127 32335.124571381377\n",
      "128 32260.62742619959\n",
      "129 32191.13140012812\n",
      "130 32126.2824297892\n",
      "131 32065.74361889105\n",
      "132 32009.19516357358\n",
      "133 31956.334199350997\n",
      "134 31906.874573894\n",
      "135 31860.54655363692\n",
      "136 31817.096467908097\n",
      "137 31776.28629867622\n",
      "138 31737.89322185875\n",
      "139 31701.709107736944\n",
      "140 31667.539985230378\n",
      "141 31635.20547850117\n",
      "142 31604.538220112816\n",
      "143 31575.3832488072\n",
      "144 31547.597395988338\n",
      "145 31521.048667041363\n",
      "146 31495.6156216971\n",
      "147 31471.186758845608\n",
      "148 31447.659909458813\n",
      "149 31424.94164037361\n",
      "150 31402.94667373838\n",
      "151 31381.597323468846\n",
      "152 31360.822951215694\n",
      "153 31340.559444529565\n",
      "154 31320.748717303035\n",
      "155 31301.338234475457\n",
      "156 31282.28056173198\n",
      "157 31263.53294005146\n",
      "158 31245.056885813818\n",
      "159 31226.817815944756\n",
      "160 31208.784697598738\n",
      "161 31190.929722639165\n",
      "162 31173.22800555719\n",
      "163 31155.657303890577\n",
      "164 31138.197760813397\n",
      "165 31120.831668313593\n",
      "166 31103.54325042416\n",
      "167 31086.318464885986\n",
      "168 31069.14482259197\n",
      "169 31052.01122337509\n",
      "170 31034.907807025564\n",
      "171 31017.82581849872\n",
      "172 31000.75748645721\n",
      "173 30983.69591410251\n",
      "174 30966.63498067365\n",
      "175 30949.569253295038\n",
      "176 30932.493908380664\n",
      "177 30915.404661243658\n",
      "178 30898.297703241242\n",
      "179 30881.169646482802\n",
      "180 30864.017473933996\n",
      "181 30846.838495956577\n",
      "182 30829.630311190245\n",
      "183 30812.39077266288\n",
      "184 30795.117957235936\n",
      "185 30777.81013921345\n",
      "186 30760.465766676552\n",
      "187 30743.083441287912\n",
      "188 30725.661899839455\n",
      "189 30708.199998401058\n",
      "190 30690.696698758955\n",
      "191 30673.151055876435\n",
      "192 30655.562207275554\n",
      "193 30637.92936383764\n",
      "194 30620.251801290076\n",
      "195 30602.52885336032\n",
      "196 30584.759905280025\n",
      "197 30566.94438823026\n",
      "198 30549.081774677597\n",
      "199 30531.171573965577\n",
      "200 30513.213328535203\n",
      "201 30495.20661049513\n",
      "202 30477.15101891609\n",
      "203 30459.046177012122\n",
      "204 30440.89173006424\n",
      "205 30422.687343003145\n",
      "206 30404.43269877269\n",
      "207 30386.127496724817\n",
      "208 30367.771450997097\n",
      "209 30349.364289133784\n",
      "210 30330.905750913753\n",
      "211 30312.395587359973\n",
      "212 30293.83355940588\n",
      "213 30275.219437044754\n",
      "214 30256.55299871431\n",
      "215 30237.83403023498\n",
      "216 30219.06232415532\n",
      "217 30200.237679025682\n",
      "218 30181.359898925733\n",
      "219 30162.428792448256\n",
      "220 30143.44417263195\n",
      "221 30124.4058562019\n",
      "222 30105.313663033357\n",
      "223 30086.167415842236\n",
      "224 30066.966939482263\n",
      "225 30047.71206092422\n",
      "226 30028.4026085\n",
      "227 30009.038411956542\n",
      "228 29989.619301700153\n",
      "229 29970.14510870182\n",
      "230 29950.615664309524\n",
      "231 29931.030799819913\n",
      "232 29911.39034619328\n",
      "233 29891.694133942827\n",
      "234 29871.941993028733\n",
      "235 29852.133752443187\n",
      "236 29832.269240017216\n",
      "237 29812.348282460756\n",
      "238 29792.370705248864\n",
      "239 29772.33633249819\n",
      "240 29752.24498645735\n",
      "241 29732.09648792686\n",
      "242 29711.890655911146\n",
      "243 29691.627307744202\n",
      "244 29671.306259001998\n",
      "245 29650.92732351146\n",
      "246 29630.490313187584\n",
      "247 29609.99503801737\n",
      "248 29589.441306382225\n",
      "249 29568.8289250365\n",
      "250 29548.157698822415\n",
      "251 29527.427431099582\n",
      "252 29506.637923770966\n",
      "253 29485.788977145923\n",
      "254 29464.88039041104\n",
      "255 29443.91196141256\n",
      "256 29422.883486821742\n",
      "257 29401.79476274348\n",
      "258 29380.645584367507\n",
      "259 29359.43574632127\n",
      "260 29338.16504264183\n",
      "261 29316.833267588227\n",
      "262 29295.440215069444\n",
      "263 29273.985679432757\n",
      "264 29252.469455685346\n",
      "265 29230.89133903854\n",
      "266 29209.251125861163\n",
      "267 29187.548613815245\n",
      "268 29165.78360180365\n",
      "269 29143.955890756373\n",
      "270 29122.06528345819\n",
      "271 29100.111585068367\n",
      "272 29078.094603111527\n",
      "273 29056.0141481469\n",
      "274 29033.870033884963\n",
      "275 29011.662077410136\n",
      "276 28989.390099872082\n",
      "277 28967.053926350913\n",
      "278 28944.65338635293\n",
      "279 28922.188314281695\n",
      "280 28899.65854944534\n",
      "281 28877.063936739407\n",
      "282 28854.404326499243\n",
      "283 28831.67957523797\n",
      "284 28808.889546156443\n",
      "285 28786.034108806056\n",
      "286 28763.113139983147\n",
      "287 28740.12652384648\n",
      "288 28717.07415209097\n",
      "289 28693.95592441625\n",
      "290 28670.771748826104\n",
      "291 28647.521542002207\n",
      "292 28624.205229742183\n",
      "293 28600.82274675207\n",
      "294 28577.374037452264\n",
      "295 28553.85905600207\n",
      "296 28530.277766479736\n",
      "297 28506.630143544142\n",
      "298 28482.916172569934\n",
      "299 28459.13584976377\n",
      "300 28435.28918231343\n",
      "301 28411.376188866077\n",
      "302 28387.39689979432\n",
      "303 28363.351357110085\n",
      "304 28339.239614924456\n",
      "305 28315.061739603065\n",
      "306 28290.817810064153\n",
      "307 28266.50791746718\n",
      "308 28242.132166008505\n",
      "309 28217.690672711393\n",
      "310 28193.183567769807\n",
      "311 28168.610994407925\n",
      "312 28143.973109157196\n",
      "313 28119.2700819584\n",
      "314 28094.50209631479\n",
      "315 28069.669349247815\n",
      "316 28044.77205134568\n",
      "317 28019.810426727927\n",
      "318 27994.784713510067\n",
      "319 27969.69516352986\n",
      "320 27944.542042076962\n",
      "321 27919.325628360064\n",
      "322 27894.046215461323\n",
      "323 27868.704109870185\n",
      "324 27843.299631877693\n",
      "325 27817.83311523109\n",
      "326 27792.304907421472\n",
      "327 27766.71536917096\n",
      "328 27741.064874823205\n",
      "329 27715.35381183643\n",
      "330 27689.58258078905\n",
      "331 27663.75159538921\n",
      "332 27637.861282188922\n",
      "333 27611.912080482274\n",
      "334 27585.904442157454\n",
      "335 27559.83883151557\n",
      "336 27533.71572512027\n",
      "337 27507.535611853393\n",
      "338 27481.29899219639\n",
      "339 27455.006378456044\n",
      "340 27428.658294232704\n",
      "341 27402.25527477433\n",
      "342 27375.797866219793\n",
      "343 27349.286625434597\n",
      "344 27322.72211997872\n",
      "345 27296.10492804057\n",
      "346 27269.43563771508\n",
      "347 27242.714847015126\n",
      "348 27215.94316346125\n",
      "349 27189.121204208033\n",
      "350 27162.249595472782\n",
      "351 27135.3289724572\n",
      "352 27108.359978802728\n",
      "353 27081.34326651113\n",
      "354 27054.27949599359\n",
      "355 27027.16933532426\n",
      "356 27000.01346008214\n",
      "357 26972.812553253963\n",
      "358 26945.567304917055\n",
      "359 26918.278411793064\n",
      "360 26890.946577179657\n",
      "361 26863.572510715447\n",
      "362 26836.156927928863\n",
      "363 26808.700550242364\n",
      "364 26781.204104364886\n",
      "365 26753.668322178022\n",
      "366 26726.093940913546\n",
      "367 26698.48170221089\n",
      "368 26670.832352194633\n",
      "369 26643.14664120802\n",
      "370 26615.42532359194\n",
      "371 26587.66915741985\n",
      "372 26559.878904227004\n",
      "373 26532.05532884613\n",
      "374 26504.199199050527\n",
      "375 26476.311285737072\n",
      "376 26448.39236202052\n",
      "377 26420.443203491384\n",
      "378 26392.464587846953\n",
      "379 26364.457294867316\n",
      "380 26336.422105859165\n",
      "381 26308.35980389516\n",
      "382 26280.271173006517\n",
      "383 26252.15699876184\n",
      "384 26224.01806733833\n",
      "385 26195.85516593701\n",
      "386 26167.66908217944\n",
      "387 26139.46060419965\n",
      "388 26111.23052025162\n",
      "389 26082.979618637954\n",
      "390 26054.708687634273\n",
      "391 26026.418515242964\n",
      "392 25998.10988909273\n",
      "393 25969.783595915957\n",
      "394 25941.440422255975\n",
      "395 25913.08115336951\n",
      "396 25884.70657357587\n",
      "397 25856.31746636301\n",
      "398 25827.914613626886\n",
      "399 25799.49879621276\n",
      "400 25771.07079304187\n",
      "401 25742.631381390896\n",
      "402 25714.18133720118\n",
      "403 25685.721434249477\n",
      "404 25657.252444451053\n",
      "405 25628.775137659617\n",
      "406 25600.29028148922\n",
      "407 25571.798641134872\n",
      "408 25543.30097969494\n",
      "409 25514.798057586773\n",
      "410 25486.290632710658\n",
      "411 25457.77946063235\n",
      "412 25429.26529357\n",
      "413 25400.748881444815\n",
      "414 25372.23097112194\n",
      "415 25343.712306628884\n",
      "416 25315.19362891772\n",
      "417 25286.675675723727\n",
      "418 25258.159181553052\n",
      "419 25229.644878021172\n",
      "420 25201.13349297105\n",
      "421 25172.62575126505\n",
      "422 25144.12237426079\n",
      "423 25115.624079937126\n",
      "424 25087.131582367874\n",
      "425 25058.645592285804\n",
      "426 25030.166816898818\n",
      "427 25001.695959688685\n",
      "428 24973.233720479107\n",
      "429 24944.78079486794\n",
      "430 24916.33787520658\n",
      "431 24887.905649766628\n",
      "432 24859.48480287398\n",
      "433 24831.076014863338\n",
      "434 24802.679962352817\n",
      "435 24774.297317709308\n",
      "436 24745.928749132356\n",
      "437 24717.574921082472\n",
      "438 24689.236493647622\n",
      "439 24660.914122901748\n",
      "440 24632.60846084238\n",
      "441 24604.3201548568\n",
      "442 24576.049848457344\n",
      "443 24547.79818072115\n",
      "444 24519.5657863934\n",
      "445 24491.353295964742\n",
      "446 24463.161335618155\n",
      "447 24434.990527114904\n",
      "448 24406.841487826892\n",
      "449 24378.714830709443\n",
      "450 24350.61116410503\n",
      "451 24322.531092325677\n",
      "452 24294.47521467826\n",
      "453 24266.44412611491\n",
      "454 24238.438417259036\n",
      "455 24210.458673836227\n",
      "456 24182.50547727076\n",
      "457 24154.579404276767\n",
      "458 24126.681026940452\n",
      "459 24098.810912856847\n",
      "460 24070.96962464265\n",
      "461 24043.15772047247\n",
      "462 24015.375753694676\n",
      "463 23987.624273205845\n",
      "464 23959.90382292293\n",
      "465 23932.21494195118\n",
      "466 23904.5581648541\n",
      "467 23876.9340214006\n",
      "468 23849.343036355564\n",
      "469 23821.785729648338\n",
      "470 23794.262616404583\n",
      "471 23766.77420699182\n",
      "472 23739.321007018567\n",
      "473 23711.903516837512\n",
      "474 23684.522232073567\n",
      "475 23657.177643567098\n",
      "476 23629.870237003935\n",
      "477 23602.60049296907\n",
      "478 23575.368887160246\n",
      "479 23548.175890280632\n",
      "480 23521.021968090066\n",
      "481 23493.90758130722\n",
      "482 23466.83318538672\n",
      "483 23439.79923078733\n",
      "484 23412.80616293483\n",
      "485 23385.854421908094\n",
      "486 23358.944442668097\n",
      "487 23332.076655158453\n",
      "488 23305.251484136108\n",
      "489 23278.469348869312\n",
      "490 23251.73066336644\n",
      "491 23225.035836488183\n",
      "492 23198.38527200175\n",
      "493 23171.779368262283\n",
      "494 23145.218518004596\n",
      "495 23118.7031086047\n",
      "496 23092.23352247094\n",
      "497 23065.810136169257\n",
      "498 23039.433320953412\n",
      "499 23013.103442917087\n",
      "500 22986.820862014294\n",
      "501 22960.585932977854\n",
      "502 22934.399005194704\n",
      "503 22908.26042225489\n",
      "504 22882.170522120025\n",
      "505 22856.129637190115\n",
      "506 22830.138094183996\n",
      "507 22804.196214006242\n",
      "508 22778.30431197221\n",
      "509 22752.462697648738\n",
      "510 22726.67167481712\n",
      "511 22700.93154152414\n",
      "512 22675.24258972692\n",
      "513 22649.60510569379\n",
      "514 22624.019369749127\n",
      "515 22598.48565635538\n",
      "516 22573.004233911073\n",
      "517 22547.575365051125\n",
      "518 22522.19930623287\n",
      "519 22496.87630795645\n",
      "520 22471.60661440688\n",
      "521 22446.390464076085\n",
      "522 22421.228089040036\n",
      "523 22396.11971538586\n",
      "524 22371.065562936597\n",
      "525 22346.065845416968\n",
      "526 22321.12077030112\n",
      "527 22296.230538714706\n",
      "528 22271.395345855617\n",
      "529 22246.615380255957\n",
      "530 22221.890824174967\n",
      "531 22197.221853876596\n",
      "532 22172.608639054644\n",
      "533 22148.0513429484\n",
      "534 22123.550122687346\n",
      "535 22099.10512878748\n",
      "536 22074.71650539813\n",
      "537 22050.38439019237\n",
      "538 22026.108914580178\n",
      "539 22001.890203679457\n",
      "540 21977.72837574322\n",
      "541 21953.623542974412\n",
      "542 21929.575810899183\n",
      "543 21905.585278521656\n",
      "544 21881.652038591226\n",
      "545 21857.776177345815\n",
      "546 21833.95777443819\n",
      "547 21810.196903195698\n",
      "548 21786.49363058409\n",
      "549 21762.848016991535\n",
      "550 21739.26011649548\n",
      "551 21715.729976821098\n",
      "552 21692.257639141568\n",
      "553 21668.843138239055\n",
      "554 21645.486502775213\n",
      "555 21622.187755064508\n",
      "556 21598.946911090356\n",
      "557 21575.76398042944\n",
      "558 21552.638966568786\n",
      "559 21529.571866819086\n",
      "560 21506.5626721498\n",
      "561 21483.611367648427\n",
      "562 21460.717932110125\n",
      "563 21437.882338528227\n",
      "564 21415.104553666497\n",
      "565 21392.384538400212\n",
      "566 21369.72224793381\n",
      "567 21347.11763131626\n",
      "568 21324.57063196129\n",
      "569 21302.081187524367\n",
      "570 21279.649230047256\n",
      "571 21257.27468595503\n",
      "572 21234.95747617691\n",
      "573 21212.69751601094\n",
      "574 21190.494715656714\n",
      "575 21168.348979748735\n",
      "576 21146.260207699605\n",
      "577 21124.228293940414\n",
      "578 21102.253127648488\n",
      "579 21080.33459309266\n",
      "580 21058.472569451344\n",
      "581 21036.666931377214\n",
      "582 21014.917548356854\n",
      "583 20993.224285563596\n",
      "584 20971.58700335688\n",
      "585 20950.00555788962\n",
      "586 20928.47980086692\n",
      "587 20907.00957941666\n",
      "588 20885.594736813662\n",
      "589 20864.235112023183\n",
      "590 20842.930540139732\n",
      "591 20821.680852274207\n",
      "592 20800.48587587856\n",
      "593 20779.345434504557\n",
      "594 20758.259348307223\n",
      "595 20737.227433907035\n",
      "596 20716.24950448086\n",
      "597 20695.325369972143\n",
      "598 20674.45483722502\n",
      "599 20653.63770969375\n",
      "600 20632.873788164747\n",
      "601 20612.16287057317\n",
      "602 20591.50475181186\n",
      "603 20570.899224379133\n",
      "604 20550.346078341496\n",
      "605 20529.84510100849\n",
      "606 20509.39607733314\n",
      "607 20488.998790176334\n",
      "608 20468.653020225513\n",
      "609 20448.358546168478\n",
      "610 20428.11514462123\n",
      "611 20407.92259044956\n",
      "612 20387.780656658746\n",
      "613 20367.689114830468\n",
      "614 20347.647734716837\n",
      "615 20327.65628485918\n",
      "616 20307.714532195383\n",
      "617 20287.822242654696\n",
      "618 20267.97918068131\n",
      "619 20248.18510976452\n",
      "620 20228.43979236154\n",
      "621 20208.742989961782\n",
      "622 20189.094463216265\n",
      "623 20169.493972011696\n",
      "624 20149.94127536883\n",
      "625 20130.436132144016\n",
      "626 20110.978300212562\n",
      "627 20091.56753730248\n",
      "628 20072.203600552803\n",
      "629 20052.88624677664\n",
      "630 20033.615232702417\n",
      "631 20014.390314866916\n",
      "632 19995.211249330918\n",
      "633 19976.077792439803\n",
      "634 19956.98970034969\n",
      "635 19937.94672946235\n",
      "636 19918.9486361922\n",
      "637 19899.995177099387\n",
      "638 19881.086108881173\n",
      "639 19862.221188640488\n",
      "640 19843.400173841947\n",
      "641 19824.62282223244\n",
      "642 19805.88889188681\n",
      "643 19787.198141350927\n",
      "644 19768.550329667905\n",
      "645 19749.945216510838\n",
      "646 19731.382562072675\n",
      "647 19712.86212716075\n",
      "648 19694.38367310846\n",
      "649 19675.946962027738\n",
      "650 19657.551756591456\n",
      "651 19639.197820249286\n",
      "652 19620.884917336316\n",
      "653 19602.61281293901\n",
      "654 19584.38127269454\n",
      "655 19566.190063329286\n",
      "656 19548.038952234776\n",
      "657 19529.927707837458\n",
      "658 19511.856099156797\n",
      "659 19493.823896413513\n",
      "660 19475.830870625654\n",
      "661 19457.876793797295\n",
      "662 19439.96143866978\n",
      "663 19422.08457899907\n",
      "664 19404.245989605337\n",
      "665 19386.445446300688\n",
      "666 19368.682725742063\n",
      "667 19350.95760551433\n",
      "668 19333.269864414466\n",
      "669 19315.61928198742\n",
      "670 19298.005639065646\n",
      "671 19280.428717337774\n",
      "672 19262.88829940797\n",
      "673 19245.38416897696\n",
      "674 19227.9161107692\n",
      "675 19210.48391046438\n",
      "676 19193.08735476217\n",
      "677 19175.72623169802\n",
      "678 19158.400329805652\n",
      "679 19141.10943879545\n",
      "680 19123.853349302448\n",
      "681 19106.63185329868\n",
      "682 19089.44474353907\n",
      "683 19072.29181378088\n",
      "684 19055.172858919774\n",
      "685 19038.08767451782\n",
      "686 19021.036057519814\n",
      "687 19004.017805394164\n",
      "688 18987.0327170899\n",
      "689 18970.080592243463\n",
      "690 18953.161231358576\n",
      "691 18936.27443615389\n",
      "692 18919.420009269892\n",
      "693 18902.597754224706\n",
      "694 18885.807475325564\n",
      "695 18869.04897806639\n",
      "696 18852.32206906232\n",
      "697 18835.62655525227\n",
      "698 18818.96224495585\n",
      "699 18802.328947422608\n",
      "700 18785.726472475108\n",
      "701 18769.1546310214\n",
      "702 18752.613235021334\n",
      "703 18736.102097273873\n",
      "704 18719.62103151817\n",
      "705 18703.16985195384\n",
      "706 18686.748373971557\n",
      "707 18670.356413878282\n",
      "708 18653.99378874924\n",
      "709 18637.660316739806\n",
      "710 18621.355816467127\n",
      "711 18605.080107674414\n",
      "712 18588.83301087325\n",
      "713 18572.614347429637\n",
      "714 18556.42393942814\n",
      "715 18540.261609915484\n",
      "716 18524.127182778804\n",
      "717 18508.020482741063\n",
      "718 18491.941335281845\n",
      "719 18475.889566626112\n",
      "720 18459.865003831128\n",
      "721 18443.86747483116\n",
      "722 18427.896808342866\n",
      "723 18411.95283390456\n",
      "724 18396.03538176199\n",
      "725 18380.144283073354\n",
      "726 18364.27936963316\n",
      "727 18348.4404740008\n",
      "728 18332.62742970659\n",
      "729 18316.84007094453\n",
      "730 18301.078232886153\n",
      "731 18285.341751207823\n",
      "732 18269.63046236297\n",
      "733 18253.944203682004\n",
      "734 18238.28281337347\n",
      "735 18222.646130303594\n",
      "736 18207.033994056157\n",
      "737 18191.446245014842\n",
      "738 18175.882724366817\n",
      "739 18160.34327399722\n",
      "740 18144.827736628446\n",
      "741 18129.335955660943\n",
      "742 18113.867775475876\n",
      "743 18098.423040993803\n",
      "744 18083.001597984854\n",
      "745 18067.603292813557\n",
      "746 18052.227972934204\n",
      "747 18036.87548627652\n",
      "748 18021.545681751202\n",
      "749 18006.23840879787\n",
      "750 17990.95351780366\n",
      "751 17975.69085989971\n",
      "752 17960.450286871164\n",
      "753 17945.231651350547\n",
      "754 17930.03480675215\n",
      "755 17914.859607104354\n",
      "756 17899.70590745384\n",
      "757 17884.57356344921\n",
      "758 17869.462431680287\n",
      "759 17854.37236915026\n",
      "760 17839.303233798128\n",
      "761 17824.25488453239\n",
      "762 17809.22718086798\n",
      "763 17794.219983054383\n",
      "764 17779.23315220051\n",
      "765 17764.266550031418\n",
      "766 17749.320039332797\n",
      "767 17734.39348353506\n",
      "768 17719.48674664963\n",
      "769 17704.59969357247\n",
      "770 17689.732189983126\n",
      "771 17674.884102532196\n",
      "772 17660.05529839807\n",
      "773 17645.24564550894\n",
      "774 17630.45501287189\n",
      "775 17615.68326997605\n",
      "776 17600.93028721662\n",
      "777 17586.195935705004\n",
      "778 17571.480087239048\n",
      "779 17556.78261455069\n",
      "780 17542.10339114567\n",
      "781 17527.442291179264\n",
      "782 17512.799189606514\n",
      "783 17498.173962223198\n",
      "784 17483.566485557858\n",
      "785 17468.976636931973\n",
      "786 17454.404294259053\n",
      "787 17439.84933653453\n",
      "788 17425.311643246143\n",
      "789 17410.791094576565\n",
      "790 17396.287571702353\n",
      "791 17381.80095640368\n",
      "792 17367.33113120925\n",
      "793 17352.877979502246\n",
      "794 17338.441385323007\n",
      "795 17324.0212333115\n",
      "796 17309.61740895321\n",
      "797 17295.229798620592\n",
      "798 17280.858289216038\n",
      "799 17266.502768206767\n",
      "800 17252.16312399251\n",
      "801 17237.839245658608\n",
      "802 17223.531022755757\n",
      "803 17209.238345797894\n",
      "804 17194.96110586898\n",
      "805 17180.69919463413\n",
      "806 17166.452504575704\n",
      "807 17152.22092852594\n",
      "808 17138.004360113522\n",
      "809 17123.802693773945\n",
      "810 17109.615824147386\n",
      "811 17095.443646937438\n",
      "812 17081.28605812333\n",
      "813 17067.14295414821\n",
      "814 17053.014232479807\n",
      "815 17038.899790651514\n",
      "816 17024.799526897074\n",
      "817 17010.71333985008\n",
      "818 16996.641128798015\n",
      "819 16982.582793478294\n",
      "820 16968.538234059444\n",
      "821 16954.50735091877\n",
      "822 16940.490045144845\n",
      "823 16926.48621807279\n",
      "824 16912.495771462407\n",
      "825 16898.518607373535\n",
      "826 16884.55462801903\n",
      "827 16870.603736116293\n",
      "828 16856.665834758955\n",
      "829 16842.740827004633\n",
      "830 16828.828616064366\n",
      "831 16814.929105701212\n",
      "832 16801.04219938163\n",
      "833 16787.16780101953\n",
      "834 16773.305814290015\n",
      "835 16759.456143234598\n",
      "836 16745.618691818447\n",
      "837 16731.793363885663\n",
      "838 16717.980063114574\n",
      "839 16704.17869324983\n",
      "840 16690.389158006237\n",
      "841 16676.61136081104\n",
      "842 16662.84520492697\n",
      "843 16649.090593101428\n",
      "844 16635.347427925186\n",
      "845 16621.61561189768\n",
      "846 16607.895047010326\n",
      "847 16594.185634774145\n",
      "848 16580.487276322438\n",
      "849 16566.79987239546\n",
      "850 16553.12332330291\n",
      "851 16539.45752882511\n",
      "852 16525.802387971657\n",
      "853 16512.15779928733\n",
      "854 16498.523660951407\n",
      "855 16484.899870531593\n",
      "856 16471.286324817265\n",
      "857 16457.682920177722\n",
      "858 16444.089552359474\n",
      "859 16430.506116691362\n",
      "860 16416.93250782986\n",
      "861 16403.368620248704\n",
      "862 16389.814347912252\n",
      "863 16376.269584343136\n",
      "864 16362.7342229645\n",
      "865 16349.208157037823\n",
      "866 16335.691279739422\n",
      "867 16322.183484297911\n",
      "868 16308.6846643906\n",
      "869 16295.194714062129\n",
      "870 16281.713527628915\n",
      "871 16268.241000305545\n",
      "872 16254.777028348712\n",
      "873 16241.321509201372\n",
      "874 16227.874341623652\n",
      "875 16214.435425800206\n",
      "876 16201.00466386708\n",
      "877 16187.58196022623\n",
      "878 16174.16722136349\n",
      "879 16160.76035644406\n",
      "880 16147.36127738114\n",
      "881 16133.969899241134\n",
      "882 16120.586140157027\n",
      "883 16107.209921550453\n",
      "884 16093.841168627505\n",
      "885 16080.47981043655\n",
      "886 16067.125779739927\n",
      "887 16053.779013296738\n",
      "888 16040.439451960721\n",
      "889 16027.107040616862\n",
      "890 16013.781728534186\n",
      "891 16000.463469139524\n",
      "892 15987.152219852376\n",
      "893 15973.847942218375\n",
      "894 15960.550601641256\n",
      "895 15947.26016768922\n",
      "896 15933.976613725199\n",
      "897 15920.699916543186\n",
      "898 15907.43005665389\n",
      "899 15894.167017992511\n",
      "900 15880.910787592255\n",
      "901 15867.661355490043\n",
      "902 15854.418714672933\n",
      "903 15841.1828608093\n",
      "904 15827.95379211876\n",
      "905 15814.73150904843\n",
      "906 15801.516014202283\n",
      "907 15788.307312253457\n",
      "908 15775.1054098064\n",
      "909 15761.910314980074\n",
      "910 15748.722037712638\n",
      "911 15735.540589034745\n",
      "912 15722.36598117866\n",
      "913 15709.198227822366\n",
      "914 15696.037343508624\n",
      "915 15682.883343980218\n",
      "916 15669.736245522006\n",
      "917 15656.596065320526\n",
      "918 15643.462821300347\n",
      "919 15630.336531611538\n",
      "920 15617.217215310093\n",
      "921 15604.104891701616\n",
      "922 15590.999580437336\n",
      "923 15577.901301751212\n",
      "924 15564.810076075852\n",
      "925 15551.72592405182\n",
      "926 15538.648866558418\n",
      "927 15525.578924595444\n",
      "928 15512.516119137235\n",
      "929 15499.46047172998\n",
      "930 15486.412003812533\n",
      "931 15473.370736878052\n",
      "932 15460.336692357407\n",
      "933 15447.309891889725\n",
      "934 15434.290357221038\n",
      "935 15421.278109942408\n",
      "936 15408.273171732273\n",
      "937 15395.275564214577\n",
      "938 15382.285309010433\n",
      "939 15369.302427636596\n",
      "940 15356.326941837544\n",
      "941 15343.358873076668\n",
      "942 15330.39824268492\n",
      "943 15317.445072264598\n",
      "944 15304.499383206381\n",
      "945 15291.561196952234\n",
      "946 15278.6305347365\n",
      "947 15265.707417858443\n",
      "948 15252.791867506929\n",
      "949 15239.883904913044\n",
      "950 15226.983551179492\n",
      "951 15214.090827357093\n",
      "952 15201.205754547473\n",
      "953 15188.328353555165\n",
      "954 15175.45864550536\n",
      "955 15162.596651243986\n",
      "956 15149.742391683898\n",
      "957 15136.895887470706\n",
      "958 15124.057159411599\n",
      "959 15111.226228260846\n",
      "960 15098.403114606095\n",
      "961 15085.587839191054\n",
      "962 15072.780422612554\n",
      "963 15059.980885459403\n",
      "964 15047.189248125522\n",
      "965 15034.40553129829\n",
      "966 15021.629755546031\n",
      "967 15008.861941136644\n",
      "968 14996.102108674957\n",
      "969 14983.350278493925\n",
      "970 14970.60647102165\n",
      "971 14957.87070669115\n",
      "972 14945.143005706397\n",
      "973 14932.423388419464\n",
      "974 14919.711875091978\n",
      "975 14907.008486047978\n",
      "976 14894.313241581889\n",
      "977 14881.626161982826\n",
      "978 14868.94726749152\n",
      "979 14856.276578348312\n",
      "980 14843.614114691814\n",
      "981 14830.959896773757\n",
      "982 14818.313944695225\n",
      "983 14805.676278653062\n",
      "984 14793.046918876451\n",
      "985 14780.42588548861\n",
      "986 14767.813198626958\n",
      "987 14755.208878395906\n",
      "988 14742.61294497666\n",
      "989 14730.025418383904\n",
      "990 14717.446318674447\n",
      "991 14704.87566598116\n",
      "992 14692.313480430767\n",
      "993 14679.759781969191\n",
      "994 14667.214590482019\n",
      "995 14654.677925985963\n",
      "996 14642.149808473401\n",
      "997 14629.630258022073\n",
      "998 14617.119294617205\n",
      "999 14604.616938058552\n"
     ]
    }
   ],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self) -> None:\n",
    "        # self.rnn = RNN(hidden_size=20,output_size=30, return_sequences=False, bidirectional=False, init='glorot_uniform')\n",
    "        self.lstm = LSTM(hidden_size=20, return_sequences=False, bidirectional=False)\n",
    "        self.layernorm = LayerNorm(D=1)\n",
    "        # self.rnn2 = RNN(hidden_size=20,output_size=30, return_sequences=False, init='glorot_uniform')\n",
    "        self.flatten = FlattenLayer()\n",
    "        # self.linear1 = FCLayer(output_dim=20)\n",
    "        self.linear2 = FCLayer(output_dim=1)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        x = self.lstm(inputs)\n",
    "        # print(f'before layernorm: {x}')\n",
    "        x1 = self.layernorm(x)\n",
    "        # print(f'after layernorm: {x1}')\n",
    "        # x = self.rnn2(x)\n",
    "        x2 = self.flatten(x1)\n",
    "        # x3 = self.linear1(x2)\n",
    "        x4 = self.linear2(x2)\n",
    "        return x4\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "batch_size = 32\n",
    "model = Model()\n",
    "x_train = Tensor(x_train_shuffled)\n",
    "y_train = Tensor(np.expand_dims(y_train_shuffled,axis=-1))\n",
    "i = 0\n",
    "for epoch in range(1000):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for start in range(0, len(x_train_shuffled), batch_size):\n",
    "        end = start + batch_size\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = x_train[start:end]\n",
    "\n",
    "        predicted = model(inputs)\n",
    "        actual = y_train[start:end]\n",
    "        # print(predicted.shape, actual.shape)\n",
    "        loss = mse(y_true=actual, y_pred=predicted)\n",
    "        if i==0:\n",
    "            draw_computation_graph(loss)\n",
    "        i+=1\n",
    "        # for param in model.parameters():\n",
    "        #     print(param.name, param.id)\n",
    "        # print(start, loss)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        optimizer.step(model)\n",
    "    epoch_loss /= (train_data//batch_size + train_data % batch_size)\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25b522a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-121.],\n",
       "       [-114.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[10,11,12], [11,12,13]])\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "test = Tensor(test)\n",
    "print(test.shape)\n",
    "out = model(test)\n",
    "np.round(out.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a37fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss=0.05099273701819455\n",
      "epoch 2: loss=13249.740727372679\n",
      "epoch 3: loss=14749.407963355394\n",
      "epoch 4: loss=15250.752942708701\n",
      "epoch 5: loss=15492.481381005673\n",
      "epoch 6: loss=15622.466162419676\n",
      "epoch 7: loss=15695.41284485934\n",
      "epoch 8: loss=15737.109844625535\n",
      "epoch 9: loss=15761.141082024957\n",
      "epoch 10: loss=15775.042803035232\n",
      "epoch 11: loss=15783.09837473718\n",
      "epoch 12: loss=15787.76982950266\n",
      "epoch 13: loss=15790.479695590704\n",
      "epoch 14: loss=15792.051856900975\n",
      "epoch 15: loss=15792.963996259414\n",
      "epoch 16: loss=15793.493200779878\n",
      "epoch 17: loss=15793.800228715167\n",
      "epoch 18: loss=15793.978352795524\n",
      "epoch 19: loss=15794.081690347573\n",
      "epoch 20: loss=15794.141639857253\n",
      "epoch 21: loss=15794.176417997249\n",
      "epoch 22: loss=15794.19659337004\n",
      "epoch 23: loss=15794.208297317833\n",
      "epoch 24: loss=15794.215086847486\n",
      "epoch 25: loss=15794.219025468892\n",
      "epoch 26: loss=15794.221310260786\n",
      "epoch 27: loss=15794.222635661932\n",
      "epoch 28: loss=15794.223404521004\n",
      "epoch 29: loss=15794.22385053153\n",
      "epoch 30: loss=15794.224109259143\n",
      "epoch 31: loss=15794.224259344925\n",
      "epoch 32: loss=15794.224346408413\n",
      "epoch 33: loss=15794.22439691318\n",
      "epoch 34: loss=15794.224426210523\n",
      "epoch 35: loss=15794.224443205623\n",
      "epoch 36: loss=15794.2244530643\n",
      "epoch 37: loss=15794.224458783203\n",
      "epoch 38: loss=15794.22446210072\n",
      "epoch 39: loss=15794.22446402514\n",
      "epoch 40: loss=15794.224465141517\n",
      "epoch 41: loss=15794.224465789084\n",
      "epoch 42: loss=15794.224466164736\n",
      "epoch 43: loss=15794.22446638269\n",
      "epoch 44: loss=15794.22446650907\n",
      "epoch 45: loss=15794.22446658242\n",
      "epoch 46: loss=15794.224466624943\n",
      "epoch 47: loss=15794.224466649632\n",
      "epoch 48: loss=15794.224466663925\n",
      "epoch 49: loss=15794.224466672225\n",
      "epoch 50: loss=15794.224466677038\n",
      "epoch 51: loss=15930.649908907928\n",
      "epoch 52: loss=15939.264734232322\n",
      "epoch 53: loss=15948.58238361161\n",
      "epoch 54: loss=15953.986863057577\n",
      "epoch 55: loss=15957.122304554452\n",
      "epoch 56: loss=15958.941586224675\n",
      "epoch 57: loss=15959.997270124293\n",
      "epoch 58: loss=15960.609883998388\n",
      "epoch 59: loss=15960.96539313605\n",
      "epoch 60: loss=15961.171703524114\n",
      "epoch 61: loss=15961.291431359525\n",
      "epoch 62: loss=15961.360913200611\n",
      "epoch 63: loss=15961.401235821391\n",
      "epoch 64: loss=15961.424636416967\n",
      "epoch 65: loss=15961.438216595921\n",
      "epoch 66: loss=15961.446097650123\n",
      "epoch 67: loss=15961.450671303874\n",
      "epoch 68: loss=15961.453325556908\n",
      "epoch 69: loss=15961.454865914031\n",
      "epoch 70: loss=15961.455759837934\n",
      "epoch 71: loss=15961.45627861366\n",
      "epoch 72: loss=15961.456579677691\n",
      "epoch 73: loss=15961.456754395815\n",
      "epoch 74: loss=15961.456855790992\n",
      "epoch 75: loss=15961.456914634151\n",
      "epoch 76: loss=15961.456948782956\n",
      "epoch 77: loss=15961.45696860073\n",
      "epoch 78: loss=15961.456980101626\n",
      "epoch 79: loss=15961.456986776077\n",
      "epoch 80: loss=15961.45699064951\n",
      "epoch 81: loss=15961.456992897345\n",
      "epoch 82: loss=15961.456994201864\n",
      "epoch 83: loss=15961.456994958962\n",
      "epoch 84: loss=15961.456995398261\n",
      "epoch 85: loss=15961.456995653254\n",
      "epoch 86: loss=15961.456995801243\n",
      "epoch 87: loss=15961.45699588706\n",
      "epoch 88: loss=15961.456995936904\n",
      "epoch 89: loss=15961.456995965844\n",
      "epoch 90: loss=15961.456995982642\n",
      "epoch 91: loss=15961.456995992376\n",
      "epoch 92: loss=15961.456995998022\n",
      "epoch 93: loss=15961.45699600127\n",
      "epoch 94: loss=15961.456996003191\n",
      "epoch 95: loss=15961.456996004295\n",
      "epoch 96: loss=15961.45699600493\n",
      "epoch 97: loss=15961.456996005327\n",
      "epoch 98: loss=15961.456996005525\n",
      "epoch 99: loss=15961.45699600566\n",
      "epoch 100: loss=15961.456996005729\n"
     ]
    }
   ],
   "source": [
    "def learning_rate_decay(epoch, optimizer):\n",
    "    if epoch != 0 and epoch % 50 == 0:\n",
    "        optimizer.learning_rate *= 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f56c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf15-gpu",
   "language": "python",
   "name": "tf15-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
