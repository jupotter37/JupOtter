{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto: Análise de Sentimentos em Discursos Políticos\n",
    "\n",
    "<font color='black'>**Parte 1: Coleta de dados**</font><br>\n",
    "<font color='gray'>Parte 2: Limpeza de dados</font><br>\n",
    "<font color='gray'>Parte 3: Modelagem e Análise de Sentimentos</font>\n",
    "\n",
    "![Alt text](./images/web-scraping-1.svg)\n",
    "\n",
    "Esta é a primeira parte deste projeto, no qual vamos obter os discursos políticos do presidente atual do Brasil, diretamente de suas fontes oficiais, através de um `Web Scraping`, no site público dos [discursos](https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos) do governo.\n",
    "\n",
    "Nosso objetivo é coletar as transcrições de seus discursos oficiais em 2019 e 2020.\n",
    "\n",
    "Vamos coletar os dados!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas\n",
    "\n",
    "Iniciaremos importando as bibliotecas que serão utilizadas. As principais delas são:\n",
    "* **BeautifulSoup:** é uma biblioteca Python de extração de dados de arquivos HTML e XML. Ele cria uma árvore de análise para páginas analisadas que podem ser usadas para extrair dados de HTML, o que é útil para web scraping.\n",
    "* **Selenium:** usado para testar aplicações web pelo browser de forma automatizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação de dados\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Definimos o parâmetro abaixo para evitar o erro: RecursionError: maximum recursion depth exceeded\n",
    "# Ao gravar os resultados em disco\n",
    "sys.setrecursionlimit(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir o **chromedriver** para abrir o navegador e executar as instruções automaticamente. O chromedriver deve ser da mesma versão do navegador google chrome instalado na máquina e ser colocado na mesma pasta do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizar chromedriver\n",
    "chromedriver = \"./chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então vamos começar criando uma lista vazia para armazenar as urls para coleta. Neste caso, já analisado previamente, as páginas seguem um padrão com o número no final, múltiplos de 30, portanto, podemos criar um loop para inserirmos o número de cada página. Lembrando que nem todas as páginas são assim e dependendo da página deverá analisar a melhor forma de definir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria um lista vazia para receber as urls\n",
    "lista_urls = []\n",
    "\n",
    "# loop para criar a lista\n",
    "for i in range(0, 271, 30):\n",
    "    full_url = 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?' + 'b_start:int=' + str(i)\n",
    "    lista_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada na lista, no nosso caso são 10 urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=0',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=30',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=60',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=90',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=120',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=150',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=180',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=210',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=240',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos?b_start:int=270']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizando as listas\n",
    "lista_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos definir um contador, este contador é o número de páginas que queremos coletar, a princípio podemos coletar um pouco e aumentarmos se desejamos. A quantidade de páginas para coleta está diretamente ligada ao tempo da coleta, então vamos iniciar com 5 páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o contador\n",
    "contador = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As URLs definidas anteriormente são os locais onde contém os discursos, cada URL contém 30 links, correspondentes à 30 discursos, que serão as próximas coletas.\n",
    "\n",
    "Criaremos agora uma função para começar as coletas.<br>\n",
    "Observação: quando vamos coletar várias páginas, não devemos deixar o loop rodando direto, ou seja, deve-se ter uma pausa antes da página ir para a próxima, essa boa prática deve ser utilizada, pois se não o servidor da página pode sobrecarregar e seremos derrubados. Na função abaixo colocamos um tempo de 5s `time.sleep(5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a função para scraping\n",
    "def scraping_urls(urls):\n",
    "    \n",
    "    # definindo o driver\n",
    "    driver = webdriver.Chrome(\"./chromedriver\")\n",
    "    \n",
    "    # lista para receber o resultado\n",
    "    soup_list = []\n",
    "    \n",
    "    # contador para o loop\n",
    "    count = 0\n",
    "    \n",
    "    # iniciando a contagem do tempo\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # loop pelas urls\n",
    "    for i in urls:\n",
    "        if count < contador:\n",
    "            driver.get(i)\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "            soup_list.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "        count += 1  \n",
    "    driver.close()\n",
    "    \n",
    "    # tempo decorrido do loop\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print(\"Tempo total da coleta: \" + str(int(elapsed_time))  + \" segundos.\")\n",
    "    \n",
    "    # retornando o resultado da função\n",
    "    return soup_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a função criada, então vamos executá-la e em seguida visualizar o resultado da coleta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total da coleta: 51 segundos.\n"
     ]
    }
   ],
   "source": [
    "# executando a função\n",
    "soups = scraping_urls(lista_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando\n",
    "# soups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado é o código fonte, html da página, agora utilizaremos os métodos da biblioteca `BeautifulSoup` e eventualmente podemos utilizar também as expressões regulares `regex`.\n",
    "\n",
    "Mas antes vamos dar uma olhada, à quantas páginas correspondem esses códigos com a função `len()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora criaremos a função que extrairá as URLs de cada discurso e cada URL será armazenada em uma lista.\n",
    "\n",
    "Mas como vamos descobrir em qual parte do código HTML constam essas URLs?<br>\n",
    "Deve-se procurar pelas *tags* e *classes* que formam a página, uma forma mais fácil, pelo google chrome clicando com o botão direito em algum lugar da página, clique na opção *inspecionar*,  ai abrirá a janela de desenvolvedor, com isso poderá ir com a seta em cima do link na página e será direcionada para esta parte do código, então você insere nos métodos da biblioteca *BeautifulSoup*. Vá aos poucos e assim vai aprendendo em que momentos colocará *tag* e *classes* para extrair o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a função\n",
    "def extrai_links(soup_object):\n",
    "    \n",
    "    # lista para receber o resultado\n",
    "    links_list = []\n",
    "    \n",
    "    # loop pela código da pagina\n",
    "    for s in soup_object:\n",
    "        links = s.find_all('a', {'class':'summary url'})\n",
    "        for i in links:\n",
    "            full_link = i['href']\n",
    "            links_list.append(full_link)\n",
    "            \n",
    "    # retorna o resultado\n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos excutar a função e dar uma olhada no resultado, do que sobrou do código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos/2020/discurso-do-presidente-da-republica-jair-bolsonaro-no-lancamento-dos-programas-codex-e-super-br-e-8o-revogacao-palacio-do-planalto',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos/2020/discurso-do-presidente-da-republica-jair-bolsonaro-na-cupula-do-g20-videoconferencia-palacio-do-planalto-1',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos/2020/discurso-do-presidente-da-republica-jair-bolsonaro-na-cupula-do-g20-videoconferencia-palacio-do-planalto',\n",
       " 'https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos/2020/discurso-do-presidente-da-republica-jair-bolsonaro-na-cerimonia-de-entrega-de-titulos-de-propriedade-rural-em-goias-flores-de-goias']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list = extrai_links(soups)\n",
    "link_list[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Era isso mesmo que queríamos! vamos dar uma olhada em quantos links, no nosso caso, são 5 páginas de coletas com 30 discursos, dando um total de 150 links de discursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos salvar esses links, caso precisarmos mais tarde, não será necessário rodar novamente, vamos utilizar a extensão `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em disco\n",
    "with open('dados/db_links.pickle', 'wb') as to_write:\n",
    "    pickle.dump(link_list, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizarmos as coletas, nos falta a parte mais importante que são os discursos, também criaremos uma função para isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando a função que extrai o discurso político do código fonte\n",
    "def extrai_discurso(urls):\n",
    "    \n",
    "    # instancia o driver do chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    \n",
    "    # lista vazia para receber o resultado\n",
    "    doc_source = []\n",
    "    \n",
    "    # inicia o tempo para o loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # loop para coleta dos dados\n",
    "    for i in urls:\n",
    "        driver.get(i)\n",
    "        time.sleep(5)\n",
    "        doc_source.append(BeautifulSoup(driver.page_source, 'html.parser'))\n",
    "    driver.close()\n",
    "\n",
    "    # tempo decorrido do loop\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print(\"Tempo total da coleta dos discursos: \" + str(int(elapsed_time))  + \" segundos.\")\n",
    "    \n",
    "    # retorna o resultado da função\n",
    "    return doc_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executando a função!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total da coleta dos discursos: 1049 segundos.\n"
     ]
    }
   ],
   "source": [
    "discursos = extrai_discurso(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim vamos salvar o código fonte HTML da forma que coletamos e a extração dos discursos e limpeza ficarão para o próximo *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando em disco\n",
    "with open('dados/db_discursos.pickle', 'wb') as to_write:\n",
    "    pickle.dump(discursos, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
