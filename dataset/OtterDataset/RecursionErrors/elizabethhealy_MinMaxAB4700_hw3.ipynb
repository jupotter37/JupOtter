{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework 3: Adversarial Search\n",
    "<h3 style=\"color: red;\">\n",
    "    Due: Monday, March 4, 1:24pm on Gradescope\n",
    "    <br><br>\n",
    "    Late submission: Wednesday, March 6, 1:24pm on Gradescope (50% penalty)\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which other students did you interact with on this assignment? Provide their NetID(s) below consistent with the [homework policy](http://www.cs.cornell.edu/courses/cs4700/2019sp/)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eah252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written Question\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">25 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following game tree.  Each game state has two actions that can be taken: Left and Right.  The numbers at the terminal nodes are the values given by the evaluation function.\n",
    "  1. Which move should be made at the top level (Left or Right)?  What is its value?\n",
    "  1. Which nodes get pruned if you use alpha-beta pruning when you explore successors of states going left-to-right? \n",
    "  1. Give the answer to (B) when successors are explored right-to-left.\n",
    "  \n",
    "![](game_tree.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 1A: Right - value = 5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 1B: S, K, V, W, O, DD, EE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 1C: D, H, I, P, Q, R, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider using minimax search with alpha–beta pruning on a game whose branching factor is $b$ and where you search to depth $m$. If successor nodes are searched in the worst-case order no pruning happens and the algorithm takes $O(b^m)$ time.  (Make sure you understand why that is true.) It has been shown that if successors are ordered in the best-case optimal fashion the algorithm’s time complexity will be $O(b^\\frac{m}{2})$. This means the best-case scenario will take less time to search to the same depth $m$. Let’s say the worst-case ordering of successors runs for time $T$.  How much deeper can you search in the best-case scenario if it is also allowed to run for time $T$?  To make this as simple as possible, assume the constants that are “hidden” in both big-O time complexity expressions are the same."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ANSWER TO QUESTION 2: you can search to depth 2m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the assignment will provide you with programming exercises to help cement the key ideas about adversarial search. For this purpose, we have chosen a simple 2-player strategy game called __Neutron__ as our domain of interest. It is played on a 5 x 5 board. Each player has 5 soldier pieces and a respective home row. The soldiers for each player start by occupying their home row; for player 1, this is row 1, and for player 2, this is row 5. In addition, there is a neutral piece called the Neutron located in the center of the board that both players control.\n",
    "\n",
    "For our purposes, we have chosen to make each piece distinct from one another. The soldier pieces for player 1 are marked by the first 5 letters of the alphabet. The soldier pieces for player 2 are marked by the first 5 positive integers. The Neutron piece is marked by a \\*. The initial position of the board (as well row and column numbers) can be illustrated as:\n",
    "\n",
    "```\n",
    "          1   2   3   4   5\n",
    "\n",
    "        ---------------------\n",
    "1       | A | B | C | D | E |\n",
    "        ---------------------\n",
    "2       |   |   |   |   |   |\n",
    "        ---------------------\n",
    "3       |   |   | * |   |   |\n",
    "        ---------------------\n",
    "4       |   |   |   |   |   |\n",
    "        ---------------------\n",
    "5       | 1 | 2 | 3 | 4 | 5 |\n",
    "        ---------------------\n",
    "```\n",
    "\n",
    "__RULES:__ The game begins with player 1 and then alternates between players. In the first turn, player 1 must select 1 of their own soldier pieces to move. Then, for every turn after this, a player must make 2 sequential moves. First, they must move the Neutron piece. Second, they must select 1 of their own soldier pieces to move. The movement of a piece (soldier or Neutron) can be in either of the 4 directions: up, left, down, right. A piece can move in a direction as long as the path is clear. A piece cannot take another piece, jump over a piece, or otherwise change directions. They must move as many spaces as possible in the direction chosen until they collide with another piece or a wall.\n",
    "\n",
    "For example, let us use the initial state illustrated above. Since this is the beginning of the game, player 1 has 5 available choices. Note, coordinates are in (row,column) format.\n",
    "\n",
    "1. Move A from (1,1) down to (4,1)\n",
    "1. Move B from (1,2) down to (4,2)\n",
    "1. Move C from (1,3) down to (2,3)\n",
    "1. Move D from (1,4) down to (4,4)\n",
    "1. Move E from (1,5) down to (4,5)\n",
    "\n",
    "Let's say player 1 decides to use move 4 from the list above. The resulting state would be:\n",
    "\n",
    "```\n",
    "          1   2   3   4   5\n",
    "\n",
    "        ---------------------\n",
    "1       | A | B | C |   | E |\n",
    "        ---------------------\n",
    "2       |   |   |   |   |   |\n",
    "        ---------------------\n",
    "3       |   |   | * |   |   |\n",
    "        ---------------------\n",
    "4       |   |   |   | D |   |\n",
    "        ---------------------\n",
    "5       | 1 | 2 | 3 | 4 | 5 |\n",
    "        ---------------------\n",
    "```\n",
    "\n",
    "It is now player 2's turn. Since this is no longer the initial state of the game, the player must make 2 sequential moves (Neutron followed by a soldier). Therefore, player 2 has 15 possible actions to choose from:\n",
    "\n",
    "1. Move * from (3,3) up to (2,3), then move 1 from (5,1) up to (2,1)\n",
    "1. Move * from (3,3) up to (2,3), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) up to (2,3), then move 3 from (5,3) up to (3,3)\n",
    "1. Move * from (3,3) up to (2,3), then move 5 from (5,5) up to (2,5)\n",
    "1. Move * from (3,3) left to (3,1), then move 1 from (5,1) up to (4,1)\n",
    "1. Move * from (3,3) left to (3,1), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) left to (3,1), then move 3 from (5,3) up to (2,3)\n",
    "1. Move * from (3,3) left to (3,1), then move 5 from (5,5) up to (2,5)\n",
    "1. Move * from (3,3) down to (4,3), then move 1 from (5,1) up to (2,1)\n",
    "1. Move * from (3,3) down to (4,3), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) down to (4,3), then move 5 from (5,5) up to (2,5)\n",
    "1. Move * from (3,3) right to (3,5), then move 1 from (5,1) up to (2,1)\n",
    "1. Move * from (3,3) right to (3,5), then move 2 from (5,2) up to (2,2)\n",
    "1. Move * from (3,3) right to (3,5), then move 3 from (5,3) up to (2,3)\n",
    "1. Move * from (3,3) right to (3,5), then move 5 from (5,5) up to (4,5)\n",
    "\n",
    "This back-and-forth will continue until the game ends. So, how does the game of Neutron end?\n",
    "\n",
    "__GOALS:__ The main goal of the game is for a player to move the Neutron piece to their home row. This can happen in a couple of ways. Either the player is able to move the Neutron on their own turn to their home row, or can otherwise force the opponent into moving the Neutron to the player's home row. Alternatively, it is also possible to win the game without securing the Neutron by blocking your opponent from moving, i.e. when your opponent is deadlocked, you also win.\n",
    "\n",
    "Run the code cell below which contains the logic for __Neutron__. Feel free to look through the code for a better understanding of the game implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "# additional definitions\n",
    "GameState = namedtuple('GameState', 'board, player, is_initial, utility_value')\n",
    "PLAYER_1  = ['A', 'B', 'C', 'D', 'E']\n",
    "PLAYER_2  = ['1', '2', '3', '4', '5']\n",
    "NEUTRON   = '*'\n",
    "BLANK     = ' '\n",
    "\n",
    "\"\"\"\n",
    "The Neutron game. Implemented as a class so that multiple instances can be created.\n",
    "\n",
    "A state for this game is a 4-tuple, (board, player, is_initial, utility_value).\n",
    "- board, a dictionary with (i,j) coordinates as the key, and the piece as\n",
    "  the value. Valid coordinate values for i and j are 1 to 5, inclusive. The piece\n",
    "  should refer to one of the variables defined above.\n",
    "- player, refers to the player that needs to move in this state. Should reference\n",
    "  one of the variables defined above.\n",
    "- is_initial, a Boolean value that determines whether or not this state is the\n",
    "  initial state of the game.\n",
    "- utility_value, an integer value that represents the utility value for the\n",
    "  state.\n",
    "\"\"\"\n",
    "class NeutronGame:\n",
    "    \"\"\"\n",
    "    Constructor for the class. Initializes the initial state of the game, the move\n",
    "    ordering that should be used, and the number of states generated during a game.\n",
    "    The game starts with all of player 1's pieces at row 1, all of player 2's pieces\n",
    "    at row 5, and the neutron in the center. The fixed move ordering is up, down,\n",
    "    left, then right.\n",
    "    \n",
    "    Optional parameters:\n",
    "    - custom_state, a user-provided state to use as the initial state of the game\n",
    "    - move_order, a custom move order to be used in actions()\n",
    "    \"\"\"\n",
    "    def __init__(self, custom_state = None, move_order = None):\n",
    "        if custom_state:\n",
    "            self.initial_state = custom_state\n",
    "        else:\n",
    "            board = {(1, j + 1) : PLAYER_1[j] for j in range(5)}\n",
    "            board.update({(i,j) : BLANK for j in range(1,6) for i in range(2,5)})\n",
    "            board.update({(5, j + 1) : PLAYER_2[j] for j in range(5)})\n",
    "            board[(3,3)] = NEUTRON\n",
    "            self.initial_state = GameState(board = board, player = PLAYER_1,\n",
    "                is_initial = True, utility_value = 0)\n",
    "        \n",
    "        if move_order:\n",
    "            self.move_order = move_order\n",
    "        else:\n",
    "            self.move_order = [(-1,0), (1,0), (0,-1), (0,1)]\n",
    "        \n",
    "        self.num_states_gen = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Given a well-defined state as input, this function will return all the\n",
    "    possible actions that can be taken as a list. An action can consist of\n",
    "    1 or 2 moves, and so is treated as a list. A move is represented as a\n",
    "    tuple (piece, description), where piece refers to one of the global\n",
    "    variables and description is a string describing the movement of the piece.\n",
    "    \"\"\"\n",
    "    def actions(self, state):\n",
    "        acts = []\n",
    "\n",
    "        # must move the neutron piece first if this is not the initial state\n",
    "        neutron_moves = []\n",
    "        if not state.is_initial:\n",
    "            neutron_moves = [self.try_move(state.board, NEUTRON, i, j) \\\n",
    "                             for (i,j) in self.move_order]\n",
    "            neutron_moves = filter(lambda m : m != None, neutron_moves)\n",
    "            neutron_moves = map(lambda m : self.simplify_move(state.board, m),\n",
    "                neutron_moves)\n",
    "\n",
    "        # if the neutron was moved, must temporarily transition the state first\n",
    "        # then move the soldier pieces\n",
    "        for nm in neutron_moves:\n",
    "            temp_board = self.result(state, [nm]).board\n",
    "            for piece in state.player:\n",
    "                moves = [self.try_move(temp_board, piece, i, j) \\\n",
    "                         for (i,j) in self.move_order]\n",
    "                moves = filter(lambda m : m != None, moves)\n",
    "                moves = map(lambda m : self.simplify_move(temp_board, m), moves)\n",
    "                for m in moves:\n",
    "                    acts.append([nm, m])\n",
    "\n",
    "        # otherwise, just move the soldier pieces for the player\n",
    "        if not neutron_moves:\n",
    "            for piece in state.player:\n",
    "                moves = [self.try_move(state.board, piece, i, j) \\\n",
    "                         for (i,j) in self.move_order]\n",
    "                moves = filter(lambda m : m != None, moves)\n",
    "                moves = map(lambda m : self.simplify_move(state.board, m),\n",
    "                    moves)\n",
    "                for m in moves:\n",
    "                    acts.append([m])\n",
    "\n",
    "        return acts\n",
    "\n",
    "    \"\"\"\n",
    "    Try moving the piece on the board by shifting the piece's coordinates\n",
    "    by the amount (delta_i,delta_j). Return the resulting move.\n",
    "    \"\"\"\n",
    "    def try_move(self, board, piece, delta_i, delta_j):\n",
    "        move   = None\n",
    "        (i,j)  = self.find_piece(board, piece)\n",
    "        temp_i = i + delta_i\n",
    "        temp_j = j + delta_j\n",
    "        while temp_i >= 1 and temp_i <= 5 and temp_j >= 1 and temp_j <= 5 \\\n",
    "        and board[(temp_i,temp_j)] == BLANK:\n",
    "            temp_i += delta_i\n",
    "            temp_j += delta_j\n",
    "        temp_i -= delta_i\n",
    "        temp_j -= delta_j\n",
    "        if board[(temp_i,temp_j)] == BLANK:\n",
    "            move = (piece, (temp_i,temp_j))\n",
    "        return move\n",
    "\n",
    "    \"\"\"\n",
    "    Find the piece on the board and return its coordinates.\n",
    "    \"\"\"\n",
    "    def find_piece(self, board, piece):\n",
    "        i, j = None, None\n",
    "        for (m,n) in iter(board):\n",
    "            if board[(m,n)] == piece:\n",
    "                i, j = m, n\n",
    "                break\n",
    "        return (i,j)\n",
    "\n",
    "    \"\"\"\n",
    "    Given move as the tuple (piece, description), convert the description into\n",
    "    delta values to try moving the piece on the given board. Return the\n",
    "    resulting move.\n",
    "    \"\"\"\n",
    "    def decipher_move(self, board, move):\n",
    "        (piece,desc) = move\n",
    "        (i,j)        = self.find_piece(board, piece)\n",
    "        vert         = {'UP' : -1, 'DOWN' : 1}\n",
    "        horiz        = {'LEFT' : -1, 'RIGHT' : 1}\n",
    "        return self.try_move(board, piece, vert.get(desc,0), horiz.get(desc,0))\n",
    "        \n",
    "    \"\"\"\n",
    "    Given a move in the format (piece, (i,j)), convert the movement into a simple\n",
    "    description that is user-friendly.\n",
    "    \"\"\"\n",
    "    def simplify_move(self, board, move):\n",
    "        (piece, (i,j)) = move\n",
    "        (m,n)          = self.find_piece(board, piece)\n",
    "        if i < m:\n",
    "            desc = 'UP'\n",
    "        elif i > m:\n",
    "            desc = 'DOWN'\n",
    "        elif j < n:\n",
    "            desc = 'LEFT'\n",
    "        else:\n",
    "            desc = 'RIGHT'\n",
    "        return (piece,desc)\n",
    "\n",
    "    \"\"\"\n",
    "    Given a state and an action, return the new state that our game should\n",
    "    transition to. Assumes state and action are valid.\n",
    "    \"\"\"\n",
    "    def result(self, state, action):\n",
    "        board                = state.board.copy()\n",
    "        player               = state.player\n",
    "        self.num_states_gen += 1\n",
    "        if not action:\n",
    "            return GameState(board = board, is_initial = False, player = player,\n",
    "                utility_value = 1 if state.player == PLAYER_2 else -1)\n",
    "        for a in action:\n",
    "            (piece, (i,j)) = self.decipher_move(board, a)\n",
    "            (m,n)          = self.find_piece(board, piece)\n",
    "            board[(m,n)]   = BLANK\n",
    "            board[(i,j)]   = piece\n",
    "        return GameState(board = board, is_initial = False,\n",
    "            player = PLAYER_2 if player == PLAYER_1 else PLAYER_1,\n",
    "            utility_value = self.compute_utility(board))\n",
    "\n",
    "    \"\"\"\n",
    "    Return the utility of the given state from the perspective of\n",
    "    the given player.\n",
    "    \"\"\"\n",
    "    def utility(self, state, player):\n",
    "        u = state.utility_value\n",
    "        if u == 0 and not self.actions(state):\n",
    "            return -1 if state.player == player else 1\n",
    "        return u if player == PLAYER_1 else -u\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the utility of the board from the perspective of player 1.\n",
    "    +1 if the neutron is in row 1, -1 if the neutron is in row 5,\n",
    "    0 otherwise.\n",
    "    \"\"\"\n",
    "    def compute_utility(self, board):\n",
    "        (i,j) = self.find_piece(board, NEUTRON)\n",
    "        u     = 0\n",
    "        if i == 1:\n",
    "            u = 1\n",
    "        elif i == 5:\n",
    "            u = -1\n",
    "        return u\n",
    "\n",
    "    \"\"\"\n",
    "    A state is terminal if the utility value is non-zero or if\n",
    "    there are no available actions to take in that state.\n",
    "    \"\"\"\n",
    "    def terminal_test(self, state):\n",
    "        return state.utility_value != 0 or not self.actions(state)\n",
    "\n",
    "    \"\"\"\n",
    "    Print the state to console.\n",
    "    \"\"\"\n",
    "    def display(self, state):\n",
    "        board_size = 5\n",
    "        s          = '\\n'\n",
    "        for _ in range(board_size * 4 + 1):\n",
    "            s += '-'\n",
    "        print(s)\n",
    "\n",
    "        for i in range(1, board_size + 1):\n",
    "            s = ''\n",
    "            for j in range(1, board_size + 1):\n",
    "                s += '| ' + state.board[(i,j)] + ' '\n",
    "            s += '|\\n'\n",
    "            for _ in range(board_size * 4 + 1):\n",
    "                s += '-'\n",
    "            print(s)\n",
    "\n",
    "        print('Player to move:',\n",
    "              'Player 1' if state.player == PLAYER_1 else 'Player 2')\n",
    "\n",
    "    \"\"\"\n",
    "    Let two agents play the game from the initial state. display_flag\n",
    "    toggles whether or not to show every state during gameplay.\n",
    "    experiment_flag will suppress all output.\n",
    "    \n",
    "    These are the 4 possible outputs:\n",
    "    (1,1) means player 2 wins, caused by player 1\n",
    "    (1,2) means player 1 wins, caused by player 2\n",
    "    (-1,1) means player 1 wins, caused by player 1\n",
    "    (-1,2) means player 2 wins, caused by player 2\n",
    "    \"\"\"\n",
    "    def play_game(self, first_player, second_player, display_flag, experiment_flag = False):\n",
    "        state = self.initial_state\n",
    "        if display_flag:\n",
    "            self.display(state)\n",
    "        while True:\n",
    "            for player in [first_player, second_player]:\n",
    "                action = player.play_turn(self, state)\n",
    "                state  = self.result(state, action)\n",
    "                if display_flag:\n",
    "                    self.display(state)\n",
    "                if self.terminal_test(state):\n",
    "                    if not experiment_flag:\n",
    "                        print('\\nGame is over!')\n",
    "                        self.display(state)\n",
    "                    return (self.utility(state, state.player),\n",
    "                            1 if player == first_player else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce 2 types of agents that will be able to play our game. The first, called `QueryAgent`, acts as a stand-in for the user. It will output the possible actions that the user can take and ask for input on which action to undergo. The second, called `RandomAgent`, simply chooses amongst the available actions at random. Note that an action in our implementation is a list of tuples (wherein a tuple represents a single move). Therefore, the input you provide the agent should also be a list of tuples.\n",
    "\n",
    "For example, reusing our example from the introduction, the actions for player 1 at the initial state would be displayed as:\n",
    "\n",
    "`[[('A', 'DOWN')], [('B', 'DOWN')], [('C', 'DOWN')], [('D', 'DOWN')], [('E', 'DOWN')]]`\n",
    "\n",
    "Suppose player 1 decides to move its soldier piece B down. Then, as input, it would supply: `[('B', 'DOWN')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that relies on user input.\n",
    "\"\"\"\n",
    "class QueryAgent:\n",
    "    def play_turn(self, game, state):\n",
    "        action = None\n",
    "        print('\\nAvailable actions:', game.actions(state))\n",
    "        if game.actions(state):\n",
    "            action_str = input('\\nYour action: ')\n",
    "            try:\n",
    "                action = eval(action_str)\n",
    "            except NameError:\n",
    "                action = action_str\n",
    "        else:\n",
    "            print('No legal actions remain!')\n",
    "        return action\n",
    "\n",
    "\"\"\"\n",
    "An agent that will randomly choose amongst the available actions.\n",
    "\"\"\"\n",
    "class RandomAgent:\n",
    "    def play_turn(self, game, state):\n",
    "        return random.choice(game.actions(state)) if game.actions(state) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a game and agents that can play it. To ensure you have a proper understanding of __Neutron__, let's have you play against a random agent. Run the code cell below to try your best to win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A | B | C | D | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Available actions: [[('A', 'DOWN')], [('B', 'DOWN')], [('C', 'DOWN')], [('D', 'DOWN')], [('E', 'DOWN')]]\n",
      "\n",
      "Your action: [('B', 'DOWN')]\n",
      "\n",
      "---------------------\n",
      "| A |   | C | D | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A |   | C | D | E |\n",
      "---------------------\n",
      "|   |   |   |   | 5 |\n",
      "---------------------\n",
      "| * |   |   |   |   |\n",
      "---------------------\n",
      "|   | B |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 |   |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Available actions: [[('*', 'UP'), ('A', 'RIGHT')], [('*', 'UP'), ('B', 'UP')], [('*', 'UP'), ('B', 'LEFT')], [('*', 'UP'), ('B', 'RIGHT')], [('*', 'UP'), ('C', 'DOWN')], [('*', 'UP'), ('C', 'LEFT')], [('*', 'UP'), ('D', 'DOWN')], [('*', 'DOWN'), ('A', 'DOWN')], [('*', 'DOWN'), ('A', 'RIGHT')], [('*', 'DOWN'), ('B', 'UP')], [('*', 'DOWN'), ('B', 'RIGHT')], [('*', 'DOWN'), ('C', 'DOWN')], [('*', 'DOWN'), ('C', 'LEFT')], [('*', 'DOWN'), ('D', 'DOWN')], [('*', 'RIGHT'), ('A', 'DOWN')], [('*', 'RIGHT'), ('A', 'RIGHT')], [('*', 'RIGHT'), ('B', 'UP')], [('*', 'RIGHT'), ('B', 'LEFT')], [('*', 'RIGHT'), ('B', 'RIGHT')], [('*', 'RIGHT'), ('C', 'DOWN')], [('*', 'RIGHT'), ('C', 'LEFT')], [('*', 'RIGHT'), ('D', 'DOWN')]]\n",
      "\n",
      "Your action: [('*', 'UP'), ('D', 'DOWN')]\n",
      "\n",
      "---------------------\n",
      "| A |   | C |   | E |\n",
      "---------------------\n",
      "| * |   |   |   | 5 |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   | B |   | D |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A |   | C |   | E |\n",
      "---------------------\n",
      "|   |   |   | * | 5 |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   | B |   | D |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 |   | 4 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Available actions: [[('*', 'UP'), ('A', 'DOWN')], [('*', 'UP'), ('A', 'RIGHT')], [('*', 'UP'), ('B', 'UP')], [('*', 'UP'), ('B', 'LEFT')], [('*', 'UP'), ('B', 'RIGHT')], [('*', 'UP'), ('C', 'DOWN')], [('*', 'UP'), ('C', 'LEFT')], [('*', 'UP'), ('D', 'UP')], [('*', 'UP'), ('D', 'DOWN')], [('*', 'UP'), ('D', 'LEFT')], [('*', 'UP'), ('D', 'RIGHT')], [('*', 'DOWN'), ('A', 'DOWN')], [('*', 'DOWN'), ('A', 'RIGHT')], [('*', 'DOWN'), ('B', 'UP')], [('*', 'DOWN'), ('B', 'LEFT')], [('*', 'DOWN'), ('B', 'RIGHT')], [('*', 'DOWN'), ('C', 'DOWN')], [('*', 'DOWN'), ('C', 'LEFT')], [('*', 'DOWN'), ('C', 'RIGHT')], [('*', 'DOWN'), ('D', 'DOWN')], [('*', 'DOWN'), ('D', 'LEFT')], [('*', 'DOWN'), ('D', 'RIGHT')], [('*', 'DOWN'), ('E', 'LEFT')], [('*', 'LEFT'), ('A', 'RIGHT')], [('*', 'LEFT'), ('B', 'UP')], [('*', 'LEFT'), ('B', 'LEFT')], [('*', 'LEFT'), ('B', 'RIGHT')], [('*', 'LEFT'), ('C', 'DOWN')], [('*', 'LEFT'), ('C', 'LEFT')], [('*', 'LEFT'), ('C', 'RIGHT')], [('*', 'LEFT'), ('D', 'UP')], [('*', 'LEFT'), ('D', 'DOWN')], [('*', 'LEFT'), ('D', 'LEFT')], [('*', 'LEFT'), ('D', 'RIGHT')], [('*', 'LEFT'), ('E', 'LEFT')]]\n",
      "\n",
      "Your action: [('*', 'UP'), ('A', 'DOWN')]\n",
      "\n",
      "---------------------\n",
      "|   |   | C | * | E |\n",
      "---------------------\n",
      "|   |   |   |   | 5 |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| A | B |   | D |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 |   | 4 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "|   |   | C | * | E |\n",
      "---------------------\n",
      "|   |   |   |   | 5 |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| A | B |   | D |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 |   | 4 |\n",
      "---------------------\n",
      "Player to move: Player 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = QueryAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just for fun, let's have 2 random agents go at it. Toggle the display flag (third parameter for `play_game`) to see the full sequence of states for the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A | B | C | D | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   |   |   | D |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "| * |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | D |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "| * |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | D | E |\n",
      "---------------------\n",
      "| 1 | 2 |   | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| * |   |   | D | E |\n",
      "---------------------\n",
      "| 1 | 2 | 4 |   | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | * | D |   |\n",
      "---------------------\n",
      "| 1 | 2 | 4 |   | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   | * |   |   |\n",
      "---------------------\n",
      "|   |   | 4 | D |   |\n",
      "---------------------\n",
      "| 1 | 2 |   |   | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "| * |   |   |   |   |\n",
      "---------------------\n",
      "|   |   | 4 |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   | E |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| * |   |   |   | 4 |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "| * |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | E |\n",
      "---------------------\n",
      "|   |   |   |   | 4 |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "|   |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | E |\n",
      "---------------------\n",
      "| * | 4 |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B |   |   | C |\n",
      "---------------------\n",
      "| * |   | 3 |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | E |\n",
      "---------------------\n",
      "|   | 4 |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B |   |   | C |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   |   |   |   | E |\n",
      "---------------------\n",
      "| * | 4 |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B |   |   | C |\n",
      "---------------------\n",
      "| * |   |   |   | 3 |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   | 4 |   |   | E |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B |   |   | C |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "| * | 4 |   |   | E |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B |   | D | C |\n",
      "---------------------\n",
      "| * |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   | 4 |   |   | E |\n",
      "---------------------\n",
      "| 1 | 2 |   |   | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B |   | D | C |\n",
      "---------------------\n",
      "|   |   |   |   | * |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "| 4 |   |   |   | E |\n",
      "---------------------\n",
      "| 1 | 2 |   |   | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B |   |   | C |\n",
      "---------------------\n",
      "| * |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "| 4 |   |   |   | E |\n",
      "---------------------\n",
      "| 1 | 2 |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B |   |   | C |\n",
      "---------------------\n",
      "|   | 2 |   |   | * |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "| 4 |   |   |   | E |\n",
      "---------------------\n",
      "| 1 |   |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "|   | 2 | * |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "| 4 |   |   |   | E |\n",
      "---------------------\n",
      "| 1 |   |   | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "| 4 | 2 |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   |   |   |   | E |\n",
      "---------------------\n",
      "| 1 |   | * | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| A | B | C |   |   |\n",
      "---------------------\n",
      "| 4 | 2 |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   |   |   |   | E |\n",
      "---------------------\n",
      "| 1 |   | * | D | 5 |\n",
      "---------------------\n",
      "Player to move: Player 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = RandomAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, True) # can toggle display flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Minimax and Maximin\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">20 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learned in lecture, the __minimax__ and __maximin__ search algorithms provide a way for an agent to search an adversarial search space. Specifically, player 1 (traditionally called _MAX_) wants to maximize their score while player 2 (similarly referred to as _MIN_) wishes to minimize such a score. If we envision the search as a tree, then each depth corresponds to different responses (actions) that the current player can make to the opponent's previous action. As we descend this tree, the current player alternates until the search reaches a terminal state.\n",
    "\n",
    "We can summarize this procedure as the following:\n",
    "\n",
    "$$\n",
    "\\mbox{Minimax}(s) =\n",
    "\\begin{cases} \n",
    "      \\mbox{Utility}(s) & \\mbox{if Terminal}(s) \\\\\n",
    "      argmax_{a\\ \\in \\ \\mbox{Actions}(s)} \\mbox{Maximin(Result}(s, a)) & \\mbox{if Player}(s) = \\mbox{MAX} \\\\\n",
    "      argmin_{a\\ \\in \\ \\mbox{Actions}(s)} \\mbox{Minimax(Result}(s, a)) & \\mbox{if Player}(s) = \\mbox{MIN}\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "Your first task in this assignment is to implement the pair of search algorithms. For easy reference, a modified pseudocode from lecture has been reproduced below. Note that, for this assigment, our algorithms must output two things: __a value and an action__. One handy feature about Python is that functions are allowed to return 2 outputs, so this works out. You can do this by using the syntax: `return val_1, val_2`. Lastly, remember that you __must__ also provide documentation for your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "minimax(game, state, cutoff, depth, eval_fn)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 1), empty_action\n",
    "    else\n",
    "        val = -∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = maximin(game, result(state, act), cutoff, depth + 1, eval_fn)\n",
    "            if val' > val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "    return val, best_act\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "maximin(game, state, cutoff, depth, eval_fn)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 2), empty_action\n",
    "    else\n",
    "        val = ∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = minimax(game, result(state, act), cutoff, depth + 1, eval_fn)\n",
    "            if val' < val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "    return val, best_act\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finds the best possible action and score of that actions for Player 1 \n",
    "at the current state and depth. \n",
    "Inputs:\n",
    "game[NeutronGame] - current game that is being played\n",
    "state [4-tuple] - represents current state of game - (board, player, is_initial, utility_value)\n",
    "cutoff [function] - determines when to stop search\n",
    "depth [int] - how many moves into the game\n",
    "eval_fn [function] - determines the value of an action\n",
    "Returns:\n",
    "best_act[action] - action that minimizes the terminal value\n",
    "val[number] - the value of the action returned\n",
    "\"\"\"\n",
    "def minimax(game, state, cutoff = None, depth = 0, eval_fn = None):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    #if reached cutoff, return the value of the current state\n",
    "    if cutoff(state,depth):\n",
    "        #print('reached cutoff')\n",
    "        #print('cutoff')\n",
    "        if depth == 0: print('cut wo act')\n",
    "        return eval_fn(state, state[1]), None\n",
    "        # evaluate cutoff state wrong\n",
    "    else:\n",
    "        #searches all possible actions, comparing each of their vals\n",
    "        #and keeping that with the highest val\n",
    "        val = -math.inf\n",
    "        best_act = None\n",
    "        for act in game.actions(state):\n",
    "            vall = minimax(game, game.result(state,act), cutoff, depth+1, eval_fn)[0]\n",
    "            if vall > val : \n",
    "                val = vall\n",
    "                best_act = act\n",
    "        #if best_act == None: print('no act')\n",
    "        return val, best_act\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Finds the best possible action and score of that actions for Player 2 \n",
    "at the current state and depth. \n",
    "Inputs:\n",
    "game[NeutronGame] - current game that is being played\n",
    "state [4-tuple] - represents current state of game - (board, player, is_initial, utility_value)\n",
    "cutoff [function] - determines when to stop search\n",
    "depth [int] - how many moves into the game\n",
    "eval_fn [function] - determines the value of an action\n",
    "Returns:\n",
    "best_act[action] - action that maximizes the terminal value\n",
    "val[number] - the value of the action returned\n",
    "\"\"\"\n",
    "def maximin(game, state, cutoff = None, depth = 0, eval_fn = None):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    #if reached cutoff, return the value of the current state\n",
    "    if cutoff(state,depth):\n",
    "        if depth == 0: print('cut wo act')\n",
    "        return eval_fn(state, state[1]), None\n",
    "    else:\n",
    "        #searches all possible actions, comparing each of their vals\n",
    "        #and keeping that with the lowest val\n",
    "        val = math.inf\n",
    "        best_act = None\n",
    "        for act in game.actions(state):\n",
    "            vall = maximin(game, game.result(state,act), cutoff, depth+1, eval_fn)[0]\n",
    "            if vall < val :\n",
    "                val = vall\n",
    "                best_act = act\n",
    "        #if best_act == None: print('no act')\n",
    "        return val, best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a working implementation of __minimax__ and __maximin__, we can create agents based around these algorithms. Both agents can take optional parameters but we will focus on these parameters later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that utilizes the minimax search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class MinimaxAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = minimax(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        #print('turn')\n",
    "        return best_act\n",
    "\n",
    "\"\"\"\n",
    "An agent that utilizes the maximin search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class MaximinAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = maximin(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        #print('turn')\n",
    "        return best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, we are going to start from a particular state partway through a game. This particular state was written so that the minimax agent will be able to playout until the end. Run the code cell below to check if your implementation is able to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A |   | B | C | D |\n",
      "---------------------\n",
      "| 1 | * | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "turn\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : BLANK, (1,3) : PLAYER_1[1], (1,4) : PLAYER_1[2], (1,5) : PLAYER_1[3],\n",
    "    (2,1) : PLAYER_2[0], (2,2) : NEUTRON, (2,3) : PLAYER_2[1], (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : PLAYER_1[4], (3,3) : BLANK, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : PLAYER_2[2],\n",
    "    (5,1) : BLANK, (5,2) : PLAYER_2[3], (5,3) : BLANK, (5,4) : PLAYER_2[4], (5,5) : BLANK,\n",
    "}\n",
    "custom_state = GameState(board = custom_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game         = NeutronGame(custom_state)\n",
    "p1           = MinimaxAgent()\n",
    "p2           = RandomAgent()\n",
    "game.play_game(p1, p2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to have our minimax agent play against a random agent starting from the beginning of the game. When you run the code cell below, you will find that our minimax agent is unable to process the state space search! In fact, __your Python kernel will crash__, because it exceeds the maximum recursion depth. The takeaway from this is that, unfortunately, a pure minimax approach is intractable for our __Neutron__ domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n",
      "reached cutoff\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bbed069c3b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp1\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mMinimaxAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp2\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1397c935315d>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self, first_player, second_player, display_flag, experiment_flag)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfirst_player\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mstate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdisplay_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3668abebb986>\u001b[0m in \u001b[0;36mplay_turn\u001b[0;34m(self, game, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'turn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbest_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0795f0852554>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(game, state, cutoff, depth, eval_fn)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mvall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvall\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-7-0795f0852554>\u001b[0m in \u001b[0;36mminimax\u001b[0;34m(game, state, cutoff, depth, eval_fn)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mvall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvall\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = MinimaxAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Alpha-Beta Pruning\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">20 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from part 1, while the minimax algorithm is a good start, this approach fails in our domain, due to the exponential number of states that it needs to explore. Recall from lecture that it is possible for us to improve the algorithm through the use of a pruning technique, called __alpha-beta pruning__. In particular, we use $\\alpha$ to refer to the value of the best choice (i.e. highest-value) so far for MAX, while $\\beta$ refers to the value of the best choice (i.e. lowest-value) so far for MIN. Alpha-beta search updates the values for $\\alpha$ and $\\beta$ as it goes along and prunes branches once it determines that the value of the state is worse than $\\alpha$ (for MAX) or $\\beta$ (for MIN).\n",
    "\n",
    "Your second task in this assignment is to implement the above behavior. Similar to how minimax is split into 2 functions (depending on the current player), we also split the alpha-beta pruning into 2 functions. For easy reference, modified pseudocode from lecture has been reproduced below. Use this to guide your implementation. Note, we are representing $\\alpha$ as the variable `a` below, and similarly, representing $\\beta$ as the variable `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alphabeta_minimax(game, state, cutoff, depth, eval_fn, a, b)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 1), empty_action\n",
    "    else\n",
    "        val = -∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = alphabeta_maximin(game, result(state, act), cutoff, depth + 1, eval_fn, a, b)\n",
    "            if val' > val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "            if val >= b then\n",
    "                return val, best_act\n",
    "            a = max(a, val)\n",
    "    return val, best_act\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "alphabeta_maximin(game, state, cutoff, depth, eval_fn, a, b)\n",
    "    if cutoff(state, depth) then\n",
    "        return eval_fn(state, player 2), empty_action\n",
    "    else\n",
    "        val = ∞\n",
    "        best_act = empty_action\n",
    "        foreach act in actions(state)\n",
    "            val' = alphabeta_minimax(game, result(state, act), cutoff, depth + 1, eval_fn, a, b)\n",
    "            if val' < val then\n",
    "                val = val'\n",
    "                best_act = act\n",
    "            if val <= a then\n",
    "                return val, best_act\n",
    "            b = min(b, val)\n",
    "    return val, best_act\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finds the best possible action and score of that actions for Player 1 \n",
    "at the current state and depth. \n",
    "\n",
    "Inputs:\n",
    "game[NeutronGame] - current game that is being played\n",
    "state [4-tuple] - represents current state of game - (board, player, is_initial, utility_value)\n",
    "cutoff [function] - determines when to stop search\n",
    "depth [int] - how many moves into the game\n",
    "eval_fn [function] - determines the value of an action\n",
    "a[number] - alpha value - of the actions searched thusfar, the best that minimax could\n",
    "                          do or the worst that maximin could do\n",
    "b[number] - beta value - of the actions searched, the worst that minimax could\n",
    "                          do or the best that maximin could do\n",
    "Returns:\n",
    "best_act[action] - action that minimizes the terminal value\n",
    "val[number] - the value of the action returned\n",
    "\n",
    "******* add other vars and update doc\n",
    "\"\"\"\n",
    "def alphabeta_minimax(game, state, cutoff = None, depth = 0, eval_fn = None, a = -math.inf, b = math.inf):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    #if reached cutoff, return the value of the current state\n",
    "    if cutoff(state,depth):\n",
    "        return eval_fn(state, state[1]), None\n",
    "    else:\n",
    "        #searches possible actions, comparing each of their vals\n",
    "        #and keeping that with the lowest val\n",
    "        val = -math.inf\n",
    "        best_act = None\n",
    "        for act in game.actions(state):\n",
    "            vall = alphabeta_minimax(game, game.result(state,act), cutoff, depth+1, eval_fn, a, b)[0]\n",
    "            if vall > val : \n",
    "                val = vall\n",
    "                best_act = act\n",
    "            if val >= b: #if the value is less than the best the other player can do then the search stops\n",
    "                return val, best_act\n",
    "            a = max(a, val)\n",
    "        return val, best_act\n",
    "\n",
    "\"\"\"\n",
    "Finds the best possible action and score of that actions for Player 2 \n",
    "at the current state and depth. \n",
    "\n",
    "Inputs:\n",
    "game[NeutronGame] - current game that is being played\n",
    "state [4-tuple] - represents current state of game - (board, player, is_initial, utility_value)\n",
    "cutoff [function] - determines when to stop search\n",
    "depth [int] - how many moves into the game\n",
    "eval_fn [function] - determines the value of an action\n",
    "a[number] - alpha value - of the actions searched thusfar, its val represents the best \n",
    "                          that minimax coulddo or the worst that maximin could do\n",
    "b[number] - beta value - of the actions searched thusfar, its val represents the worst that \n",
    "                          minimax could do or the best that maximin could do\n",
    "Returns:\n",
    "best_act[action] - action that minimizes the terminal value\n",
    "val[number] - the value of the action returned\n",
    "\n",
    "\"\"\"\n",
    "def alphabeta_maximin(game, state, cutoff = None, depth = 0, eval_fn = None, a = -math.inf, b = math.inf):\n",
    "    # default cutoff is just a test if the state is terminal\n",
    "    if cutoff == None:\n",
    "        cutoff = lambda s, d : game.terminal_test(s)\n",
    "    \n",
    "    # default eval is just the utility value\n",
    "    if eval_fn == None:\n",
    "        eval_fn = game.utility\n",
    "    \n",
    "    # replace the line below with your code\n",
    "    if cutoff(state,depth):\n",
    "        return eval_fn(state, state[1]), None\n",
    "    else:\n",
    "        #searches possible actions, comparing each of their vals\n",
    "        #and keeping that with the lowest val\n",
    "        val = math.inf\n",
    "        best_act = None\n",
    "        for act in game.actions(state):\n",
    "            vall = alphabeta_maximin(game, game.result(state,act), cutoff, depth+1, eval_fn, a, b)[0]\n",
    "            if vall < val : \n",
    "                val = vall\n",
    "                best_act = act\n",
    "            if val<= a: #if the value is less than the best the other player can do then the search stops\n",
    "                return val, best_act\n",
    "            b = min(b, val)\n",
    "    return val, best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a working implementation of __alphabeta_minimax__ and __alphabeta_maximin__, we can create agents based around these algorithms. Just like the minimax agents, these agents can take the optional parameters, but let's not worry about them just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An agent that utilizes the alpha-beta minimax search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class AlphabetaMinimaxAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = alphabeta_minimax(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        return best_act\n",
    "\n",
    "\"\"\"\n",
    "An agent that utilizes the alpha-beta maximin search algorithm. Can take two optional parameters.\n",
    "cutoff, that can be used to determine when to stop the search\n",
    "eval_fn, which evaluates a state's value from the perspective of a given player\n",
    "\"\"\"\n",
    "class AlphabetaMaximinAgent:\n",
    "    def __init__(self, cutoff = None, eval_fn = None):\n",
    "        self.cutoff  = cutoff\n",
    "        self.eval_fn = eval_fn\n",
    "\n",
    "    def play_turn(self, game, state):\n",
    "        _, best_act = alphabeta_maximin(game, state, cutoff = self.cutoff, eval_fn = self.eval_fn)\n",
    "        return best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, we are going to start from a particular state partway through a game. This particular state was written so that the alpha-beta agent will be able to playout until the end. Run the code cell below to check if your implementation is able to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------\n",
      "| A |   | B | C | D |\n",
      "---------------------\n",
      "| 1 | * | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 1\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| A | * | B |   | D |\n",
      "---------------------\n",
      "| 1 |   | 2 |   |   |\n",
      "---------------------\n",
      "|   | E |   |   |   |\n",
      "---------------------\n",
      "|   |   |   | C | 3 |\n",
      "---------------------\n",
      "|   | 4 |   | 5 |   |\n",
      "---------------------\n",
      "Player to move: Player 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : BLANK, (1,3) : PLAYER_1[1], (1,4) : PLAYER_1[2], (1,5) : PLAYER_1[3],\n",
    "    (2,1) : PLAYER_2[0], (2,2) : NEUTRON, (2,3) : PLAYER_2[1], (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : PLAYER_1[4], (3,3) : BLANK, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : PLAYER_2[2],\n",
    "    (5,1) : BLANK, (5,2) : PLAYER_2[3], (5,3) : BLANK, (5,4) : PLAYER_2[4], (5,5) : BLANK,\n",
    "}\n",
    "custom_state = GameState(board = custom_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game         = NeutronGame(custom_state)\n",
    "p1           = AlphabetaMinimaxAgent()\n",
    "p2           = RandomAgent()\n",
    "game.play_game(p1, p2, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to have our alpha-beta agent play against a random agent starting from the beginning of the game. When you run the code cell below, you will find that our alpha-beta agent is also unable to process the state space search, much like minimax. Once again, __your Python kernel will crash__. The takeaway from this is that even alpha-beta pruning does not help to make our __Neutron__ domain tractable for adversarial search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-a30fec1838ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp1\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mAlphabetaMinimaxAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp2\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1397c935315d>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(self, first_player, second_player, display_flag, experiment_flag)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfirst_player\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mstate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdisplay_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-fe3e2ab37beb>\u001b[0m in \u001b[0;36mplay_turn\u001b[0;34m(self, game, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphabeta_minimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbest_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-75fcc4ecc825>\u001b[0m in \u001b[0;36malphabeta_minimax\u001b[0;34m(game, state, cutoff, depth, eval_fn, a, b)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mvall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphabeta_minimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvall\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-55-75fcc4ecc825>\u001b[0m in \u001b[0;36malphabeta_minimax\u001b[0;34m(game, state, cutoff, depth, eval_fn, a, b)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mbest_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mvall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphabeta_minimax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvall\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "game = NeutronGame()\n",
    "p1   = AlphabetaMinimaxAgent()\n",
    "p2   = RandomAgent()\n",
    "game.play_game(p1, p2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Heuristics\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">40 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any way to salvage our adversarial search? It turns out, there is! We can cut off our search early and apply a heuristic function to evaluate states, effectively treating non-terminal states as if they were terminal. Of course, this begs the question, what makes a good cutoff test? And what makes a good evaluation function?\n",
    "\n",
    "This is your third task for this assignment. Come up with your own heuristics for the cutoff and evaluation functions so that our agents can finally play the game from start to finish. `custom_cutoff` should return a Boolean value and `custom_eval` should return either -1 or +1.\n",
    "\n",
    "For reference, here is some documentation on a game state:\n",
    "> A state for this game is a 4-tuple, (board, player, is_initial, utility_value).\n",
    "> - board, a dictionary with (i,j) coordinates as the key, and the piece as\n",
    "  the value. Valid coordinate values for i and j are 1 to 5, inclusive. The piece\n",
    "  should refer to one of the variables `PLAYER_1[n]`, `PLAYER_2[n]`, `NEUTRON`, or\n",
    "  `BLANK`, where `n` can have a value from 0 to 4, inclusive.\n",
    "> - player, refers to the player that needs to move in this state. Should reference\n",
    "  one of the variables `PLAYER_1` or `PLAYER_2`.\n",
    "> - is_initial, a Boolean value that determines whether or not this state is the\n",
    "  initial state of the game.\n",
    "> - utility_value, an integer value that represents the utility value for the\n",
    "  state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks if, at the current depth and state, the search should be cutoff\n",
    "Inputs:\n",
    "state [4-tuple] - represents current state of game - (board, player, is_initial, utility_value)\n",
    "depth [int>=0] - how many moves into the game\n",
    "Returns:\n",
    "boolean - whether search is cut off(True) or not(False)\n",
    "\"\"\"\n",
    "def custom_cutoff(state, depth):\n",
    "    \n",
    "    \n",
    "   \n",
    "    #dont cutoff it is initial state \n",
    "    if state.is_initial:\n",
    "        #print('cutoff 1')\n",
    "        return False \n",
    "    \n",
    "    #cutoff if the terminal_test is not 0 ie game is over\n",
    "    if state[3] !=  0: \n",
    "        #print('cutoff 2')\n",
    "        return True #do cutoff if terminal\n",
    "    if depth == 0: return False\n",
    "    \n",
    "    #cutoff if the value of the state is >0.8 or <0.8\n",
    "    if math.fabs(custom_eval(state, state[1])) >= 0.8:\n",
    "        #print('cutoff after eval')\n",
    "        return True\n",
    "    \n",
    "    #cutoff if looking more than 4 moves ahead\n",
    "    if depth>=4:\n",
    "        #print('cutoff 5')\n",
    "        return True \n",
    "    \n",
    "    \n",
    "    \n",
    "     #returns coordinates of piece(ie 'A', '*' ect.) on board\n",
    "    #def find_piece(board, piece):\n",
    "     #   i, j = None, None\n",
    "      #  for (m,n) in iter(board):\n",
    "       #     if board[(m,n)] == piece:\n",
    "        #        i, j = m, n\n",
    "         #       break\n",
    "        #print('cut off funct')\n",
    "        #return (i,j)\"\"\"\"\"\n",
    "    \n",
    "    \n",
    "    #if non of the above true - dont cutoff\n",
    "    return False\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Finds the value of the current state - how good or bad it is for the player\n",
    "Inputs:\n",
    "state [4-tuple] - represents current state of game - (board, player, is_initial, utility_value)\n",
    "player[either PLAYER_! or PLAYER_2] - the player whose turn it is\n",
    "Returns:\n",
    "val[number] - describes how good or how bad a state is - is a number from -1 to 1\n",
    "\"\"\"\n",
    "def custom_eval(state, player):\n",
    "    \n",
    "    #returns the number possible of ways for a piece at playerPosition to move (0-4)\n",
    "    def how_many_ways_to_move(playerPosition):\n",
    "        moveup = False\n",
    "        if playerPosition[1]>1: \n",
    "            if state.board[(playerPosition[0],playerPosition[1]-1)]==' ':\n",
    "                moveup=True\n",
    "        movedown = False\n",
    "        if playerPosition[1]<5:\n",
    "            if state.board[(playerPosition[0],playerPosition[1]+1)]==' ':\n",
    "                movedown = True\n",
    "        moveleft = False\n",
    "        if playerPosition[0]>1: \n",
    "            if state.board[(playerPosition[0]-1,playerPosition[1])]==' ':\n",
    "                moveleft = True\n",
    "        moveright = False\n",
    "        if playerPosition[0]<5: \n",
    "            if state.board[(playerPosition[0]+1,playerPosition[1])]==' ':\n",
    "                moveright = True\n",
    "        total = int(moveup)+int(movedown)+int(moveleft)+int(moveright)\n",
    "        return total\n",
    "    \n",
    "    #returns coordinates of piece(ie 'A', '*' ect.) on board\n",
    "    def find_piece(board, piece):\n",
    "        i, j = None, None\n",
    "        for (m,n) in iter(board):\n",
    "            if board[(m,n)] == piece:\n",
    "                i, j = m, n\n",
    "                break\n",
    "        return (i,j)\n",
    "    \n",
    "    #returns the number of blanks in Player1's home row\n",
    "    #def how_many_empties_home_row():\n",
    "     #   empties=0\n",
    "      #  for i in range(1,6):\n",
    "       #     if state.board[i,1]==BLANK: empties = empties + 1\n",
    "        #return empties\"\"\"\"\n",
    "    \n",
    "    if state.is_initial: return 0 #each player has equal chance in initial state\n",
    "    if state.utility_value != 0: return state.utility_value #if game over return val as -1 or 1 depending on who won\n",
    "\n",
    "    #checks if Player1 could win on the next move after this state - if so val=0.99\n",
    "    p1WinNext = False\n",
    "    for j in range(1,find_piece(state.board, '*')[1]) : \n",
    "        if state.board[(find_piece(state.board, '*')[0],j)]==' ':\n",
    "            p1WinNext = True\n",
    "    if ((state.player==PLAYER_1)  and p1WinNext): \n",
    "        return 0.99\n",
    "    \n",
    "    #checks if Player2 could win on the next move after this state - if so val=-0.99\n",
    "    p2WinNext = False\n",
    "    for j in range(find_piece(state.board, '*')[1],5):\n",
    "        if state.board[(find_piece(state.board, '*')[0],j)] == ' ': \n",
    "            p2WinNext = True\n",
    "    if state[1]==PLAYER_2 and p2WinNext : \n",
    "        return -0.99\n",
    "    \n",
    "    #finds score which is \n",
    "    #(num ways p1 could move - num ways p2 could move)/most posible ways to move\n",
    "    #thus higher scores are given when P1 has more available moves than P2\n",
    "    sumone = 0\n",
    "    sumtwo = 0\n",
    "    bestnummoves = 20 #max number of ways you could move all your player\n",
    "    score=0\n",
    "    for a in PLAYER_1:\n",
    "        sumone = sumone + how_many_ways_to_move(find_piece(state.board, a))\n",
    "    for b in PLAYER_2:\n",
    "        sumtwo = sumtwo + how_many_ways_to_move(find_piece(state.board, b))\n",
    "    score = (sumone - sumtwo)/bestnummoves\n",
    "    if score ==1 or score == -1: score = score*(0.9)\n",
    "    #print('end cut')\n",
    "   \n",
    "    \n",
    "    return score \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate your heuristics! First, we will try the minimax agent against the random agent. (Your kernel should not crash this time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| B | * | C | D | E |\n",
      "---------------------\n",
      "| A |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "| 1 | 2 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Time elapsed: 0.689476\n"
     ]
    }
   ],
   "source": [
    "#print('start')\n",
    "game  = NeutronGame()\n",
    "p1    = MinimaxAgent(custom_cutoff, custom_eval)\n",
    "p2    = RandomAgent()\n",
    "start = time.process_time()\n",
    "game.play_game(p1, p2, False)\n",
    "end = time.process_time()\n",
    "print('\\nTime elapsed:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will try the alpha-beta agent against the random agent. (Again, your kernel should not crash.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game is over!\n",
      "\n",
      "---------------------\n",
      "| B | * | C | D |   |\n",
      "---------------------\n",
      "| 2 |   |   |   |   |\n",
      "---------------------\n",
      "| A |   |   |   | E |\n",
      "---------------------\n",
      "|   |   |   |   |   |\n",
      "---------------------\n",
      "|   | 1 | 3 | 4 | 5 |\n",
      "---------------------\n",
      "Player to move: Player 2\n",
      "\n",
      "Time elapsed: 1.5562800000002426\n"
     ]
    }
   ],
   "source": [
    "game  = NeutronGame()\n",
    "p1    = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "p2    = RandomAgent()\n",
    "start = time.process_time()\n",
    "game.play_game(p1, p2, False)\n",
    "end = time.process_time()\n",
    "print('\\nTime elapsed:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Experiments & Questions\n",
    "\n",
    "<h3 style=\"text-align: right; margin-top: -1em;\">15 points</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, the game has been using a fixed move ordering of up, down, left, then right. However, it is possible that this move ordering affects the overall performance of our adversarial search. First, let's see how this move ordering affects the number of states generated. To do this, we will play the game between the alpha-beta (with heuristics) agent and the random agent. We will do this repeatedly and then generate a link to a graph of the data. Now, what if we change the move ordering? In the code cell below, provide your own ordering by changing the value of `custom_move_order`, then run the cell. __How did your ordering affect the number of states generated compared to the initial fixed ordering?__ Write your response in the raw text cell below the code cell.\n",
    "\n",
    "_Note_: Running the cell below will take some time.\n",
    "\n",
    "For reference, here is the list of moves as notated in the game logic and what they mean:\n",
    "+ (-1,0) is UP\n",
    "+ (1,0) is DOWN\n",
    "+ (0,-1) is LEFT\n",
    "+ (0,1) is RIGHT\n",
    "\n",
    "Please ensure your choice for `custom_move_order` is different from the initial fixed move order (up, down, left, right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This link is for the number of states generated using the initial move order:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B17650.8,22603.7,16206.3,19659.3,20031.9,11911.2,11818.9,13050.0,10332.0,32480.2%7D\n",
      "\n",
      "This link is for the number of states generated using your custom move order:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B14.0,14.0,14.0,14.0,14.0,14.0,14.0,14.0,14.0,14.0%7D\n"
     ]
    }
   ],
   "source": [
    "# provide your custom move order below\n",
    "# this should be a list of 4 tuples\n",
    "custom_move_order = [(1,0),(0,-1),(0,1),(0,-1)]\n",
    "\n",
    "# run trials for number of states generated using the initial move order\n",
    "num_trials = 10\n",
    "results    = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('This link is for the number of states generated using the initial move order:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated using the custom move order\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame(move_order = custom_move_order)\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the number of states generated using your custom move order:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The number of state generated decreased immensely. By switching up and down, and left and right, the number of generated states was only 14 for each of the trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a second experiment, let us take a look at the ordering of our agents. Thus far, we have always been placing our agents as the first player of the game. __What if we place our agent as the second player instead? Does it affect the win rate? How about the number of states generated during the search?__ Run the code cell below to run the experiment and generate links to plot the data. Analyze these plots and provide a short answer in the raw text cell below.\n",
    "\n",
    "_Note_: Running the cell below will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This link is for the win rate as first player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B0.8,0.75,0.6,0.65,0.7,0.85,0.85,0.9,0.8,0.7,0.75,0.85,0.9,0.9,0.7,0.9,0.75,0.9,0.75,0.9%7D\n",
      "\n",
      "This link is for the win rate as second player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B0.7,0.65,0.7,0.7,0.7,0.6,0.8,0.7,0.7,0.8,0.55,0.75,0.6,0.55,0.75,0.7,0.75,0.65,0.8,0.8%7D\n",
      "\n",
      "This link is for the number of states generated as first player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B15703.35,12074.7,22122.6,22762.05,22883.65,13062.2,19540.35,15008.95,24560.4,13956.1,11668.65,20966.75,12977.55,18758.55,10752.2,18483.7,16968.25,18834.75,13321.7,17964.05%7D\n",
      "\n",
      "This link is for the number of states generated as second player:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B22852.25,24147.35,19500.3,15626.55,30525.55,23383.4,14583.95,23872.95,21437.95,23289.9,21646.9,21564.85,26155.9,22020.3,27972.55,17128.05,25297.6,22780.65,17629.6,23008.95%7D\n"
     ]
    }
   ],
   "source": [
    "# run trials for win rate as first player\n",
    "num_trials = 20\n",
    "results    = []\n",
    "for m in range(num_trials):\n",
    "    win = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        v    = game.play_game(p1, p2, False, True)\n",
    "        if v == (1,2) or v == (-1,1):\n",
    "            win += 1.0\n",
    "    results.append(win / num_trials)\n",
    "print('This link is for the win rate as first player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for win rate as second player\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    win = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = RandomAgent()\n",
    "        p2   = AlphabetaMaximinAgent(custom_cutoff, custom_eval)\n",
    "        v    = game.play_game(p1, p2, False, True)\n",
    "        if v == (1,1) or v == (-1,2):\n",
    "            win += 1.0\n",
    "    results.append(win / num_trials)\n",
    "print('\\nThis link is for the win rate as second player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated as first player\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(custom_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the number of states generated as first player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated as second player\n",
    "results = []\n",
    "for m in range(num_trials):\n",
    "    avg = 0.0\n",
    "    for n in range(num_trials):\n",
    "        game = NeutronGame()\n",
    "        p1   = RandomAgent()\n",
    "        p2   = AlphabetaMaximinAgent(custom_cutoff, custom_eval)\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the number of states generated as second player:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Placing the agent as the second player did slightly lower the win rate. As the first agent, the average win rate for all the trials was 0.76 while as the second agent, the average win for all the trials rate was 0.7. The average number of states generated in all the trials increased from 17118.5 states to 22221.3 states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one final experiment, let's look at __how the depth limit as the cutoff test can affect our search__. Run the code cell below, analyze the data, and provide your answer in the raw text cell.\n",
    "\n",
    "_Note_: Running the cell below will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "This link is for the average win rate as the depth limit increases:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B0.8,0.9,0.9,0.8,0.4%7D\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "This link is for the average number of states generated as the depth limit increases:\n",
      "https://www.wolframalpha.com/input/?i=plot+%7B138.1,814.1,7390.5,189415.0,2103275.1%7D\n"
     ]
    }
   ],
   "source": [
    "# run trials for win rate\n",
    "max_depth  = 5\n",
    "results    = []\n",
    "num_trials = 10\n",
    "for depth in range(max_depth):\n",
    "    print(depth)\n",
    "    exp_cutoff = lambda s, d : d >= depth + 1\n",
    "    win        = 0.0\n",
    "    for n in range(num_trials):\n",
    "        #print(n)\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(exp_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        v    = game.play_game(p1, p2, False, True)\n",
    "        if v == (1,2) or v == (-1,1):\n",
    "            win += 1.0\n",
    "    results.append(win / num_trials)\n",
    "print('This link is for the average win rate as the depth limit increases:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')\n",
    "\n",
    "# run trials for number of states generated\n",
    "results = []\n",
    "for depth in range(max_depth):\n",
    "    print(depth)\n",
    "    exp_cutoff = lambda s, d : d >= depth + 1\n",
    "    avg        = 0.0\n",
    "    for n in range(num_trials):\n",
    "        #print(n)\n",
    "        game = NeutronGame()\n",
    "        p1   = AlphabetaMinimaxAgent(exp_cutoff, custom_eval)\n",
    "        p2   = RandomAgent()\n",
    "        game.play_game(p1, p2, False, True)\n",
    "        avg += game.num_states_gen\n",
    "    results.append(avg / num_trials)\n",
    "print('\\nThis link is for the average number of states generated as the depth limit increases:')\n",
    "s = 'https://www.wolframalpha.com/input/?i=plot+%7B'\n",
    "for r in results:\n",
    "    s += str(r) + ','\n",
    "print(s[:-1] + '%7D')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The depth limit cutoff returned a win rate>0.5 when depth was <=4 but at depth=5, the rate dropped below 0.5. Also, the average number of states generated grew approximately by a factor of 10 for each increase in depth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Just for Fun\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to close this assignment, you are now tasked with creating 2 additional game states. For the first game state, our pure minimax agent (meaning no heuristics) should be able to playout as the first player. For the second game state, the pure alpha-beta agent should be able to playout as the first player, but the pure minimax agent should not be able to. Good luck! :)\n",
    "\n",
    "__Do not use the game state from earlier in the assignment!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimax_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : PLAYER_1[1], (1,3) : PLAYER_1[2], (1,4) : PLAYER_1[3], (1,5) : PLAYER_1[4],\n",
    "    (2,1) : BLANK, (2,2) : BLANK, (2,3) : BLANK, (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : BLANK, (3,3) : NEUTRON, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : BLANK,\n",
    "    (5,1) : PLAYER_2[0], (5,2) : PLAYER_2[1], (5,3) : PLAYER_2[2], (5,4) : PLAYER_2[3], (5,5) : PLAYER_2[4]\n",
    "}\n",
    "minimax_state = GameState(board = minimax_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game          = NeutronGame(minimax_state)\n",
    "p1            = MinimaxAgent()\n",
    "p2            = RandomAgent()\n",
    "v             = game.play_game(p1, p2, False)\n",
    "if v == (1,2) or v == (-1,1):\n",
    "    print('Congrats! The minimax agent was able to win!')\n",
    "\n",
    "alphabeta_board = {\n",
    "    (1,1) : PLAYER_1[0], (1,2) : PLAYER_1[1], (1,3) : PLAYER_1[2], (1,4) : PLAYER_1[3], (1,5) : PLAYER_1[4],\n",
    "    (2,1) : BLANK, (2,2) : BLANK, (2,3) : BLANK, (2,4) : BLANK, (2,5) : BLANK,\n",
    "    (3,1) : BLANK, (3,2) : BLANK, (3,3) : NEUTRON, (3,4) : BLANK, (3,5) : BLANK,\n",
    "    (4,1) : BLANK, (4,2) : BLANK, (4,3) : BLANK, (4,4) : BLANK, (4,5) : BLANK,\n",
    "    (5,1) : PLAYER_2[0], (5,2) : PLAYER_2[1], (5,3) : PLAYER_2[2], (5,4) : PLAYER_2[3], (5,5) : PLAYER_2[4]\n",
    "}\n",
    "alphabeta_state = GameState(board = alphabeta_board, player = PLAYER_1, is_initial = False, utility_value = 0)\n",
    "game            = NeutronGame(alphabeta_state)\n",
    "p1              = AlphabetaMinimaxAgent()\n",
    "p2              = RandomAgent()\n",
    "v               = game.play_game(p1, p2, False)\n",
    "if v == (1,2) or v == (-1,1):\n",
    "    print('Congrats! The alpha-beta agent was able to win!')\n",
    "    print('But you make sure that your minimax agent cannot [as in, it should kill the Python kernel].')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will only be submitting your Jupyter notebook file, *hw3.ipynb*. Do not worry about submitting the additional *game_tree.png* file. Furthermore, as a reminder, part of your grade is your documentation. Each of the functions you implemented as part of this assignment **must** be documented.\n",
    "\n",
    "Please upload your *hw3.ipynb* file to CMS by **Monday, March 4 @ 1:24pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
