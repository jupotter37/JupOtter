{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlX3Yhiq5Enc",
        "outputId": "7ec618af-5fb0-48a1-b070-ffa97023585b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "!pip install livelossplot\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "# If on drive retrieve the images\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = '/content/drive/MyDrive/Fiber Finder/generative/images.zip'\n",
        "    local_path = '/content/'\n",
        "    !cp -r \"$data_path\" \"$local_path\"\n",
        "    !unzip \"/content/images.zip\" -d \"/content\" > /dev/null 2>&1\n",
        "    print('Data recieved and unzipped!')\n",
        "\n",
        "except Exception as e:\n",
        "    print('No Drive or Error:', e)\n",
        "\n",
        "data_path = './images'\n",
        "data_path = '../images'\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vliPBlh5Enf"
      },
      "outputs": [],
      "source": [
        "class FiberDataset(Dataset):\n",
        "    \"\"\"Creates a dataset from an image file.\"\"\"\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.files = os.listdir(image_dir)\n",
        "        self.image_paths = []\n",
        "\n",
        "        self.labels = {}\n",
        "        self.false = []\n",
        "\n",
        "        self._parse_files()\n",
        "\n",
        "    def _parse_files(self):\n",
        "        for file_name in self.files:\n",
        "            file_name = os.path.join(self.image_dir, file_name)\n",
        "            if file_name.endswith(\".png\"):\n",
        "                self.image_paths.append(file_name)\n",
        "            elif file_name.endswith(\".csv\"):\n",
        "                labels = pd.read_csv(file_name, header=None)[0]\n",
        "                labels.index += 1\n",
        "                labels.index += 2000\n",
        "                labels = dict(labels)\n",
        "                self.labels = labels\n",
        "            else:\n",
        "                self.false.append(file_name)\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.image_paths[idx]\n",
        "        image_no = int(file_name.split(\"/\")[-1].replace(\".png\", \"\"))\n",
        "        image = Image.open(file_name).convert('L')\n",
        "\n",
        "        label = self.labels[image_no]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "data_path_bio = data_path + \"/bio/\"\n",
        "data_path_bio = data_path + \"/diffusion/diffusion_voxels/\"\n",
        "\n",
        "dataset = FiberDataset(data_path_bio, transform=transform)\n",
        "exp_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "N3sIWy0g5Eng",
        "outputId": "d2fc8961-2cac-4dcd-fc01-da983aec3deb"
      },
      "outputs": [],
      "source": [
        "def show_images_grid(images, title=None, nrow=8):\n",
        "    \"\"\"Plots the images in a grid\"\"\"\n",
        "    img_grid = make_grid(images, nrow=nrow).numpy()\n",
        "    plt.imshow(np.transpose(img_grid, (1, 2, 0)))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# DataLoader for the dataset\n",
        "exp_loader = DataLoader(dataset, shuffle=True)\n",
        "\n",
        "# Thresholds\n",
        "low_threshold = 0.4\n",
        "high_threshold = 0.65\n",
        "\n",
        "# Lists to store samples\n",
        "very_low_mean_samples = []\n",
        "medium_mean_samples = []\n",
        "high_mean_samples = []\n",
        "\n",
        "for images, labels in exp_loader:\n",
        "    for img in images:\n",
        "        mean_intensity = img.mean()\n",
        "        if mean_intensity <= low_threshold and len(very_low_mean_samples) < 32:\n",
        "            very_low_mean_samples.append(img)\n",
        "        elif low_threshold < mean_intensity <= high_threshold and len(medium_mean_samples) < 32:\n",
        "            medium_mean_samples.append(img)\n",
        "        elif mean_intensity > high_threshold and len(high_mean_samples) < 32:\n",
        "            high_mean_samples.append(img)\n",
        "\n",
        "    if len(very_low_mean_samples) >= 32 and len(medium_mean_samples) >= 32 and len(high_mean_samples) >= 32:\n",
        "        break\n",
        "\n",
        "show_images_grid(very_low_mean_samples, \"Very Low Mean Pixel Intensity Images\")\n",
        "show_images_grid(medium_mean_samples, \"Medium Mean Pixel Intensity Images\")\n",
        "show_images_grid(high_mean_samples, \"High Mean Pixel Intensity Images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "2SHq5gTm5Eng",
        "outputId": "4bc1380e-bde3-4695-a073-2e67603e3ca3"
      },
      "outputs": [],
      "source": [
        "class FilteredDataset(Dataset):\n",
        "    \"\"\"Creates a filtered set from the standard dataset using tresholds.\"\"\"\n",
        "    def __init__(self, original_dataset, low_threshold=0., high_threshold=1, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = [(img, label) for img, label in original_dataset if low_threshold < img.mean() <= high_threshold]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx][0]\n",
        "        label = self.data[idx][1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "dataset = FiberDataset(data_path_bio, transform=transform)\n",
        "filtered_dataset = FilteredDataset(dataset, low_threshold=.1, high_threshold=.65)\n",
        "filtered_loader = DataLoader(filtered_dataset, batch_size=1, shuffle=False)\n",
        "filtered_samples = []\n",
        "\n",
        "for i, data in enumerate(filtered_dataset):\n",
        "    image = data[0]\n",
        "    filtered_samples.append(image)\n",
        "    if i >= 31:\n",
        "        break\n",
        "\n",
        "show_images_grid(filtered_samples, \"Sample Images from Filtered Dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IbaVbf5Enh"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.RandomVerticalFlip()])\n",
        "\n",
        "dataset = FiberDataset(data_path_bio, transform=transform)\n",
        "filtered_dataset = FilteredDataset(dataset, low_threshold=.1, high_threshold=.65)\n",
        "loader = DataLoader(filtered_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "GE3RZCro5Enh"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        # Define a simple U-Net architecture\n",
        "        # You should replace this with a more complex and effective U-Net design\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, in_channels, num_diffusion_steps):\n",
        "        super(DiffusionModel, self).__init__()\n",
        "        self.unet = UNet(1, in_channels)\n",
        "        self.num_diffusion_steps = num_diffusion_steps\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # x: Input data (e.g., an image)\n",
        "        # t: Time step in the diffusion process\n",
        "        noise = torch.randn_like(x)\n",
        "        noisy_data = self.q_sample(x, t, noise)\n",
        "        predicted_noise = self.unet(noisy_data)\n",
        "        return predicted_noise\n",
        "\n",
        "    def q_sample(model, x, t, noise):\n",
        "        # Apply noise\n",
        "        noisy_x = x + noise\n",
        "        # Pass through the model\n",
        "        predicted_noise = model(noisy_x, t)\n",
        "        return noisy_x, predicted_noise\n",
        "\n",
        "    def p_sample(self, x_t, t, predicted_noise):\n",
        "        # Denoising step using predicted noise\n",
        "        # This should be defined according to your noise schedule and model design.\n",
        "        return x_t - predicted_noise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "4m5xhJUS5Enh",
        "outputId": "a9037509-1171-4034-99ba-3ef77c5888b3"
      },
      "outputs": [
        {
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m\n\u001b[1;32m     56\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./DiffusionModel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 57\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[50], line 24\u001b[0m, in \u001b[0;36mtrain_diffusion_model\u001b[0;34m(model, data_loader, epochs, model_path, save)\u001b[0m\n\u001b[1;32m     22\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(real_data)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Call q_sample with both real_data, noise, and time step t\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m noisy_data \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m model(noisy_data, t)\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "Cell \u001b[0;32mIn[49], line 43\u001b[0m, in \u001b[0;36mDiffusionModel.q_sample\u001b[0;34m(model, x, t, noise)\u001b[0m\n\u001b[1;32m     41\u001b[0m noisy_x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m noise\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Pass through the model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m noisy_x, predicted_noise\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[49], line 35\u001b[0m, in \u001b[0;36mDiffusionModel.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# x: Input data (e.g., an image)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# t: Time step in the diffusion process\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x)\n\u001b[0;32m---> 35\u001b[0m     noisy_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(noisy_data)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_noise\n",
            "Cell \u001b[0;32mIn[49], line 43\u001b[0m, in \u001b[0;36mDiffusionModel.q_sample\u001b[0;34m(model, x, t, noise)\u001b[0m\n\u001b[1;32m     41\u001b[0m noisy_x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m noise\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Pass through the model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m noisy_x, predicted_noise\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[49], line 35\u001b[0m, in \u001b[0;36mDiffusionModel.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# x: Input data (e.g., an image)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# t: Time step in the diffusion process\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x)\n\u001b[0;32m---> 35\u001b[0m     noisy_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(noisy_data)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_noise\n",
            "    \u001b[0;31m[... skipping similar frames: DiffusionModel.q_sample at line 43 (592 times), Module._call_impl at line 1527 (591 times), Module._wrapped_call_impl at line 1518 (591 times), DiffusionModel.forward at line 35 (591 times)]\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[49], line 35\u001b[0m, in \u001b[0;36mDiffusionModel.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# x: Input data (e.g., an image)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# t: Time step in the diffusion process\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x)\n\u001b[0;32m---> 35\u001b[0m     noisy_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(noisy_data)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_noise\n",
            "Cell \u001b[0;32mIn[49], line 43\u001b[0m, in \u001b[0;36mDiffusionModel.q_sample\u001b[0;34m(model, x, t, noise)\u001b[0m\n\u001b[1;32m     41\u001b[0m noisy_x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m noise\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Pass through the model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m noisy_x, predicted_noise\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ],
      "source": [
        "def train_diffusion_model(model, data_loader, epochs, model_path, save=False):\n",
        "    # Optimizer\n",
        "    lr = 1e-5\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Loss function (modify according to your diffusion model's requirement)\n",
        "    loss_function = nn.MSELoss()\n",
        "\n",
        "    groups = {'Loss': ['total_loss']}\n",
        "    liveloss = PlotLosses(groups=groups)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        logs = {}\n",
        "        total_loss = 0\n",
        "\n",
        "        for real_data, _ in data_loader:\n",
        "            batch_size = real_data.size(0)\n",
        "            real_data = real_data.to(device)\n",
        "\n",
        "            # Assuming your diffusion model has a method to generate noise and denoise\n",
        "            t = torch.randint(0, model.num_diffusion_steps, (batch_size, 1)).to(device)\n",
        "            noise = torch.randn_like(real_data)\n",
        "            # Call q_sample with both real_data, noise, and time step t\n",
        "            noisy_data = model.q_sample(real_data, t, noise)\n",
        "            predicted_noise = model(noisy_data, t)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calculate loss - example given for an MSE loss\n",
        "            loss = loss_function(predicted_noise, noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Average losses for current epoch\n",
        "        logs['total_loss'] = total_loss / len(data_loader)\n",
        "\n",
        "        # Update livelossplot\n",
        "        liveloss.update(logs)\n",
        "        liveloss.send()\n",
        "\n",
        "        # Save model parameters every 'save_interval' epochs\n",
        "        if (epoch + 1) % 10 == 0 and save:\n",
        "            model_path = os.path.join(model_path, f'diffusion_model_epoch_{epoch + 1}.pth')\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "in_channels = 50 * 50\n",
        "num_diffusion_steps = 1000\n",
        "model = DiffusionModel(in_channels, num_diffusion_steps).to(device)\n",
        "\n",
        "# Training the Diffusion Model\n",
        "epochs = 1500\n",
        "model_path = './DiffusionModel'\n",
        "trained_model = train_diffusion_model(model, loader, epochs, model_path, save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "hqhgAQYF5Eni",
        "outputId": "170f1e36-7b00-4ad6-ebdd-c5832b5bef7e"
      },
      "outputs": [],
      "source": [
        "def plot_samples(model, num_samples=32, nrow=8, save=False, model_type='VAE'):\n",
        "    save_dir=f'./{model_type}_fiber'\n",
        "    # Generate random latent vectors\n",
        "    z = torch.randn(num_samples, model.fc[0].in_features).to(device)\n",
        "\n",
        "    # Decode the latent vectors\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        imgs = model(z).view(-1, 1, 50, 50).to('cpu')\n",
        "    # Create a grid of images and display\n",
        "    img_grid = make_grid(imgs, nrow=nrow)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(np.transpose(img_grid.numpy(), (1, 2, 0)), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the images if requested\n",
        "    if save:\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        for i, img in enumerate(imgs):\n",
        "            fname = f'{str(i+1).zfill(3)}.jpeg'\n",
        "            img_np = img.squeeze().numpy()\n",
        "            if img_np.max() <= 1:\n",
        "                img_np = (img_np * 255).astype(np.uint8)\n",
        "            img_pil = Image.fromarray(img_np)\n",
        "            img_pil.save(os.path.join(save_dir, fname))\n",
        "\n",
        "trained_generator = gan[0]\n",
        "\n",
        "plot_samples(trained_generator, num_samples=32, nrow=8, save=False, model_type='GAN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVzjzTCL5Enj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
