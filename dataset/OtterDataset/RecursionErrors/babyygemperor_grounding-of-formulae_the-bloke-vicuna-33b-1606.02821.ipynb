{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e50065-dd79-48b6-9bc3-87eb6522a15f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting auto-gptq\n",
      "  Downloading auto_gptq-0.3.2.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/1b/79/5a3a7d877a9b0a72f528e9977ec65cdb9fad800fa4f5110f87f2acaaf6fe/auto_gptq-0.3.2.tar.gz has inconsistent version: expected '0.3.2', but metadata has '0.3.2+cu118'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.1.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz (from https://pypi.org/simple/auto-gptq/) (requires-python:>=3.8.0)\u001b[0m: \u001b[33mRequested auto-gptq from https://files.pythonhosted.org/packages/3f/5c/28d57f833498e014e9d066ea0199a503b5e59d4c8e8e701a460223e143da/auto_gptq-0.3.1.tar.gz has inconsistent version: expected '0.3.1', but metadata has '0.3.1+cu1180'\u001b[0m\n",
      "  Downloading auto_gptq-0.3.0.tar.gz (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.19.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from auto-gptq)\n",
      "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.0.1+cu118)\n",
      "Collecting peft (from auto-gptq)\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->auto-gptq) (5.9.5)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->auto-gptq) (15.0.7)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->auto-gptq)\n",
      "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->auto-gptq)\n",
      "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->auto-gptq)\n",
      "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m225.7/225.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.2.1)\n",
      "Building wheels for collected packages: auto-gptq\n",
      "  Building wheel for auto-gptq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for auto-gptq: filename=auto_gptq-0.3.0-cp310-cp310-linux_x86_64.whl size=5664438 sha256=4e7e987e449022a25a112145c129d74293fa0b9cbf5edac908016835d4ec60e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/ca/da/632464db8c2071b514d3b659652383c789f53cc2fc917737bb\n",
      "Successfully built auto-gptq\n",
      "Installing collected packages: tokenizers, safetensors, pytz, xxhash, unidecode, tzdata, tqdm, rouge, regex, pyarrow, multidict, fsspec, frozenlist, einops, dill, async-timeout, yarl, tiktoken, pandas, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, accelerate, peft, auto-gptq\n",
      "Successfully installed accelerate-0.21.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 auto-gptq-0.3.0 datasets-2.14.3 dill-0.3.7 einops-0.6.1 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 multidict-6.0.4 multiprocess-0.70.15 pandas-2.0.3 peft-0.4.0 pyarrow-12.0.1 pytz-2023.3 regex-2023.6.3 rouge-1.0.1 safetensors-0.3.1 tiktoken-0.4.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.31.0 tzdata-2023.3 unidecode-1.3.6 xxhash-3.3.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode tiktoken transformers einops auto-gptq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5a187-7c86-4ea4-863a-55817112cfb5",
   "metadata": {},
   "source": [
    "## Time to install packages 4 mins 0 seconds\n",
    "## Time started: 23:59:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4255ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import itertools\n",
    "import tiktoken\n",
    "import ast\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9777c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--Vicuna-33B-1-3-SuperHOT-8K-GPTQ/snapshots/6bd46c2caeac69d4329b48f66bbd05f8546998a8/vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\"\n",
    "model_basename = \"vicuna-33b-1.3-superhot-8k-GPTQ-4bit--1g.act.order\"\n",
    "model_name = 'vicuna-33b'\n",
    "file_code = '1606.02821'\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "model.seqlen = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f3d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(prompt_template, model=\"gpt-3.5-turbo\"):\n",
    "    return len(tokenizer(prompt_template, return_tensors='pt').input_ids.cuda().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_messages(text):\n",
    "    # This is a placeholder for your actual implementation\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def split_into_chunks(text, max_tokens=2000):\n",
    "    messages = split_into_messages(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "    for message in messages:\n",
    "        message_tokens = num_tokens_from_messages(message, model=\"gpt-3.5-turbo\")\n",
    "        if current_tokens + message_tokens > max_tokens:\n",
    "            # If adding this message would exceed the max tokens, start a new chunk\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "            current_chunk = [message]\n",
    "            current_tokens = message_tokens\n",
    "        else:\n",
    "            # Otherwise, add the message to the current chunk\n",
    "            current_chunk.append(message)\n",
    "            current_tokens += message_tokens\n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e285df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_string(input_str):\n",
    "    if \"</s>\" in input_str:\n",
    "        return input_str\n",
    "    else:\n",
    "        last_comma_index = input_str.rfind(',')\n",
    "        if last_comma_index == -1:\n",
    "            return input_str  # No comma found, return the string as is\n",
    "        else:\n",
    "            return input_str[:last_comma_index] + '}' + input_str[last_comma_index+1:]\n",
    "        \n",
    "def get_json_of_string(incorrect, pattern=r'\\{[^\\}]*\\}'):\n",
    "    match = re.search(r'{(.*)}', incorrect, re.DOTALL)\n",
    "    if match:\n",
    "        return \"{\" + match.group(1).replace('{', '[').replace('}', ']') + \"}\"\n",
    "    else:\n",
    "        return \"{}\"\n",
    "\n",
    "\n",
    "def string_to_dict(my_string):\n",
    "    # Load the JSON string into a list of tuples\n",
    "    tuples_list = json.JSONDecoder(object_pairs_hook=list).decode(my_string)\n",
    "\n",
    "    # Create a new dictionary to hold the final result\n",
    "    final_dict = {}\n",
    "\n",
    "    # Iterate over the list of tuples\n",
    "    for key, value in tuples_list:\n",
    "        # If the key is already in the final dictionary, append the value\n",
    "        # to the list of values for that key\n",
    "        if key in final_dict:\n",
    "            # Ensure the value is in a list form\n",
    "            if not isinstance(final_dict[key], list):\n",
    "                final_dict[key] = [final_dict[key]]\n",
    "            final_dict[key].append(value)\n",
    "        else:\n",
    "            # If the key is not in the final dictionary, add it with the value\n",
    "            final_dict[key] = value\n",
    "\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c1858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dictionaries(dict1, dict2):\n",
    "    union_dict = dict1.copy()\n",
    "\n",
    "    for key, value in dict2.items():\n",
    "        if key in union_dict:\n",
    "            if isinstance(union_dict[key], list):\n",
    "                if value not in union_dict[key]:\n",
    "                    union_dict[key].append(value)\n",
    "            else:\n",
    "                if union_dict[key] != value:\n",
    "                    union_dict[key] = [union_dict[key], value]\n",
    "        else:\n",
    "            union_dict[key] = value\n",
    "\n",
    "    return union_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff36d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path, clean=True):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    if clean:\n",
    "        matches = re.findall(r'<mi(.*?)</mi>', texts)\n",
    "        for match in matches:\n",
    "            original_string = f'<mi{match}</mi>'\n",
    "            replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'<|\\1|>', original_string)\n",
    "            texts = texts.replace(original_string, replaced_string)\n",
    "\n",
    "    return texts\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb9f95a7-690d-49a1-947e-2d1c107a5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "USER: {prompt[3]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0568d3-d765-4c53-ac27-c67650dad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_pattern(input_string):\n",
    "    pattern = r\"<\\|[^<\\|>]*\\|>\"\n",
    "    if re.search(pattern, input_string):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f85cb-e25a-4a21-81a4-c8c143a47c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(output):\n",
    "    pos = output.index('Do not include the angle brackets in the dictionary')\n",
    "    end_of_string = output[pos:]\n",
    "    print(end_of_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf7ee5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1250 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "text = page\n",
    "chunks = split_into_chunks(text, max_tokens=512)\n",
    "i = 0\n",
    "\n",
    "while not contains_pattern(chunks[i]):\n",
    "    i += 1\n",
    "print(i)\n",
    "\n",
    "question = chunks[i]\n",
    "\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "\n",
    "prompt = [\n",
    "        {'role': 'system',\n",
    "         'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                    'dictionary. The goal is to identify and classify each individual mathematical symbol, variable,'\n",
    "                    ' and identifier in the text marked between \"<||>\". The dictionary should store the identifiers as '\n",
    "                    'keys and their corresponding definitions as values in an array format. '},\n",
    "        {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>‚Ä≤=(<|X|>,<|R|>,\n",
    "        <|v|>), where <|X|> is a set of states, <|R|><|‚äÜ|><|X|><|√ó|><|X|> is a binary relation on <|X|>, \n",
    "        and <|v|>:<|ùñØùóãùóàùóâ|>‚Üí2<|X|> is a valuation. Given a relational model <|M|>‚Ä≤, the satisfaction relation between \n",
    "        points <|x<|‚àà<|X<| and formulas <|œÜ<|‚àà<|‚Ñí<|<|ùñ™ùñ†<| is defined inductively by <|M|>‚Ä≤,<|x|>‚ä®<|ùñ™|><|œÜ|>‚áî for all \n",
    "        <|y|>‚àà<|X|>,<|x|><|R|><|y|> implies <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|><|M|>‚Ä≤,<|x|>‚ä®<|ùñ†|><|œÜ|><||>‚áî for all <|y|>‚àà<|X|>,\n",
    "        <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|>'''},\n",
    "        {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ùñØùóãùóàùóâ\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"œÜ\": \"Formula in ùñ™ùñ†\",\n",
    "            \"‚Ñí_{ùñ™ùñ†}\": \"Set of formulas\",\n",
    "            \"ùñ™\": \"Modal operator K\",\n",
    "            \"ùñ†\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"‚ä®\": \"Satisfaction relation\",\n",
    "            \"‚áî\": \"If and only if operator\",\n",
    "            \"‚àà\": \"Element of a set\",\n",
    "            \"‚äÜ\": \"Subset of a set\",\n",
    "            \"√ó\": \"Cartesian product operator\",\n",
    "            \"‚Üí\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "        {'role': 'user', 'content': f'Generate a JSON dictionary for the following text\\n```txt\\n{question}```. '\n",
    "                                    'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                    'Do not consider any other identifier other than those marked. Consider all the '\n",
    "                                    'identifiers individually. Do not skip any identifier, mention all the identifiers '\n",
    "                                    'inside \"<||>\" in your dictionary. Do not include the angle brackets in the '\n",
    "                                    'dictionary.'}\n",
    "    ]\n",
    "\n",
    "\n",
    "open_prompt = get_prompt(prompt)\n",
    "\n",
    "prompt_size = num_tokens_from_messages(open_prompt)\n",
    "print(f\"{prompt_size} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "612640fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ\n",
      "Do not include the angle brackets in the dictionary.\n",
      "ASSISTANT:  identifiers = {\n",
      "\"nouns\": [\"nouns\", \"Nouns\", \"verbs\": \"Verbs\"],\n",
      "\"cultural shifts\": \"Cultural shifts\",\n",
      "\"linguistic shifts\": \"Linguistic shifts\",\n",
      "\"Gentner and France, 1988\": \"Gentner, T. & France, R. 1988.\",\n",
      "\"Traugott and Dasher, 2001\": \"Traugott, E. & Dasher, B. 2001.\",\n",
      "\"nominal domain\": \"Nominal domain\",\n",
      "\"local semantic neighborhood\": \"Local semantic neighborhood\",\n",
      "\"global notion of change\": \"Global notion of change\",\n",
      "\"large-scale statistical study\": \"Large-scale statistical study\",\n",
      "\"historical corpora\": \"Historical corpora\",\n",
      "\"consecutive decades\": \"Consecutive decades\",\n",
      "\"word2vec embeddings\": \"Word2vec embeddings\",\n",
      "\"Hamilton et al., 2016\": \"Hamilton, A. J., Yakhnenko, A., & Berg-Kirkpatrick, T. 2016.\",\n",
      "\"previous work\": \"Previous work\",\n",
      "\"URL\": \"URL\",\n",
      "\"vector representation\": \"Vector representation\",\n",
      "\"Turney and Pantel, 2010\": \"Turney, P. & Pantel, P. 2010.\"\n",
      "}</s>\n",
      "Time taken: 0:00:21.633835\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using {model_name_or_path}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "output_string = tokenizer.decode(output[0])\n",
    "print_output(output_string)\n",
    "\n",
    "total_time_taken = datetime.now() - start_time\n",
    "print(f\"Time taken: {total_time_taken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507e7993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585 335 1250\n"
     ]
    }
   ],
   "source": [
    "actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff56fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting ',' delimiter: line 2 column 36 (char 37)\n"
     ]
    }
   ],
   "source": [
    "# Safely convert the dictionary string to a dictionary using json.loads()\n",
    "try:\n",
    "    ind = output_string.index('Do not include the angle brackets in the dictionary.')\n",
    "    correct_output_string = modify_string(output_string[ind:])\n",
    "    dic_output_string = get_json_of_string(correct_output_string)\n",
    "    dictionary = [string_to_dict(dic_output_string)]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    dictionary = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acbf76ad-2acb-41dc-be3d-aab9c19c15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{}]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d18ab90-38c2-4724-9a03-239fc73078d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_loop(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT: {prompt[2]['content']}\n",
    "SYSTEM: {prompt[3]['content']}\n",
    "USER: {prompt[4]['content']}\n",
    "ASSISTANT: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bab7543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 of 13\n",
      "Iteration 3 of 13\n",
      "Iteration 4 of 13\n",
      "Iteration 5 of 13\n",
      "Iteration 6 of 13\n",
      "\n",
      "\n",
      "\n",
      "1275 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: \n",
      "```json\n",
      "{\n",
      "\"d\": \"Cosine distance\",\n",
      "\"G\": \"Function or operator\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"t\": \"Time\",\n",
      "\"k\": \"Number of neighbors\",\n",
      "\"e\": \"Set of vectors\",\n",
      "\"n\": \"Neighborhood measure\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"v\": \"Vector\",\n",
      "\"A\": \"Set of vectors\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"R\": \"Relationship\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"N\": \"Neighbors\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"H\": \"Set of vectors\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"L\": \"Local neighborhood measure\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"E\": \"Vector\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"Sim\": \"Similarity\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\": \"Word\",\n",
      "\"i\": \"Index\",\n",
      "\"w\":\n",
      "848 2525\n",
      "Actual total tokens till now: 3373\n",
      "Iteration 7 of 13\n",
      "\n",
      "\n",
      "\n",
      "1559 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: jsonDictionary = {\n",
      "\"ùê¨\": {\n",
      "\"Cosine similarity\": \"Cosine similarity\",\n",
      "\"ùê∞\": \"Semantic neighbors\",\n",
      "\"ùê±\": \"Vectors\",\n",
      "\"ùêπ\": \"Nearest neighbors\",\n",
      "\"ùêπ\\_t\": \"Nearest neighbors in time-periods t and t+1\",\n",
      "\"ùêπ\\_i\": \"Nearest semantic neighbors\",\n",
      "\"ùê∫\": \"Full vocabulary\",\n",
      "\"ùê∫\": \"Word's nearest neighbors\",\n",
      "\"ùêª\": \"Local neighborhood measure\",\n",
      "\"ùêº\": \"Time-periods\",\n",
      "\"ùêΩ\": \"Second-order vector\",\n",
      "\"ùêº\\_i\": \"Time-period\",\n",
      "\"ùêº\\_t\": \"Time\",\n",
      "\"ùêº\\_t+1\": \"Time t+1\",\n",
      "\"ùêº\\_e\": \"Epsilon\",\n",
      "\"ùêº\\_k\": \"Number of neighbors\",\n",
      "\"ùêº\\_w\": \"Words\",\n",
      "\"ùêº\\_L\": \"Local neighborhood measure\",\n",
      "\"ùêº\\_E\": \"Similarity\"\n",
      "}\n",
      "}</s>\n",
      "1139 4084\n",
      "Actual total tokens till now: 5223\n",
      "INCORRECT DICTIONARY\n",
      "Iteration 8 of 13\n",
      "\n",
      "\n",
      "\n",
      "1589 prompt tokens counted.\n",
      "\n",
      "Do not include the angle brackets in your dictionary.\n",
      "ASSISTANT: Math identifiers from text:\n",
      "```css\n",
      "d: d, G, e: e, v: v, n: n, k: k, t: t, i: i, R: R, L: l, H: h, d: d, N: n, E: e, Sim: sim\n",
      "```\n",
      "JSON dictionary:\n",
      "```json\n",
      "{\n",
      "  \"identifiers\": {\n",
      "    \"d\": \"Cosine distance\",\n",
      "    \"G\": \"Function or operator\",\n",
      "    \"w\": [\"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\", \"Word\"],\n",
      "    \"i\": [\"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\", \"Index\"],\n",
      "    \"t\": \"Time\",\n",
      "    \"k\": \"Number of neighbors\",\n",
      "    \"e\": \"Set of vectors\",\n",
      "    \"n\": \"Neighbors\",\n",
      "    \"n\": \"Neighborhood measure\",\n",
      "    \"v\": \"Vector\",\n",
      "    \"A\": \"Set of vectors\",\n",
      "    \"R\": \"Relationship\",\n",
      "    \"N\": \"Neighbors\",\n",
      "    \"H\": \"Set of vectors\",\n",
      "    \"L\": \"Local neighborhood measure\",\n",
      "    \"E\": \"Vector\",\n",
      "    \"Sim\": \"Similarity\"\n",
      "}\n",
      "```</s>\n",
      "1562 5673\n",
      "Actual total tokens till now: 7235\n",
      "INCORRECT DICTIONARY\n",
      "Iteration 9 of 13\n",
      "Iteration 10 of 13\n",
      "Iteration 11 of 13\n",
      "Iteration 12 of 13\n",
      "Iteration 13 of 13\n",
      "Iteration 14 of 13\n",
      "Time taken: 0:01:19.017523\n",
      "Total time taken: 0:01:40.651300\n",
      "7235 1562 5673\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "number_of_dictionaries = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Iteration {i} of {len(chunks)}\")\n",
    "    i += 1\n",
    "    if chunk == question:\n",
    "        continue\n",
    "    if not contains_pattern(chunk):\n",
    "        continue\n",
    "    question = chunk\n",
    "    \n",
    "    if prompt_size > 1600:\n",
    "        number_of_dictionaries += 1\n",
    "        print(\"\\nNew dictionary\\n\")\n",
    "        dictionary.append({})\n",
    "    \n",
    "    prompt = [\n",
    "            {'role': 'system',\n",
    "             'content': 'You are a helpful research assistant tasked with converting long paragraphs into a JSON '\n",
    "                        'dictionary. '\n",
    "                        'The goal is to identify and classify each individual mathematical symbol, variable, '\n",
    "                        'and identifier in the text marked between \"<||>\"'\n",
    "                        'The dictionary should store the identifiers as keys and their corresponding definitions as '\n",
    "                        'values in an array format. '},\n",
    "            {'role': 'system', 'name': 'example_user', 'content': '''A relational model is a triple <|M|>‚Ä≤=(<|X|>,<|R|>,\n",
    "            <|v|>), where <|X|> is a set of states, <|R|><|‚äÜ|><|X|><|√ó|><|X|> is a binary relation on <|X|>, \n",
    "            and <|v|>:<|ùñØùóãùóàùóâ|>‚Üí2<|X|> is a valuation. Given a relational model <|M|>‚Ä≤, the satisfaction relation \n",
    "            between points <|x<|‚àà<|X<| and formulas <|œÜ<|‚àà<|‚Ñí<|<|ùñ™ùñ†<| is defined inductively by <|M|>‚Ä≤,\n",
    "            <|x|>‚ä®<|ùñ™|><|œÜ|>‚áî for all <|y|>‚àà<|X|>,<|x|><|R|><|y|> implies <|M|>‚Ä≤,<|y|>‚ä®<|œÜ|><|M|>‚Ä≤,\n",
    "            <|x|>‚ä®<|ùñ†|><|œÜ|><||>‚áî for all <|y|>‚àà<|X|>,<|M|>‚Ä≤,<|y|>‚ä®<|œÜ|>'''},\n",
    "            {'role': 'system', 'name': 'example_assistant', 'content': '''identifiers = {\n",
    "            \"M\": [\"Model\", \"Expertise Model\"],\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"X\": \"Set of states\",\n",
    "            \"R\": \"Binary relation on X\",\n",
    "            \"v\": \"Valuation\",\n",
    "            \"ùñØùóãùóàùóâ\": \"Set of propositions\",\n",
    "            \"M'\": \"Relational model\",\n",
    "            \"x\": \"Point in X\",\n",
    "            \"œÜ\": \"Formula in ùñ™ùñ†\",\n",
    "            \"‚Ñí_{ùñ™ùñ†}\": \"Set of formulas\",\n",
    "            \"ùñ™\": \"Modal operator K\",\n",
    "            \"ùñ†\": \"Modal operator A\",\n",
    "            \"y\": \"Point in X\",\n",
    "            \"‚ä®\": \"Satisfaction relation\",\n",
    "            \"‚áî\": \"If and only if operator\",\n",
    "            \"‚àà\": \"Element of a set\",\n",
    "            \"‚äÜ\": \"Subset of a set\",\n",
    "            \"√ó\": \"Cartesian product operator\",\n",
    "            \"‚Üí\": \"Function or implication operator\",\n",
    "            \"for all\": \"Universal quantifier\"\n",
    "            }'''},\n",
    "            {'role': 'system',\n",
    "             'content': f'Given is already a pre existing dictionary. Your job is to extend this dictionary. Do not '\n",
    "                        f'remove any pre existing definitions from this dictionary.'\n",
    "                        f'\\n{dictionary[number_of_dictionaries]}. If there is nothing to mention, reply with an empty '\n",
    "                        f'dictionary'},\n",
    "            {'role': 'user', 'content': f'Generate a JSON dictionary for the following text: {question}. '\n",
    "                                        'Only consider the mathematical identifiers inside \"<||>\" for the dictionary. '\n",
    "                                        'Do not consider any other identifier other than those marked. '\n",
    "                                        'Consider all the identifiers individually. Do not skip any identifier, mention'\n",
    "                                        ' all the identifiers inside \"<||>\" in your dictionary. '\n",
    "                                        'Do not include the angle brackets in your dictionary.'}\n",
    "        ]\n",
    "    \n",
    "    open_prompt = get_prompt_loop(prompt)\n",
    "    \n",
    "    prompt_size = num_tokens_from_messages(open_prompt)\n",
    "    print(f\"\\n\\n\\n{prompt_size} prompt tokens counted.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "            output_string = tokenizer.decode(output[0])\n",
    "            \n",
    "            actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "            completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "            prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "            ind = output_string.index('Do not include the angle brackets in your dictionary.')\n",
    "            print(output_string[ind:])\n",
    "            \n",
    "            print(completion_tokens, prompt_tokens)\n",
    "    \n",
    "            print(f\"Actual total tokens till now: {actual_total_tokens}\")\n",
    "\n",
    "            try:\n",
    "                correct_output_string = modify_string(output_string[ind:])\n",
    "                dic_output_string = get_json_of_string(correct_output_string)\n",
    "                new_dictionary = string_to_dict(dic_output_string)\n",
    "            except Exception as e:\n",
    "                print(\"INCORRECT DICTIONARY\")\n",
    "            dictionary[number_of_dictionaries] = merge_dictionaries(dictionary[number_of_dictionaries], new_dictionary)\n",
    "            \n",
    "            break\n",
    "        except Exception as e:\n",
    "            number_of_dictionaries += 1\n",
    "            dictionary.append({})\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            print(\"Retrying...\")\n",
    "total_time_taken += (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04278ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dct = {}\n",
    "for dic in dictionary:\n",
    "    dct = merge_dictionaries(dct, dic)\n",
    "# pprint.pprint(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4212dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'w': '['Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', 'Word', [...]]'\n",
      "'i': '['Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', 'Index', [...]]'\n"
     ]
    }
   ],
   "source": [
    "for key, value in dct.items():\n",
    "    if type(value) == list:\n",
    "        print(f\"'{key}': '{value}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed252bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        if isinstance(i, list):\n",
    "            output_list.extend(flatten_list(i))\n",
    "        else:\n",
    "            output_list.append(i)\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def remove_duplicates(input_list):\n",
    "    output_list = []\n",
    "    for item in input_list:\n",
    "        if item not in output_list:\n",
    "            output_list.append(item)\n",
    "    if len(output_list) == 1:\n",
    "        return output_list[0]\n",
    "    return output_list\n",
    "\n",
    "\n",
    "def process_value(v):\n",
    "    if isinstance(v, str):\n",
    "        new_v = v.replace('$', '')\n",
    "        while '\\\\\\\\' in new_v:\n",
    "            new_v = new_v.replace('\\\\\\\\', '\\\\').replace('\\n', '')\n",
    "    else:  # Assuming it's a list\n",
    "        new_v = flatten_list([process_value(val) for val in v])\n",
    "        \n",
    "    return remove_duplicates(new_v) if isinstance(new_v, list) else new_v\n",
    "\n",
    "\n",
    "def reduce_pairs(dictionary):\n",
    "    new_dict = {}\n",
    "    for k, v in dictionary.items():\n",
    "        # reduce key backslashes\n",
    "        new_k = k.replace('$', '')\n",
    "        while '\\\\\\\\' in new_k:\n",
    "            new_k = new_k.replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "        # process value\n",
    "        new_v = process_value(v)\n",
    "\n",
    "        new_dict[new_k] = new_v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c8083f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dict_without_backslashes \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdct\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 41\u001b[0m, in \u001b[0;36mreduce_pairs\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m     38\u001b[0m         new_k \u001b[38;5;241m=\u001b[39m new_k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# process value\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     new_dict[new_k] \u001b[38;5;241m=\u001b[39m new_v\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_dict\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mprocess_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     25\u001b[0m         new_v \u001b[38;5;241m=\u001b[39m new_v\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Assuming it's a list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m flatten_list([process_value(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remove_duplicates(new_v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m new_v\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m         new_v \u001b[38;5;241m=\u001b[39m new_v\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Assuming it's a list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m flatten_list([\u001b[43mprocess_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remove_duplicates(new_v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m new_v\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mprocess_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     25\u001b[0m         new_v \u001b[38;5;241m=\u001b[39m new_v\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Assuming it's a list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m flatten_list([process_value(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remove_duplicates(new_v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m new_v\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m         new_v \u001b[38;5;241m=\u001b[39m new_v\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Assuming it's a list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m flatten_list([\u001b[43mprocess_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remove_duplicates(new_v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m new_v\n",
      "    \u001b[0;31m[... skipping similar frames: <listcomp> at line 27 (1483 times), process_value at line 27 (1483 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36mprocess_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     25\u001b[0m         new_v \u001b[38;5;241m=\u001b[39m new_v\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Assuming it's a list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m flatten_list([process_value(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remove_duplicates(new_v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m new_v\n",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m         new_v \u001b[38;5;241m=\u001b[39m new_v\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Assuming it's a list\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     new_v \u001b[38;5;241m=\u001b[39m flatten_list([\u001b[43mprocess_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m v])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m remove_duplicates(new_v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_v, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m new_v\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "dict_without_backslashes = reduce_pairs(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab472ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(dict_without_backslashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b426f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_without_backslashes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parsed_json \u001b[38;5;241m=\u001b[39m \u001b[43mdict_without_backslashes\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_without_backslashes' is not defined"
     ]
    }
   ],
   "source": [
    "parsed_json = dict_without_backslashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dec9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{file_code}_mcdict.json', 'r', encoding='utf-8') as f:\n",
    "    mc_dict_original = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4284b146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m mc_dict_original[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_author\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_name_or_path\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Iterate over your dictionary and fill the new one\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, values \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparsed_json\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Determine the base key and the affix\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     base_key \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^[^*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_^,(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[]*\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\u001b[38;5;241m.\u001b[39mgroup()\n\u001b[1;32m     11\u001b[0m     affix \u001b[38;5;241m=\u001b[39m key[\u001b[38;5;28mlen\u001b[39m(base_key):]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_json' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode().hex()\n",
    "\n",
    "mc_dict_original['_author'] = model_name_or_path\n",
    "\n",
    "# Iterate over your dictionary and fill the new one\n",
    "for key, values in parsed_json.items():\n",
    "    # Determine the base key and the affix\n",
    "    base_key = re.match(r\"^[^*'_^,(\\[]*\", key).group()\n",
    "    affix = key[len(base_key):]\n",
    "\n",
    "    hex_code = get_hex_code(base_key)\n",
    "    values = values if isinstance(values, list) else [values]\n",
    "\n",
    "    if hex_code in mc_dict_original[\"concepts\"]:\n",
    "        k = list(mc_dict_original[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "        new_identifier = []\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][k].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "    else:\n",
    "        if hex_code not in mc_dict_original[\"concepts\"]:\n",
    "            mc_dict_original[\"concepts\"][hex_code] = {\n",
    "                \"_surface\": {\n",
    "                    \"text\": base_key,\n",
    "                    \"unicode_name\": base_key if len(base_key) != 1 else unicodedata.name(base_key)\n",
    "                },\n",
    "                \"identifiers\": {\n",
    "                    'default': []\n",
    "                }\n",
    "            }\n",
    "\n",
    "        for value in values:\n",
    "            mc_dict_original[\"concepts\"][hex_code][\"identifiers\"][\"default\"].append({\n",
    "                \"affixes\": [affix] if affix else [],\n",
    "                \"arity\": 0,\n",
    "                \"description\": value\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert new dictionary to a sorted dictionary\n",
    "sorted_dict = dict(sorted(mc_dict_original[\"concepts\"].items(), key=lambda x: (len(x[0]), x[0])))\n",
    "mc_dict_original[\"concepts\"] = sorted_dict\n",
    "\n",
    "# Convert new dictionary to JSON\n",
    "json_str = json.dumps(mc_dict_original, indent=4, ensure_ascii=False)\n",
    "\n",
    "#print(json_str)\n",
    "\n",
    "with open(f'{file_code}-{model_name}_mcdict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(mc_dict_original, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a79ec2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_tags(element):\n",
    "    if isinstance(element, NavigableString):\n",
    "        return element\n",
    "    if element.name == 'mi':\n",
    "        return str(element)\n",
    "    return ''.join(get_text_from_tags(child) for child in element.children)\n",
    "\n",
    "def parse_html(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as html_file:\n",
    "        soup = BeautifulSoup(html_file, 'html.parser')\n",
    "\n",
    "    texts = get_text_from_tags(soup)\n",
    "    return texts\n",
    "\n",
    "def find_mi_strings(text):\n",
    "    pattern = r'(<mi.*?</mi>)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "# call the function with your HTML file path\n",
    "page = parse_html(f'{file_code}.html')\n",
    "matches = find_mi_strings(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64a0cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dict = mc_dict_original\n",
    "with open(f'{file_code}_anno.json', encoding='utf-8') as fp:\n",
    "    parsed_annotation = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "163356dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index_from_char_index(message, key, char_index):\n",
    "    i = 0\n",
    "    index = -1\n",
    "    for word in message:\n",
    "        if key in word:\n",
    "            index = i\n",
    "        i += 1\n",
    "    return index\n",
    "\n",
    "def expand_string_to_tokens(message, index, num_tokens_right=25, num_tokens_left=75):\n",
    "    words = message.split()  # Split the message into words\n",
    "\n",
    "    # Start at the index where the center word is\n",
    "    left_index = right_index = index\n",
    "\n",
    "    tokens_counter_right = num_tokens_from_messages(words[right_index])\n",
    "    tokens_counter_left = num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the left from the center index until you reach num_tokens_left\n",
    "    while tokens_counter_left < num_tokens_left and left_index > 0:\n",
    "        left_index -= 1\n",
    "        tokens_counter_left += num_tokens_from_messages(words[left_index])\n",
    "\n",
    "    # Expand to the right from the center index until you reach num_tokens_right\n",
    "    while tokens_counter_right < num_tokens_right and right_index < len(words) - 1:\n",
    "        right_index += 1\n",
    "        tokens_counter_right += num_tokens_from_messages(words[right_index])\n",
    "\n",
    "    # Combine the words back into a string and return\n",
    "    return ' '.join(words[left_index:right_index + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_text(text, replacement, exception):\n",
    "    # Find all matches\n",
    "    matches = re.findall(r'<mi(.*?)</mi>', text)\n",
    "    \n",
    "    for match in matches:\n",
    "        original_string = f'<mi{match}</mi>'\n",
    "        \n",
    "        # Skip exception\n",
    "        if original_string == exception:\n",
    "            continue\n",
    "        \n",
    "        # Replace match\n",
    "        replaced_string = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', original_string)\n",
    "        text = text.replace(original_string, replaced_string)\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_context(match):\n",
    "    match_len = len(match)\n",
    "    new_page = replace_text(page, '', match)\n",
    "    char_index = new_page.index(match) + int(match_len/2)\n",
    "    word_index = get_word_index_from_char_index(new_page, char_index)\n",
    "    section = expand_string_to_tokens(new_page, word_index)\n",
    "    section = re.sub(r'<.*?>(.*?)</.*?>', r'<<\\1>>', section)\n",
    "    return match, section\n",
    "\n",
    "# Function to create a hex code (a binary representation of the key)\n",
    "def get_hex_code(key):\n",
    "    return key.encode('utf-8').hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7706ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_tags(s):\n",
    "    parts = re.split('(<mi)', s)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        if '>' not in parts[i + 1]:\n",
    "            parts[i] = ''\n",
    "            parts[i + 1] = ''\n",
    "    return ''.join(parts)\n",
    "\n",
    "def get_definition_of_id(dict_id, identifier):\n",
    "    \n",
    "    try:\n",
    "        hex_code = get_hex_code(identifier)\n",
    "        index = parsed_annotation['mi_anno'][dict_id]['concept_id']\n",
    "        key = list(parsed_dict['concepts'][hex_code]['identifiers'].keys())[0]\n",
    "        return f\"({parsed_dict['concepts'][hex_code]['identifiers'][key][index]['description']})\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_context(match):\n",
    "    key_word = page.index(match) + len(match)\n",
    "    last_index = min(len(page), key_word + 500)\n",
    "    first_index = max(0, key_word - 3000)\n",
    "    context_window = page[first_index:last_index]\n",
    "    \n",
    "    reg_matches = re.findall(r'<mi(.*?)</mi>', context_window)\n",
    "    \n",
    "    identifier = None\n",
    "    \n",
    "    for reg_match in reg_matches:\n",
    "        original_string = f'<mi{reg_match}</mi>'\n",
    "        soup = BeautifulSoup(original_string, 'html.parser')\n",
    "        \n",
    "        \n",
    "        tags = soup.find_all('mi')\n",
    "        \n",
    "        if original_string == match:\n",
    "            identifier = tags[0].text\n",
    "            continue\n",
    "\n",
    "        context_window = context_window.replace(original_string,\n",
    "                                                f\"{tags[0].text}{get_definition_of_id(tags[0].get('id'), tags[0].text)}\")\n",
    "    \n",
    "    context_window = re.sub(r'<mi.*?>(.*?)<\\/mi>', r'<<\\1>>', context_window)\n",
    "    \n",
    "    context_window = remove_trailing_tags(context_window)\n",
    "    context_window = re.sub(r'^(?!.*<mi.*).*<\\/mi>', '', context_window, flags=re.DOTALL)\n",
    "        \n",
    "    index = 0\n",
    "    for word in context_window.split():\n",
    "        if f\"<<{identifier}>>\" in word:\n",
    "            word_index = index\n",
    "        index += 1\n",
    "    \n",
    "    if word_index == -1:\n",
    "        return context_window\n",
    "    else:\n",
    "        context_window = expand_string_to_tokens(context_window, word_index)\n",
    "    return context_window\n",
    "\n",
    "#print(get_context('<mi id=\"S1.p2.1.m1.1.1.3\" xref=\"S1.p2.1.m1.1.1.3.cmml\">œÜ</mi>'))\n",
    "#get_context('<mi id=\"S1.p2.1.m1.1.1.2\" xref=\"S1.p2.1.m1.1.1.2.cmml\">ùñ§</mi>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62ec5036-7177-46d0-9728-571edb9f6bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_anno(prompt):\n",
    "    return f'''SYSTEM: {prompt[0]['content']}\n",
    "USER: {prompt[1]['content']}\n",
    "ASSISTANT:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f57696ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 87: None\n",
      "Iteration 2 of 87: None\n",
      "Iteration 3 of 87: None\n",
      "Iteration 4 of 87: None\n",
      "Iteration 5 of 87: None\n",
      "Iteration 6 of 87: None\n",
      "Iteration 7 of 87: None\n",
      "Iteration 8 of 87: None\n",
      "Iteration 9 of 87: None\n",
      "Iteration 10 of 87: None\n",
      "Iteration 11 of 87: None\n",
      "Iteration 12 of 87: None\n",
      "Iteration 13 of 87: None\n",
      "Iteration 14 of 87: None\n",
      "Iteration 15 of 87: None\n",
      "Iteration 16 of 87: None\n",
      "Iteration 17 of 87: None\n",
      "Iteration 18 of 87: None\n",
      "Iteration 19 of 87: None\n",
      "Iteration 20 of 87: None\n",
      "Iteration 21 of 87: None\n",
      "Iteration 22 of 87: None\n",
      "Iteration 23 of 87: None\n",
      "Iteration 24 of 87: None\n",
      "Iteration 25 of 87: None\n",
      "Iteration 26 of 87: None\n",
      "Iteration 27 of 87: None\n",
      "Iteration 28 of 87: None\n",
      "Iteration 29 of 87: None\n",
      "Iteration 30 of 87: None\n",
      "Iteration 31 of 87: None\n",
      "Iteration 32 of 87: None\n",
      "Iteration 33 of 87: None\n",
      "Iteration 34 of 87: None\n",
      "Iteration 35 of 87: None\n",
      "Iteration 36 of 87: None\n",
      "Iteration 37 of 87: None\n",
      "Iteration 38 of 87: None\n",
      "Iteration 39 of 87: None\n",
      "Iteration 40 of 87: None\n",
      "Iteration 41 of 87: None\n",
      "Iteration 42 of 87: None\n",
      "Iteration 43 of 87: None\n",
      "Iteration 44 of 87: None\n",
      "Iteration 45 of 87: None\n",
      "Iteration 46 of 87: None\n",
      "Iteration 47 of 87: None\n",
      "Iteration 48 of 87: None\n",
      "Iteration 49 of 87: None\n",
      "Iteration 50 of 87: None\n",
      "Iteration 51 of 87: None\n",
      "Iteration 52 of 87: None\n",
      "Iteration 53 of 87: None\n",
      "Iteration 54 of 87: None\n",
      "Iteration 55 of 87: None\n",
      "Iteration 56 of 87: None\n",
      "Iteration 57 of 87: None\n",
      "Iteration 58 of 87: None\n",
      "Iteration 59 of 87: None\n",
      "Iteration 60 of 87: None\n",
      "Iteration 61 of 87: None\n",
      "Iteration 62 of 87: None\n",
      "Iteration 63 of 87: None\n",
      "Iteration 64 of 87: None\n",
      "Iteration 65 of 87: None\n",
      "Iteration 66 of 87: None\n",
      "Iteration 67 of 87: None\n",
      "Iteration 68 of 87: None\n",
      "Iteration 69 of 87: None\n",
      "Iteration 70 of 87: None\n",
      "Iteration 71 of 87: None\n",
      "Iteration 72 of 87: None\n",
      "Iteration 73 of 87: None\n",
      "Iteration 74 of 87: None\n",
      "Iteration 75 of 87: None\n",
      "Iteration 76 of 87: None\n",
      "Iteration 77 of 87: None\n",
      "Iteration 78 of 87: None\n",
      "Iteration 79 of 87: None\n",
      "Iteration 80 of 87: None\n",
      "Iteration 81 of 87: None\n",
      "Iteration 82 of 87: None\n",
      "Iteration 83 of 87: None\n",
      "Iteration 84 of 87: None\n",
      "Iteration 85 of 87: None\n",
      "Iteration 86 of 87: None\n",
      "Iteration 87 of 87: Key does not exist in the dictionary of concepts  \n",
      "Annotation completed\n",
      "Time taken: 0:00:00.479644\n",
      "Total time taken: 0:00:00.479613\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "actual_total_tokens = 0\n",
    "completion_tokens = 0\n",
    "prompt_tokens = 0\n",
    "no_tags = 0\n",
    "no_keys = 0\n",
    "no_anno = 0\n",
    "i = 1\n",
    "for match in matches:\n",
    "    print(f\"Iteration {i} of {len(matches)}: \", end='')\n",
    "    i += 1\n",
    "    context = get_context(match)\n",
    "    match_variable = re.sub(r'<.*?>(.*?)</.*?>', r'\\1', match)\n",
    "    context_index = context.index(f\"<<{match_variable}>>\") + len(match_variable)\n",
    "    possible_affix = str(context[context_index+4:context_index+5]).replace(\"‚Ä≤\", \"'\")\n",
    "    soup = BeautifulSoup(match, 'html.parser')\n",
    "    mi_tag = soup.find('mi')\n",
    "    if mi_tag is not None and 'id' in mi_tag.attrs:\n",
    "        anno_id = mi_tag['id']\n",
    "    else:\n",
    "        print('TAG NOT FOUND', match)\n",
    "        no_tags += 1\n",
    "        continue\n",
    "    \n",
    "    hex_code = get_hex_code(match_variable)\n",
    "    if hex_code not in parsed_dict['concepts']:\n",
    "        match_variable = f\"{unidecode(match_variable)}\"\n",
    "        hex_code = get_hex_code(match_variable)\n",
    "        if hex_code not in parsed_dict['concepts']:\n",
    "            print(\"Key does not exist in the dictionary of concepts\", match_variable, hex_code)\n",
    "            no_keys += 1\n",
    "            continue\n",
    "    \n",
    "    if anno_id not in parsed_annotation['mi_anno']:\n",
    "        print(\"Annotation ID does not exist in annotation.json\", anno_id)\n",
    "        no_anno += 1\n",
    "        continue\n",
    "\n",
    "    k = list(parsed_dict[\"concepts\"][hex_code][\"identifiers\"].keys())[0]\n",
    "    mcdict = parsed_dict['concepts'][hex_code]['identifiers'][k]\n",
    "    \n",
    "    if len(mcdict) == 1:\n",
    "        parsed_annotation['mi_anno'][anno_id]['concept_id'] = 0\n",
    "        print('0')\n",
    "    elif len(mcdict) > 1:\n",
    "        prompt_mcdict = []\n",
    "\n",
    "        index = 0\n",
    "        for val in mcdict:\n",
    "            prompt_mcdict.append({'index': f\"{index}\", 'identifier': f\"{match_variable}{'' if len(val['affixes']) == 0 else val['affixes'][0]}\", 'description': val['description']})\n",
    "            index += 1\n",
    "            \n",
    "        prompt = [\n",
    "            {'role': 'system', 'content': 'You are a professional annotater API. Your job is to select a fitting annotation from a dictionary for a mathematical identifier.'},\n",
    "            {'role': 'user', 'content': f'''Given the following possible annotations:\\n```json\\n{prompt_mcdict}```.\n",
    "             Select the index for the most fitting description for the identifier <<{match_variable}>> from the following text.\n",
    "             The potential affix of the indentifier could be <<{possible_affix}>>. Take the affixes of the possible annotations into account.\n",
    "             Only return the value of the index and nothing else.\n",
    "             Do not add any explanation otherwise the API breaks.\n",
    "             The identifier has been marked with <<>>.\n",
    "             If you can't come up with an index, write 'None'\n",
    "             ```txt\n",
    "             {context}\n",
    "             ```'''}\n",
    "        ]\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                open_prompt = get_prompt_anno(prompt)\n",
    "                \n",
    "                input_ids = tokenizer(open_prompt, return_tensors='pt').input_ids.cuda()\n",
    "                output = model.generate(inputs=input_ids, temperature=0.5, max_new_tokens=512, repetition_penalty=1.05)\n",
    "                output_string = tokenizer.decode(output[0])\n",
    "                \n",
    "                actual_total_tokens += num_tokens_from_messages(output_string)\n",
    "                completion_tokens += num_tokens_from_messages(output_string) - num_tokens_from_messages(open_prompt)\n",
    "                prompt_tokens += num_tokens_from_messages(open_prompt)\n",
    "\n",
    "                ind = output_string.index('ASSISTANT:')\n",
    "                value = output_string[ind:]\n",
    "                print(value)\n",
    "                \n",
    "                print(completion_tokens, prompt_tokens)\n",
    "\n",
    "                try:\n",
    "                    index = int(int(re.search('\\d+', value).group()))\n",
    "                    print(index)\n",
    "                    parsed_annotation['mi_anno'][anno_id]['concept_id'] = index\n",
    "                except Exception as f:\n",
    "                    print(f)\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred\\n{e}\")\n",
    "                print(\"Retrying...\")\n",
    "    else:\n",
    "        print('None')\n",
    "\n",
    "print('Annotation completed')\n",
    "\n",
    "    \n",
    "total_time_taken = (datetime.now() - start_time)\n",
    "print(f\"Time taken: {datetime.now() - start_time }\")\n",
    "print(f\"Total time taken: {total_time_taken}\")\n",
    "print(actual_total_tokens, completion_tokens, prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfeab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_annotation['_annotator'] = model_name_or_path\n",
    "with open(f'{file_code}-{model_name}_anno.json', 'w') as fp:\n",
    "    json.dump(parsed_annotation, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9180c363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "items = 0\n",
    "for key, value in parsed_annotation['mi_anno'].items():\n",
    "    if value['concept_id'] is not None:\n",
    "        #print(key, value)\n",
    "        items += 1\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe4c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
