{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.markers import MarkerStyle\n",
    "from matplotlib.legend_handler import HandlerBase\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "\n",
    "from timeeval import DatasetManager, DefaultMetrics, TimeEval, Status, Datasets\n",
    "from timeeval import TimeEval, ResourceConstraints, DefaultMetrics, Algorithm, TrainingType, InputDimensionality\n",
    "from timeeval.timeeval import Experiment\n",
    "from timeeval.timeeval import Times\n",
    "from timeeval.utils.datasets import extract_labels\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "algo_meta = {\n",
    "    \"DeepAnT\": {\"name\": \"DeepAnT\", \"research_area\": \"Deep Learning\", \"method_family\": \"forecasting\", \"image_name\": \"deepant\", \"display_name\": \"DeepAnT\", \"tex_command\": \"\\\\deepant[cite]{}\"},\n",
    "    \"EncDec-AD\": {\"name\": \"EncDec-AD\", \"research_area\": \"Deep Learning\", \"method_family\": \"reconstruction\", \"image_name\": \"encdec_ad\", \"display_name\": \"EncDec-AD\", \"tex_command\": \"\\\\encdecad[cite]{}\"},\n",
    "    \"FastMCD\": {\"name\": \"FastMCD\", \"research_area\": \"Statistics (Regression & Forecasting)\", \"method_family\": \"distribution\", \"image_name\": \"fast_mcd\", \"display_name\": \"FastMCD\", \"tex_command\": \"\\\\fastmcd[cite]{}\"},\n",
    "    \"IF-LOF\": {\"name\": \"IF-LOF\", \"research_area\": \"Ourlier Detection\", \"method_family\": \"trees\", \"image_name\": \"if_lof\", \"display_name\": \"IF-LOF\", \"tex_command\": \"\\\\iflof[cite]{}\"},\n",
    "    \"iForest\": {\"name\": \"Isolation Forest (iForest)\", \"research_area\": \"Ourlier Detection\", \"method_family\": \"trees\", \"image_name\": \"iforest\", \"display_name\": \"iForest\", \"tex_command\": \"\\\\iforest[cite]{}\"},\n",
    "    \"KMeans\": {\"name\": \"KMeans\", \"research_area\": \"Classic Machine Learning\", \"method_family\": \"distance\", \"image_name\": \"kmeans\", \"display_name\": \"KMeans\", \"tex_command\": \"\\\\kmeans[cite]{}\"},\n",
    "    \"KNN\": {\"name\": \"KNN\", \"research_area\": \"Classic Machine Learning\", \"method_family\": \"distance\", \"image_name\": \"knn\", \"display_name\": \"KNN\", \"tex_command\": \"\\\\knn[cite]{}\"},    \n",
    "    \"LOF\": {\"name\": \"LOF\", \"research_area\": \"Ourlier Detection\", \"method_family\": \"distance\", \"image_name\": \"lof\", \"display_name\": \"LOF\", \"tex_command\": \"\\\\lof[cite]{}\"},\n",
    "    \"LSTM-AD\": {\"name\": \"LSTM-AD\", \"research_area\": \"Deep Learning\", \"method_family\": \"forecasting\", \"image_name\": \"lstm_ad\", \"display_name\": \"LSTM-AD\", \"tex_command\": \"\\\\lstmad[cite]{}\"},\n",
    "    \"PCC\": {\"name\": \"PCC\", \"research_area\": \"Classic Machine Learning\", \"method_family\": \"reconstruction\", \"image_name\": \"pcc\", \"display_name\": \"PCC\", \"tex_command\": \"\\\\pcc[cite]{}\"},\n",
    "    \"Roburst PCA\": {\"name\": \"Roburst PCA\", \"research_area\": \"Classic Machine Learning\", \"method_family\": \"reconstruction\", \"image_name\": \"robust_pca\", \"display_name\": \"Roburst PCA\", \"tex_command\": \"\\\\robustpca[cite]{}\"},\n",
    "}\n",
    "\n",
    "algorithm_names = ['DeepAnT', 'EncDec-AD', 'KMeans', 'KNN', 'LOF', 'LSTM-AD',\n",
    "                   'FastMCD', 'PCC', 'iForest', 'IF-LOF', 'Roburst PCA']\n",
    "unsupervised_algorithm_names = [\n",
    "    'KMeans', 'KNN', 'LOF', 'PCC', 'iForest', 'IF-LOF']\n",
    "semi_supervised_algorithm_names = ['DeepAnT', 'EncDec-AD',  'LSTM-AD', 'FastMCD', 'Roburst PCA']\n",
    "\n",
    "unsupervised_dataset_id = ('custom', 'MOVE-II_unsupervised')\n",
    "semi_supervised_dataset_id = ('custom', 'MOVE-II_semi-supervised')\n",
    "storePlotDir = r'plots'\n",
    "\n",
    "class CalculatedMetrics:\n",
    "    def __init__(self):\n",
    "        self.ROC_AUC = 'ROC_AUC'\n",
    "        self.PR_AUC = 'PR_AUC'\n",
    "        self.RANGE_PR_AUC = 'RANGE_PR_AUC'\n",
    "        self.OVERALL_RUN_TIME = 'overall_time'\n",
    "\n",
    "CALCULATED_METRICS = CalculatedMetrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_column_figwidth = 10\n",
    "double_column_figwidth = 20\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    \"font.weight\": \"regular\",  # \"bold\"\n",
    "    \"figure.figsize\": (double_column_figwidth, 10)\n",
    "})\n",
    "os.makedirs(storePlotDir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and configuration\n",
    "data_path = Path(r\"data\")\n",
    "result_root_path = Path(r\"results\")\n",
    "result_paths = [d for d in result_root_path.iterdir() if d.is_dir()]\n",
    "print(result_paths)\n",
    "result_path = result_root_path / \"2024_07_20_13_41_02\"\n",
    "print(f\"Reading results from {result_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm\tcollection\tdataset\talgo_training_type\talgo_input_dimensionality\tdataset_training_type\tdataset_input_dimensionality\ttrain_preprocess_time\ttrain_main_time\texecute_preprocess_time repetition\thyper_params\thyper_params_id\tROC_AUC\tPR_AUC\tRANGE_PR_AUC\ttrain_postprocess_time\tdataset_name\toverall_time\talgorithm-index\n",
    "\n",
    "custom_dataset = Path(r\"data/datasets.json\")\n",
    "dm = DatasetManager(Path.cwd() / \"data\", create_if_missing=True,\n",
    "                    custom_datasets_file=custom_dataset)\n",
    "\n",
    "RESULT_DF  = utils.load_results(result_path)\n",
    "MOVE_II_DF = dm.get_dataset_df(unsupervised_dataset_id)\n",
    "MOVE_II_SEMI_SUPERVISE_DF = dm.get_dataset_df(semi_supervised_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# algorithm_names = ['LOF']\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve\n",
    "from utils import range_precision_recall_curve\n",
    "\n",
    "metrics_functions = {}\n",
    "\n",
    "metrics_functions[CALCULATED_METRICS.ROC_AUC] = roc_curve\n",
    "metrics_functions[CALCULATED_METRICS.PR_AUC] = precision_recall_curve\n",
    "metrics_functions[CALCULATED_METRICS.RANGE_PR_AUC] = range_precision_recall_curve\n",
    "\n",
    "\n",
    "def plot_curves(metric: str, store_plot= False):\n",
    "    fig = go.Figure()\n",
    "    for algo in algorithm_names:\n",
    "        if algo in unsupervised_algorithm_names:\n",
    "            scoreDf: np.ndarray = utils.load_scores_df(\n",
    "            RESULT_DF, result_path, algo, unsupervised_dataset_id).iloc[:, 0]\n",
    "            labels = extract_labels(MOVE_II_DF)\n",
    "        elif algo in semi_supervised_algorithm_names:\n",
    "            scoreDf: np.ndarray = utils.load_scores_df(\n",
    "            RESULT_DF, result_path, algo, semi_supervised_dataset_id).iloc[:, 0]\n",
    "            labels = extract_labels(MOVE_II_SEMI_SUPERVISE_DF)\n",
    "        y_true, y_scores = Experiment.scale_scores(labels, scoreDf)\n",
    "        x, y, area = utils.curve_store(\n",
    "            y_true, y_scores, metrics_functions[metric])\n",
    "        name = f'{algo} | area = {area:.4f}'\n",
    "        fig.add_trace(go.Scatter(x=x, y=y,\n",
    "                             mode='lines',\n",
    "                                  name=name))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={\"text\": f\"{metric} curves\",\n",
    "               \"xanchor\": \"center\", \"x\": 0.5},\n",
    "        legend_title=\"Metrics\")\n",
    "    fig.show()\n",
    "    if store_plot:\n",
    "        fig.write_image(file=f'{storePlotDir}/{metric}_curves_plot.pdf',\n",
    "                        format='pdf')\n",
    "\n",
    "    # {metrics_functions[metric].__name__}\n",
    "\n",
    "\n",
    "plot_curves(CALCULATED_METRICS.ROC_AUC, True)\n",
    "plot_curves(CALCULATED_METRICS.PR_AUC, True)\n",
    "plot_curves(CALCULATED_METRICS.RANGE_PR_AUC, True)\n",
    "\n",
    "# algorithm_names = ['EncDec-AD']\n",
    "\n",
    "#  ROC Scores \n",
    "# fig = go.Figure()\n",
    "\n",
    "# for algo in algorithm_names:\n",
    "#     # scoreDf: np.ndarray = utils.load_scores_df(\n",
    "#     #     df, result_path, algo, dataset_id).iloc[:, 0]\n",
    "\n",
    "#     if algo in unsupervised_algorithm_names:\n",
    "#         scoreDf: np.ndarray = utils.load_scores_df(\n",
    "#             RESULT_DF, result_path, algo, unsupervised_dataset_id).iloc[:, 0]\n",
    "#         labels = extract_labels(MOVE_II_DF)\n",
    "#     elif algo in semi_supervised_algorithm_names:\n",
    "#         scoreDf: np.ndarray = utils.load_scores_df(\n",
    "#             RESULT_DF, result_path, algo, semi_supervised_dataset_id).iloc[:, 0]\n",
    "#         labels = extract_labels(MOVE_II_SEMI_SUPERVISE_DF)\n",
    "\n",
    "#     y_true, y_scores = Experiment.scale_scores(labels, scoreDf)\n",
    "#     # fig_curv = go.Figure()\n",
    "\n",
    "#     # roc_curve_x, roc_curve_y, roc_curve_area = utils.curve_store(y_true, y_scores, roc_curve)\n",
    "#     # roc_curve_name = f'{algo} {roc_curve.__name__} | area = {roc_curve_area:.4f}'\n",
    "#     # fig.add_trace(go.Scatter(x=roc_curve_x, y=roc_curve_y,\n",
    "#     #                          mode='lines',\n",
    "#     #                               name=roc_curve_name))\n",
    "    \n",
    "\n",
    "#     # pr_curve_x, pr_curve_y, pr_curve_area = utils.curve_store(\n",
    "#     #     y_true, y_scores, precision_recall_curve)\n",
    "#     # pr_curve_name = f'{precision_recall_curve.__name__} | area = {pr_curve_area:.4f}'\n",
    "#     # fig_curv.add_trace(go.Scatter(x=pr_curve_x, y=pr_curve_y,\n",
    "#     #                          mode='lines',\n",
    "#     #                               name=pr_curve_name))\n",
    "\n",
    "#     # range_pr_curve_x, range_pr_curve_y, range_pr_curve_area = utils.curve_store(\n",
    "#     #     y_true, y_scores, range_precision_recall_curve)\n",
    "#     # range_pr_curve_name = f'{range_precision_recall_curve.__name__} | area = {range_pr_curve_area:.4f}'\n",
    "#     # fig_curv.add_trace(go.Scatter(x=range_pr_curve_x, y=range_pr_curve_y,\n",
    "#     #                          mode='lines',\n",
    "#     #                               name=range_pr_curve_name))\n",
    "\n",
    "#     # utils.auc_plot_matplotlib(algo, y_true, y_scores,\n",
    "#     #                           roc_curve, True, True, storePlotDir, 'png')\n",
    "#     # utils.auc_plot_matplotlib(algo, y_true, y_scores,\n",
    "#     #                           precision_recall_curve, True, True, storePlotDir, 'png')\n",
    "#     # utils.auc_plot_matplotlib(algo, y_true, y_scores,\n",
    "#     #                           range_precision_recall_curve, True, True, storePlotDir, 'png')\n",
    "\n",
    "#     # roc_score, roc_fig = utils.auc_plot(y_true, y_scores, roc_curve)\n",
    "#     # pr_score, pr_fig = utils.auc_plot(y_true, y_scores, precision_recall_curve)\n",
    "#     # range_pr_score, range_pr_fig = utils.auc_plot(y_true, y_scores, range_precision_recall_curve)\n",
    "#     # print(\n",
    "#     #     f'Algorithm:{algo} => ROC_AUC = {roc_score}:  PR_AUC={pr_score}: RANGE_PR_AUC={range_pr_score}')\n",
    "#     # roc_fig.show()\n",
    "#     # pr_fig.show()\n",
    "#     # range_pr_fig.show()\n",
    "\n",
    "# # fig.update_layout(\n",
    "# #     title={\"text\": f\"{CALCULATED_METRICS.ROC_AUC}Evaluation evaluation curves\",\n",
    "# #                \"xanchor\": \"center\", \"x\": 0.5},\n",
    "# #         legend_title=\"Metrics\")\n",
    "# # fig.show()\n",
    "# # fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.ROC_AUC}_metrics_curves_plot.png',\n",
    "# #                          format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for algo in algorithm_names:\n",
    "    if algo in unsupervised_algorithm_names:\n",
    "        collection = unsupervised_dataset_id[0]\n",
    "        dataset_name = unsupervised_dataset_id[1]\n",
    "    elif algo in semi_supervised_algorithm_names:\n",
    "        collection = semi_supervised_dataset_id[0]\n",
    "        dataset_name = semi_supervised_dataset_id[1]\n",
    "    \n",
    "    fig = utils.plot_scores(\n",
    "        algorithm_name=algo, collection_name=collection, dataset_name=dataset_name, df=RESULT_DF,\n",
    "        dmgr=dm, result_path=result_path)\n",
    "    print(f'{algo} Plot')\n",
    "    # plotly.io.config.default_scale 1920 x 1080 pixels\n",
    "    fig.show()\n",
    "    fig.write_image(file=f'{storePlotDir}/{algo}_score_plot.png',\n",
    "                    format='png', scale=1.0, width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DF[\"algo_family\"] = RESULT_DF[\"algorithm\"].apply(\n",
    "    lambda algo: algo_meta[algo][\"method_family\"])\n",
    "RESULT_DF[\"algo_area\"] = RESULT_DF[\"algorithm\"].apply(\n",
    "    lambda algo: algo_meta[algo][\"research_area\"])\n",
    "RESULT_DF[\"algo_display_name\"] = RESULT_DF[\"algorithm\"].apply(\n",
    "    lambda algo: algo_meta[algo][\"display_name\"])\n",
    "\n",
    "RESULT_DF[[\"algorithm\", \"algo_display_name\",\n",
    "           \"algo_family\", \"algo_area\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_counts = RESULT_DF.pivot_table(index=[\"algo_training_type\", \"algorithm\"], columns=[\n",
    "                                 \"status\"], values=\"repetition\")\n",
    "df_error_counts = df_error_counts.fillna(value=0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of errors per algorithm grouped by algorithm training type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tpe in [\"SEMI_SUPERVISED\", \"UNSUPERVISED\"]:\n",
    "    print(tpe)\n",
    "    fig = ff.create_table(df_error_counts.loc[tpe], index=True)\n",
    "    fig.write_image(file=f'{storePlotDir}/{tpe}_Run_plot.png',\n",
    "                    format='png')\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm quality assessment\n",
    "Overall algorithm performance based on ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregatemetric = 'ROC_AUC'  # ROC_AUC\n",
    "\n",
    "aggregations = [\"min\", \"mean\", \"median\", \"max\"]\n",
    "df_overall_scores = RESULT_DF.pivot_table(\n",
    "    index=\"algorithm\", values=CALCULATED_METRICS.ROC_AUC, aggfunc=aggregations)\n",
    "df_overall_scores.columns = aggregations\n",
    "df_overall_scores = df_overall_scores.sort_values(by=\"median\", ascending=False)\n",
    "\n",
    "df_overall_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_asl = RESULT_DF.pivot(index=\"algorithm\", columns=\"dataset_name\",\n",
    "                         values=CALCULATED_METRICS.ROC_AUC)\n",
    "df_asl = df_asl.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "df_asl[\"median\"] = df_asl.median(axis=1)\n",
    "df_asl = df_asl.sort_values(by=\"median\", ascending=True)\n",
    "df_asl = df_asl.drop(columns=\"median\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.tips()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_fig = utils.plot_barplot(RESULT_DF, title=f\"{CALCULATED_METRICS.ROC_AUC} Bar plot\",\n",
    "                                 ax_label=\"Algorithms\", ay_label=f\"{CALCULATED_METRICS.ROC_AUC} score\", metric=CALCULATED_METRICS.ROC_AUC)\n",
    "pr_auc_fig = utils.plot_barplot(RESULT_DF, title=f\"{CALCULATED_METRICS.PR_AUC} Bar plot\",\n",
    "                                ax_label=\"Algorithms\", ay_label=f\"{CALCULATED_METRICS.PR_AUC} score\", metric=CALCULATED_METRICS.PR_AUC)\n",
    "range_pr_auc_fig = utils.plot_barplot(RESULT_DF, title=f\"{CALCULATED_METRICS.RANGE_PR_AUC} Bar plot\",\n",
    "                                      ax_label=\"Algorithms\", ay_label=f\"{CALCULATED_METRICS.RANGE_PR_AUC} score\", metric=CALCULATED_METRICS.RANGE_PR_AUC)\n",
    "\n",
    "overall_time_plot = utils.plot_barplot(RESULT_DF, title=\"Overall runtime Bar plot\",\n",
    "                                         ax_label=\"Algorithms\", ay_label=\"Overall runtime (in seconds)\", metric=CALCULATED_METRICS.OVERALL_RUN_TIME)\n",
    "\n",
    "py.iplot(roc_auc_fig)\n",
    "roc_auc_fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.ROC_AUC}_score_plot.png',\n",
    "                    format='png', scale=1.0, width=1920, height=1080)\n",
    "py.iplot(pr_auc_fig)\n",
    "pr_auc_fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.PR_AUC}_score_plot.png',\n",
    "                        format='png', scale=1.0, width=1920, height=1080)\n",
    "py.iplot(range_pr_auc_fig)\n",
    "range_pr_auc_fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.RANGE_PR_AUC}_score_plot.png',\n",
    "                        format='png', scale=1.0, width=1920, height=1080)\n",
    "py.iplot(overall_time_plot)\n",
    "overall_time_plot.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.OVERALL_RUN_TIME}_plot.png',\n",
    "                        format='png', scale=1.0, width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_datasets = RESULT_DF.pivot(index=\"dataset_name\",\n",
    "#                        columns=\"algorithm\", values=aggregatemetric)\n",
    "# # df_datasets = df_datasets.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# df_datasets[\"median\"] = df_datasets.median(axis=1)\n",
    "# df_datasets = df_datasets.sort_values(by=\"median\", ascending=True)\n",
    "# df_datasets = df_datasets.drop(columns=\"median\").T\n",
    "\n",
    "\n",
    "# CALCULATED_METRICS.RANGE_PR_AUC\n",
    "\n",
    "\n",
    "# def plot_dataset_boxplot(characteristic):\n",
    "#     df_c = df_datasets.drop(\n",
    "#         columns=[c for c in df_datasets.columns if characteristic != c.split(\"-\")[1]])\n",
    "#     fig = go.Figure()\n",
    "#     for i, c in enumerate(df_c.columns):\n",
    "#         base_osci = c.split(\"-\")[0]\n",
    "#         fig.add_trace(go.Box(\n",
    "#             x=df_c[c],\n",
    "#             name=c,\n",
    "#             boxpoints=False,\n",
    "#             legendgroup=base_osci,\n",
    "#             visible=\"legendonly\" if base_osci != \"sinus\" else None\n",
    "#         ))\n",
    "#     fig.update_layout(\n",
    "#         title={\"text\": f\"Dataset scores by characteristic '{characteristic}'\",\n",
    "#                \"xanchor\": \"center\", \"x\": 0.5},\n",
    "#         xaxis_title=f\"{aggregatemetric} score\",\n",
    "#         legend_title=\"Datasets\"\n",
    "#     )\n",
    "#     return py.iplot(fig)\n",
    "\n",
    "def plot_box_for_metrics_base_on_learning_type(metric:str):\n",
    "\n",
    "    df_learning_type = RESULT_DF.pivot(index=\"algo_training_type\",\n",
    "                                       columns=\"algorithm\", values=metric)\n",
    "    df_learning_type[\"median\"] = df_learning_type.median(axis=1)\n",
    "    df_learning_type = df_learning_type.sort_values(by=\"median\", ascending=True)\n",
    "    df_learning_type = df_learning_type.drop(columns=\"median\").T\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, c in enumerate(df_learning_type.columns):\n",
    "        fig.add_trace(go.Box(\n",
    "            x=df_learning_type[c],\n",
    "            name=c,\n",
    "            boxpoints=False,\n",
    "            # text_auto=True\n",
    "            # visible=None if i < n_show or i > len(\n",
    "            #     df_learning_type.columns)-n_show-1 else \"legendonly\"\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        title={\"text\": f\"{metric} box plots by Algorithm learning type\",\n",
    "               \"xanchor\": \"center\", \"x\": 0.5},\n",
    "        xaxis_title=f\"{metric} score\",\n",
    "        legend_title=\"Algorithm Learning Type\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roc_auc_box_fig = plot_box_for_metrics_base_on_learning_type(\n",
    "    CALCULATED_METRICS.ROC_AUC)\n",
    "pr_auc__box_fig = plot_box_for_metrics_base_on_learning_type(\n",
    "    CALCULATED_METRICS.PR_AUC)\n",
    "range_pr_auc_box_fig = plot_box_for_metrics_base_on_learning_type(\n",
    "    CALCULATED_METRICS.RANGE_PR_AUC)\n",
    "py.iplot(roc_auc_box_fig)\n",
    "roc_auc_box_fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.ROC_AUC}_score_Algorithms_box_plot.png',\n",
    "                        format='png', scale=1.0, width=1920, height=1080)\n",
    "py.iplot(pr_auc__box_fig)\n",
    "pr_auc__box_fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.PR_AUC}_score_Algorithms_box_plot.png',\n",
    "                       format='png', scale=1.0, width=1920, height=1080)\n",
    "py.iplot(range_pr_auc_box_fig)\n",
    "range_pr_auc_box_fig.write_image(file=f'{storePlotDir}/{CALCULATED_METRICS.RANGE_PR_AUC}_score_Algorithms_box_plot.png',\n",
    "                             format='png', scale=1.0, width=1920, height=1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok = \"- OK -\"\n",
    "# oom = \"- OOM -\"\n",
    "# timeout = \"- TIMEOUT -\"\n",
    "# error_mapping = {\n",
    "#     \"TimeoutError\": timeout,\n",
    "#     \"status code '137'\": oom,\n",
    "#     \"MemoryError: Unable to allocate\": oom,\n",
    "#     \"ValueError: Expected 2D array, got 1D array instead\": \"Wrong shape error\",\n",
    "#     \"could not broadcast input array from shape\": \"Wrong shape error\",\n",
    "#     # shapes (20,) and (19,500) not aligned\n",
    "#     \"not aligned\": \"Wrong shape error\",\n",
    "#     \"array must not contain infs or NaNs\": \"unexpected Inf or NaN\",\n",
    "#     \"contains NaN\": \"unexpected Inf or NaN\",\n",
    "#     \"cannot convert float NaN to integer\": \"unexpected Inf or NaN\",\n",
    "#     \"Error(s) in loading state_dict\": \"Model loading error\",\n",
    "#     \"EOFError\": \"Model loading error\",\n",
    "#     \"Restoring from checkpoint failed\": \"Model loading error\",\n",
    "#     \"RecursionError: maximum recursion depth exceeded in comparison\": \"Max recursion depth exceeded\",\n",
    "#     # ValueError: X has 44 features, but PCA is expecting 43 features as input.\n",
    "#     \"but PCA is expecting\": \"BROKEN Exathlon DATASETS\",\n",
    "#     \"input.size(-1) must be equal to input_size\": \"BROKEN Exathlon DATASETS\",\n",
    "#     \"ValueError: The condensed distance matrix must contain only finite values.\": \"LinAlgError\",\n",
    "#     \"LinAlgError\": \"LinAlgError\",\n",
    "#     \"NameError: name 'nan' is not defined\": \"Not converged\",\n",
    "#     \"Could not form valid cluster separation\": \"Not converged\",\n",
    "#     \"contamination must be in\": \"Invariance/assumption not met\",\n",
    "#     \"Data must not be constant\": \"Invariance/assumption not met\",\n",
    "#     \"Cannot compute initial seasonals using heuristic method with less than two full seasonal cycles in the data\": \"Invariance/assumption not met\",\n",
    "#     \"ValueError: Anom detection needs at least 2 periods worth of data\": \"Invariance/assumption not met\",\n",
    "#     \"`dataset` input should have multiple elements\": \"Invariance/assumption not met\",\n",
    "#     \"Cannot take a larger sample than population\": \"Invariance/assumption not met\",\n",
    "#     \"num_samples should be a positive integer value\": \"Invariance/assumption not met\",\n",
    "#     \"Cannot use heuristic method to compute initial seasonal and levels with less than periods + 10 datapoints\": \"Invariance/assumption not met\",\n",
    "#     \"ValueError: The window size must be less than or equal to 0\": \"Invariance/assumption not met\",\n",
    "#     \"The window size must be less than or equal to\": \"Incompatible parameters\",\n",
    "#     \"window_size has to be greater\": \"Incompatible parameters\",\n",
    "#     \"Set a higher piecewise_median_period_weeks\": \"Incompatible parameters\",\n",
    "#     \"OutOfBoundsDatetime: cannot convert input with unit 'm'\": \"Incompatible parameters\",\n",
    "#     \"`window_size` must be at least 4\": \"Incompatible parameters\",\n",
    "#     \"elements of 'k' must be between\": \"Incompatible parameters\",\n",
    "#     \"Expected n_neighbors <= n_samples\": \"Incompatible parameters\",\n",
    "#     \"PAA size can't be greater than the timeseries size\": \"Incompatible parameters\",\n",
    "#     \"All window sizes must be greater than or equal to\": \"Incompatible parameters\",\n",
    "#     \"ValueError: __len__() should return >= 0\": \"Bug\",\n",
    "#     \"stack expects a non-empty TensorList\": \"Bug\",\n",
    "#     \"expected non-empty vector\": \"Bug\",\n",
    "#     \"Found array with 0 feature(s)\": \"Bug\",\n",
    "#     \"ValueError: On entry to DLASCL parameter number 4 had an illegal value\": \"Bug\",\n",
    "#     \"Sample larger than population or is negative\": \"Bug\",\n",
    "#     \"ZeroDivisionError\": \"Bug\",\n",
    "#     \"IndexError\": \"Bug\",\n",
    "#     \"status code '139'\": \"Bug\",  # segfault\n",
    "#     \"replacement has length zero\": \"Bug\",\n",
    "#     \"missing value where TRUE/FALSE needed\": \"Bug\",\n",
    "#     \"invalid subscript type 'list'\": \"Bug\",\n",
    "#     \"subscript out of bounds\": \"Bug\",\n",
    "#     \"invalid argument to unary operator\": \"Bug\",\n",
    "#     \"negative length vectors are not allowed\": \"Bug\",\n",
    "#     \"negative dimensions are not allowed\": \"Bug\",\n",
    "#     \"`std` must be positive\": \"Bug\",\n",
    "#     \"does not have key\": \"Bug\",  # State '1' does not have key '1'\n",
    "#     \"Less than 2 uniques breaks left\": \"Bug\",\n",
    "#     \"The encoder for value is invalid\": \"Bug\",\n",
    "#     \"arange: cannot compute length\": \"Bug\",\n",
    "#     \"n_components=3 must be between 0 and min(n_samples, n_features)\": \"Bug\",\n",
    "#     \"must match the size of tensor b\": \"Wrong shape error\",\n",
    "# }\n",
    "\n",
    "# def get_folder(df, index):\n",
    "#     series = df.loc[index]\n",
    "#     # result_path = result_path\n",
    "#     dataset_name = series[\"dataset\"]\n",
    "#     path = (\n",
    "#         result_path /\n",
    "#         series[\"algorithm\"] /\n",
    "#         series[\"hyper_params_id\"] /\n",
    "#         series[\"collection\"] /\n",
    "#         dataset_name /\n",
    "#         str(series[\"repetition\"])\n",
    "#     )\n",
    "#     return path\n",
    "\n",
    "\n",
    "# def category_from_logfile(logfile):\n",
    "#     with logfile.open() as fh:\n",
    "#         log = fh.read()\n",
    "#     for error in error_mapping:\n",
    "#         if error in log:\n",
    "#             return error_mapping[error]\n",
    "#     # print(log)\n",
    "#     return \"other\"\n",
    "\n",
    "\n",
    "# def extract_category(series):\n",
    "#     status = series[\"status\"]\n",
    "#     msg = series[\"error_message\"]\n",
    "#     if status == \"Status.OK\":\n",
    "#         return ok\n",
    "#     elif status == \"Status.TIMEOUT\":\n",
    "#         return timeout\n",
    "#     # status is ERROR:\n",
    "#     elif \"DockerAlgorithmFailedError\" in msg:\n",
    "#         path = get_folder(series.name) / \"execution.log\"\n",
    "#         if path.exists():\n",
    "#             return category_from_logfile(path)\n",
    "#         return \"DockerAlgorithmFailedError\"\n",
    "#     else:\n",
    "#         m = re.search(\"^([\\w]+)\\(.*\\)\", msg)\n",
    "#         if m:\n",
    "#             error = m.group(1)\n",
    "#         else:\n",
    "#             error = msg\n",
    "#         return f\"TimeEval:{error}\"\n",
    "\n",
    "\n",
    "# RESULT_DF[\"error_category\"] = RESULT_DF.apply(\n",
    "#     extract_category, axis=\"columns\", raw=False)\n",
    "# df_error_category_overview = RESULT_DF.pivot_table(\n",
    "#     index=\"error_category\", columns=\"algorithm\", values=\"repetition\", aggfunc=\"count\")\n",
    "# df_error_category_overview.insert(\n",
    "#     0, \"ALL (sum)\", df_error_category_overview.sum(axis=1))\n",
    "\n",
    "# with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "#     display(df_error_category_overview.style.format(\"{:.0f}\", na_rep=\"\"))\n",
    "\n",
    "# df_error_summary = pd.DataFrame(df_error_category_overview.sum(axis=1))\n",
    "# df_error_summary.columns = [\"count\"]\n",
    "\n",
    "# df_error_summary.loc[\"- ERROR -\",\n",
    "#                      \"count\"] = df_error_summary[~df_error_summary.index.str.startswith(\"- \")].sum().item()\n",
    "# df_error_summary = df_error_summary.drop(\n",
    "#     df_error_summary[~df_error_summary.index.str.startswith(\"- \")].index)\n",
    "\n",
    "# all_count = df_error_summary.sum().item()\n",
    "# df_error_summary[\"percentage\"] = df_error_summary / all_count\n",
    "# df_error_summary[\"count\"] = df_error_summary[\"count\"].astype(np.int_)\n",
    "# df_error_summary.style.format({\"percentage\": \"{:06.2%}\".format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dominant_aggregation = \"mean\"\n",
    "# index_columns = [\"algo_input_dimensionality\",\n",
    "#                  \"algo_training_type\", \"algorithm\"]\n",
    "\n",
    "\n",
    "# df_asl = RESULT_DF.pivot(index=index_columns, columns=[\n",
    "#                   \"collection\", \"dataset\"], values=\"ROC_AUC\")\n",
    "# df_asl = df_asl.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# df_asl[dominant_aggregation] = df_asl.agg(dominant_aggregation, axis=1)\n",
    "# df_asl = df_asl.reset_index().sort_values(\n",
    "#     by=index_columns[:-1] + [dominant_aggregation], ascending=True).set_index(index_columns)\n",
    "# df_asl = df_asl.drop(columns=dominant_aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_asl_pr = RESULT_DF.pivot(index=index_columns, columns=[\n",
    "#                      \"collection\", \"dataset\"], values=\"PR_AUC\")\n",
    "# df_asl_pr = df_asl_pr.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# df_asl_pr = df_asl_pr.reindex(df_asl.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_asl_rangepr = RESULT_DF.pivot(index=index_columns, columns=[\n",
    "#                           \"collection\", \"dataset\"], values=\"RANGE_PR_AUC\")\n",
    "# df_asl_rangepr = df_asl_rangepr.dropna(\n",
    "#     axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# df_asl_rangepr = df_asl_rangepr.reindex(df_asl.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_asl_gt = RESULT_DF[RESULT_DF[\"collection\"] == \"custom\"].pivot(\n",
    "#     index=index_columns, columns=[\"collection\", \"dataset\"], values=\"ROC_AUC\")\n",
    "# df_asl_gt = df_asl_gt.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# df_asl_gt = df_asl_gt.reindex(df_asl.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative = True\n",
    "\n",
    "# df_algorithm_error_counts = RESULT_DF.pivot_table(index=[\"algorithm\"], columns=[\n",
    "#                                            \"error_category\"], values=\"repetition\", aggfunc=\"count\")\n",
    "# df_algorithm_error_counts = df_algorithm_error_counts.fillna(\n",
    "#     value=0).astype(np.int64)\n",
    "# error_categories = [\n",
    "#     c for c in df_algorithm_error_counts.columns if not c.startswith(\"-\")]\n",
    "# df_algorithm_error_counts[\"- ERROR -\"] = df_algorithm_error_counts[error_categories].sum(\n",
    "#     axis=1)\n",
    "# df_algorithm_error_counts = df_algorithm_error_counts.drop(\n",
    "#     columns=error_categories)\n",
    "# df_algorithm_error_counts[\"- ALL -\"] = df_algorithm_error_counts.sum(axis=1)\n",
    "# df_algorithm_error_counts.columns = [\n",
    "#     c.split(\" \")[1] for c in df_algorithm_error_counts.columns]\n",
    "\n",
    "\n",
    "# def get_error_count(algo, tpe=\"ERROR\"):\n",
    "#     if relative:\n",
    "#         return df_algorithm_error_counts.loc[algo, tpe] / df_algorithm_error_counts.loc[algo, \"ALL\"]\n",
    "#     else:\n",
    "#         return df_algorithm_error_counts.loc[algo, tpe]\n",
    "\n",
    "# # overview = []\n",
    "# # for d, l, a in df_asl.index:\n",
    "# #     overview.append([d,l,a])\n",
    "# # df_overview_table = pd.DataFrame(overview, columns=[\"dimensionality\", \"learning type\", \"algorithm\"])\n",
    "# # df_overview_table[\"# TIMEOUT\"] = df_overview_table[\"algorithm\"].apply(get_error_count, tpe=\"TIMEOUT\")\n",
    "# # df_overview_table[\"# OOM\"] = df_overview_table[\"algorithm\"].apply(get_error_count, tpe=\"OOM\")\n",
    "# # df_overview_table[\"# ERROR\"] = df_overview_table[\"algorithm\"].apply(get_error_count, tpe=\"ERROR\")\n",
    "# # df_overview_table[\"algorithm\"] = df_overview_table[\"algorithm\"].apply(lambda algo: algo_meta[algo][\"display_name\"])\n",
    "\n",
    "# # percent_format = \"{:03.0%}\"\n",
    "# # with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "# #     display(df_overview_table.style.format({\"# TIMEOUT\": percent_format, \"# OOM\": percent_format, \"# ERROR\": percent_format}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fliers = False\n",
    "# show_labels = False\n",
    "# title = \"Overall algorithm quality\"\n",
    "# df_boxplot = df_asl.T\n",
    "\n",
    "# labels = df_boxplot.columns\n",
    "# labels = [f\"{c[2]}\" for c in labels]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(20, 20), sharey=True)\n",
    "# ax = axs[0]\n",
    "# ax.boxplot([df_boxplot[c].dropna().values for c in df_boxplot.columns], labels=labels,\n",
    "#            sym=None if fliers else \"\", vert=False, meanline=True, showmeans=True, showfliers=fliers, manage_ticks=True)\n",
    "# ax.set_xlabel(\"ROC_AUC\")\n",
    "# ax.set_title(\"ROC_AUC all datasets\")\n",
    "# ax.set_xlim(-0.05, 1.05)\n",
    "# if not show_labels:\n",
    "#     ax.tick_params(axis=\"y\", which=\"both\", left=True,\n",
    "#                    right=False, labelleft=False, labelright=False)\n",
    "\n",
    "# ax = axs[1]\n",
    "# ax.boxplot([df_asl_pr.T[c].dropna().values for c in df_boxplot.columns], labels=labels,\n",
    "#            sym=None if fliers else \"\", vert=False, meanline=True, showmeans=True, showfliers=fliers, manage_ticks=True)\n",
    "# ax.set_xlabel(\"PR_AUC\")\n",
    "# ax.set_title(\"PR_AUC all datasets\")\n",
    "# ax.set_xlim(-0.05, 1.05)\n",
    "# ax.tick_params(axis=\"y\", which=\"both\", left=False,\n",
    "#                right=False, labelleft=False, labelright=False)\n",
    "\n",
    "# ax = axs[2]\n",
    "# ax.boxplot([df_asl_rangepr.T[c].dropna().values for c in df_boxplot.columns], labels=labels,\n",
    "#            sym=None if fliers else \"\", vert=False, meanline=True, showmeans=True, showfliers=fliers, manage_ticks=True)\n",
    "# ax.set_xlabel(\"RANGE_PR_AUC\")\n",
    "# ax.set_title(\"RANGE_PR_AUC all datasets\")\n",
    "# ax.set_xlim(-0.05, 1.05)\n",
    "# ax.tick_params(axis=\"y\", which=\"both\", left=False,\n",
    "#                right=False, labelleft=False, labelright=False)\n",
    "\n",
    "# ax = axs[3]\n",
    "# ax.boxplot([df_asl_gt.T[c].dropna().values for c in df_boxplot.columns], labels=labels,\n",
    "#            sym=None if fliers else \"\", vert=False, meanline=True, showmeans=True, showfliers=fliers, manage_ticks=True)\n",
    "# ax.set_xlabel(\"ROC_AUC\")\n",
    "# # ax.xaxis.set_label_position(\"top\")\n",
    "# ax.set_title(\"ROC_AUC GutenTAG datasets only\")\n",
    "# ax.set_xlim(-0.05, 1.05)\n",
    "# ax.tick_params(axis=\"y\", which=\"both\", left=False,\n",
    "#                right=False, labelleft=False, labelright=False)\n",
    "\n",
    "# # ax = axs[3]\n",
    "# # ax.axis(\"tight\")\n",
    "# # ax.set_axis_off()\n",
    "# table = ax.table(cellText=df_boxplot.columns,\n",
    "#                  loc=\"center\", edges=\"open\", fontsize=200)\n",
    "\n",
    "# # fig.tight_layout()\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = RESULT_DF.groupby(by=\"algorithm\")[[\"ROC_AUC\"]].mean()\n",
    "# df_tmp = df_tmp.sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "# df_tmp[\"Rank\"] = df_tmp.rank(ascending=False)\n",
    "\n",
    "# df_tmp_gt = RESULT_DF[RESULT_DF[\"collection\"] == \"custom\"].groupby(by=\"algo_display_name\")[\n",
    "#     [\"ROC_AUC\"]].mean()\n",
    "# df_tmp_gt = df_tmp_gt.sort_values(by=\"ROC_AUC\", ascending=False)\n",
    "# df_tmp_gt[\"Rank\"] = df_tmp_gt.rank(ascending=False)\n",
    "# df_tmp = pd.merge(df_tmp_gt, df_tmp, left_index=True,\n",
    "#                   right_index=True, how=\"inner\", suffixes=(\"_gt\", \"_all\"))\n",
    "# df_tmp[\"Diff_rank\"] = df_tmp[\"Rank_all\"] - df_tmp[\"Rank_gt\"]\n",
    "# df_tmp[\"Diff ROC_AUC\"] = df_tmp[\"ROC_AUC_all\"] - df_tmp[\"ROC_AUC_gt\"]\n",
    "# df_tmp.sort_values(\"Diff ROC_AUC\").style.format(\n",
    "#     {\"Diff_rank\": \"{:+03.0f}\".format, \"Diff ROC_AUC\": \"{:+0.2f}\".format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability of our metric scores and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_family_colormap = plt.get_cmap(\"Dark2\")\n",
    "# learning_type_colormap = plt.get_cmap(\"tab20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def method_family_marker_map(family):\n",
    "#     mapping = {\n",
    "#         \"encoding\": \"v\",\n",
    "#         \"distance\": \"o\",\n",
    "#         \"distribution\": \"^\",\n",
    "#         \"forecasting\": \"*\",\n",
    "#         \"reconstruction\": \"s\",\n",
    "#         \"trees\": \"P\"\n",
    "#     }\n",
    "#     return mapping[family]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colormap = method_family_colormap\n",
    "# method_families = sorted(\n",
    "#     set([algo_meta[algo][\"method_family\"] for algo in algo_meta]) - {\"baseline\"})\n",
    "# # annotated_algorithms = [\n",
    "# #     \"LOF\",\n",
    "# #     \"KNN\",\n",
    "# #     \"KMeans\",\n",
    "# #     \"PCC\",\n",
    "# #     \"iForest\",\n",
    "# #     \"IF-LOF\",\n",
    "# #     \"LSTM-AD\",\n",
    "# #     \"Roburst PCA\",\n",
    "# #     \"FastMCD\",\n",
    "# #     \"EncDec-AD\",\n",
    "# #     \"DeepAnT\",\n",
    "# # ]\n",
    "\n",
    "# # annotated_algorithms_supervised = []\n",
    "# # annotated_algorithms_unsupervised = []\n",
    "\n",
    "# cycler = plt.cycler(marker=[\"o\", \"+\", \"*\", \"x\", \".\", \"X\"]) * \\\n",
    "#     plt.cycler(color=plt.get_cmap(\"tab20\").colors)\n",
    "\n",
    "# algo_auroc = df_asl.T.mean()\n",
    "# dataset_count_lut = df_algorithm_error_counts[\"OK\"] / \\\n",
    "#     df_algorithm_error_counts[\"ALL\"]\n",
    "\n",
    "\n",
    "# df_tmp = algo_auroc.copy().to_frame(\"auroc\")\n",
    "# df_tmp.loc[:, \"reliability\"] = df_tmp.index.map(\n",
    "#     lambda x: dataset_count_lut[x[2]])\n",
    "# df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def percentage_above(pct):\n",
    "#     print(\n",
    "#         f\"{len(df_tmp[df_tmp.reliability > pct]) / len(df_tmp):.0%} of all algorithms reached a reliability of above {pct}.\")\n",
    "\n",
    "\n",
    "# def percentage_below(pct):\n",
    "#     print(f\"{len(df_tmp[df_tmp.reliability < pct]) / len(df_tmp):.0%} ({len(df_tmp[df_tmp.reliability < pct])}) of all algorithms reached a reliability of below {pct}.\")\n",
    "\n",
    "\n",
    "# percentage_above(0.7)\n",
    "# percentage_above(0.99)\n",
    "\n",
    "\n",
    "# def algorithm_auroc_reliability(alg):\n",
    "#     s = df_tmp.xs(alg, level=\"algorithm\")\n",
    "#     print(\n",
    "#         f\"{alg} has a ROC_AUC of {s.auroc.iloc[0]:.2f} with a reliability of {s.reliability.iloc[0]:.0%}.\")\n",
    "\n",
    "\n",
    "# algorithm_auroc_reliability(\"LOF\")\n",
    "# algorithm_auroc_reliability(\"Roburst PCA\")\n",
    "# algorithm_auroc_reliability(\"EncDec-AD\")\n",
    "# algorithm_auroc_reliability(\"KNN\")\n",
    "\n",
    "# percentage_below(0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(double_column_figwidth, 10))\n",
    "# ax = fig.gca()\n",
    "# ax.set_prop_cycle(cycler)\n",
    "\n",
    "# scatter_plots = defaultdict(lambda: defaultdict(list))\n",
    "# algo_positions = {}\n",
    "\n",
    "# for (_, _, algo), r in df_tmp.iterrows():\n",
    "#     algo_metadata = algo_meta[algo]\n",
    "#     name = algo_metadata[\"display_name\"]\n",
    "#     method_family = algo_metadata[\"method_family\"]\n",
    "#     x = r.auroc\n",
    "#     y = r.reliability\n",
    "#     scatter_plots[method_family][\"xs\"].append(x)\n",
    "#     scatter_plots[method_family][\"ys\"].append(y)\n",
    "#     scatter_plots[method_family][\"cs\"].append(\n",
    "#         colormap(method_families.index(method_family)))\n",
    "#     if name in algorithm_names:\n",
    "#         algo_positions[name] = (x, y)\n",
    "\n",
    "# for family, scatter in scatter_plots.items():\n",
    "#     xs = scatter[\"xs\"]\n",
    "#     ys = scatter[\"ys\"]\n",
    "#     cs = scatter[\"cs\"]\n",
    "#     ax.scatter(xs, ys, color=cs, s=40, marker=method_family_marker_map(family))\n",
    "\n",
    "# ax.legend(handles=[\n",
    "#     Line2D([0], [0], color=\"w\", marker=method_family_marker_map(\n",
    "#         fam), markerfacecolor=colormap(i), markeredgecolor=colormap(i), label=fam, markersize=8)\n",
    "#     for i, fam in enumerate(method_families)\n",
    "# ], loc=\"upper right\")\n",
    "\n",
    "# for a in algorithm_names:\n",
    "#     ha = \"left\"\n",
    "#     text_position = (2, -15)\n",
    "#     # if a in [\"LSTM-AD\", \"EncDec-AD\", \"IF-LOF\", \"KNN\"]:\n",
    "#     #     text_position = (0, -15)\n",
    "#     # if a in [\"KMeans\"]:\n",
    "#     #     ha = \"right\"\n",
    "#     ax.annotate(\n",
    "#         a, algo_positions[a], textcoords=\"offset points\", xytext=text_position, ha=ha)\n",
    "\n",
    "# # add vline to separate bad and good algos\n",
    "# # ax.vlines([0.75], 0, 1, colors=\"grey\", linestyles=\"dashed\")\n",
    "# ax.hlines([0.5], 0.45, 1, colors=\"grey\", linestyles=\"dashed\")\n",
    "# ax.set_xlabel(\"AUC-ROC\")\n",
    "# ax.set_ylabel(\"Successfully processed algorithms on MOVE II dataset (relative)\")\n",
    "# # ax.set_title(\"Reliability of the ROC_AUC values\")\n",
    "# ax.set_xlim(0.44, 1.065)\n",
    "# ax.set_ylim(-0.05, 1.08)\n",
    "# # ax.legend(ncol=2, loc=\"upper left\", bbox_to_anchor=(1, 1.01))\n",
    "# fig.savefig(r\"plots/reliability.pdf\", bbox_inches=\"tight\")\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
