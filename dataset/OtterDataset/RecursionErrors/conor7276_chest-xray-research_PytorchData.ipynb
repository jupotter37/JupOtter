{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86ee9a09-ca9b-4bf9-ae86-b3eaf94385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2788f63-a0e4-4c2a-85f0-e9e1e9bf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/chest_xray_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e039d761-4b6a-4214-9fe8-faf0f0bd8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4adfdb-2853-4aa3-95b5-2570288068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d232810f-1a44-4e54-a88f-e1c1b63e249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(X_train.values, dtype= torch.int16)\n",
    "test_tensor = torch.tensor(y_test.values, dtype = torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70aa980e-2123-4874-92bf-ef4f8adf008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4,   3,   3,  ...,   0,   0,   0],\n",
       "        [  0,  13,  45,  ...,   0,   0,   0],\n",
       "        [ 20,  20,  20,  ...,  20,  20,  20],\n",
       "        ...,\n",
       "        [ 92, 102, 179,  ...,  23,  24,  23],\n",
       "        [ 52,  65,  73,  ...,   0,   0,   0],\n",
       "        [ 10,  23,  62,  ...,  24,  26,  37]], dtype=torch.int16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e094d1d-9a61-45cc-8dd3-c1f0f1746601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c49a7599-ef06-4655-8b5d-78c651de9a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
       "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
       "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
       "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
       "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
       "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
       "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65cbe147-35b5-441a-bc5e-7e30d7ecaf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7185290-c5b5-438f-8e20-57ed271a82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f65d7306-3fa2-4e85-a7e5-89f310b2c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "class ChestxrayImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=pil_transform_tensor, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c28a7f39-af79-4efa-85ef-d2d81f2a670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\train_images\\\\train_images\"\n",
    "annotations_file = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\labels_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7b3e41b-5e7b-4bb1-b55f-bb14ed6e4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_dataset = ChestxrayImageDataset(annotations_file=annotations_file, img_dir=img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e82414f7-45a0-4de4-8ff1-40c31793533b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: ChestxrayImageDataset.__getitem__ at line 31 (1478 times), pil_transform_tensor at line 2 (1478 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 27\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 27\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m read_image(img_path)\n\u001b[0;32m     29\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4002\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;124;03mQuickly retrieve single value at passed column and index.\u001b[39;00m\n\u001b[0;32m   3985\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m`self.columns._index_as_unique`; Caller is responsible for checking.\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m takeable:\n\u001b[1;32m-> 4002\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[0;32m   4005\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(col)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   3801\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[0;32m   3803\u001b[0m col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39miget(i)\n\u001b[1;32m-> 3804\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_box_col_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_as_cached(label, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4392\u001b[0m, in \u001b[0;36mDataFrame._box_col_values\u001b[1;34m(self, values, loc)\u001b[0m\n\u001b[0;32m   4390\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n\u001b[0;32m   4391\u001b[0m \u001b[38;5;66;03m# We get index=self.index bc values is a SingleDataManager\u001b[39;00m\n\u001b[1;32m-> 4392\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4393\u001b[0m obj\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   4394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:656\u001b[0m, in \u001b[0;36mDataFrame._constructor_sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced \u001b[38;5;129;01mis\u001b[39;00m Series:\n\u001b[1;32m--> 656\u001b[0m         ser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m         ser\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:652\u001b[0m, in \u001b[0;36mDataFrame._sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:352\u001b[0m, in \u001b[0;36mNDFrame._from_mgr\u001b[1;34m(cls, mgr, axes)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mConstruct a new object of this type from a Manager object and axes.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03min the event that axes are refactored out of the Manager objects.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:279\u001b[0m, in \u001b[0;36mNDFrame.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_item_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mFlags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\flags.py:55\u001b[0m, in \u001b[0;36mFlags.__init__\u001b[1;34m(self, obj, allows_duplicate_labels)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: NDFrame, \u001b[38;5;241m*\u001b[39m, allows_duplicate_labels: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "chest_dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3ee334-1623-4826-ba0d-3eeea74380c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chest_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PIL method\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chest_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# PIL method\n",
    "test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "test_img[0][0].shape # access image in tensor\n",
    "test_img = test_img[0][0] # image\n",
    "test_img.shape\n",
    "test_img = np.array(test_img) # turn to np array\n",
    "test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "img_gray = ImageOps.grayscale(test_pil_img)\n",
    "image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "image_resized\n",
    "image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "image_resized_tensor\n",
    "image_resized_tensor[0]\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(image_resized_tensor[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d48af6-a227-49ee-a61b-e9da4dd9cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_transform_tensor(tensor_img):\n",
    "    # test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "    # test_img[0][0].shape # access image in tensor\n",
    "    test_img = test_img[0][0] # image\n",
    "    # test_img.shape\n",
    "    test_img = np.array(test_img) # turn to np array\n",
    "    test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "    img_gray = ImageOps.grayscale(test_pil_img)\n",
    "    image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "    # image_resized\n",
    "    image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "    # image_resized_tensor\n",
    "    return image_resized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90489696-f947-45c7-889d-36e40ad57ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd7c0-49f0-4a63-8e8e-b7c96c4b22f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41e763af-9992-4e3a-a3ba-1c4e094b76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        # # get all pixel columns\n",
    "        # selected_features = list(df.columns)\n",
    "        # selected_features.remove(\"file_name\")\n",
    "        # selected_features.remove(\"class_id\")\n",
    "        \n",
    "        # # Set train and target\n",
    "        # X = df[selected_features]\n",
    "        # y = df['class_id']\n",
    "        \n",
    "        # # Get training and test data\n",
    "        # X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "        # # Turn into np arrays \n",
    "        # X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)\n",
    "        # y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)\n",
    "\n",
    "        # # Turn into PyTorch Sensors\n",
    "        # X_train_torch = torch.from_numpy(X_train_np)\n",
    "        # y_test_torch = torch.from_numpy(y_test_np)\n",
    "\n",
    "        # self.X = X_train_torch\n",
    "        # self.y = y_test_torch\n",
    "\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "        \n",
    "        # if train == True:\n",
    "        #     X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # else:\n",
    "        #     y_train, y_test = y.iloc[train_indices], y[test_indices]\n",
    "        \n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "        \n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806b5a98-38bd-4686-a278-2eebafc6f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c970b6d6-43d0-4684-8aec-53b837d2175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>file_name</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002194571005371555.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002972834724824498.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>img_1004160693662088646.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1011159426506457600.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>img_1014387197248837154.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       4       3       3       2       1       0       0       0       0   \n",
       "1       0      13      45      71      86      95     109     103      94   \n",
       "2      20      20      20      20      20      20      20      20      20   \n",
       "3       1      11      27      49      83     107     118     123     131   \n",
       "4      47      77     117      79     140      64      55      62      69   \n",
       "\n",
       "   pixel9  ...  pixel4088  pixel4089  pixel4090  pixel4091  pixel4092  \\\n",
       "0       2  ...        108         41          0          0          0   \n",
       "1      96  ...         99         33          0          0          0   \n",
       "2      20  ...         53         13         21         20         20   \n",
       "3     140  ...        149        135        112         99         65   \n",
       "4      67  ...        136        117         68         51         36   \n",
       "\n",
       "   pixel4093  pixel4094  pixel4095                    file_name  class_id  \n",
       "0          0          0          0  img_1002194571005371555.jpg         1  \n",
       "1          0          0          0  img_1002972834724824498.jpg         1  \n",
       "2         20         20         20  img_1004160693662088646.jpg         0  \n",
       "3         32         10          0  img_1011159426506457600.jpg         2  \n",
       "4         17         27         36  img_1014387197248837154.jpg         1  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92afaa7d-6d2e-46c3-9327-0525fb0f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc7aa4c-c39e-4980-b6a8-249f95ee57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc06ae7f-1f50-49eb-92c2-45e497cc69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aecfb10-6bd3-4650-b007-98aa3d65ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4205, 4096)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a2bc951-718f-48d4-9767-37167241002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973af36e-595e-4928-87a0-76cb0ca4c448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b2233d2-0c1e-4c96-b0c9-963c69ff1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f91ad91a-8944-44d8-ac76-42f5e6c54d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016b5ac1-df21-421f-a9d4-f1a28eb25f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_torch = torch.from_numpy(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "786db0ea-9c4f-4129-8ac0-1c329e446406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505144b4-974f-4bad-95fc-4d2cc8c335c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chest_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChestXRayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mChestXRayDataset.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m flattened_img \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[row,\u001b[38;5;241m0\u001b[39m:columns_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;66;03m# skip class_id\u001b[39;00m\n\u001b[0;32m     70\u001b[0m class_id \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[row,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# get class_id\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m matrix_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# turn flattened image back into 2d image\u001b[39;00m\n\u001b[0;32m     74\u001b[0m df_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_X, [matrix_img],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# append image\u001b[39;00m\n\u001b[0;32m     75\u001b[0m df_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_y, class_id) \u001b[38;5;66;03m# append class\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1469\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(a, new_shape)\u001b[0m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_shape, (\u001b[38;5;28mint\u001b[39m, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m   1467\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m (new_shape,)\n\u001b[1;32m-> 1469\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1471\u001b[0m new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim_length \u001b[38;5;129;01min\u001b[39;00m new_shape:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1874\u001b[0m, in \u001b[0;36mravel\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chest_dataset = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefda4a-750c-4ebb-abec-18eac9b8ba52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chest_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec14a3-6092-4b2b-9ff0-9f79f4a67f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_dataset, batch_size = 100, shuffle = True, num_workers = 2)\n",
    "test_dataloader = DataLoader(dataset = chest_dataset, batch_size = 5000, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffac4dd3-67bc-41f6-8cf1-dce746cfbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn pandas df into np array\n",
    "#dimensions [[64,64],1] [[64x64 image], class_id]\n",
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a4f2a11-76e9-4d06-b917-9bfd52a4a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = np.array([],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0b5a10-cc3c-4b9d-8d5a-e7e536dfa170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7ccb023-e4e1-4472-ba97-486e562b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"file_name\"]) # unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7da4b67f-ac74-4d27-a5fe-c6f976fcb81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4087</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>203</td>\n",
       "      <td>186</td>\n",
       "      <td>167</td>\n",
       "      <td>63</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>77</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>103</td>\n",
       "      <td>115</td>\n",
       "      <td>126</td>\n",
       "      <td>121</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>161</td>\n",
       "      <td>124</td>\n",
       "      <td>60</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>92</td>\n",
       "      <td>102</td>\n",
       "      <td>179</td>\n",
       "      <td>120</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>52</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>80</td>\n",
       "      <td>104</td>\n",
       "      <td>133</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>171</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "      <td>148</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4672 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          4       3       3       2       1       0       0       0       0   \n",
       "1          0      13      45      71      86      95     109     103      94   \n",
       "2         20      20      20      20      20      20      20      20      20   \n",
       "3          1      11      27      49      83     107     118     123     131   \n",
       "4         47      77     117      79     140      64      55      62      69   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4667     203     186     167      63      30      35      41      52      51   \n",
       "4668      88      90      93     103     115     126     121     126     128   \n",
       "4669      92     102     179     120      73      73      83      83      74   \n",
       "4670      52      65      73      81      87      80     104     133     175   \n",
       "4671      10      23      62      57      67      68      70      72      73   \n",
       "\n",
       "      pixel9  ...  pixel4087  pixel4088  pixel4089  pixel4090  pixel4091  \\\n",
       "0          2  ...        135        108         41          0          0   \n",
       "1         96  ...        129         99         33          0          0   \n",
       "2         20  ...        110         53         13         21         20   \n",
       "3        140  ...        130        149        135        112         99   \n",
       "4         67  ...        138        136        117         68         51   \n",
       "...      ...  ...        ...        ...        ...        ...        ...   \n",
       "4667      50  ...         86         95         77         48         17   \n",
       "4668     129  ...        192        186        177        161        124   \n",
       "4669      67  ...         99         51         10         17         22   \n",
       "4670     203  ...        179        170        171        132         93   \n",
       "4671      75  ...        188        185        178        148         83   \n",
       "\n",
       "      pixel4092  pixel4093  pixel4094  pixel4095  class_id  \n",
       "0             0          0          0          0         1  \n",
       "1             0          0          0          0         1  \n",
       "2            20         20         20         20         0  \n",
       "3            65         32         10          0         2  \n",
       "4            36         17         27         36         1  \n",
       "...         ...        ...        ...        ...       ...  \n",
       "4667          9         15         16         18         1  \n",
       "4668         60         36         48         51         1  \n",
       "4669         24         23         24         23         1  \n",
       "4670         40          0          0          0         0  \n",
       "4671         31         24         26         37         1  \n",
       "\n",
       "[4672 rows x 4097 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c17dce9-a425-403e-867c-adf20d9c0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "print(columns_len)\n",
    "\n",
    "# We have to get the first image in order to initalize array\n",
    "first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "first_img = np.resize(first_img,(64,64))\n",
    "\n",
    "# Get first class to initalize array\n",
    "second = df.loc[0,\"class_id\"]\n",
    "\n",
    "df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "\n",
    "df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "\n",
    "for row in range(1,len(df)):\n",
    "    \n",
    "    flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "    class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "    matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "\n",
    "    df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "    df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b3a5a02-bb63-4490-a80c-ba8cf806d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672, 64, 64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5514a5b-deae-45ea-bdcd-00324500665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ea1074cf-f930-4feb-a96b-ecf5deee30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = df_X[train_indices,:], df_X[test_indices,:] \n",
    "y_train, y_test = df_y[train_indices], df_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4564017d-bc0a-42a0-9a30-fac2198503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4205, 64, 64) (467, 64, 64) (4205,) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a63a626-341a-4d9a-8b75-6431c2d8b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2dff90c7-6af5-4026-b7d4-2aab34a49d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b25b160c-c768-4cf5-b31d-9795f85fc3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467, 64, 64])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch = torch.from_numpy(X_test)\n",
    "X_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7787a457-e0af-441c-8231-bf81a432da35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
       "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
       "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
       "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
       "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
       "         ...,\n",
       "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
       "\n",
       "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
       "         [  0,   2,  26,  ...,  39,  14,   1],\n",
       "         [  0,   0,  21,  ...,  47,  14,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
       "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
       "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
       "         ...,\n",
       "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
       "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
       "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
       "\n",
       "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
       "         [  1,   0,  33,  ...,   0,   0,   0],\n",
       "         [  5,   0,  29,  ...,   0,   0,   4],\n",
       "         ...,\n",
       "         [  5,   0,   0,  ...,   0,   0,   5],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
       "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
       "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
       "         ...,\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 30,  28,  26,  ...,  22,  23,  24]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54ed148-3e32-488f-aefe-a8785092d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "#Testing New Class\n",
    "chest_xray = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee9d896-bce6-44c0-8d1d-f64f7786dc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chest_xray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c4e734-c1e1-4b7d-bc62-37bf62edbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_train = chest_xray[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4db8bd-9e46-43dc-8a52-93fb1e9fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4205, 64, 64]) (4205,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_train[0].shape, chest_xray_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27acb10a-314c-4427-befb-919b7667d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_test = chest_xray[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7506351d-6cbf-4653-9edc-40b3d43c6e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64]) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_test[0].shape, chest_xray_test[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "444ce630-36cd-4e9f-99ed-48bf5398ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray_train, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b11acb0-437c-4fe3-8dee-9c6795b8da96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e54e007d90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f914543b-66dd-42b3-8fb2-e64377acce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataiter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d663e579-8c0f-4705-a35a-da047fecd376",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92075df2-ad1d-43d4-a591-3383e7e0db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8866d93a-61e1-4a8e-8039-ab21c11c5522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 65,  72,  86,  ...,  24,  19,  12],\n",
       "         [ 74,  86,  86,  ...,  29,  25,  22],\n",
       "         [ 78,  91,  94,  ...,  30,  28,  24],\n",
       "         ...,\n",
       "         [ 45,  43,  35,  ...,  44,  45,  46],\n",
       "         [ 44,  44,  33,  ...,  45,  46,  46],\n",
       "         [ 44,  43,  31,  ...,  45,  46,  46]],\n",
       "\n",
       "        [[ 48,  61,  59,  ...,  61,  50,  30],\n",
       "         [ 38,  58,  60,  ...,  57,  42,  19],\n",
       "         [ 25,  52,  62,  ...,  51,  40,  15],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   4,   0,   0],\n",
       "         [  0,   0,   0,  ...,   6,   0,   0],\n",
       "         [  0,   0,   0,  ...,   9,   0,   0]],\n",
       "\n",
       "        [[  8,  34,  51,  ..., 192, 195, 211],\n",
       "         [ 12,  48,  48,  ..., 186, 189, 203],\n",
       "         [ 39,  48,  49,  ..., 188, 191, 195],\n",
       "         ...,\n",
       "         [ 32,  30,  29,  ...,  28,  29,  33],\n",
       "         [ 32,  30,  29,  ...,  28,  28,  32],\n",
       "         [ 32,  30,  29,  ...,  28,  29,  31]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[122, 121, 141,  ...,   2,   6,  12],\n",
       "         [146, 129, 118,  ...,  20,  26,  31],\n",
       "         [ 87, 145, 141,  ...,  40,  45,  50],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 18,  34,  48,  ...,  85,  84,  89],\n",
       "         [ 31,  43,  57,  ...,  92, 100,  99],\n",
       "         [ 41,  58,  83,  ..., 109, 112, 104],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 55,  68,  85,  ...,  53,  52,  44],\n",
       "         [ 57,  64,  72,  ...,  56,  47,  44],\n",
       "         [ 59,  69,  79,  ...,  61,  51,  49],\n",
       "         ...,\n",
       "         [ 21,  14,  14,  ...,  14,  20,  26],\n",
       "         [ 19,  14,  16,  ...,  17,  16,  25],\n",
       "         [ 19,  13,  18,  ...,  22,  13,  26]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc5e629-1563-4141-82fa-3f5d44f7e4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,  17,  60,  ..., 121, 120,  93],\n",
      "         [  0,  15,  57,  ..., 117, 111,  91],\n",
      "         [  0,  13,  54,  ..., 118, 108,  87],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 19,  19,  15,  ...,  48,  45,  36],\n",
      "         [ 20,  19,   3,  ...,  43,  45,  33],\n",
      "         [ 16,  10,  16,  ...,  45,  37,  90],\n",
      "         ...,\n",
      "         [ 30,  29,  29,  ...,  28,  29,  30],\n",
      "         [ 30,  30,  29,  ...,  28,  29,  30],\n",
      "         [ 32,  29,  29,  ...,  29,  28,  32]],\n",
      "\n",
      "        [[ 40,  44,  44,  ...,  16,  23,  14],\n",
      "         [ 39,  43,  49,  ...,  14,  20,  14],\n",
      "         [ 41,  46,  55,  ...,  11,  16,  16],\n",
      "         ...,\n",
      "         [ 26,  25,  18,  ...,  27,  27,  28],\n",
      "         [ 26,  25,  17,  ...,  27,  27,  28],\n",
      "         [ 25,  24,  16,  ...,  27,  28,  28]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[117, 115, 122,  ..., 102, 114, 132],\n",
      "         [130, 123, 127,  ..., 110, 120, 128],\n",
      "         [121, 140, 134,  ..., 110, 120, 127],\n",
      "         ...,\n",
      "         [  3,  30,  58,  ...,  10,   0,   0],\n",
      "         [  3,  30,  61,  ...,  12,   0,   0],\n",
      "         [  5,  31,  62,  ...,  15,   0,   1]],\n",
      "\n",
      "        [[116,  88,  80,  ..., 147, 130, 120],\n",
      "         [ 97, 103, 115,  ..., 153, 151, 138],\n",
      "         [ 87,  92, 110,  ..., 145, 169, 153],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 34,  48,  56,  ...,  64,  47, 155],\n",
      "         [ 48,  59,  64,  ...,  79,  75, 172],\n",
      "         [ 48,  63,  71,  ...,  37,  41, 144],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ..., 117,  33,   1],\n",
      "         [  0,   0,   0,  ...,  99,  36,   2],\n",
      "         [  0,   0,   0,  ...,  89,  37,   3]]]) tensor([0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 2, 0, 2, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 0,\n",
      "        1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0,\n",
      "        0, 0, 2, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 2,\n",
      "        1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ef92e41-37c9-48cf-8b36-cefefac420ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "# Test if train = True and Train = False works\n",
    "chest_xray = ChestXRayDataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5829ecd4-a130-48bb-ae3e-035b03beac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffb4bc3b-54fe-4f16-a599-c46d8fe3d6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34639f73-ab63-4764-ab33-09445ee2f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray, batch_size = 100, shuffle = True)\n",
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ccd7686-f592-4d0d-a3e7-901bf8d981e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 32, 41,  ..., 83, 80, 76],\n",
       "        [32, 47, 58,  ..., 82, 75, 68],\n",
       "        [39, 57, 72,  ..., 80, 71, 59],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ..., 17,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 23,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 27,  0,  1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a93895ac-2cea-4424-a6af-db9453992995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 31,  40,  53,  ...,  18,  23,  23],\n",
      "         [ 35,  46,  60,  ...,  14,  16,  23],\n",
      "         [ 37,  54,  69,  ...,  32,  14,  11],\n",
      "         ...,\n",
      "         [ 19,  13,  18,  ...,  22,  23,  24],\n",
      "         [ 19,  13,  19,  ...,  23,  24,  25],\n",
      "         [ 18,  13,  19,  ...,  24,  27,  25]],\n",
      "\n",
      "        [[ 77,  83,  92,  ...,   0,   0,   0],\n",
      "         [128, 101,  91,  ...,  10,  18,  27],\n",
      "         [137, 150, 128,  ...,  45,  51,  57],\n",
      "         ...,\n",
      "         [  1,  25,  64,  ...,   2,   0,   0],\n",
      "         [  2,  26,  74,  ...,   3,   0,   0],\n",
      "         [  2,  28,  82,  ...,   5,   0,   0]],\n",
      "\n",
      "        [[  0,   0,   4,  ...,   0,   0,   0],\n",
      "         [  0,   0,   8,  ...,   0,   2,   0],\n",
      "         [  0,   0,  14,  ...,   0,   5,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 55,  76,  93,  ...,  78,  73,  54],\n",
      "         [ 52,  74,  89,  ...,  79,  69,  49],\n",
      "         [ 48,  72,  88,  ...,  85,  67,  45],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   9,   0,   1],\n",
      "         [  0,   1,   0,  ...,  11,   0,   1],\n",
      "         [  0,   1,   0,  ...,  12,   0,   1]],\n",
      "\n",
      "        [[ 18,  36,  61,  ...,  59,  44,  39],\n",
      "         [ 56,  62,  42,  ...,  41,  39,  44],\n",
      "         [ 41,  39,  45,  ...,  41,  38,  51],\n",
      "         ...,\n",
      "         [ 33,  31,  32,  ...,  13,  21,  21],\n",
      "         [ 33,  30,  31,  ...,  13,  21,  22],\n",
      "         [ 34,  30,  30,  ...,  13,  20,  22]],\n",
      "\n",
      "        [[ 88,  13,  26,  ..., 192, 204, 225],\n",
      "         [ 23,  17,  39,  ..., 193, 208, 224],\n",
      "         [ 17,  24,  41,  ..., 194, 208, 221],\n",
      "         ...,\n",
      "         [ 28,  25,  23,  ...,  25,  25,  30],\n",
      "         [ 27,  25,  23,  ...,  25,  25,  29],\n",
      "         [ 26,  25,  22,  ...,  25,  26,  27]]]) tensor([1, 0, 2, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 1,\n",
      "        2, 1, 0, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1,\n",
      "        0, 0, 2, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n",
      "        1, 0, 1, 1])\n",
      "torch.Size([100, 64, 64]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f3905e9-48d6-4a17-94c5-025d839a761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64])\n",
      "torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "chest_xray_validation = ChestXRayDataset(False)\n",
    "print(chest_xray_validation.X.shape)\n",
    "print(chest_xray_validation.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87771576-dccc-46d7-a20b-d6f6a9c9ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(dataset = chest_xray_validation, batch_size = 500)\n",
    "valid_dataiter = iter(validation_dataloader)\n",
    "valid_data = next(valid_dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f53f02b4-3a91-439d-81b5-152fcdf000e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
      "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
      "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
      "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
      "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
      "         ...,\n",
      "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
      "\n",
      "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
      "         [  0,   2,  26,  ...,  39,  14,   1],\n",
      "         [  0,   0,  21,  ...,  47,  14,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
      "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
      "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
      "         ...,\n",
      "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
      "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
      "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
      "\n",
      "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
      "         [  1,   0,  33,  ...,   0,   0,   0],\n",
      "         [  5,   0,  29,  ...,   0,   0,   4],\n",
      "         ...,\n",
      "         [  5,   0,   0,  ...,   0,   0,   5],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
      "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
      "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
      "         ...,\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 30,  28,  26,  ...,  22,  23,  24]]]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
      "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
      "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
      "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
      "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
      "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
      "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
      "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([467, 64, 64]) torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "for x,y in validation_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc203ee7-24b1-42f0-98d2-8d516c667f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN please work time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9331f960-9c7a-4a2c-94d1-44f9cc71ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Library\n",
    "import torch\n",
    "# PyTorch Neural Network\n",
    "import torch.nn as nn\n",
    "# Allows us to transform data\n",
    "import torchvision.transforms as transforms\n",
    "# Allows us to download the dataset\n",
    "import torchvision.datasets as dsets\n",
    "# Used to graph data and loss curves\n",
    "import matplotlib.pylab as plt\n",
    "# Allows us to use arrays to manipulate and store data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d2198d-716b-4ef1-8f2f-d1c7fd7ce1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the channels\n",
    "\n",
    "def plot_channels(W):\n",
    "    n_out = W.shape[0]\n",
    "    n_in = W.shape[1]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(n_out, n_in)\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "    out_index = 0\n",
    "    in_index = 0\n",
    "    \n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "        if in_index > n_in-1:\n",
    "            out_index = out_index + 1\n",
    "            in_index = 0\n",
    "        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index = in_index + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the parameters\n",
    "\n",
    "def plot_parameters(W, number_rows=1, name=\"\", i=0):\n",
    "    W = W.data[:, i, :, :]\n",
    "    n_filters = W.shape[0]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_filters:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.suptitle(name, fontsize=10)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the activations\n",
    "\n",
    "def plot_activations(A, number_rows=1, name=\"\", i=0):\n",
    "    A = A[0, :, :, :].detach().numpy()\n",
    "    n_activations = A.shape[0]\n",
    "    A_min = A.min().item()\n",
    "    A_max = A.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n",
    "    fig.subplots_adjust(hspace = 0.9)    \n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_activations:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cd973b1-a617-4db0-a1f3-c355d5ab14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        with open('static_data/train.pickle', 'rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "\n",
    "        with open('static_data/test.pickle', 'rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "        df = df.drop(columns=[\"file_name\"])\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            flattened_img = df.iloc[row, :-1].to_numpy()\n",
    "            matrix_img = np.resize(flattened_img, (64, 64))\n",
    "\n",
    "            X.append(matrix_img)\n",
    "            y.append(df.loc[row, \"class_id\"])\n",
    "\n",
    "        if train:\n",
    "            X = [X[i] for i in train_indices]\n",
    "            y = [y[i] for i in train_indices]\n",
    "        else:\n",
    "            X = [X[i] for i in test_indices]\n",
    "            y = [y[i] for i in test_indices]\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f96e4f2a-b3c8-4e10-a5d7-860c75440b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        # The reason we start with 1 channel is because we have a single black and white image\n",
    "        # Channel Width after this layer is 16\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        # Channel Wifth after this layer is 8\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Channel Width after this layer is 8\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        # Channel Width after this layer is 4\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n",
    "        # The output is a value for each class\n",
    "        # self.fc1 = nn.Linear(out_2 * 4 * 4, 3)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 3)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    # Outputs result of each stage of the CNN, relu, and pooling layers\n",
    "    def activations(self, x):\n",
    "        # Outputs activation this is not necessary\n",
    "        z1 = self.cnn1(x)\n",
    "        a1 = torch.relu(z1)\n",
    "        out = self.maxpool1(a1)\n",
    "        \n",
    "        z2 = self.cnn2(out)\n",
    "        a2 = torch.relu(z2)\n",
    "        out1 = self.maxpool2(a2)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return z1, a1, z2, a2, out1,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3162fdcb-166e-433a-9f41-d485b36f3e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAHACAYAAADdk3CpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdPklEQVR4nO3deViUZfcH8O+wyTq4IkgEQgKaK2qGZmJi+rrhT9MiUzG3ejXQ0tBXLXfD9FV7TSv3THJJUzNzF0UxTQ3NDTcUTNTKZBFZ5/79QUxOgHPGXGLm+7kurouZOXPf93meZ2YOz8wcNEopBSIiIrJoVo97AURERPT4sSAgIiIiFgRERETEgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIjAgoAeo7i4OGg0Gty6davMGI1Gg/Xr1z+yNZkiIiICXbt2fdzLKFVISAiGDRt2X/ddunQpKlas+LfXcO3aNbRt2xZOTk4PZLzHSXKs/tU/+fggKg0LAjJq2rRpaNq0KVxcXODm5oauXbsiKSnpcS+L/uFmzZqFtLQ0JCYm4uzZs4907kuXLkGj0SAxMfGBjNe8eXOkpaXB1dVVfJ85c+Zg6dKlD2R+okeBBQEZtWfPHgwZMgTff/89tm/fjvz8fLz44ou4ffv2417aI5GXl2dW8zwqFy5cQOPGjVGrVi24ubnd1xgPe5tIx7ezs4O7uzs0Go14bFdX13J/ZoQsCwsCMmrLli2IiIjA008/jQYNGmDp0qVISUnBkSNH9DEajQYLFy7E//3f/8HR0RG1atXCxo0bDcbZvHkz/P394eDggNatW+PSpUsmryU6Ohr+/v5wdHSEr68vxo0bh/z8fABFfxVaWVnh8OHDBveZPXs2vL29odPpAAAnTpzAv/71Lzg7O6N69ero3bs3fv31V318SEgIhg4dimHDhqFq1apo166daG0//PADqlWrhpiYGADArVu3MGDAAFSrVg1arRYvvPACjh07po8fP348GjZsiIULF6JmzZqwt7cHINuWxnL4q3nz5qFWrVqwt7dH9erV8dJLLxnNZ/369fr7tGvXDqmpqQa3b9iwAUFBQbC3t4evry8mTJiAgoICAICPjw/Wrl2Lzz//HBqNBhEREQCAlJQUhIWFwdnZGVqtFj179sT169eNbhNj2/KvatasCQBo1KgRNBoNQkJCAPx5Gn/KlCmoUaMGAgICAADLly9HkyZN4OLiAnd3d7z66qu4ceOGfry/vmVQ/LbK1q1bUbt2bTg7O6N9+/ZIS0vT3+evbxmEhIQgMjIS7777LipXrgx3d3eMHz/eYN1nzpzBc889B3t7e9SpUwc7duz4R79tRuaFBQGZLD09HQBQuXJlg+snTJiAnj174vjx4+jQoQN69eqFmzdvAgBSU1PRrVs3dO7cGYmJiRgwYABGjRpl8twuLi5YunQpTp06hTlz5mDBggWYNWsWgKIXodDQUCxZssTgPkuWLEFERASsrKxw69YtvPDCC2jUqBEOHz6MLVu24Pr16+jZs6fBfZYtWwY7Ozvs378fn3zyidF17dq1C23btsWUKVMQHR0NAOjRowdu3LiB7777DkeOHEFQUBDatGmj3yYAcP78eaxduxbr1q0zOL19r20pzaHY4cOHERkZiYkTJyIpKQlbtmzB888/f898srOzMWXKFHz++efYv38/bt26hVdeeUV/e3x8PPr06YOoqCicOnUKn376KZYuXYopU6YAKCqO2rdvj549eyItLQ1z5syBTqdDWFgYbt68iT179mD79u24ePEiXn75ZYO5S9smkm15t0OHDgEAduzYgbS0NKxbt05/286dO5GUlITt27dj06ZNAID8/HxMmjQJx44dw/r163Hp0iV9EXOvbTRjxgwsX74ce/fuRUpKCkaMGHHP+yxbtgxOTk44ePAgpk+fjokTJ2L79u0AgMLCQnTt2hWOjo44ePAgPvvsM4wZM+ae4xE9UIrIBIWFhapjx46qRYsWBtcDUGPHjtVfzsrKUgDUd999p5RSavTo0apOnToG94mOjlYA1O+//17mfADU119/XebtH374oWrcuLH+8qpVq1SlSpVUTk6OUkqpI0eOKI1Go5KTk5VSSk2aNEm9+OKLBmOkpqYqACopKUkppVSrVq1Uo0aNypyzWN++fVVYWJhat26dcnZ2VitXrtTfFh8fr7RarX4dxfz8/NSnn36qlFLq/fffV7a2turGjRslcr7XtpTmEBUVpZRSau3atUqr1aqMjAyjOSml1JIlSxQA9f333+uvO336tAKgDh48qJRSqk2bNmrq1KkG91u+fLny8PDQXw4LC1N9+/bVX962bZuytrZWKSkp+utOnjypAKhDhw6VuU0k2/KvkpOTFQD1448/Glzft29fVb16dZWbm3vPbfDDDz8oACozM1MppdTu3bsNjtXibXT+/Hn9fT7++GNVvXp1g7nCwsL0l1u1aqWee+45g3maNm2qoqOjlVJKfffdd8rGxkalpaXpb9++fbvRxwDRg8IzBGSSIUOG4MSJE1i5cmWJ2+rXr6//3cnJCVqtVn/a9fTp02jWrJlBfHBwsMnzr1q1Ci1atIC7uzucnZ0xduxYpKSk6G/v2rUrrK2t8fXXXwMoOrXbunVr+Pj4AACOHTuG3bt3w9nZWf8TGBgIoOg972KNGzcWrefgwYPo0aMHli9fbvCX7rFjx5CVlYUqVaoYzJWcnGwwj7e3N6pVq1Zi3HttS2kOxdq2bQtvb2/4+vqid+/eWLFiBbKzs++Zl42NDZo2baq/HBgYiIoVK+L06dP6NUycONFgDQMHDkRaWlqZY58+fRpeXl7w8vLSX1enTh2DcUvbJtJtKVWvXj3Y2dkZXHfkyBF07twZTz75JFxcXNCqVSsAMDi2/srR0RF+fn76yx4eHgZvM5Tm7v361/skJSXBy8sL7u7u+tufeeYZWVJED4DN414AlR9Dhw7Fpk2bsHfvXjzxxBMlbre1tTW4rNFo9O/bPwgHDhxAr169MGHCBLRr1w6urq5YuXIlZs6cqY+xs7NDnz59sGTJEnTr1g2xsbGYM2eO/vasrCx07txZ/z7/3Tw8PPS/Ozk5idbk5+eHKlWqYPHixejYsaN+G2RlZcHDwwNxcXEl7nP3B83Kmude21KaQzEXFxccPXoUcXFx2LZtG9577z2MHz8eP/zww31/6C0rKwsTJkxAt27dStxW/L7//frrNpFuy/sd//bt22jXrh3atWuHFStWoFq1akhJSUG7du3u+aHD0vaRUuqecz/sxwjR38GCgIxSSuGtt97C119/jbi4OP0HtkxRu3btEh+M+/77700aIyEhAd7e3gbvq16+fLlE3IABA1C3bl3MmzcPBQUFBi9aQUFBWLt2LXx8fGBj8/cP/6pVq2LdunUICQlBz549sXr1atja2iIoKAjXrl2DjY2N/uzEg3I/OdjY2CA0NBShoaF4//33UbFiRezatavUF3QAKCgowOHDh/V/oSYlJeHWrVuoXbu2fg1JSUl46qmnxOuuXbs2UlNTkZqaqj9LcOrUKdy6dQt16tS5Z76mbsviMwCFhYVGY8+cOYPffvsNH3zwgX5df/1g6qMQEBCA1NRUXL9+HdWrVwdQ9FkMokeFbxmQUUOGDMEXX3yB2NhYuLi44Nq1a7h27Rru3LkjHuONN97AuXPnMHLkSCQlJSE2Ntbk72jXqlULKSkpWLlyJS5cuICPPvpI/9bA3WrXro1nn30W0dHRCA8Ph4ODg0EuN2/eRHh4OH744QdcuHABW7duRb9+/UQvHqVxc3PDrl27cObMGYSHh6OgoAChoaEIDg5G165dsW3bNly6dAkJCQkYM2bM336xMTWHTZs24aOPPkJiYiIuX76Mzz//HDqdTv8J+9LY2trirbfewsGDB3HkyBFERETg2Wef1RcI7733Hj7//HNMmDABJ0+exOnTp7Fy5UqMHTu2zDFDQ0NRr1499OrVC0ePHsWhQ4fQp08ftGrVCk2aNLnn/Uzdlm5ubnBwcNB/4LL4g7ClefLJJ2FnZ4f//e9/uHjxIjZu3IhJkyaVGf+wtG3bFn5+fujbty+OHz+O/fv367enKV93JLpfLAjIqPnz5yM9PR0hISHw8PDQ/6xatUo8xpNPPom1a9di/fr1aNCgAT755BNMnTrVpHV06dIFw4cPx9ChQ9GwYUMkJCRg3Lhxpcb2798feXl5eP311w2ur1GjBvbv34/CwkK8+OKLqFevHoYNG4aKFSvCyur+Hw7u7u7YtWsXfvrpJ/Tq1Qs6nQ6bN2/G888/j379+sHf3x+vvPIKLl++rP/r736ZmkPFihWxbt06vPDCC6hduzY++eQTfPnll3j66afLnMPR0RHR0dF49dVX0aJFCzg7Oxvs73bt2mHTpk3Ytm0bmjZtimeffRazZs2Ct7d3mWNqNBps2LABlSpVwvPPP4/Q0FD4+voaPY40Go3J29LGxgYfffQRPv30U9SoUQNhYWFljl+tWjUsXboUa9asQZ06dfDBBx9gxowZ91zTw2BtbY3169cjKysLTZs2xYABA/Rnw/7u2zBEEhpl7E0vonJo0qRJWLNmDY4fP/64l0J03/bv34/nnnsO58+fN/gAI9HDwM8QkFnJysrCpUuXMHfuXEyePPlxL4fIJF9//TWcnZ1Rq1YtnD9/HlFRUWjRogWLAXok+JYBmZWhQ4eicePGCAkJKfF2AdE/XWZmJoYMGYLAwEBERESgadOm2LBhw+NeFlkIvmVAREREPENARERELAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgIgI0kSKfT4erVq3BxcYFGo3nYa3oklFLIzMxEjRo1YGVVdl1kybkD5pe/JecO8Li31H1vybkDPO6l+x5KIDU1VQEwy5/U1FTmboH5W3LukvwtOXdzzt+Sc5fkb8m5K6WU6AyBi4sLAGAiAHtB/LtIkAyL9BNaURwAuNaNl43Z93tRXEZeHry+/FKfW1mKbx8FoIJgXDvR7MDQ1auFkUCTMe1EcYdfeFcUl5GXB68lS4zmDvyZf6VKqbCyMr6/fvstUbSGX35pKIoDgGrVdopjjcsG8JpJuY8alQp7e+O59x3vKlpBTcwVxQHAxo29RXFduowSjpgHYJH4uN+zJxXOzsZz//hj2eyz4pvIAgGMbntYFFdlnmy75wKYDpi074ODU2FjYzz/PvGyNfQURRXxrpguijt5y/jcmQACYVruqfv2QevsbDTeteEBozFFzgjjgPbtJ4riVhV0F8VlFBTAKy5OfNynvv02tBWMP+PvmTZNNH8XDBTFFflZFBUT86UoLicnA++/7yXa96KCoPjUiT0AB9ESjB9EAKAVLPBPspm1dtKX5CLGTgsV314BsmJIOrvW0VEYCVhbywqnB5373TFWVlpRQSDe91p5MQg4mRArY0ru9vZaUUEgz0h2LAOAk5N0VEm5+ifpce/srBUVBNJDT2ttLQsEUKGCLHfJ4/Jupux7GxutqCCQPppNOeo1GuHj3qQx5blrnZ2Fz9HS41l+jNraCnPX2IrHBOTHvbZCBWjtjR9Z8mcmU56bZTlJnpPuJtn3/FAhERERsSAgIiIiFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEYWOiYoMrVIBW0Nxgoks90XgTl8rnVm8eFMVp5svHNEVTyJpQ7BOO99GFjuK5z5wZIQv0EnYCKygQz13st9/6QdIwQ418UjRe5IggE+Z+URSXk2M8JjMzA4GB4qkBAMOH5EGrzTMeaDtTNJ56rq54bk2zXrIx5zQTxWXk5MA1Wjw9nmzsKmp803u/kg1Y+03x3FknZHGddsrmvn07A+gi6yhYbNPnN6HVCh4v7ZuKxtP84C2e+5ffZb30nXca7+Spu30b6NJFPDcAYNo0Ucep+VguGi57pvAYAdDnHeH/ERg5UhaXmwvs2CGe/+i0aaIWay8sWiQbsP+z4rlV+hOiOI3rp8IR74jn5hkCIiIiYkFARERELAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgIJnYqnPHuDdjbG+9b9ssYWce2iIh35JOf6SoK81gv62qn02Xg+vVl4ulbBgdDa2N8c3U7sVc03u9RU8RzK/cVojjN9ivCETMAVBbPDwB9+y6BnZ3xfa/5UNZZ79gx+dyV2zSSBR45YjQkw1Enn/gPrtUGQNKlcfjwNaLx2jcTdmEDEB4u6+5mJ2xmqVQGAHmrwh8h69DZ3P2ibEBJO8k/BC2QbacXzrcWxWXcR4dO15pjABjv1peMH4QjThbPXQ11RHHq9GnjQXfk3eqKJf77Mzg7G3/Mv7lc1pkW7wwXz/22l5cobuGHH4riTM0+aPBgaAVdGjX9j4rG++CD18Vza1yjRHGX8ZEoLhOAtDcqzxAQERERCwIiIiJiQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBEREQwsXWxqyvg4GA8Tk2WtSfVeHcWzz1y5DeiuLS0G8IRM8VzA4DrgYEAHI3G1awpG69TpzHiuTXLXxVGStun3hbPXeyjlK6i1s2f1bokGq/bePnc6zrLjhNtReP1rVKm18ALFnwOR0fjLVx79ZIde1NNmNv/S1n73tgvvhDFZWRnw3WQfP4X8RokrXvV7Nmi8TT/6yGee5Y0UDg3srKAFi3E8wPANMTCXhDndlvWYnq7k7xtdYow7lRr43NnZWUAGCmeGwASWrmKcgc+EUWpTbJWzACAPcZbhQNAhvt0UVxOTgYwxlU+/+TJgNb4Yx7/WyIarvMo+X6vtVZ2LDl3l7UuNqVZO88QEBEREQsCIiIiYkFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREMLFToVTzb2Vd+E5D3r1p04eyWGUv662VoRRcc8XTY8GC7sJudbKOccnJHvLJIeiYBUCFyzpXZeTnw/UrE6YH8Nvu3cgTxL3dW9Zl6+vlM8Vz7/h6kiju3LWJRmMyM4FatcRTAwAGDlwMwHiLzj17BovGu9VKPvctaaCLiyzO2lo+OYCijp7Gu8Zp/ifr0piE58Uz/yoNXLpUFpdrwgP+D5cGp8POzvjjz8lJ2nW1t3juk1guirN52vhz4/080f/7ww+hFbSmXbdKdtxXlqeOAQM6ysZ8R/a6IHnuMvDEE4DG+Ng/QtaVt04PeYfOOv0rieLWC8fLFs/MMwREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEYQMrpYq6z+XkZIgGLSiQTZ4lCyuaWxiXoWSd8orjlJH44tvv3JHlDtwRxt0WxgHSui0jP9+kOGO53x2TKRoZyMuTbifpHpVvqcxM43MXx5iSu3Stt2/LcpduS0D+GMnIlvUjK46THveA7JiSxpnymJfu9wxhB8KMvKJ+dabse/nxLN1O8p550m0leRIvHsuU3DNyZMd9QYFsGwmfmgHIm0pKn0WKh5Me99LXEfHjU/jc/MciRGHSDoTFcZJ9DyWQmpqqAJjlT2pqKnO3wPwtOXdJ/pacuznnb8m5S/K35NyVUkqjlPGyQafT4erVq3BxcYFG0N+5PFBKITMzEzVq1ICVVdl/gVty7oD55W/JuQM87i1131ty7gCPe+m+FxUEREREZN74oUIiIiJiQUBEREQmFAQhISEYNmzYQ1zK/fHx8cHs2bMf6hzMfdhDneN+PIrcAcvOn7kPe6hz3A/mPvuhz2PJ+Zv9GYKcnBxERESgXr16sLGxQdeuXR/3kh6ZuLg4hIWFwcPDA05OTmjYsCFWrFjxuJf1SCQlJaF169aoXr067O3t4evri7FjxyLflK//mInz58/DxcUFFStWfNxLeSQuXboEjUZT4uf7779/3Et7JJRSmDFjBvz9/VGhQgV4enpiypQpj3tZD9348eNL3e9OTk6Pe2mPzNatW/Hss8/CxcUF1apVQ/fu3XHp0iXx/R9LQZCXJ/8u7t9VWFgIBwcHREZGIjQ09JHNW5ZHmXtCQgLq16+PtWvX4vjx4+jXrx/69OmDTZs2PbI13O1R5m5ra4s+ffpg27ZtSEpKwuzZs7FgwQK8//77j2wNf/Uo8y+Wn5+P8PBwtGzZ8pHPfbfHkfuOHTuQlpam/2ncuPEjXwPw6HOPiorCwoULMWPGDJw5cwYbN27EM88880jXUOxR5j5ixAiD/Z2WloY6deqgR48ej2wNf/Uo809OTkZYWBheeOEFJCYmYuvWrfj111/RrVs38Rj3XRB8++23cHV1xYoVK5CamoqePXuiYsWKqFy5MsLCwgyqkoiICHTt2hVTpkxBjRo1EBAQoK/i161bh9atW8PR0RENGjTAgQMHDObZt28fWrZsCQcHB3h5eSEyMhK3b8ub+jg5OWH+/PkYOHAg3N3d7zfdcpn7f/7zH0yaNAnNmzeHn58foqKi0L59e6xbt87sc/f19UW/fv3QoEEDeHt7o0uXLujVqxfi4+PvO/fylH+xsWPHIjAwED179vxbeQPlL/cqVarA3d1d/2Nra2v2uZ8+fRrz58/Hhg0b0KVLF9SsWRONGzdG27ZtzT53Z2dng/19/fp1nDp1Cv3797/v3MtT/keOHEFhYSEmT54MPz8/BAUFYcSIEUhMTBSfGb2vgiA2Nhbh4eFYsWIFevbsiXbt2sHFxQXx8fHYv38/nJ2d0b59e4PqaOfOnUhKSsL27dsN/kIdM2aMftH+/v4IDw9HwR+tDi9cuID27duje/fuOH78OFatWoV9+/Zh6NChZa4tIiICISEh95OWReSenp6OypUrW1zu58+fx5YtW9CqVav7yr085r9r1y6sWbMGH3/88X3nXF5zB4AuXbrAzc0Nzz33HDZu3GgRuX/zzTfw9fXFpk2bULNmTfj4+GDAgAG4efOm2ef+VwsXLoS/v//fOjtWnvJv3LgxrKyssGTJEhQWFiI9PR3Lly9HaGiovBg22rroD61atVJRUVFq7ty5ytXVVcXFxSmllFq+fLkKCAhQOp1OH5ubm6scHBzU1q1blVJK9e3bV1WvXl3l5ubqY5KTkxUAtXDhQv11J0+eVADU6dOnlVJK9e/fXw0aNMhgHfHx8crKykrduXNHKaWUt7e3mjVrlv72UaNGqd69e5eaQ9++fVVYWJg0ZbPKXSmlVq1apezs7NSJEycsJvfg4GBVoUIFBUANGjRIFRYWinMvz/n/+uuvysvLS+3Zs0cppdSSJUuUq6urReT+yy+/qJkzZ6rvv/9eHTp0SEVHRyuNRqM2bNhg9rkPHjxYVahQQTVr1kzt3btX7d69WzVs2FC1bt3a7HO/2507d1SlSpVUTEyMOG9zyD8uLk65ubkpa2trBUAFBwer33//XZy76H8ZFPvqq69w48YN7N+/H02bNgUAHDt2TP+hpbvl5OTgwoUL+sv16tWDnZ1diTHr16+v/93DwwMAcOPGDQQGBuLYsWM4fvy4wQfhlFLQ6XRITk5G7dq1S4w3bdo0U1ISK++57969G/369cOCBQvw9NNPC7MuUp5zX7VqFTIzM3Hs2DGMHDkSM2bMwLvvvmtC9uUz/4EDB+LVV1/F888/b1Kuf1Uec69atSrefvtt/eWmTZvi6tWr+PDDD9GlSxezzl2n0yE3Nxeff/45/P39AQCLFi1C48aNkZSUhICAALPN/W5ff/01MjMz0bdvX1G+f1Ue87927RoGDhyIvn37Ijw8HJmZmXjvvffw0ksvYfv27aLOiyYVBI0aNcLRo0exePFiNGnSBBqNBllZWWjcuHGpn16vVq2a/veyPul596mM4gXrdDoAQFZWFgYPHozIyMgS93vyySdNWfrfVp5z37NnDzp37oxZs2ahT58+Jt0XKN+5e3l5AQDq1KmDwsJCDBo0CO+88w6sra3FY5TH/Hft2oWNGzdixowZAP58crGxscFnn32G119/XTROecy9NM2aNcP27dtNuk95zN3DwwM2Njb6YgCA/sUkJSVFXBCUx9zvtnDhQnTq1AnVq1c3+b5A+cz/448/hqurK6ZPn66/7osvvoCXlxcOHjyIZ5991ugYJhUEfn5+mDlzJkJCQmBtbY25c+ciKCgIq1atgpubG7RarSnDGRUUFIRTp07hqaeeeqDj3o/ymntcXBw6deqEmJgYDBo06L7GKK+5/5VOp0N+fj50Op1JBUF5zP/AgQMoLCzUX96wYQNiYmKQkJAAT09P8TjlMffSJCYm6v8qkyqPubdo0QIFBQW4cOEC/Pz8AABnz54FAHh7e4vHKY+5F0tOTsbu3bv/1udGymP+2dnZJf5XQfHzXHHhYYzJHyr09/fH7t27sXbtWgwbNgy9evVC1apVERYWhvj4eCQnJyMuLg6RkZG4cuWKqcMbiI6ORkJCAoYOHYrExEScO3cOGzZsuOcHLUaPHl3ir+BTp04hMTERN2/eRHp6OhITE5GYmGjyespb7rt370bHjh0RGRmJ7t2749q1a7h27dp9fcCovOW+YsUKrF69GqdPn8bFixexevVqjB49Gi+//PJ9fdq8vOVfu3Zt1K1bV//j6ekJKysr1K1bF5UqVTJpPeUt92XLluHLL7/EmTNncObMGUydOhWLFy/GW2+9ZfJ6ylvuoaGhCAoKwuuvv44ff/wRR44cweDBg9G2bVuDswYS5S33YosXL4aHhwf+9a9//a01lbf8O3bsiB9++AETJ07EuXPncPToUfTr1w/e3t5o1KiRaB0mnSEoFhAQgF27dumrp7179yI6OhrdunVDZmYmPD090aZNm79dRdWvXx979uzBmDFj0LJlSyil4Ofnh5dffrnM+6SlpSElJcXgug4dOuDy5cv6y8UbR93H/3UqT7kvW7YM2dnZmDZtmsH7Ta1atUJcXJzJaypPudvY2CAmJgZnz56FUgre3t4YOnQohg8fft/rKk/5P2jlLfdJkybh8uXLsLGxQWBgIFatWoWXXnrpvtZUnnK3srLCN998g7feegvPP/88nJyc8K9//QszZ868rzWVp9yBor+Ely5dioiICJPOApalPOX/wgsvIDY2FtOnT8f06dPh6OiI4OBgbNmyBQ4ODqJ18L8dEhERkfm3LiYiIiLjWBAQERERCwIiIiJiQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBERERgQUBEREQAbCRBOp0OV69ehYuLCzQazcNe0yOhlEJmZiZq1KgBK6uy6yJLzh0wv/wtOXeAx72l7ntLzh3gcS/d91ACqampCoBZ/qSmpjJ3C8zfknOX5G/JuZtz/pacuyR/S85dKaVEZwhcXFwAAKnjxkFrb2803nVML8mwALKFcUBysqcobmFNV1FcLoAZ+DO3shTfvmxZKhwdtUbHPdtDNr+tKKrI4BMnRHGudT8QjpgHYLXR3IE/898LwFkw8m3hCqKC0oWRQGysLO6HH4zH3LmTgUGDvEzK/fLlVGi1xvd9v37G5weA9euXywIBAIWiqPQnZ4riMnQ6eF25Ij7uU+fNg9bBwei4rv1CRfMDgp30h+rV24jizm4+L4rLyMqCV6tWJu371QAcBWNXEq0AqHvmjDASQOfOojDXcy8LonIBfGhS7j4+qbCyMn7cb7koe87bs0D+mO95e4ksMDhYFJaRlQWvNm3Ex/24camwtzeee8EYWe5Pi6KKfNNXtp06L5PNnQ2gD4y/1gHCtwyKT51o7e1FBQFgfEMWsRbGQfSEDACS1d3N2Gmh4tsdHbWigkA6v50wDgC0gh1p+qjGc787xhmAZBXSk2zW1tJjBJCm7yh55v6DKblrtVrR8WcrrvKMv8D+SVYQaI2dCvwL6XGvdXCAVrRhpfvTSRgH0YsRAGidJaXqn0zZ946QrVi6AvljGYC19PlR/qxnSu5WVlrR41SakeT5s5hWJ3yMPOB9X3y7vb1WVhAI55Uf9YCdnWw7mTImINv3/FAhERERsSAgIiIiFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEYR8CvZQUwM74d92nTJG16ahYUdrOA7hZRfYN97Fr14riMrKzMaV3b/H8Tk5FP8a83aOHbMCcHPHcmD1bFJabu0gUl5GRgWrVvpDPD2Dj+HTR93Lfeks2XhcneVvQ555TorgZM8RDmuRWJVfoBHFzheOtwWbx3NWq/UsWWHGeLK5Q1tegWPcV3WBjY3y/S7u8KpUmnvvqs91EcasDvhbFydug/all167QShpMfCF7PC2uUEE8d3+sEcUdOfKS0ZisrAy0ajVZPDcAbNwo6wFS4C0b79WlL4rnvrZ9uyhur3A8U/f90D4ZkLS+0bzzmWi8mTMHiucOfUf2YNo1Uva8mJubAXwka2LEMwRERETEgoCIiIhYEBARERFYEBARERFYEBARERFYEBARERFYEBARERFYEBARERFYEBARERFM7FS4LXQ6HB2Nt2/6zzB/0Xiac4PFc38sjHtqzBhZoIkd2zp0+AGA8VaFKvAn0XgOl06L524rjPusgqzDVaZ45j8N/9cZaJ2djcaddXpaNN445MknT/5SFNYttKPRmIyMDPm8fxjeNR22tsaP+9XfOMgGzNkonjvhlw6iOM0vsq52RT3b+orn37FjMQDjealCYSe2X4WdFwFoqrsJI6Ud+LIANBPPDwB7Xl8GJyfj+36f8LFX1YS5K1Uy3oEQAHwaG5/b9KMeuHpV1p21+ZEjovE0jZPEc6sLn4jiXvYbLxwxD8Aq8fxfeXnBURQp65X4dvUV4rk3fyvrQPhhR2m32TviuXmGgIiIiFgQEBEREQsCIiIiAgsCIiIiAgsCIiIiAgsCIiIiAgsCIiIiAgsCIiIiAgsCIiIiAgsCIiIigomti190PQitoJel5tws0XjqiId4bk3j6aK4IWd+F46YC+C/4vnjEQrjjXuBN0NkbSdnfiJrdQoAQ76RtfH0wCbhiNkAeornBwC35wOh0Rhv4ZqD3qLxwsNtxXO7u4eL4gaNMB6TZ0LH5GKhoYCDoCvxF2tyhCM+L577Scj2PdBaGGdq4+oEAMb3lcZ6v2i0T7FcPLPaJDuep3eStcvOAfC+ePYiB7u4wl4Q10U43nET5r75WqQoLmif8eecwsIM4LirCbMDu1+U5d4Cu0XjffCB7HEMAHl+sufHDcLxsgHIZwcKARQI4k6ebCkab/3T8ud7LV4TxamRI0VxGbm5cP1INjfPEBARERELAiIiImJBQERERGBBQERERGBBQERERGBBQERERGBBQERERGBBQERERGBBQERERDCxU6FveDNYWRnvVnf5smy8M97y7k2qYUNRnNvPP4ridLoM/PabvFPhDkDUtWv+yIuyAVe4iOf+9wzZdjo7uJMoLgtAY/HsRXJzdwIw3qWyUqXPReN9+WVXE2afIIr6HQ2NxmQAWGbCzACweDFgbW08TpY5ANwRRx4VxqWjqiguA4CXeHYAaAvAeJvGpk1l3dVCfpB3KpzaSXY8PykcT77V/9QYkqMeaDhwoGi8RguknVSBLv/7nyjuR4wWRJnaoRKYhgRA1J81XjTekSMh4rmj3pB1fP0/YcfXQvHMRV7euFHUlTdL2IEwpJosHwC4sT5BFJfdsLksLiMD+EjWqpBnCIiIiIgFAREREbEgICIiIrAgICIiIrAgICIiIrAgICIiIrAgICIiIrAgICIiIrAgICIiIgg7FSpV1GVJp8sQDZopbIplSuewjEJZrynpGovjinMrS/HtuaJRgQxp8kbmNXBHtqWyhMMVxxnL3TAmWzS2UrLtD+QL4wBpZpKZi/eOKbkXFspykm5/U47828I46VaX5v/n7bK1PvhtBOQI46RbszjOlH0vO+qBjLw8YaT8uJfuU1kXwqItb9pjXnr0yfZAfr48I+nmlK6weD9Kj/uMbNmelx7P0tclAMi4LcsqO0P6eix7rSsOMio1NVUBMMuf1NRU5m6B+Vty7pL8LTl3c87fknOX5G/JuSullEYp42WDTqfD1atX4eLiAo1G/v8H/smUUsjMzESNGjVgZVX2OyeWnDtgfvlbcu4Aj3tL3feWnDvA416670UFAREREZk3fqiQiIiIWBAQERERCwIiIiKCCQVBSEgIhg0b9hCXcn98fHwwe/bsBzaepeRZFkvOn7kPe6hz3A8e99z3D5Ml514aizhDsHr1ajRs2BCOjo7w9vbGhx9++LiX9MDl5OQgIiIC9erVg42NDbp27VpqXFxcHIKCglChQgU89dRTWLp06SNd58MiyT8tLQ2vvvoq/P39YWVl9Y98IrgfktzXrVuHtm3bolq1atBqtQgODsbWrVsf/WIfMEnu+/btQ4sWLVClShU4ODggMDAQs2bNevSLfQikj/ti+/fvh42NDRo2bPhI1vcwSXKPi4uDRqMp8XPt2rVHv+AHSLrfc3NzMWbMGHh7e6NChQrw8fHB4sWLyxz3sRQEeeImHn/fd999h169euGNN97AiRMnMG/ePMyaNQtz58596HM/yjwLCwvh4OCAyMhIhIaGlhqTnJyMjh07onXr1khMTMSwYcMwYMCAh/bC8E/LPzc3F9WqVcPYsWPRoEGDh7qef1rue/fuRdu2bbF582YcOXIErVu3RufOnfHjjz8+8PX803J3cnLC0KFDsXfvXpw+fRpjx47F2LFj8dlnnz2UNf3T8i9269Yt9OnTB23atHlo6/mn5p6UlIS0tDT9j5ub2wNfzz8x9549e2Lnzp1YtGgRkpKS8OWXXyIgIKDM+PsuCL799lu4urpixYoVSE1NRc+ePVGxYkVUrlwZYWFhuHTpkj42IiICXbt2xZQpU1CjRg0EBATg0qVL0Gg0WLduHVq3bg1HR0c0aNAABw4cMJhn3759aNmyJRwcHODl5YXIyEjcFnZyAoDly5eja9eueOONN+Dr64uOHTti9OjRiImJEXVuKi95Ojk5Yf78+Rg4cCDc3d1Ljfnkk09Qs2ZNzJw5E7Vr18bQoUPx0ksv3fOvJXPK38fHB3PmzEGfPn3g6upqdExzyn327Nl499130bRpU9SqVQtTp05FrVq18M0335h97o0aNUJ4eDiefvpp+Pj44LXXXkO7du0QHx9f5rjmlH+xN954A6+++iqCg4PvGWeOubu5ucHd3V3/U9b38c0p9y1btmDPnj3YvHkzQkND4ePjg+DgYLRo0aLMce+rIIiNjUV4eDhWrFiBnj17ol27dnBxcUF8fDz2798PZ2dntG/f3qBi2rlzJ5KSkrB9+3Zs2rRJf/2YMWMwYsQIJCYmwt/fH+Hh4SgoKAAAXLhwAe3bt0f37t1x/PhxrFq1Cvv27cPQoUPLXFtERARCQkL0l3Nzc2Fvb28Q4+DggCtXruDy5ctmk6fEgQMHSlST7dq1K3Gwmmv+pjD33HU6HTIzM1G5cmWLy/3HH39EQkICWrVqVert5pj/kiVLcPHiRbz//vv3jDPH3AGgYcOG8PDwQNu2bbF//36LyH3jxo1o0qQJpk+fDk9PT/j7+2PEiBG4c69W+EZ7Gf6hVatWKioqSs2dO1e5urqquLg4pZRSy5cvVwEBAUqn0+ljc3NzlYODg9q6datSSqm+ffuq6tWrq9zcXH1McnKyAqAWLlyov+7kyZMKgDp9+rRSSqn+/furQYMGGawjPj5eWVlZqTt37iillPL29lazZs3S3z5q1CjVu3dv/eVPP/1UOTo6qh07dqjCwkKVlJSkAgMDFQCVkJBgNnnerW/fviosLKzE9bVq1VJTp041uO7bb79VAFR2drbZ53+34jxLu87cc1dKqZiYGFWpUiV1/fp1i8nd09NT2dnZKSsrKzVx4kSD28w5/7Nnzyo3NzeVlJSklFLq/fffVw0aNLCI3M+cOaM++eQTdfjwYbV//37Vr18/ZWNjo44cOWL2ubdr105VqFBBdezYUR08eFB9++23ytvbW0VERJQ6jlJKif65UbGvvvoKN27cwP79+9G0aVMAwLFjx3D+/Hm4uLgYxObk5ODChQv6y/Xq1YOdnV2JMevXr6//3cPDAwBw48YNBAYG4tixYzh+/DhWrFhxdwEDnU6H5ORk1K5du8R406ZNM7g8cOBAXLhwAZ06dUJ+fj60Wi2ioqIwfvz4Mk8blcc8HyRLzt8Sco+NjcWECROwYcMGg/dSzT33+Ph4ZGVl4fvvv8eoUaPw1FNPITw8XH+7OeZfWFiIV199FRMmTIC/v3+ZceaYOwAEBAQYvGfevHlzXLhwAbNmzcLy5cvNOnedTgeNRoMVK1bo3yL973//i5deegnz5s2Dg4NDifuYVBA0atQIR48exeLFi9GkSRNoNBpkZWWhcePGBskVq1atmv53JyenUse0tbXV/17cO1qn0wEAsrKyMHjwYERGRpa435NPPilas0ajQUxMDKZOnYpr166hWrVq2LlzJwDA19fXbPKUcHd3x/Xr1w2uu379OrRarcHBYa75S5h77itXrsSAAQOwZs2aEm8fmXvuNWvWBFD0JH79+nWMHz/eoCAwx/wzMzNx+PBh/Pjjj/pT0jqdDkop2NjYYNu2bQDMM/eyPPPMM9i3b5/+srnm7uHhAU9PT4PPS9WuXRtKKVy5cgW1atUqcR+TCgI/Pz/MnDkTISEhsLa2xty5cxEUFIRVq1bBzc0NWq3272dxl6CgIJw6dQpPPfXU3x7L2toanp6eAIAvv/wSwcHBBjv2buU5z3sJDg7G5s2bDa7bvn17iQ8ZmWv+Euac+5dffonXX38dK1euRMeOHUvcbs65/5VOp0NuruE/NTfH/LVaLX766SeD6+bNm4ddu3bhq6++0hdJ5ph7WRITE/V/tQPmm3uLFi2wZs0aZGVlwdnZGQBw9uxZWFlZ4Yknnij1PiZ/qNDf3x+7d+/G2rVrMWzYMPTq1QtVq1ZFWFgY4uPjkZycjLi4OERGRuLKlSt/K6Ho6GgkJCRg6NChSExMxLlz57Bhw4Z7fvhi9OjR6NOnj/7yr7/+ik8++QRnzpxBYmIioqKisGbNGqNNH8pbngBw6tQpJCYm4ubNm0hPT0diYiISExP1t7/xxhu4ePEi3n33XZw5cwbz5s3D6tWrMXz4cIvIH4D+uqysLPzyyy9ITEzEqVOnzD732NhY9OnTBzNnzkSzZs1w7do1XLt2Denp6Waf+8cff4xvvvkG586dw7lz57Bo0SLMmDEDr732WonxzS1/Kysr1K1b1+DHzc0N9vb2qFu3rsFfuOaWO1D07ZoNGzbg/PnzOHHiBIYNG4Zdu3ZhyJAhBuOYY+6vvvoqqlSpgn79+uHUqVPYu3cvRo4ciddff73UtwsAE88QFAsICMCuXbv0FdXevXsRHR2Nbt26ITMzE56enmjTps3frqzq16+PPXv2YMyYMWjZsiWUUvDz88PLL79c5n3S0tKQkpJicN2yZcswYsQIKKUQHByMuLg4PPPMM2aXZ4cOHQy+OdGoUSMA0H+9smbNmvj2228xfPhwzJkzB0888QQWLlyIdu3aWUT+d18HAEeOHEFsbCy8vb0Nvk4EmF/un332GQoKCjBkyBCDJ8O+ffuWaE5lbrnrdDqMHj0aycnJsLGxgZ+fH2JiYjB48OBS5zC3/E1hbrnn5eXhnXfewc8//wxHR0fUr18fO3bsQOvWrc0+d2dnZ2zfvh1vvfUWmjRpgipVqqBnz56YPHlymfPw3x8TERGRZbQuJiIiontjQUBEREQsCIiIiIgFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREQGwkQTpdDpcvXoVLi4u0Gg0D3tNj4RSCpmZmahRowasrMquiyw5d8D88rfk3AEe95a67y05d4DHvXTfQwmkpqYqAGb5k5qaytwtMH9Lzl2SvyXnbs75W3LukvwtOXellBKdIXBxcQEA2NmlQqPRGo3Pzf1dMizSI/8rigOAnSGTRHE/dHMVxeUC+C/+zK0sxbenTpgArb290XFdo7NF81/EBFEcAPjiV1HcDlQVxd0GEAbjueOumNSqVaE1Vl0CcL0RLlpD+/YTRXEAMHKkLO5GG+P7PhtAf5iW+1dfpcLR0fhx799Bduw9hauiOACoUsVJFFe/vmy8goIMxMd7iY97IByAnWDk10Xzjx/fUBRXFHtJHCuTBaCFSfsemAPAwWj8unUvi1bg7CwKAwC8+GKcKC7duZfRmAyl4HX7tkm5jxqVCnt748d9k/Gy477ly7JtBABNjn4mirO1lY1XWJiBpCT5cf8hJHsd6O3rK5rf9eISURwAtG/fUBS3ZUs/4Yj5AL4R7XtRQVB86kSj0YoKAqBQMiy0FSqI4gDAyUkyL2D8JduQsdNCxbdr7e1FBQGgE81rfNfcTZa77KXjT5JTYvr8raxEBQEg26e2trKcAMBJmJijeETTcnd01IqOP/k+leduZSVL3kb0SP6T9LgvKgYkBYHslU7yAvMn0x4lUqbs+6KXBeMvDdLnJ+mx/Ee0KEprwqltU3K3t9eK9pc0Ja2d5DgqYm0t257W1uIhAciPe9leB7TiBcgrQflzo7Aa+oNk3/NDhURERMSCgIiIiFgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREYSNiYpdzHUVtVRx3rRJNuCA5eK5s56dLop7r2lTUVxGYSGmHT0qnv9sdLSotcQa4XhVL18Wzw3vLFFYMNYJB8wG8Jp8fgDnb9wQ5b9oUYxovGf6y5upPPGNLC59qzIaU3g7AxB2sywW38FV1PBq00jj8wOASnxRPnn79rK4SbJOnhlKwZTst2CZqPHMcxVlR/7ZUbfEc3sL4zI+lW33O3cyMGyYeHoAQLrvB6LmM+/tkz2eJlaUd2cF9srC5s41HnPnDvDGGybMDYwffxNFXe7urV8/2fbfu0T+mF8A2WuDrIdr0TOe8X6Of2q0Px3OzsZf7TZeEg4YJmvWBwDffHNbGCl9HrkDCF8beIaAiIiIWBAQERERCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiKCiZ0KpS7W7iiK87uWIh5zUXdhlysfH1mcTieeGwCaogMAW6Nx6ud5sgEvXRLPrfotlAV+/bUoLEMpuKaLpwcAHF2QDkdH4527QnvJ9pM3hN0sASShkyjuxfeDjcZkFBSI5y3mD8BREPerj3DAZ03oGLdliyjs4pHfRXGZmRlAQ3mvwuDgYGhtjD9NzIiPF433WS1ZVzsAEDQIBAB0Pi+Ly80VT63nejEQosf9r/8Wjbd30nzx3MZ7BBbR9K0iiMoTz1ssfUkctI7Gj/xTdXuKxjuzRD53S/QWxXXu/LkoLj8/A9giP+7rbvoAWnvj/Ukbjn9PNJ6aOUc8N0aNEoUd/V62T7OyMtCq1VBRLM8QEBEREQsCIiIiYkFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREYEFAREREMLF1sfPo0XAWtHN0vnVUNN5ONBbP3QaylrNZw2X9TnNyMoBoeSvLzZu/hJOT8da9XYRdaTeOuiSeW7NkkDBS2ps1H8Aa8fwAMHDgVAAVjMbdvi1rTeviLp9bmymLe7XmAaMx+fkZwGH5fgeAeU3TYWNjfN+vHCJr2zzehLnHnzwpivPzuy0cURpXxPXAM5Dsd2C5aDwVKmvxCwB47TVR2N4WLURxtwF8JJ8dALACm0VtqzXz74jGCwwUtjYHcNq+kSguLXGx0ZhMFLXgNsnatYCt8bbNdb56STRcnd6ydsQA8Mty2fHU7ZasdbHJHcsrVgQEr3WJibLhNl95Wzx1h+dkbd2Dft0misu4LX/M8wwBERERsSAgIiIiFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEFgREREQEEzsVuk6zA2BnNC7mfVkHQnnvJuCgcKnNoroJR8w3YXYgrYOrqGNZiw9knfpWpTYXz/3xx7K4yZOfFcXpdBm4ft20ToXpwXuhtRHsgy88ReOdOCHtvgh4eK8Uxe3+0ninwNsAvhLPXGTzZkBrvFEh9kka+gEIMmFuzdNJwsj/CePyTJgdiMccOAviGkG4zu+OySf/7jtRWNpK2WMuOzsDeN20LpWBgCh/e0FXOwD43xlZN0sA6PZ/srzWjVhhNMYxOxsYJH/MAQAmTACcjWd//ITs78q6S2VdBQEgQdipcK/NC6K4DBTAlD0/LTpa1J/zRUSJxuuIM+K5Fy3aJYp7fUR92YCFheK5eYaAiIiIWBAQERERCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiKCsFOhUsUds3JFg+YIJ88QxgFAljhS2oGwKO7P3EpXfPsd4ag5ObKssrOFAwK4I5xcp5PGFa3RWO53x2QUFMgGFy42M9OUvS/bWLdNGMmU3KVrlcx/9xoebLS0A2FRnPS4l+YkfdxlSA9SE2Rny/bPnTumH/fS5x2lHuwxAgD5+bIxMwRPJhl/PC5NesxnybLPEj7nZZjwkJce9dLnpeI46XEve6UzZX/KX8GKj1NjMoQdCIvjJPseSiA1NVUBMMuf1NRU5m6B+Vty7pL8LTl3c87fknOX5G/JuSullEYp42WDTqfD1atX4eLiAo1G3ov7n0wphczMTNSoUQNWVmW/c2LJuQPml78l5w7wuLfUfW/JuQM87qX7XlQQEBERkXnjhwqJiIiIBQERERGVURCEhIRg2LBhj3gpxvn4+GD27NkPdQ5Lzh2w7PyZ+7CHOsf94HHPff8wWXLupTGrMwQ5OTmIiIhAvXr1YGNjg65du5aIiYiIgEajKfHz9NNPP/oFP0CS3AFgxYoVaNCgARwdHeHh4YHXX38dv/3226Nd7EMgzf/jjz9G7dq14eDggICAAHz++eePdqEPQVxcHMLCwuDh4QEnJyc0bNgQK1asKBG3Zs0aBAYGwt7eHvXq1cPmzZsfw2ofLEnuJ0+eRPfu3eHj4wONRvNYnmgfFkn+CxYsQMuWLVGpUiVUqlQJoaGhOHTo0GNa8YMjyX3dunVo0qQJKlasqI9Zvnz5Y1rxgyN9zBdbuXIlNBpNmc+LxR56QZCXJ/1+9N9XWFgIBwcHREZGIjQ0tNSYOXPmIC0tTf+TmpqKypUro0ePHg98Pf+03Pfv348+ffqgf//+OHnyJNasWYNDhw5h4MCBD2VN/7T858+fj9GjR2P8+PE4efIkJkyYgCFDhuCbb7554Ot5lLknJCSgfv36WLt2LY4fP45+/fqhT58+2LRpk0FMeHg4+vfvjx9//BFdu3ZF165dceLEiQe+nn9a7tnZ2fD19cUHH3wAd3f3h76mf1r+cXFxCA8Px+7du3HgwAF4eXnhxRdfxM8///zA1/NPy71y5coYM2YMDhw4oI/p168ftm7d+sDX80/LvdilS5cwYsQItGzZ0vjApX0XsVWrVioqKkp/edOmTUqr1aovvvhCpaSkqB49eihXV1dVqVIl1aVLF5WcnKyP7du3rwoLC1OTJ09WHh4eysfHRyUnJysAau3atSokJEQ5ODio+vXrq4SEBIN54+Pj1XPPPafs7e3VE088od566y2VlZWlv93b21vNmjXL6Hcp716HMV9//bXSaDTq0qVLZp/7hx9+qHx9fQ2u++ijj5Snp6f+sjnnHxwcrEaMGGFw3dtvv61atGhhNrkX69Chg+rXr5/+cs+ePVXHjh0NYpo1a6YGDx5s9rnfrazxLCV/pZQqKChQLi4uatmyZRaXu1JKNWrUSI0dO9Yici8oKFDNmzdXCxcuFL0mGj1DEBsbi/DwcKxYsQI9e/ZEu3bt4OLigvj4eOzfvx/Ozs5o3769QXW0c+dOJCUlYfv27QYVy5gxYzBixAgkJibC398f4eHhKPijg9SFCxfQvn17dO/eHcePH8eqVauwb98+DB06tMy1RUREICQkxHjVcw+LFi1CaGgovL29zT734OBgpKamYvPmzVBK4fr16/jqq6/QoUOHUuPNLf/c3FzY29sbXOfg4IBDhw4hP9+w0155zz09PR2VK1fWXz5w4ECJMyft2rXDgQMHStzX3HI3lbnnn52djfz8/FJjzDl3pZR+rc8//7xF5D5x4kS4ubmhf//+97yvXmlVQnHVNHfuXOXq6qri4uKUUkotX75cBQQEKJ1Op4/Nzc1VDg4OauvWrUqpoqqpevXqKjc3Vx9TXDUtXLhQf93JkycVAHX69GmllFL9+/dXgwYNMlhHfHy8srKyUnfu3FFKlayaRo0apXr37l1qpSOphn7++WdlbW2tVq1aZTG5r169Wjk7OysbGxsFQHXu3Fnl5eVZRP6jR49W7u7u6vDhw0qn06kffvhBVa9eXQFQV69eNYvclVJq1apVys7OTp04cUJ/na2trYqNjTWI+/jjj5Wbm5tSyjz2e1m5383YGQJzz18ppd58803l6+urn8Pcc79165ZycnJSNjY2qkKFCmrRokX628w59/j4eOXp6al++eUX/XqNvSaW+b8MvvrqK9y4cQP79+9H06ZNAQDHjh3D+fPn4eLiYhCbk5ODCxcu6C/Xq1cPdnZ2JcasX7++/ncPDw8AwI0bNxAYGIhjx47h+PHjBh+MUEpBp9MhOTkZtWvXLjHetGnTylq+yLJly1CxYsUSH7Qw19xPnTqFqKgovPfee2jXrh3S0tIwcuRIvPHGG1i0aJHZ5z9u3Dhcu3YNzz77LJRSqF69Ovr27Yvp06frO3iV99x3796Nfv36YcGCBSZ/UNaScwcsI/8PPvgAK1euRFxcnMHZMnPO3cXFBYmJicjKysLOnTvx9ttvw9fXV/8XtznmnpmZid69e2PBggWoWrVqmff9qzILgkaNGuHo0aNYvHgxmjRpAo1Gg6ysLDRu3LjUTzNWq1ZN/7uTk1OpY9ra2up/L24Lqfvjn51kZWVh8ODBiIyMLHG/J598UpiOnFIKixcvRu/evUvsUHPNfdq0aWjRogVGjhwJoOigdXJyQsuWLTF58mT9gWuu+Ts4OGDx4sX49NNPcf36dXh4eOCzzz6Di4uLPofynPuePXvQuXNnzJo1C3369DG4zd3dHdevXze47vr16wYfsjPX3KXMPf8ZM2bggw8+wI4dOwxesADzzt3KygpPPfUUAKBhw4Y4ffo0pk2bpi8IzDH3Cxcu4NKlS+jcubP+uuL5bWxskJSUBD8/vxLjlVkQ+Pn5YebMmQgJCYG1tTXmzp2LoKAgrFq1Cm5ubtBqtSYt3JigoCCcOnVKv+Metj179uD8+fOlvrdirrlnZ2fDxsZwl1tbWwOAwX/CMtf8i9na2uKJJ54AUPR1nE6dOunPEJTX3OPi4tCpUyfExMRg0KBBJW4PDg7Gzp07Db5zvX37dgQHB+svm2vuUuac//Tp0zFlyhRs3boVTZo0KXG7Oef+VzqdDrm5f/4/Q3PMPTAwED/99JPBdWPHjkVmZibmzJkDLy+vUse854cK/f39sXv3bqxduxbDhg1Dr169ULVqVYSFhSE+Ph7JycmIi4tDZGQkrly58reSi46ORkJCAoYOHYrExEScO3cOGzZsuOcHLUaPHl2iIjx16hQSExNx8+ZNpKenIzExEYmJiSXuu2jRIjRr1gx169a1mNw7d+6MdevWYf78+bh48SL279+PyMhIPPPMM6hRo4bZ53/27Fl88cUXOHfuHA4dOoRXXnkFJ06cwNSpU8t17rt370bHjh0RGRmJ7t2749q1a7h27Rpu3rypj4mKisKWLVswc+ZMnDlzBuPHj8fhw4dLzGOOuefl5emPhby8PPz8889ITEzE+fPnS4xvjvnHxMRg3LhxWLx4MXx8fPQxWX/598bmmPu0adOwfft2XLx4EadPn8bMmTOxfPlyvPbaa2adu729PerWrWvwU7FiRbi4uKBu3bqlvs0B3OMMQbGAgADs2rVLXz3t3bsX0dHR6NatGzIzM+Hp6Yk2bdr87Sqqfv362LNnD8aMGYOWLVtCKQU/Pz+8/PLLZd4nLS0NKSkpBtd16NABly9f1l9u1KgRAMO/gNPT07F27VrMmTPnnmsyt9wjIiKQmZmJuXPn4p133kHFihXxwgsvICYmxiLyLywsxMyZM5GUlARbW1u0bt0aCQkJ8PHxKde5L1u2DNnZ2Zg2bZrBe42tWrVCXFwcAKB58+aIjY3F2LFj8Z///Ae1atXC+vXrSy2IzS33q1ev6o8FoOjU+YwZMwxizDn/+fPnIy8vDy+99JLBWO+//z7Gjx9v1rnfvn0b//73v3HlyhU4ODggMDAQX3zxRanzmFvu94P/7ZCIiIjMq3UxERER3R8WBERERMSCgIiIiFgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREVgQEBEREQAbSZBOp8PVq1fh4uICjUbzsNf0SCilkJmZiRo1asDKquy6yJJzB8wvf0vOHeBxb6n73pJzB3jcS/c9lEBqaqoCYJY/qampzN0C87fk3CX5W3Lu5py/Jecuyd+Sc1dKKdEZAhcXFwBAang4tHZ2RuM3LlsmGRa+oqgiv21MF8UVFMjGy87OwGuveelzK0vx7YcPp8LZWWt0XI+BnWQLuHhRFgfg5XqnRHFdu8rGu3MnA8OHG88dwF0x4QCM73tgumgNJ07Yi+IAwKsgWRbYrJnRkAyl4JWXZ1LuqSNHQluhgtH4cRkjja8RwCTfJaI4AMCOHbI4T09RWEZeHryWLBEf98ePp8LFxfhxf7imq2j+4+Nkj2MAmDRpkCguPeicKC6jsBBex46Ztu8nTYLW3vix2ne/bK3LNlURxQHAJuGTWS8sFkTdATDEtNwDAqC1tjYa73pqvmB+AIgRxgHLsVkU1xtvCkfMA7BIfNynPvEEtMb+mgbgmiLL6d//7iCKAwCvebLHUm3heNkAXgVE+15UEBSfOtHa2YkKAkfJoACchXEAkONk/EkJkBcExYydFiq+3dlZK3pi1NqINikgONiK2drKcndwEA8JwHjuhjF2kBUEsrW6uMgLAm2B8QMZAGDCKT5TctdWqCB6UaiQK8tda8qOsrWVxQkel3eTHvcuLlpotcbzkj7m7e1l26iILCfJi9bdTNr39vai/SV9jGpNOEal29SUSJNyt7YWblvpM7nwWIYpGRkv1O8mPe61VlaigkC60goV5Me99JnRSTxiEcm+54cKiYiIiAUBERERsSAgIiIisCAgIiIisCAgIiIisCAgIiIisCAgIiIiCPsQFEsb/RGyBN/Ff3b8Z6LxamyRNNQoomnTTxgprXHyxHMDwNmzgJPgi581BN9XB4CbqaniuTfWlTW1+KLvd6K4+6sCxwEw3g9gLWTfsd+3T4ln7tXrbVHcp8gxGnNHPOufsiZPFm2z6TGy71lrBoeZMLvsOLmDyaI441vI0JGarqLvO0u/iR40Rv49/CnCOM0P0u2ZL567WPbIkaInyWcxVDSe7JmxyGC8Kor77beXjcZkZGSgZs3XTZgdgKMjIOirshWNRcPJO48ArTBDFKdWeYniMrKz4dpP2kBJbhJkx97i9fLnO9mzHXB1mWzMO3cygDdkzY54hoCIiIhYEBARERELAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIgILAiIiIoKJnQqHDQNsBc3YNn5wSjSepn8LE2a/LYrKRaQoLgNANRNmT0kBHARN+G58J+sWWB3dxHMve2WtKK7PgHWiuIzsbAzq3Vs8f5FIAMZ3/pU5su5Zkd/L9hMAPHthvSjOz6+XICofwBrx3ADgiZsAjHfoRHSyaLwY1BLPHY2boriLwk6FWeKZizj+8WNMw3TZftfeShHPPW/ok6K4nz+RjZeZmYHAQFnHtmLpZ9KhE3RnfcdztnDEeBNmbyOKqlIlQxAliTHkerg1gApG4+LxvWi8QybMvQQjRHG/Gm/SCADINGFuAFidkiI67sdhiGi8DwbL5+7QQ/ZY8u0VLIrLKCjAG8K5eYaAiIiIWBAQERERCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiICCwIiIiKCia2LW7cG7O0FgTt2iMabMkXevvY/z10Xxf33sKztY05OBjBG3sa099lx0FYw3sZTljmwHbI2wwDw+lhZ3Fx3WTvkwkLT25gCEwE438f9yvDUU+JQP795orifEGs0JguArOHnnxagsqiN6atz5ojG00TNMGH28aKorrVkx31hYQZwUX7cp3ySDgcH4617n78ibFf+tKy1d5EnRFFxcbL+tUq2iQxUCXSVNK3Gnj2ywZ+/5SueOyMsTBSnrTbK+Fg6HVx/E08NAEh/KRlaQa/6J/fJck9NvSGeW705XhTnv0P23GDqcT8Q8wEIetXjimi86GOviufWjMoXxanbO2UDZmQAHh6iUJ4hICIiIhYERERExIKAiIiIwIKAiIiIwIKAiIiIwIKAiIiIwIKAiIiIwIKAiIiIwIKAiIiIYGKnwujongCMd66qi02i8boiSjx3ojDuynBZ16zcXPHUAIDdH30EJ0Hcv4Ud486dixfPrU7IOgued5V14soEECSevYizcy1oNMZ7tj0bpRGNt8+Eub8UxkmOuhwT5i32FGQ9Gv/1nbTz5g/iuW/jGVGc07lhwhEzxXMDgJMT4Chp0zh+vGi8+VgjnntePdlj6ZWfZMdcDoBJ4tmLzAUgac4a0Uq2Brz5pnjuvcK4s7/8YjTmfo770W6LUKGC8cd8amqhcMSvxHN/P3++KO6c+G/aPPHcAJDeZBG0NoKXx5gY0XiftBK2mwWgfv5ZFKdxekc4ojx3niEgIiIiFgRERETEgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIjAgoCIiIgg7FSoVHHHsALRoLeFk2cJ4wB5r6XcXFlXv7y8org/cytd8e3ZwvkLC2Xzy7cSkJEhG1Pag654uxvL/e4YpWRrkO7TfGEcIN/2km5sxQ0qTclduqcKCqT7Xn7kS0c0de9Lj/s7d2QryMiX7dE7oqgi0seStAvf/ex7aUNT6dZ3zJN3jXuQx31xjCm5Fz9HGiftVCjf+/JnR+n2LIqTHvcZhcKcbstWaspxn5EpPZoebO7FQUalpqYqAGb5k5qaytwtMH9Lzl2SvyXnbs75W3LukvwtOXellNIoZbxs0Ol0uHr1KlxcXKDRCHt2/8MppZCZmYkaNWrAyqrsd04sOXfA/PK35NwBHveWuu8tOXeAx71034sKAiIiIjJv/FAhERERsSAgIiIiFgRERESEMgqCkJAQDBs27BEvxTgfHx/Mnj37oc5hybkDlp0/cx/2UOe4Hzzuue8fJkvOvTRmdYYgLi4OYWFh8PDwgJOTExo2bIgVK1YYxCxduhQajcbgx97e/jGt+MGR5A4At27dwpAhQ+Dh4YEKFSrA398fmzdvfgwrfrAk+YeEhJTY9xqNBh07dnxMq34wpPt+9uzZCAgIgIODA7y8vDB8+HDk5Ei/xf/PJMk9Pz8fEydOhJ+fH+zt7dGgQQNs2bLlMa34wUpKSkLr1q1RvXp12Nvbw9fXF2PHjkX+X/pCrFmzBoGBgbC3t0e9evXM4jEvyf3kyZPo3r07fHx8oNFoHsuL7MMgyX3BggVo2bIlKlWqhEqVKiE0NBSHDh2657iixkR/R15eHuzs7B72NACAhIQE1K9fH9HR0ahevTo2bdqEPn36wNXVFZ06ddLHabVaJCUl6S8/rK+X/NNyz8vLQ9u2beHm5oavvvoKnp6euHz5MipWrPhQ1vRPy3/dunXIu6sxzG+//YYGDRqgR48eD3w9/7TcY2NjMWrUKCxevBjNmzfH2bNnERERAY1Gg//+978PdD3/tNzHjh2LL774AgsWLEBgYCC2bt2K//u//0NCQgIaNWr0wNf0KPO3tbVFnz59EBQUhIoVK+LYsWMYOHAgdDodpk6dCqBoG4WHh2PatGno1KkTYmNj0bVrVxw9ehR169Z9oOv5p+WenZ0NX19f9OjRA8OHD3+o6/mn5R4XF4fw8HA0b94c9vb2iImJwYsvvoiTJ0/C09Oz9IFLa07QqlUrFRUVpb+8adMmpdVq1RdffKFSUlJUjx49lKurq6pUqZLq0qWLSk5O1sf27dtXhYWFqcmTJysPDw/l4+OjkpOTFQC1du1aFRISohwcHFT9+vVVQkKCwbzx8fHqueeeU/b29uqJJ55Qb731lsrKytLf7u3trWbNmmW0ucLdOnTooPr166e/vGTJEuXq6lpmvDnnPn/+fOXr66vy8vIsMv+/mjVrlnJxcdHPY865DxkyRL3wwgsGMW+//bZq0aKF2efu4eGh5s6daxDTrVs31atXL/1lc8p/+PDh6rnnntNf7tmzp+rYsaNBTLNmzdTgwYPNPve7lTaepeSulFIFBQXKxcVFLVu2rMwYo28ZxMbGIjw8HCtWrEDPnj3Rrl07uLi4ID4+Hvv374ezszPat29v8JfXzp07kZSUhO3bt2PTpk3668eMGYMRI0YgMTER/v7+CA8PR0FBUTvkCxcuoH379ujevTuOHz+OVatWYd++fRg6dGiZa4uIiEBISMg915+eno7KlSsbXJeVlQVvb294eXkhLCwMJ0+etIjcN27ciODgYAwZMgTVq1dH3bp1MXXqVBSW0abT3PL/q0WLFuGVV16Bk5OT2efevHlzHDlyRH/K8OLFi9i8eTM6dOhg9rnn5uaWeFvQwcEB+/btK/X+5Tn/8+fPY8uWLWjVqpX+ugMHDiA0NNQgrl27djhw4IDZ524Kc889Ozsb+fn593xOvOcZgrlz5ypXV1cVFxenlFJq+fLlKiAgQOl0On1sbm6ucnBwUFu3blVKFVVN1atXV7m5ufqY4qpp4cKF+utOnjypAKjTp08rpZTq37+/GjRokME64uPjlZWVlbpz545SqmTVNGrUKNW7d+8yq51Vq1YpOzs7deLECf11CQkJatmyZerHH39UcXFxqlOnTkqr1erbOppz7gEBAapChQrq9ddfV4cPH1YrV65UlStXVuPHj9fHmHP+dzt48KACoA4ePGgxuc+ZM0fZ2toqGxsbBUC98cYbFpF7eHi4qlOnjjp79qwqLCxU27ZtUw4ODsrOzs5s8g8ODlYVKlRQANSgQYNUYWGh/jZbW1sVGxtrEP/xxx8rNzc3s8/9bvc6Q2DuuSul1Jtvvql8fX31c5SmzILA09NT2draqkOHDumvHzFihLK2tlZOTk4GPxqNRs2bN0+/kUJDQw3GK95Id4918+ZNBUDt2bNHKaVUkyZNlJ2dncG4jo6OCoA6depUqRvpXnbt2qUcHR3veXpEKaXy8vKUn5+fGjt2rNnnXqtWLeXl5aUKCgr0182cOVO5u7vrL5tz/ncbNGiQqlevnsF15pz77t27VfXq1dWCBQvU8ePH1bp165SXl5eaOHGi2ed+48YNFRYWpqysrJS1tbXy9/dX//73v5W9vb0+prznn5KSok6ePKliY2OVp6eniomJ0d8mKQjMNfe7lVUQWELu06ZNU5UqVVLHjh2753hlfqiwUaNGOHr0KBYvXowmTZpAo9EgKysLjRs3LvUTzNWqVdP/XtopWKDogxDFij/Ip9PpABSdxh88eDAiIyNL3O/JJ58sa5ml2rNnDzp37oxZs2ahT58+94y1tbVFo0aNcP78ef115pq7h4cHbG1tYW1trb+udu3auHbtmsEHYsw1/2K3b9/GypUrMXHixBK3mWvu48aNQ+/evTFgwAAAQL169XD79m0MGjQIY8aMAWC+uVerVg3r169HTk4OfvvtN9SoUQOjRo2Cr6+vQVx5zt/LywsAUKdOHRQWFmLQoEF45513YG1tDXd3d1y/ft0g/vr163B3d9dfNtfcJcw99xkzZuCDDz7Ajh07UL9+/XuOV2ZB4Ofnh5kzZyIkJATW1taYO3cugoKCsGrVKri5uUGr1Zq0cGOCgoJw6tQpPPXUU39rnLi4OHTq1AkxMTEYNGiQ0fjCwkL89NNPBu+lmmvuLVq0QGxsLHQ6nf6fXJw9exYeHh4Gn4411/yLrVmzBrm5uXjttddK3GauuWdnZ5f4xybFTxrqj39nYq65F7O3t4enpyfy8/Oxdu1a9OzZ0+D28pr/X+l0OuTn50On08Ha2hrBwcHYuXOnwfftt2/fjuDgYP1lc81dwpxznz59OqZMmYKtW7eiSZMmRse454cK/f39sXv3bqxduxbDhg1Dr169ULVqVYSFhSE+Ph7JycmIi4tDZGQkrly58reSiY6ORkJCAoYOHYrExEScO3cOGzZsuOcHLUaPHm3w18Du3bvRsWNHREZGonv37rh27RquXbuGmzdv6mMmTpyIbdu24eLFizh69Chee+01XL58Wf+Xkznn/uabb+LmzZuIiorC2bNn8e2332Lq1KkYMmRIifHNMf9iixYtQteuXVGlSpVSxzbH3Dt37oz58+dj5cqVSE5Oxvbt2zFu3Dh07tzZ4InTHHM/ePAg1q1bh4sXLyI+Ph7t27eHTqfDu+++W2L88pb/ihUrsHr1apw+fRoXL17E6tWrMXr0aLz88sv6v1KjoqKwZcsWzJw5E2fOnMH48eNx+PDhEvOYY+55eXlITExEYmIi8vLy8PPPPyMxMdHgjLC55h4TE4Nx48Zh8eLF8PHx0T82srKyypzHaB+CgIAA7Nq1S1897d27F9HR0ejWrRsyMzPh6emJNm3a/O0qqn79+tizZw/GjBmDli1bQikFPz8/vPzyy2XeJy0tDSkpKfrLy5YtQ3Z2NqZNm4Zp06bpr2/VqhXi4uIAAL///jsGDhyIa9euoVKlSmjcuDESEhJQp04ds8/dy8sLW7duxfDhw1G/fn14enoiKioK0dHRpc5hbvkDRQ099u3bh23btt1zTeaW+9ixY6HRaDB27Fj8/PPPqFatGjp37owpU6aYfe45OTkYO3YsLl68CGdnZ3To0AHLly8vs/9GecrfxsYGMTExOHv2LJRS8Pb2xtChQw2+c9+8eXPExsZi7Nix+M9//oNatWph/fr1pfYgMLfcr169atBrYsaMGZgxY0aJ5wVzzH3+/PnIy8vDSy+9ZDDW+++/j/Hjx5c6D//9MREREZlX62IiIiK6PywIiIiIiAUBERERsSAgIiIisCAgIiIisCAgIiIisCAgIiIisCAgIiIisCAgIiIisCAgIiIisCAgIiIisCAgIiIiAP8Pwg+vvRBgb6wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN(out_1=16, out_2=32)\n",
    "plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\n",
    "plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24b446dc-e5f0-4226-96c4-121c1440d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXRayDataset(True)\n",
    "validation_dataset = ChestXRayDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32a7d16b-672d-4fae-b0fe-b4d275d85ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4205, 64, 64]) torch.Size([4205]) torch.Size([467, 64, 64]) torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.X.shape,train_dataset.y.shape,validation_dataset.X.shape,validation_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8e55608-98ae-470b-82f9-4f12c3f857b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset.X = train_dataset.X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf7d0270-1c1a-4646-a5c9-03a225ad7eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ffe137c-755f-4b2d-9d19-9848c75d5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a criterion which will measure loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "# Create an optimizer that updates model parameters using the learning rate and gradient\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# Create a Data Loader for the training data with a batch size of 100 \n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "# Create a Data Loader for the validation data with a batch size of 5000 \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd09b9ee-f883-4b1d-a6e6-edcd9d432fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 1, 64, 64]' is invalid for input of size 20480",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m N_test\n\u001b[0;32m     53\u001b[0m         accuracy_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[75], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Makes a prediction based on X value\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[0;32m     26\u001b[0m z \u001b[38;5;241m=\u001b[39m model(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[100, 1, 64, 64]' is invalid for input of size 20480"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Number of times we want to train on the taining dataset\n",
    "n_epochs=3\n",
    "# List to keep track of cost and accuracy\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "# Size of the validation dataset\n",
    "N_test=len(validation_dataset)\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(n_epochs):\n",
    "    # Loops for each epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        # Keeps track of cost for each epoch\n",
    "        COST=0\n",
    "        # For each batch in train loader\n",
    "        for x, y in train_loader:\n",
    "            # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
    "            optimizer.zero_grad()\n",
    "            # Makes a prediction based on X value\n",
    "            x = torch.reshape(x,(100,1,64,64))\n",
    "\n",
    "            # print(x.shape)\n",
    "            # break\n",
    "            z = model(x)\n",
    "            \n",
    "            # Measures the loss between prediction and acutal Y value\n",
    "            # print(z[0][0], type(z))\n",
    "            loss = criterion(z[0], y[0])\n",
    "            # Calculates the gradient value with respect to each weight and bias\n",
    "            loss.backward()\n",
    "            # Updates the weight and bias according to calculated gradient value\n",
    "            optimizer.step()\n",
    "            # Cumulates loss \n",
    "            COST+=loss.data\n",
    "        \n",
    "        # Saves cost of training data of epoch\n",
    "        cost_list.append(COST)\n",
    "        # Keeps track of correct predictions\n",
    "        correct=0\n",
    "        # Perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            # Makes a prediction\n",
    "            z = model(x_test)\n",
    "            # The class with the max value is the one we are predicting\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            # Checks if the prediction matches the actual value\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        \n",
    "        # Calcualtes accuracy and saves it\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f158610-53e1-4b3c-b3ea-8e65cd20cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f52abd-4bd4-4296-a30d-5adab9a17c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ced57-ebfc-492b-ac6b-869bff5d879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211127-2ab7-4dea-8f8a-e062bda41662",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859b85e-06bc-4406-a349-bcb6709d579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a3d53-c1b6-4372-8461-49bd71bbad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83398d5d-85a1-4616-b659-0f976933b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f780ed74-c63e-4e5a-99dc-a8c8f51c9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        with open('static_data/train.pickle', 'rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "\n",
    "        with open('static_data/test.pickle', 'rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "        df = df.drop(columns=[\"file_name\"])\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            flattened_img = df.iloc[row, :-1].to_numpy()\n",
    "            matrix_img = np.resize(flattened_img, (64, 64))\n",
    "\n",
    "            X.append(matrix_img)\n",
    "            y.append(df.loc[row, \"class_id\"])\n",
    "\n",
    "        if train:\n",
    "            X = [X[i] for i in train_indices]\n",
    "            y = [y[i] for i in train_indices]\n",
    "        else:\n",
    "            X = [X[i] for i in test_indices]\n",
    "            y = [y[i] for i in test_indices]\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3a22be4-0d60-4a0a-8c1d-0a016eec7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        # The reason we start with 1 channel is because we have a single black and white image\n",
    "        # Channel Width after this layer is 16\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        # Channel Width after this layer is 8\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Channel Width after this layer is 8\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        # Channel Width after this layer is 4\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Calculate the size of the input to the fully connected layer dynamically\n",
    "        # This depends on the input size and the operations performed by the CNN layers\n",
    "        # You can calculate it based on your architecture\n",
    "        self.fc_input_size = out_2 * 16  # Adjust this calculation based on your architecture\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 3)  # Adjust the number of output features\n",
    "        \n",
    "    # Forward method\n",
    "    def forward(self, x):\n",
    "        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a545b961-80d4-4794-b886-5e34faec0b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x256 and 512x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[85], line 51\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Assuming x is the input data and y is the labels\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Reshape the input tensor if needed\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# x = torch.reshape(x, (x.size(0), 1, 64, 64))\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[84], line 38\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool2(x)\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x256 and 512x3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assuming you have defined your CNN model class 'CNN' somewhere before\n",
    "# Define your CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Assuming you have defined your criterion (loss function) and optimizer elsewhere\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming you have defined your datasets elsewhere\n",
    "# Assuming train_dataset and validation_dataset are your training and validation datasets, respectively\n",
    "\n",
    "# Define the data loaders for the entire datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset)\n",
    "\n",
    "# Define the number of epochs\n",
    "n_epochs = 3\n",
    "\n",
    "# List to keep track of cost and accuracy\n",
    "cost_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Size of the validation dataset\n",
    "N_test = len(validation_dataset)\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        # Keeps track of cost for each epoch\n",
    "        COST = 0\n",
    "        # Keeps track of correct predictions\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()  # Set the model to train mode\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Assuming x is the input data and y is the labels\n",
    "            # Reshape the input tensor if needed\n",
    "            # x = torch.reshape(x, (x.size(0), 1, 64, 64))\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, y)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            # Cumulate loss\n",
    "            COST += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = correct / total\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_loss = COST / len(train_loader)\n",
    "        cost_list.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {avg_loss:.4f}, Accuracy: {100 * accuracy:.2f}%')\n",
    "\n",
    "# Start training\n",
    "train_model(n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2ba51-f22f-4cd7-9fd6-dc5fda572efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
