{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b812fd61",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad344ad",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a synthetic dataset with missing values to simulate the pandas missing values dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Creating a dictionary with some missing values\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', np.nan, 'David', 'Eva'],\n",
    "    'Age': [25, 30, np.nan, 22, 29],\n",
    "    'Score': [88, 92, 85, np.nan, 95],\n",
    "    'Country': ['USA', 'Canada', np.nan, 'USA', 'Germany']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Get the shape of the DataFrame (rows, columns)\n",
    "df_shape = df.shape\n",
    "df_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1e0e5",
   "metadata": {},
   "source": [
    "My own defition on \"observations\" and \"varaibles\": \"observations\", which is rows according\n",
    "to the dateset in statistics, is a collection of measurements obtained by an individual. \n",
    "\"variabels\", which is columns according to the dateset in statistics, is a feature of\n",
    "something or someone that differs from other individuals that is measured for each\n",
    "observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3c80f",
   "metadata": {},
   "source": [
    "df.shape: This gives the total number of rows and columns in the DataFrame. It reports the full structure of the dataset, regardless of the type of data in each column.\n",
    "df.describe(): By default, df.describe() only provides summary statistics for numerical columns (e.g., integers and floats). This means it excludes non-numerical columns like strings (object type) unless specified otherwise (e.g., df.describe(include='all')).\n",
    "\n",
    "df.shape[0]: This represents the total number of rows (observations) in the dataset, whether or not there are missing values in those rows.\n",
    "df.describe() \"count\": The \"count\" column in df.describe() reports the number of non-missing (non-null) values for each variable. It excludes rows with missing values in the corresponding column.\n",
    "\n",
    "\n",
    "Number of columns analyzed: df.describe() by default only includes numerical columns, while df.shape counts all columns.\n",
    "\"Count\" values in df.describe(): These represent the number of non-missing values in each numerical column, whereas df.shape counts the total number of rows, even if some contain missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91840192",
   "metadata": {},
   "source": [
    "Attributes:\n",
    "Represent static properties or values.\n",
    "Accessed without parentheses (df.shape).\n",
    "Do not change the DataFrame or perform any computation.\n",
    "Methods:\n",
    "Perform actions, calculations, or transformations.\n",
    "Accessed with parentheses (df.describe()).\n",
    "May take arguments and often return a modified DataFrame or computed result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd8685",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/66f1ee10-3744-8000-9eae-8fc66383b7be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55cc42",
   "metadata": {},
   "source": [
    "Count:\n",
    "The number of non-null (non-missing) entries for each variable/column.\n",
    "Mean:\n",
    "The arithmetic average of the data in the column. It's calculated as the sum of all values divided by the count (number of values).\n",
    "Std (Standard Deviation):\n",
    "A measure of the dispersion or spread of data from the mean. It indicates how much the data points typically deviate from the mean.\n",
    "Min (Minimum):\n",
    "The smallest value in the data for each variable/column.\n",
    "25% (1st Quartile):\n",
    "The 25th percentile, also known as the first quartile (Q1). It represents the value below which 25% of the data fall. It gives insight into the lower range of the data.\n",
    "50% (Median or 2nd Quartile):\n",
    "The median or 50th percentile, representing the middle value in the dataset when sorted. It divides the data into two equal halves and is less affected by outliers compared to the mean.\n",
    "75% (3rd Quartile):\n",
    "The 75th percentile, also known as the third quartile (Q3). It represents the value below which 75% of the data fall, providing insight into the upper range of the data.\n",
    "Max (Maximum):\n",
    "The largest value in the dataset for each variable/column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a42516",
   "metadata": {},
   "source": [
    "When missing data is scattered across rows, dropping rows might be a better choice.\n",
    "When missing data is concentrated within certain columns, dropping columns may be a more efficient option. \n",
    "In many cases, a combination of both methods may provide the most efficient use of non-missing data.\n",
    "First, use del df['col'] to remove columns that have a large proportion of missing data, reducing noise and potential bias.\n",
    "Then, use df.dropna() to remove any remaining rows with missing data, as the overall missingness would have reduced after removing the problematic columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f15eaa",
   "metadata": {},
   "source": [
    "Scenario: You have a dataset containing customer feedback with multiple features, including ratings, comments, and demographics.\n",
    "Use Case:\n",
    "Suppose the dataset has 1,000 entries, but only 10 rows have missing values in the Rating column.\n",
    "Since the Rating is crucial for your analysis (e.g., customer satisfaction), and only a small number of entries are missing, you would use df.dropna() to remove just those 10 rows.\n",
    "This preserves the integrity of the dataset, allowing you to retain a majority of the data for analysis without losing important columns.\n",
    "\n",
    "Scenario: You have a dataset of student grades with columns for Student ID, Name, Assignment Scores, and Extracurricular Activities.\n",
    "Use Case:\n",
    "Imagine that the Extracurricular Activities column has 80% missing values while the other columns are complete.\n",
    "In this case, using del df['Extracurricular Activities'] would be preferred.\n",
    "Since this column contributes little information due to the high percentage of missing data, deleting it allows you to keep all the rows intact and focus on the more reliable columns for your analysis.\n",
    "\n",
    "When using both methods, applying del df['col'] before df.dropna() can help in the following ways:\n",
    "Reduce Missingness: Removing columns with high missing values can decrease the overall missingness across the dataset, meaning that fewer rows will be dropped when df.dropna() is applied afterward.\n",
    "Maintain Row Integrity: If you have several columns with scattered missing values, dropping the ones with excessive missing data first may allow you to keep more rows that contain valuable information.\n",
    "Streamlined Analysis: It simplifies the dataset for analysis by eliminating unreliable data early on, making the subsequent row removal process more effective.\n",
    "    \n",
    "Justification for Approach\n",
    "By removing the Extracurricular Activities column first, which had a high percentage of missing values, I ensured that I retained the maximum number of rows with valid data in the Score column. After removing this column, I used df.dropna() to clean up any remaining missing values, allowing me to maintain a clean and effective dataset for further analysis. This strategy minimizes data loss while focusing on retaining the most relevant information.\n",
    "\n",
    "The remove action using some combination of del df['col'] and/or df.dropna() is as follows in the next In."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f765be7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name  Score\n",
      "0   1    Alice   85.0\n",
      "2   3  Charlie   90.0\n",
      "3   4    David   78.0\n",
      "5   6    Frank   92.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "    'Score': [85, np.nan, 90, 78, np.nan, 92],\n",
    "    'Extracurricular Activities': ['Football', 'Basketball', np.nan, 'Soccer', np.nan, 'Chess']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Remove the column with high missingness\n",
    "del df['Extracurricular Activities']\n",
    "\n",
    "# Step 2: Drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Final DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7dd36",
   "metadata": {},
   "source": [
    "df.groupby(\"col1\"): This part of the command groups the DataFrame by the unique values in col1. Each unique value in col1 forms a separate group.\n",
    "[\"col2\"]: This specifies that we are interested in the col2 column for our analysis.\n",
    ".describe(): This function generates descriptive statistics for col2 for each group created by col1. The output will typically include:\n",
    "count: Number of non-null entries in col2.\n",
    "mean: Average of col2 for the group.\n",
    "std: Standard deviation of col2.\n",
    "min: Minimum value of col2.\n",
    "25%: 25th percentile (1st quartile) of col2.\n",
    "50%: Median (2nd quartile) of col2.\n",
    "75%: 75th percentile (3rd quartile) of col2.\n",
    "max: Maximum value of col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37475ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],\n",
    "    'Values': [10, 20, 30, 15, 25, 35, 5, 15, 25, None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd11474",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          count  mean   std   min   25%   50%   75%   max\n",
      "Category                                                 \n",
      "A           3.0  20.0  10.0  10.0  15.0  20.0  25.0  30.0\n",
      "B           3.0  25.0  10.0  15.0  20.0  25.0  30.0  35.0\n",
      "C           3.0  15.0  10.0   5.0  10.0  15.0  20.0  25.0\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Category' and describe 'Values'\n",
    "result = df.groupby(\"Category\")[\"Values\"].describe()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c40c29",
   "metadata": {},
   "source": [
    "Count: The number of valid (non-null) entries for Values in each category:\n",
    "Category A: 3\n",
    "Category B: 3\n",
    "Category C: 3\n",
    "Mean: The average of Values for each category:\n",
    "Category A: (10 + 20 + 30) / 3 = 20.0\n",
    "Category B: (15 + 25 + 35) / 3 = 25.0\n",
    "Category C: (5 + 15 + 25) / 3 = 15.0\n",
    "Standard Deviation (std): Measures the spread of Values for each category:\n",
    "All categories have a standard deviation of 10.0, indicating how much the values vary around the mean.\n",
    "Min, 25%, 50%, 75%, Max: These values provide insights into the distribution of Values:\n",
    "Min: The smallest value in each category.\n",
    "25% (1st quartile): The value below which 25% of the data fall.\n",
    "50% (Median): The middle value of Values.\n",
    "75% (3rd quartile): The value below which 75% of the data fall.\n",
    "Max: The largest value in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55646fba",
   "metadata": {},
   "source": [
    "While df.describe() provides an overall view of the dataset, including how many entries are\n",
    "missing across all columns, df.groupby(\"col1\")[\"col2\"].describe() gives a detailed \n",
    "breakdown that highlights the distribution of valid data across specific categories. \n",
    "This distinction is crucial for understanding the context and reliability of the statistics,\n",
    "particularly when considering how missing data impacts analysis and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a494033",
   "metadata": {},
   "source": [
    "question8.3:Different errors I get in the ChatBot session.\n",
    "A:NameError: name 'pd' is not defined\n",
    "\n",
    "B:FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv'\n",
    "\n",
    "C:NameError: name 'DF' is not defined\n",
    "\n",
    "D:SyntaxError: unexpected EOF while parsing\n",
    "\n",
    "E:AttributeError: 'DataFrame' object has no attribute 'group_by'\n",
    "\n",
    "F:KeyError: 'Sex'\n",
    "\n",
    "G:NameError: name 'sex' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ed55a",
   "metadata": {},
   "source": [
    "It's ok to work with ChatBot to troubleshoot and fix the coding errors, and I think a google search for the error doesn't provide the necessary toubleshooting help more quickly than ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9135c5",
   "metadata": {},
   "source": [
    "question 9:Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab631a",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/66f5fdb1-3200-8000-865a-982cfce45b7e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
