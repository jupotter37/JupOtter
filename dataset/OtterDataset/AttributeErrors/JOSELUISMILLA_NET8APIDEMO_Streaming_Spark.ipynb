{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JOSELUISMILLA/NET8APIDEMO/blob/main/Streaming_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuaderno de: Ricardo Alonzo Fernandez Salguero"
      ],
      "metadata": {
        "id": "HpxhKeQeLxzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming en PySpark: Procesamiento Continuo de Datos en Big Data\n",
        "\n",
        "El procesamiento de datos en tiempo real es fundamental en el mundo actual, donde la velocidad y la toma de decisiones basada en datos son esenciales. PySpark, una biblioteca de procesamiento de datos distribuida basada en Apache Spark, ofrece una solución poderosa para el procesamiento continuo de datos en grandes volúmenes.\n",
        "\n",
        "## ¿Qué es el Streaming de PySpark?\n",
        "\n",
        "Streaming en PySpark se refiere al procesamiento continuo y en tiempo real de datos. A diferencia del procesamiento por lotes, donde los datos se procesan en conjuntos fijos y predefinidos, el streaming permite que los datos se procesen a medida que llegan, lo que habilita aplicaciones en tiempo real, como análisis de tendencias, detección de anomalías y generación de alertas.\n",
        "\n",
        "El streaming de PySpark se basa en el motor de procesamiento distribuido de Apache Spark, lo que lo hace escalable y adecuado para trabajar con grandes volúmenes de datos en clústeres de computadoras. PySpark proporciona una API fácil de usar para configurar, ejecutar y administrar flujos de datos en tiempo real.\n",
        "\n",
        "## Principios Clave del Streaming en PySpark\n",
        "\n",
        "Para comprender mejor el streaming en PySpark, es fundamental conocer algunos de sus principios clave:\n",
        "\n",
        "### Micro-Batches:\n",
        "Aunque el término \"streaming\" puede sugerir un flujo de datos ininterrumpido, en PySpark, los datos generalmente se procesan en micro-batches. Un micro-batch es una pequeña porción de datos que se recopila durante un intervalo de tiempo definido y luego se procesa. Esta abstracción permite la paralelización y la administración eficiente de recursos en entornos distribuidos.\n",
        "\n",
        "### Tolerancia a Fallos:\n",
        "PySpark está diseñado para ser altamente tolerante a fallos. En caso de que un nodo falle durante el procesamiento en tiempo real, los datos se pueden recuperar y reprocesar, lo que garantiza la confiabilidad y la integridad de los resultados.\n",
        "\n",
        "### Ventanas Temporales:\n",
        "El procesamiento de streaming a menudo implica el análisis de datos dentro de ventanas temporales. Las ventanas temporales permiten realizar cálculos y agregaciones en un conjunto específico de datos durante un período de tiempo definido, lo que facilita el análisis de tendencias y la generación de resultados en tiempo real.\n",
        "\n",
        "## Configuración Básica del Streaming en PySpark\n",
        "\n",
        "A continuación, exploraremos los pasos básicos para configurar un proceso de streaming en PySpark:\n",
        "\n",
        "### Paso 1: Crear una Sesión Spark\n",
        "La primera tarea en la configuración del streaming es crear una sesión Spark. La sesión Spark proporciona un punto de entrada para interactuar con el clúster de Spark y define configuraciones clave, como el modo de ejecución y el nombre de la aplicación.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"StreamingDemo\") \\\n",
        "    .getOrCreate()\n",
        "```\n",
        "\n",
        "En este ejemplo, se crea una sesión Spark con el nombre \"StreamingDemo\" y se ejecuta en modo local para fines de desarrollo. En un entorno de producción, se especificaría la URL del clúster Spark en lugar de \"local\".\n",
        "\n",
        "### Paso 2: Definir el Esquema de Datos\n",
        "Para procesar datos en streaming de manera efectiva, es fundamental definir un esquema para los datos entrantes. El esquema especifica la estructura de los datos, incluidos los nombres de columna y los tipos de datos.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "    StructField(\"sensor_id\", IntegerType(), True),\n",
        "    StructField(\"value\", DoubleType(), True)\n",
        "])\n",
        "```\n",
        "\n",
        "En este ejemplo, se define un esquema para datos que contienen un campo de tiempo (timestamp), un identificador de sensor (sensor_id) y un valor numérico (value). El tipo de datos de cada campo se especifica adecuadamente.\n",
        "\n",
        "### Paso 3: Configurar el Origen de Datos\n",
        "El siguiente paso es configurar el origen de datos que proporcionará los datos en tiempo real. PySpark admite varias fuentes de datos, como Kafka, sistemas de archivos, sockets y más. La elección de la fuente de datos depende de la aplicación específica.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "source = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"sensor_data\") \\\n",
        "    .load()\n",
        "\n",
        "# Deserializar datos JSON\n",
        "source = source.selectExpr(\"CAST(value AS STRING) as json\")\n",
        "```\n",
        "\n",
        "En este ejemplo, se configura una fuente Kafka que escucha en el tema \"sensor_data\". Los datos se leen en formato JSON y se convierten en una columna llamada \"json\". La elección de Kafka es común para flujos de datos en tiempo real debido a su capacidad de transmisión escalable.\n",
        "\n",
        "### Paso 4: Aplicar Transformaciones y Agregaciones\n",
        "Una vez que los datos fluyen desde la fuente, es posible aplicar transformaciones y agregaciones en tiempo real utilizando las capacidades de PySpark.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import window, avg\n",
        "\n",
        "result = source \\\n",
        "    .selectExpr(\"CAST(json AS struct<timestamp: string, sensor_id: int, value: double>)\") \\\n",
        "    .withColumn(\"timestamp\", source[\"timestamp\"].cast(StringType())) \\\n",
        "    .withColumn(\"value\", source[\"value\"].cast(DoubleType())) \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(\"timestamp\", \"5 minutes\"),\n",
        "        \"sensor_id\"\n",
        "    ) \\\n",
        "    .agg(avg(\"value\").alias(\"avg_value\"))\n",
        "```\n",
        "\n",
        "En este ejemplo, se realiza una serie de transformaciones en los datos, incluida la conversión de tipos de datos, la definición de una ventana temporal de 5 minutos y el cálculo del valor promedio por sensor dentro de cada ventana. Estas transformaciones son solo un ejemplo, y las posibilidades son amplias según los requisitos de la aplicación.\n",
        "\n",
        "### Paso 5: Escribir Resultados a un Destino\n",
        "Una vez que se han aplicado las transformaciones y agregaciones, los resultados se pueden escribir en un destino específico para su posterior análisis o visualización.\n",
        "\n",
        "```python\n",
        "query = result \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "\n",
        "En este ejemplo, los resultados se escriben en la consola para fines de depuración y\n",
        "\n",
        " pruebas. Sin embargo, PySpark admite una variedad de destinos, como sistemas de archivos, bases de datos, sistemas de mensajería y más. El proceso de streaming se inicia con `start()`, y `awaitTermination()` hace que el proceso espere hasta que se detenga manualmente o debido a un error.\n",
        "\n",
        "## Ventanas Temporales en Streaming\n",
        "Una característica poderosa del streaming en PySpark es la capacidad de definir ventanas temporales. Las ventanas temporales permiten agrupar y analizar datos dentro de un intervalo de tiempo específico. Esto es fundamental para el análisis en tiempo real y la generación de resultados basados en ventanas de tiempo. PySpark admite varias formas de definir ventanas temporales:\n",
        "\n",
        "### Ventana Fija:\n",
        "En una ventana fija, los datos se agrupan en intervalos de tiempo de igual duración. Por ejemplo, es posible definir una ventana de 5 minutos que agrupe los datos en intervalos de 5 minutos cada uno.\n",
        "\n",
        "```python\n",
        "windowed_data = source \\\n",
        "    .groupBy(\n",
        "        window(\"timestamp\", \"5 minutes\"),\n",
        "        \"sensor_id\"\n",
        "    ) \\\n",
        "    .agg(avg(\"value\").alias(\"avg_value\"))\n",
        "```\n",
        "\n",
        "En este ejemplo, los datos se agrupan en ventanas de 5 minutos.\n",
        "\n",
        "### Ventana Deslizante:\n",
        "Una ventana deslizante permite que los datos se superpongan en múltiples ventanas. Esto significa que un punto de datos puede estar presente en más de una ventana si cumple con los criterios de tiempo.\n",
        "\n",
        "```python\n",
        "windowed_data = source \\\n",
        "    .groupBy(\n",
        "        window(\"timestamp\", \"5 minutes\", \"1 minute\"),\n",
        "        \"sensor_id\"\n",
        "    ) \\\n",
        "    .agg(avg(\"value\").alias(\"avg_value\"))\n",
        "```\n",
        "\n",
        "En este ejemplo, se define una ventana de 5 minutos que se desliza cada 1 minuto. Esto permite un solapamiento de ventanas y un análisis más granular de los datos.\n",
        "\n",
        "### Ventana de Agua:\n",
        "Una ventana de agua define un período de tolerancia para los datos retrasados o desordenados. Esto es útil cuando los datos pueden llegar fuera de orden o con retraso.\n",
        "\n",
        "```python\n",
        "source = source \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
        "```\n",
        "\n",
        "En este ejemplo, se define una ventana de agua de 10 minutos, lo que significa que se aceptarán datos que lleguen hasta 10 minutos después de su marca de tiempo.\n",
        "\n",
        "## Ventajas del Streaming en PySpark\n",
        "\n",
        "El streaming en PySpark ofrece una serie de ventajas clave para el procesamiento de datos en tiempo real:\n",
        "\n",
        "### Escalabilidad:\n",
        "PySpark se ejecuta en un clúster distribuido, lo que permite escalar horizontalmente para manejar grandes volúmenes de datos y cargas de trabajo de procesamiento intensivas.\n",
        "\n",
        "### Tolerancia a Fallos:\n",
        "PySpark está diseñado para ser altamente tolerante a fallos, lo que garantiza la disponibilidad y la integridad de los resultados en caso de fallas en los nodos.\n",
        "\n",
        "### Abstracción de Micro-Batch:\n",
        "El uso de micro-batches permite una administración eficiente de recursos y una mayor flexibilidad en la programación y el procesamiento de datos en tiempo real.\n",
        "\n",
        "### Integración con Fuentes de Datos:\n",
        "PySpark ofrece amplia compatibilidad con diversas fuentes de datos, como Kafka, sistemas de archivos, sockets, bases de datos y más, lo que facilita la ingestión de datos en tiempo real desde múltiples fuentes.\n",
        "\n",
        "### Ventanas Temporales:\n",
        "La capacidad de definir ventanas temporales facilita el análisis de tendencias, la detección de patrones y la generación de resultados basados en intervalos de tiempo específicos.\n",
        "\n",
        "## Casos de Uso del Streaming en PySpark\n",
        "\n",
        "El streaming en PySpark se utiliza en una amplia variedad de casos de uso en la actualidad. Algunos ejemplos incluyen:\n",
        "\n",
        "### Análisis de Redes Sociales:\n",
        "El streaming se utiliza para analizar y rastrear tendencias en redes sociales en tiempo real, lo que permite a las empresas tomar decisiones informadas y responder rápidamente a eventos y comentarios de los usuarios.\n",
        "\n",
        "### Monitorización de Infraestructuras:\n",
        "Las organizaciones utilizan el streaming para monitorizar la salud y el rendimiento de sus infraestructuras, generando alertas en tiempo real en caso de problemas.\n",
        "\n",
        "### Detección de Anomalías:\n",
        "El streaming se aplica en la detección de anomalías, donde se analizan datos continuos para identificar patrones inusuales o comportamientos anómalos.\n",
        "\n",
        "### Procesamiento de Eventos IoT:\n",
        "En el Internet de las cosas (IoT), el streaming se utiliza para procesar eventos generados por sensores y dispositivos conectados en tiempo real, lo que permite el monitoreo y la automatización de procesos.\n",
        "\n",
        "### Seguridad y Vigilancia:\n",
        "Las aplicaciones de seguridad y vigilancia aprovechan el streaming para analizar flujos de video y detectar eventos sospechosos o actividades inusuales.\n",
        "\n",
        "## Desafíos del Streaming en PySpark\n",
        "\n",
        "Si bien el streaming en PySpark ofrece muchas ventajas, también presenta algunos desafíos:\n",
        "\n",
        "### Latencia:\n",
        "El procesamiento en tiempo real a menudo implica latencia mínima para tomar decisiones rápidas. Optimizar el rendimiento y reducir la latencia puede ser un desafío en entornos de alta carga.\n",
        "\n",
        "### Escalabilidad Correcta:\n",
        "Si bien PySpark es altamente escalable, es importante diseñar y configurar el clúster correctamente para garantizar un rendimiento óptimo en escenarios de carga variable.\n",
        "\n",
        "### Gestión de Ventanas Temporales:\n",
        "Definir y gestionar ventanas temporales puede ser complejo, especialmente en aplicaciones con múltiples fuentes de datos y requisitos de análisis en tiempo real.\n",
        "\n",
        "### Mantenimiento y Monitoreo:\n",
        "Los flujos de datos en tiempo real requieren un mantenimiento y monitoreo constante para garantizar que los resultados sean precisos y que cualquier problema se aborde de manera oportuna."
      ],
      "metadata": {
        "id": "39WvG44ELG2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming PySpark\n",
        "\n",
        "### Parte 1: Configuración del Streaming de Datos de Personas\n",
        "Esta parte del código se encarga de configurar un proceso de streaming de datos que simula la lectura y procesamiento de datos de personas desde un archivo CSV.\n",
        "\n",
        "#### Paso 1: Instalar PySpark\n",
        "```markdown\n",
        "!pip install pyspark\n",
        "```\n",
        "Este paso instala la biblioteca PySpark, que se utiliza para el procesamiento de datos en el entorno de Spark.\n",
        "\n",
        "#### Paso 2: Crear una sesión Spark\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Streaming\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "```\n",
        "En este paso, se crea una sesión Spark que se utiliza para interactuar con Spark y realizar operaciones de procesamiento de datos.\n",
        "\n",
        "#### Paso 3: Definir el esquema para los datos CSV\n",
        "```python\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"first\", StringType(), True),\n",
        "    StructField(\"last\", StringType(), True),\n",
        "    StructField(\"middle\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"phone\", StringType(), True),\n",
        "    StructField(\"fax\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])\n",
        "```\n",
        "En este paso, se define el esquema de datos para los datos CSV que se van a leer en el proceso de streaming. El esquema especifica el nombre y el tipo de datos de cada columna en el archivo CSV.\n",
        "\n",
        "#### Paso 4: Leer datos CSV desde una URL\n",
        "```python\n",
        "import os\n",
        "\n",
        "# URL de los datos CSV\n",
        "csv_url = \"https://raw.githubusercontent.com/lawlesst/vivo-sample-data/master/data/csv/people.csv\"\n",
        "\n",
        "# Descargar el archivo CSV\n",
        "!wget {csv_url} -O people.csv\n",
        "\n",
        "# Mover el archivo CSV a un directorio de streaming simulado\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "!mv people.csv {streaming_directory}\n",
        "```\n",
        "En este paso, se descarga un archivo CSV desde una URL y se mueve a un directorio de streaming simulado. Este paso es necesario porque Spark no puede leer directamente desde una URL con `readStream`.\n",
        "\n",
        "#### Paso 5: Configurar el streaming\n",
        "```python\n",
        "people_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(streaming_directory)\n",
        "```\n",
        "Aquí se configura el proceso de streaming. Se utiliza `readStream` para leer datos de forma continua desde el directorio de streaming simulado. Se especifica el formato como \"csv\", se aplica el esquema definido en el Paso 3 y se establece la opción \"header\" en True para indicar que la primera fila contiene nombres de columna.\n",
        "\n",
        "#### Paso 6: Definir la transformación y almacenar los resultados\n",
        "```python\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "query = people_df.writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .queryName(\"selectTable\") \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "En este paso, se define la transformación que se realizará en los datos y se configura la salida de resultados. Los datos se escriben en formato JSON en el directorio de resultados especificado. El proceso de streaming se inicia con `start()`, y `awaitTermination()` hace que el proceso espere hasta que se detenga de manera manual o debido a un error.\n",
        "\n",
        "### Parte 2: Generación de Datos Simulados de Personas en Streaming\n",
        "Esta parte del código simula la generación de datos de personas en streaming para el proceso de Spark.\n",
        "\n",
        "#### Paso 1: Instalar PySpark (repetido)\n",
        "Este paso ya se ha explicado anteriormente en la Parte 1.\n",
        "\n",
        "#### Paso 2: Crear una sesión Spark (repetido)\n",
        "Este paso también se ha explicado anteriormente y se crea una nueva sesión Spark.\n",
        "\n",
        "#### Paso 3: Definir el esquema para los datos CSV (repetido)\n",
        "Este paso define el esquema de datos para los datos CSV simulados.\n",
        "\n",
        "#### Paso 4: Simular datos en streaming\n",
        "```python\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import threading\n",
        "\n",
        "# Función para generar datos simulados y escribirlos en un archivo\n",
        "def generate_and_write_data(batch_size, batch_number, directory):\n",
        "    for i in range(batch_number):\n",
        "        # Generar datos simulados\n",
        "        data = {\n",
        "            \"person_ID\": [j for j in range(i * batch_size, (i + 1) * batch_size)],\n",
        "            \"name\": [\"Name_\" + str(j) for j in range(i * batch_size, (i + 1) * batch_size)],\n",
        "            # Añade aquí más campos según el esquema\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Escribir en un archivo CSV en el directorio especificado\n",
        "        file_path = os.path.join(directory, f\"file_{i}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "\n",
        "        # Esperar antes de generar el siguiente lote\n",
        "        time.sleep(1)\n",
        "\n",
        "# Directorio para los archivos CSV simulados\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "\n",
        "# Iniciar la generación de datos en un hilo separado\n",
        "thread = threading.Thread(target=generate_and_write_data, args=(10, 100, streaming_directory))\n",
        "thread.start()\n",
        "```\n",
        "En este paso, se define una función `generate_and_write_data` que genera datos simulados y los escribe en archivos CSV en el directorio de streaming simulado. Luego, se inicia un hilo separado para ejecutar esta función y simular la generación de datos en streaming.\n",
        "\n",
        "#### Paso 5: Configurar el streaming (repetido)\n",
        "Este paso configura el proceso de streaming para leer los datos generados en el directorio de streaming simulado.\n",
        "\n",
        "#### Paso 6: Procesar y Almacenar los Resultados\n",
        "```python\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "# Generar un nombre de consulta único\n",
        "import uuid\n",
        "unique_query_name = \"selectTable_\" + str(uuid.uuid4())\n",
        "\n",
        "os.makedirs(results_directory, exist_ok=True)\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "query = people_df.writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .queryName(unique_query_name) \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination\n",
        "\n",
        "()\n",
        "```\n",
        "En este paso, se configura la salida de resultados en formato JSON en un directorio específico. Se utiliza un nombre de consulta único para evitar conflictos. El proceso de streaming se inicia y espera su terminación.\n",
        "\n",
        "### Parte 3: Generación de Datos Simulados de Temperatura en Streaming\n",
        "Esta parte del código simula la generación de datos de temperatura en streaming para el proceso de Spark.\n",
        "\n",
        "#### Paso 1: Instalar PySpark y Matplotlib (repetido)\n",
        "Estos pasos ya se han explicado anteriormente.\n",
        "\n",
        "#### Paso 2: Crear una sesión Spark (repetido)\n",
        "Este paso también se ha explicado anteriormente y se crea una nueva sesión Spark.\n",
        "\n",
        "#### Paso 3: Definir el esquema para los datos CSV (repetido)\n",
        "Este paso define el esquema de datos para los datos de temperatura simulados.\n",
        "\n",
        "#### Paso 4: Simular datos en streaming\n",
        "```python\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import threading\n",
        "\n",
        "def generate_and_write_data(batch_size, batch_number, directory):\n",
        "    for i in range(batch_number):\n",
        "        data = {\n",
        "            \"timestamp\": [pd.Timestamp.now() for _ in range(batch_size)],\n",
        "            \"temperature\": [random.randint(50, 60) for _ in range(batch_size)]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        file_path = os.path.join(directory, f\"file_{i}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "        time.sleep(1)\n",
        "\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "\n",
        "thread = threading.Thread(target=generate_and_write_data, args=(10, 100, streaming_directory))\n",
        "thread.start()\n",
        "```\n",
        "En este paso, se define una función `generate_and_write_data` que genera datos simulados de temperatura y los escribe en archivos CSV en el directorio de streaming simulado. Luego, se inicia un hilo separado para ejecutar esta función y simular la generación de datos en streaming.\n",
        "\n",
        "#### Paso 5: Configurar el streaming\n",
        "```python\n",
        "from pyspark.sql.functions import window\n",
        "\n",
        "people_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", False) \\\n",
        "    .load(streaming_directory)\n",
        "```\n",
        "Este paso configura el proceso de streaming para leer los datos generados en el directorio de streaming simulado. La diferencia aquí es que se utiliza `window` de PySpark para configurar una ventana de tiempo en la que se agruparán los datos.\n",
        "\n",
        "#### Paso 6: Procesar y Almacenar los Resultados\n",
        "```python\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "os.makedirs(results_directory, exist_ok=True)\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "from pyspark.sql.functions import current_timestamp\n",
        "import uuid\n",
        "\n",
        "unique_query_name = \"selectTable_\" + str(uuid.uuid4())\n",
        "\n",
        "query = people_df \\\n",
        "    .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
        "    .writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .queryName(unique_query_name) \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "En este paso, se configura la salida de resultados en formato JSON en un directorio específico. Se utiliza un nombre de consulta único y se agrega una columna de marca de tiempo actual a los datos. El proceso de streaming se inicia y espera su terminación.\n",
        "\n",
        "### Parte 4: Combina Datos de Archivos CSV\n",
        "Esta parte del código combina datos de múltiples archivos CSV en un DataFrame único.\n",
        "\n",
        "```python\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "directory_path = '/content/streaming/'\n",
        "\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(directory_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        df = pd.read_csv(file_path)\n",
        "        combined_df = combined_df.append(df, ignore_index=True)\n",
        "\n",
        "combined_df.sort_values('timestamp', inplace=True)\n",
        "```\n",
        "\n",
        "Este código utiliza la biblioteca `pandas` para combinar los datos de múltiples archivos CSV en un DataFrame llamado `combined_df`. Luego, se ordenan los datos por la columna \"timestamp\". Este paso es útil cuando se desean combinar resultados de múltiples procesos de streaming en uno solo."
      ],
      "metadata": {
        "id": "dXXDzUtuKszg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalar PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Paso 2: Crear una sesión Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Streaming\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Paso 3: Definir el esquema para los datos CSV\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"first\", StringType(), True),\n",
        "    StructField(\"last\", StringType(), True),\n",
        "    StructField(\"middle\", StringType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"phone\", StringType(), True),\n",
        "    StructField(\"fax\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Paso 4: Leer datos CSV desde una URL\n",
        "# Nota: Normalmente no puedes leer directamente desde una URL con readStream.\n",
        "# Para simular el streaming, primero descargamos los datos y luego los leemos como un stream.\n",
        "\n",
        "import os\n",
        "\n",
        "# URL de los datos CSV\n",
        "csv_url = \"https://raw.githubusercontent.com/lawlesst/vivo-sample-data/master/data/csv/people.csv\"\n",
        "\n",
        "# Descargar el archivo CSV\n",
        "!wget {csv_url} -O people.csv\n",
        "\n",
        "# Mover el archivo CSV a un directorio de streaming simulado\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "!mv people.csv {streaming_directory}\n",
        "\n",
        "# Paso 5: Configurar el streaming\n",
        "people_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(streaming_directory)\n",
        "\n",
        "# Paso 6: Definir la transformación y almacenar los resultados\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "query = people_df.writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .queryName(\"selectTable\") \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "u1e0a4sYIXKu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33988bfc-d453-42fd-ddb7-b70cad82b205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=a9629650f54863e025f1cdd37f56d1d21cdd0df47880bacddad2cab4f93c8fb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n",
            "--2024-09-08 17:04:27--  https://raw.githubusercontent.com/lawlesst/vivo-sample-data/master/data/csv/people.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4095 (4.0K) [text/plain]\n",
            "Saving to: ‘people.csv’\n",
            "\n",
            "people.csv          100%[===================>]   4.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-08 17:04:27 (37.6 MB/s) - ‘people.csv’ saved [4095/4095]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=41>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o16.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "An error occurred while calling o45.awaitTermination",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e441ff006a3f>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o45.awaitTermination"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalar PySpark\n",
        "!pip install pyspark\n",
        "\n",
        "# Paso 2: Crear una sesión Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Streaming\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Paso 3: Definir el esquema para los datos CSV\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"person_ID\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    # Agrega aquí más campos según sea necesario\n",
        "])\n",
        "\n",
        "# Paso 4: Simular datos en streaming\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import threading\n",
        "\n",
        "# Función para generar datos simulados y escribirlos en un archivo\n",
        "def generate_and_write_data(batch_size, batch_number, directory):\n",
        "    for i in range(batch_number):\n",
        "        # Generar datos simulados\n",
        "        data = {\n",
        "            \"person_ID\": [j for j in range(i * batch_size, (i + 1) * batch_size)],\n",
        "            \"name\": [\"Name_\" + str(j) for j in range(i * batch_size, (i + 1) * batch_size)],\n",
        "            # Añade aquí más campos según el esquema\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Escribir en un archivo CSV en el directorio especificado\n",
        "        file_path = os.path.join(directory, f\"file_{i}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "\n",
        "        # Esperar antes de generar el siguiente lote\n",
        "        time.sleep(1)\n",
        "\n",
        "# Directorio para los archivos CSV simulados\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "\n",
        "# Iniciar la generación de datos en un hilo separado\n",
        "thread = threading.Thread(target=generate_and_write_data, args=(10, 100, streaming_directory))\n",
        "thread.start()\n",
        "\n",
        "# Paso 5: Configurar el streaming\n",
        "people_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", False) \\\n",
        "    .load(streaming_directory)\n",
        "\n",
        "# Paso 6: Procesar y Almacenar los Resultados\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "# Generar un nombre de consulta único\n",
        "import uuid\n",
        "unique_query_name = \"selectTable_\" + str(uuid.uuid4())\n",
        "\n",
        "os.makedirs(results_directory, exist_ok=True)\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "query = people_df.writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .queryName(unique_query_name) \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()\n"
      ],
      "metadata": {
        "id": "LJkKtbJiHqt4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "f860df8a-b72f-40d9-9e0d-92d90e89869a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cc93c5eddab2>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalar PySpark\n",
        "!pip install pyspark matplotlib\n",
        "\n",
        "# Paso 2: Crear una sesión Spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Streaming\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Paso 3: Definir el esquema para los datos CSV\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
        "import random\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"temperature\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Paso 4: Simular datos en streaming\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import threading\n",
        "\n",
        "def generate_and_write_data(batch_size, batch_number, directory):\n",
        "    for i in range(batch_number):\n",
        "        data = {\n",
        "            \"timestamp\": [pd.Timestamp.now() for _ in range(batch_size)],\n",
        "            \"temperature\": [random.randint(50, 60) for _ in range(batch_size)]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        file_path = os.path.join(directory, f\"file_{i}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "        time.sleep(1)\n",
        "\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "\n",
        "thread = threading.Thread(target=generate_and_write_data, args=(10, 100, streaming_directory))\n",
        "thread.start()\n",
        "\n",
        "# Paso 5: Configurar el streaming\n",
        "from pyspark.sql.functions import window\n",
        "\n",
        "people_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", False) \\\n",
        "    .load(streaming_directory)\n",
        "\n",
        "# Paso 6: Procesar y Almacenar los Resultados\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "os.makedirs(results_directory, exist_ok=True)\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "from pyspark.sql.functions import current_timestamp\n",
        "import uuid\n",
        "\n",
        "# Generar un nombre de consulta único\n",
        "unique_query_name = \"selectTable_\" + str(uuid.uuid4())\n",
        "\n",
        "# Asegúrate de que las líneas siguientes estén correctamente indentadas\n",
        "query = people_df \\\n",
        "    .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
        "    .writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .queryName(unique_query_name) \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "ewWLozjBLkJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b129f7f-04ee-4c12-fb55-7f98ee81da8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark matplotlib\n",
        "# Importaciones necesarias\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "import pandas as pd\n",
        "import random\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Paso 1: Crear una sesión Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"Streaming\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Paso 2: Definir el esquema para los datos CSV\n",
        "schema = StructType([\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"temperature\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Paso 3: Función para generar datos en streaming\n",
        "def generate_and_write_data(batch_size, batch_number, directory):\n",
        "    for i in range(batch_number):\n",
        "        data = {\n",
        "            \"timestamp\": [pd.Timestamp.now() for _ in range(batch_size)],\n",
        "            \"temperature\": [random.randint(50, 60) for _ in range(batch_size)]\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        file_path = os.path.join(directory, f\"file_{i}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "        time.sleep(1)\n",
        "\n",
        "streaming_directory = \"streaming\"\n",
        "os.makedirs(streaming_directory, exist_ok=True)\n",
        "\n",
        "# Iniciar la generación de datos en un hilo separado\n",
        "thread = threading.Thread(target=generate_and_write_data, args=(10, 100, streaming_directory))\n",
        "thread.start()\n",
        "\n",
        "# Paso 4: Configurar el streaming\n",
        "people_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", False) \\\n",
        "    .load(streaming_directory)\n",
        "\n",
        "# Paso 5: Procesar y Almacenar los Resultados (sin bloquear)\n",
        "results_directory = \"results\"\n",
        "checkpoint_directory = \"checkpoint\"\n",
        "\n",
        "os.makedirs(results_directory, exist_ok=True)\n",
        "os.makedirs(checkpoint_directory, exist_ok=True)\n",
        "\n",
        "query = people_df \\\n",
        "    .writeStream \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"checkpointLocation\", checkpoint_directory) \\\n",
        "    .option(\"path\", results_directory) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the directory where the CSV files are stored\n",
        "directory_path = '/content/streaming/'\n",
        "\n",
        "# Initialize an empty DataFrame to hold all combined data\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "# Loop through all the files in the directory\n",
        "for file_name in os.listdir(directory_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        # Read the CSV file into a DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "        # Append the DataFrame to the combined DataFrame\n",
        "        combined_df = combined_df.append(df, ignore_index=True)\n",
        "\n",
        "# Now combined_df contains all the data from the CSV files\n",
        "print(combined_df)\n",
        "\n",
        "\n",
        "# Sort the dataframe by timestamp just to be sure\n",
        "combined_df.sort_values('timestamp', inplace=True)\n",
        "\n",
        "# Asegúrate de detener el streaming query cuando ya no sea necesario\n",
        "query.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "gMobeEH3QZtP",
        "outputId": "83caf2db-3561-4c36-e664-0094f8bd77d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-decc557bb1dd>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Append the DataFrame to the combined DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Now combined_df contains all the data from the CSV files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ]
    }
  ]
}