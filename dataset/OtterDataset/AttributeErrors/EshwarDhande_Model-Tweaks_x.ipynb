{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 'fc1.weight' requires gradient computation.\n",
      "Parameter 'fc1.bias' requires gradient computation.\n",
      "Parameter 'fc2.weight' requires gradient computation.\n",
      "Parameter 'fc2.bias' requires gradient computation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Iterate through named parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter '{name}' requires gradient computation.\")\n",
    "\n",
    "# Output:\n",
    "# Parameter 'fc1.weight' requires gradient computation.\n",
    "# Parameter 'fc1.bias' requires gradient computation.\n",
    "# Parameter 'fc2.weight' requires gradient computation.\n",
    "# Parameter 'fc2.bias' requires gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eshwar.dhande/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-2.9189e-01, -1.5491e-01, -1.3439e-01,  ..., -2.0054e-01,\n",
      "           2.1889e-01,  8.6199e-01],\n",
      "         [-1.0991e+00, -5.8966e-01,  3.3140e-01,  ..., -8.8954e-01,\n",
      "           5.4775e-01,  4.2283e-01],\n",
      "         [-7.6762e-01, -5.9382e-01,  7.7160e-01,  ..., -4.1291e-04,\n",
      "           1.6690e-01,  7.7435e-01],\n",
      "         ...,\n",
      "         [-6.0538e-01,  3.7993e-01,  1.4017e-01,  ..., -1.9302e-01,\n",
      "           1.2256e-01, -3.9497e-01],\n",
      "         [ 2.6391e-03, -1.0609e+00,  2.5788e-01,  ...,  7.4172e-01,\n",
      "           5.8270e-01, -5.2550e-01],\n",
      "         [ 1.2781e+00, -1.9431e-02, -3.4141e-01,  ...,  6.4128e-01,\n",
      "          -8.0450e-01, -4.4906e-01]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.8300, -0.1933, -0.1548,  0.6392,  0.0525,  0.0583,  0.7980,  0.0883,\n",
      "         -0.1336, -1.0000,  0.0833,  0.5763,  0.9801, -0.1575,  0.8668, -0.4265,\n",
      "          0.1190, -0.4740,  0.1472, -0.3702,  0.4592,  0.9947,  0.6133,  0.2071,\n",
      "          0.1942,  0.7303, -0.5276,  0.9212,  0.9494,  0.7115, -0.6075,  0.0428,\n",
      "         -0.9872, -0.0465, -0.4037, -0.9852,  0.1453, -0.6084,  0.2335,  0.1979,\n",
      "         -0.8680,  0.0679,  0.9996, -0.6049, -0.0385, -0.1956, -0.9999,  0.1670,\n",
      "         -0.8481,  0.3610,  0.2075,  0.1049, -0.0306,  0.3319,  0.3265,  0.3054,\n",
      "         -0.3377, -0.0221, -0.0284, -0.3911, -0.6045,  0.0705, -0.2188, -0.8782,\n",
      "          0.0024, -0.0203,  0.1168, -0.1537,  0.1759, -0.2249,  0.8048,  0.0085,\n",
      "          0.1409, -0.7844, -0.1568,  0.0575, -0.5822,  1.0000, -0.3171, -0.9729,\n",
      "          0.2461, -0.0042,  0.4474,  0.4755, -0.3367, -1.0000,  0.3869,  0.0440,\n",
      "         -0.9882, -0.1670,  0.2964, -0.0421, -0.1121,  0.4975, -0.2119, -0.0869,\n",
      "         -0.1019, -0.0439, -0.0418,  0.0420, -0.2249, -0.1116,  0.0447, -0.2724,\n",
      "          0.0715, -0.3113, -0.2509,  0.3268, -0.3018,  0.5744,  0.3830, -0.1902,\n",
      "          0.1503, -0.9427,  0.5014, -0.1512, -0.9842, -0.5550, -0.9840,  0.5358,\n",
      "          0.0716,  0.1091,  0.9455,  0.4854,  0.1481,  0.1639, -0.3110, -1.0000,\n",
      "         -0.3668, -0.4304,  0.2707, -0.0882, -0.9715, -0.9575,  0.4533,  0.9645,\n",
      "          0.0091,  0.9988,  0.0014,  0.9263,  0.1784, -0.2011, -0.2087, -0.2304,\n",
      "          0.4904,  0.0149, -0.4816,  0.0727,  0.1665,  0.0935, -0.1649, -0.0990,\n",
      "         -0.2869, -0.9136, -0.2381,  0.9231,  0.0164, -0.1619,  0.5843, -0.0960,\n",
      "         -0.1662,  0.7906,  0.4042,  0.1253,  0.0848,  0.2478, -0.3102,  0.3487,\n",
      "         -0.8444,  0.3553,  0.2513, -0.1121, -0.2492, -0.9742, -0.0807,  0.3642,\n",
      "          0.9872,  0.6358,  0.0932,  0.2867, -0.2207,  0.3858, -0.9483,  0.9772,\n",
      "          0.0879,  0.2027,  0.0791, -0.0971, -0.8296, -0.4105,  0.7898, -0.1863,\n",
      "         -0.7931,  0.1138, -0.4512, -0.2444, -0.1153,  0.3207, -0.2290, -0.3072,\n",
      "          0.0833,  0.9082,  0.9368,  0.6079, -0.4289,  0.4764, -0.8317, -0.2752,\n",
      "         -0.0859,  0.0606, -0.1830,  0.9927, -0.2180,  0.0285, -0.9306, -0.9837,\n",
      "         -0.2997, -0.8707,  0.0417, -0.6324,  0.2773,  0.4882, -0.1279,  0.1766,\n",
      "         -0.9429, -0.7227,  0.2204, -0.1878,  0.2567, -0.0808,  0.7452,  0.4931,\n",
      "         -0.4651,  0.3960,  0.9184, -0.1764, -0.6690,  0.6907, -0.1708,  0.8475,\n",
      "         -0.5064,  0.9704,  0.5354,  0.3791, -0.8558, -0.0526, -0.8245,  0.0848,\n",
      "          0.0714, -0.6512,  0.2664,  0.5830,  0.1180,  0.8415, -0.4140,  0.9808,\n",
      "         -0.6622, -0.9494, -0.1348,  0.2884, -0.9861,  0.3006,  0.0993, -0.4018,\n",
      "         -0.2551, -0.3709, -0.9397,  0.7364, -0.0480,  0.9696,  0.1985, -0.8074,\n",
      "         -0.3117, -0.8724, -0.4144, -0.0362,  0.4379, -0.3398, -0.9366,  0.3754,\n",
      "          0.4880,  0.2829,  0.1961,  0.9888,  0.9998,  0.9687,  0.8491,  0.7479,\n",
      "         -0.9716, -0.1807,  1.0000, -0.8317, -1.0000, -0.9180, -0.5570,  0.2596,\n",
      "         -1.0000, -0.0101,  0.2292, -0.8554, -0.0431,  0.9739,  0.9543, -1.0000,\n",
      "          0.8043,  0.9241, -0.5800,  0.5419, -0.1655,  0.9725,  0.3050,  0.2653,\n",
      "          0.0214,  0.0737, -0.4025, -0.7531,  0.0697,  0.2065,  0.9048, -0.1096,\n",
      "         -0.5660, -0.8547, -0.1426,  0.1307, -0.4964, -0.9517, -0.0336, -0.3935,\n",
      "          0.6004, -0.0491,  0.1299, -0.6293,  0.1117, -0.4649, -0.0739,  0.6877,\n",
      "         -0.9202, -0.4906, -0.0626, -0.5568,  0.2066, -0.9580,  0.9517, -0.1861,\n",
      "          0.3343,  1.0000, -0.2774, -0.7240,  0.3068,  0.0674, -0.5593,  1.0000,\n",
      "          0.4973, -0.9797, -0.5310,  0.3058, -0.2859, -0.3368,  0.9983, -0.0576,\n",
      "          0.0215,  0.3229,  0.9683, -0.9905,  0.8747, -0.8823, -0.9589,  0.9543,\n",
      "          0.9285, -0.1276, -0.6619, -0.1533, -0.1367,  0.0663, -0.9142,  0.5389,\n",
      "          0.2989,  0.0873,  0.8683, -0.7236, -0.5731,  0.2424, -0.0607,  0.3183,\n",
      "          0.4320,  0.3134, -0.1928, -0.2002, -0.0082, -0.3004, -0.9698,  0.1868,\n",
      "          1.0000,  0.1235, -0.0415, -0.1373,  0.1007, -0.4802,  0.3064,  0.3644,\n",
      "         -0.1315, -0.7556,  0.1703, -0.8388, -0.9861,  0.6193, -0.0201, -0.1165,\n",
      "          0.9989,  0.0313, -0.0284, -0.0933,  0.6401, -0.2532,  0.3578, -0.1358,\n",
      "          0.9761, -0.1044,  0.5303,  0.7386, -0.1503, -0.1749, -0.5989, -0.1889,\n",
      "         -0.9193,  0.2212, -0.9245,  0.9574,  0.0865,  0.1843, -0.0579,  0.2352,\n",
      "          1.0000, -0.2948,  0.4598,  0.1572,  0.6305, -0.9809, -0.6736, -0.1288,\n",
      "          0.1167,  0.1134, -0.1383,  0.1007, -0.9543,  0.1096, -0.0914, -0.9568,\n",
      "         -0.9849,  0.2969,  0.6215, -0.1911, -0.7397, -0.4979, -0.5561,  0.3192,\n",
      "         -0.0957, -0.8884,  0.5232, -0.0990,  0.2959, -0.1085,  0.5589, -0.0131,\n",
      "          0.9244, -0.0947,  0.2566,  0.1247, -0.7052,  0.7559, -0.7210, -0.1795,\n",
      "          0.0406,  1.0000, -0.4125,  0.4991,  0.6193,  0.5294, -0.0145,  0.0966,\n",
      "          0.4044, -0.0067,  0.2521, -0.1069, -0.0778, -0.1330,  0.4761, -0.2634,\n",
      "         -0.0156,  0.6968,  0.4267, -0.2376,  0.1897, -0.1647,  0.9928,  0.1428,\n",
      "          0.0313, -0.3398,  0.2061, -0.1083,  0.1545,  1.0000,  0.2210, -0.2115,\n",
      "         -0.9878, -0.3516, -0.8430,  0.9994,  0.7556, -0.7242,  0.5008,  0.2707,\n",
      "          0.0061,  0.4851,  0.0320, -0.1282, -0.0140, -0.0189,  0.9298, -0.2843,\n",
      "         -0.9562, -0.5040,  0.1455, -0.9510,  0.9854, -0.4068, -0.0129, -0.0657,\n",
      "          0.4993,  0.0021, -0.3008, -0.9752, -0.0012, -0.1066,  0.9441,  0.0119,\n",
      "         -0.5407, -0.8710, -0.0708,  0.2456, -0.0411, -0.9265,  0.9569, -0.9746,\n",
      "          0.2623,  1.0000,  0.2922, -0.5675, -0.0704, -0.3098,  0.1115,  0.3312,\n",
      "          0.5867, -0.9436, -0.0470,  0.0212,  0.0269,  0.0522,  0.3063,  0.5139,\n",
      "          0.0107, -0.4663, -0.4366,  0.1348,  0.2524,  0.6419, -0.0489,  0.0052,\n",
      "         -0.0295,  0.0463, -0.7975, -0.0822, -0.2167, -0.9970,  0.5292, -1.0000,\n",
      "         -0.2506, -0.4344, -0.0531,  0.7593,  0.2471,  0.2786, -0.6318, -0.0273,\n",
      "          0.8237,  0.7018,  0.0219,  0.1141, -0.5398,  0.0113,  0.1075, -0.0088,\n",
      "         -0.0794,  0.8029,  0.0391,  1.0000, -0.1369, -0.3874, -0.9180,  0.0587,\n",
      "          0.0312,  0.9999, -0.8069, -0.9388,  0.1253, -0.5257, -0.7687,  0.1684,\n",
      "         -0.1738, -0.5990, -0.5795,  0.9298,  0.6928, -0.5742,  0.2901, -0.0887,\n",
      "         -0.4606, -0.0899,  0.1866,  0.9845,  0.2915,  0.8336,  0.7524,  0.0265,\n",
      "          0.9636,  0.1016,  0.2032, -0.0785,  1.0000,  0.2292, -0.9201,  0.5347,\n",
      "         -0.9800,  0.0803, -0.9445,  0.0884, -0.0901,  0.8481, -0.1281,  0.9277,\n",
      "         -0.0774, -0.1449,  0.0163,  0.2573,  0.1819, -0.8592, -0.9836, -0.9825,\n",
      "          0.3324, -0.4040,  0.0309,  0.1849, -0.0727,  0.2201,  0.2333, -1.0000,\n",
      "          0.9251,  0.2352,  0.1542,  0.9393,  0.1790,  0.2254,  0.1233, -0.9820,\n",
      "         -0.8953, -0.0829, -0.2577,  0.6594,  0.3636,  0.8201,  0.1712, -0.4407,\n",
      "         -0.1584,  0.0321, -0.3519, -0.9921,  0.2578,  0.0325, -0.8807,  0.9603,\n",
      "         -0.4774,  0.0303,  0.5855, -0.1738,  0.8170,  0.6597,  0.1265, -0.1225,\n",
      "          0.4113,  0.8139,  0.9307,  0.9873, -0.1928,  0.6120,  0.1175,  0.2564,\n",
      "          0.6503, -0.9289, -0.0317, -0.1137,  0.0249, -0.0029,  0.0050, -0.9196,\n",
      "          0.6266, -0.0210,  0.4212, -0.1986,  0.4100, -0.3299, -0.0970, -0.7247,\n",
      "         -0.5377,  0.6207,  0.1296,  0.8498,  0.4317,  0.1606, -0.6066,  0.0278,\n",
      "          0.1014, -0.9079,  0.8449,  0.1979,  0.5009,  0.2115, -0.2859,  0.8690,\n",
      "         -0.3009, -0.2279, -0.0782, -0.6288,  0.7546, -0.0951, -0.3717, -0.2939,\n",
      "          0.5055,  0.1671,  0.9924, -0.1050, -0.2125, -0.0799, -0.0956,  0.3160,\n",
      "         -0.2466, -1.0000,  0.2670,  0.1401,  0.0244, -0.1710, -0.0447, -0.0092,\n",
      "         -0.9546, -0.0443,  0.0796,  0.1160, -0.3890, -0.2291,  0.5405,  0.3925,\n",
      "          0.6557,  0.8585,  0.1367,  0.6078,  0.5974, -0.3028, -0.5000,  0.8661]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to multiply each weight by a factor\n",
    "def multiply_weights(model, factor=1.1):\n",
    "    \"\"\"\n",
    "    Multiplies each weight parameter of the model by a given factor.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model whose weights will be multiplied.\n",
    "    factor (float): The factor by which weights will be multiplied.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:  # Ensure the parameter requires gradient\n",
    "            param.data.mul_(factor)\n",
    "\n",
    "# Example usage with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # Modify weights without tracking operations for gradient computation\n",
    "    multiply_weights(model, factor=1.1)\n",
    "\n",
    "# Example input to test the model\n",
    "inputs = tokenizer(\"This is a test input.\", return_tensors=\"pt\")\n",
    "\n",
    "# Run the model with the modified weights (still in no_grad context)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example interpretation for a text classification task\n",
    "logits = outputs.last_hidden_state  # Assuming last_hidden_state contains logits\n",
    "predictions = torch.argmax(logits, dim=-1)  # Get predicted classes\n",
    "\n",
    "# Example interpretation for a token classification task\n",
    "token_embeddings = outputs.last_hidden_state  # Assuming last_hidden_state contains token embeddings\n",
    "predicted_labels = torch.argmax(token_embeddings, dim=-1)  # Get predicted labels for each token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[205, 386, 242, 470, 183, 160, 346, 555]])\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example interpretation for token classification (NER) task\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m token_logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m  \u001b[38;5;66;03m# Assuming logits are token-level predictions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Decode labels for each token (assuming token_logits shape is [batch_size, seq_length, num_labels])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "# Example interpretation for token classification (NER) task\n",
    "token_logits = outputs.logits  # Assuming logits are token-level predictions\n",
    "\n",
    "# Decode labels for each token (assuming token_logits shape is [batch_size, seq_length, num_labels])\n",
    "predicted_labels = torch.argmax(token_logits, dim=-1).tolist()\n",
    "\n",
    "# Convert token IDs to actual tokens using tokenizer\n",
    "token_ids = inputs['input_ids'].tolist()[0]  # Get token IDs from inputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Print tokens with predicted labels\n",
    "for token, label_id in zip(tokens, predicted_labels):\n",
    "    label = label_mapping[label_id]  # Map label ID to human-readable label\n",
    "    print(f\"Token: {token}, Predicted Label: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
