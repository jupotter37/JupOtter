{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tmarchitan/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from lib.utils import get_device\n",
    "from lib.utils.constants import Subtask, Track, PreprocessTextLevel, DatasetType\n",
    "from lib.utils.training import EarlyStopping\n",
    "from lib.data.loading import load_train_dev_test_df, build_data_loader\n",
    "from lib.data.tokenizer import get_tokenizer\n",
    "from lib.models import get_model\n",
    "from lib.training.loss import get_loss_fn\n",
    "from lib.training.metric import get_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Will save results to: ../runs/SubtaskC\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE_PATH = os.path.relpath(\"../config.json\")\n",
    "\n",
    "config = {}\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "results_dir = os.path.relpath(\"../runs/SubtaskC\")\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "print(f\"Will save results to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Track not specified in config for subtask: Subtask.SubtaskC\n",
      "Loading train data...\n",
      "Train/dev split... (df_train.shape: (3649, 3))\n",
      "Loading test data....././data/original_data/SubtaskC/SubtaskC_dev.jsonl\n",
      "df_train.shape: (3284, 3)\n",
      "df_dev.shape: (365, 3)\n",
      "df_test.shape: (505, 3)\n"
     ]
    }
   ],
   "source": [
    "task = None\n",
    "if \"task\" in config:\n",
    "    task = Subtask(config[\"task\"])\n",
    "else:\n",
    "    raise ValueError(\"Task not specified in config\")\n",
    "\n",
    "track = None\n",
    "if \"track\" in config:\n",
    "    track = Track(config[\"track\"])\n",
    "else:\n",
    "    print(f\"Warning: Track not specified in config for subtask: {task}\")\n",
    "\n",
    "dataset_type = DatasetType.TransformerTruncationDataset\n",
    "if \"dataset_type\" in config[\"data\"]:\n",
    "    dataset_type = DatasetType(config[\"data\"][\"dataset_type\"])\n",
    "\n",
    "dataset_type_settings = None\n",
    "if \"dataset_type_settings\" in config[\"data\"]:\n",
    "    dataset_type_settings = config[\"data\"][\"dataset_type_settings\"]\n",
    "\n",
    "df_train, df_dev, df_test = load_train_dev_test_df(\n",
    "    task=task,\n",
    "    track=track,\n",
    "    data_dir=f\"../{config['data']['data_dir']}\",\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    test_size=config[\"data\"][\"test_size\"],\n",
    "    preprocess_text_level=PreprocessTextLevel(\n",
    "        config[\"data\"][\"preprocess_text_level\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"df_train.shape: {df_train.shape}\")\n",
    "print(f\"df_dev.shape: {df_dev.shape}\")\n",
    "print(f\"df_test.shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(20)\n",
    "df_dev = df_dev.sample(20)\n",
    "df_test = df_test.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(**config[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = DatasetType.TransformerTruncationDataset\n",
    "if \"dataset_type\" in config[\"data\"]:\n",
    "    dataset_type = DatasetType(config[\"data\"][\"dataset_type\"])\n",
    "\n",
    "dataset_type_settings = None\n",
    "if \"dataset_type_settings\" in config[\"data\"]:\n",
    "    dataset_type_settings = config[\"data\"][\"dataset_type_settings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetType.LongformerDataset: 'longformer_dataset'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = build_data_loader(\n",
    "    df_train,\n",
    "    tokenizer,\n",
    "    max_len=config[\"data\"][\"max_len\"],\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    shuffle=True,\n",
    "    dataset_type=dataset_type,\n",
    "    dataset_type_settings=dataset_type_settings,\n",
    "    device=DEVICE,\n",
    ")\n",
    "dev_dataloader = build_data_loader(\n",
    "    df_dev,\n",
    "    tokenizer,\n",
    "    max_len=config[\"data\"][\"max_len\"],\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    dataset_type=dataset_type,\n",
    "    dataset_type_settings=dataset_type_settings,\n",
    "    device=DEVICE,\n",
    ")\n",
    "test_dataloader = build_data_loader(\n",
    "    df_test,\n",
    "    tokenizer,\n",
    "    max_len=config[\"data\"][\"max_len\"],\n",
    "    batch_size=config[\"data\"][\"batch_size\"],\n",
    "    label_column=config[\"data\"][\"label_column\"],\n",
    "    has_targets=False if config[\"data\"][\"test_size\"] is None else True,\n",
    "    dataset_type=dataset_type,\n",
    "    dataset_type_settings=dataset_type_settings,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Longformer' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m get_loss_fn(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m], DEVICE)\n\u001b[1;32m      4\u001b[0m optimizer_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Developer/ml_research/machine-generated_text_detection/notebooks/../lib/models/__init__.py:42\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_name, model_config)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GPT2(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_config)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLongformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such model!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/ml_research/machine-generated_text_detection/notebooks/../lib/models/longformer/longformer.py:72\u001b[0m, in \u001b[0;36mLongformer.__init__\u001b[0;34m(self, pretrained_model_name, out_size, dropout_p, selected_layers, selected_layers_merge_strategy, selected_layers_dropout_p, fc, out_activation)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_activation \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSigmoid()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlongformer_type \u001b[38;5;241m==\u001b[39m LongformerType\u001b[38;5;241m.\u001b[39mLONGFORMER_WITH_LAYER_SELECTION:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreeze_transformer_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/ml_research/machine-generated_text_detection/notebooks/../lib/models/longformer/longformer.py:119\u001b[0m, in \u001b[0;36mLongformer.freeze_transformer_layer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreeze_transformer_layer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    120\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/ml_research/machine-generated_text_detection/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Longformer' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "model = get_model(config[\"model\"], config[\"model_config\"]).to(DEVICE)\n",
    "loss_fn = get_loss_fn(config[\"training\"][\"loss\"], DEVICE)\n",
    "optimizer_config = config[\"training\"][\"optimizer\"]\n",
    "scheduler_config = config[\"training\"][\"scheduler\"]\n",
    "metric_fn, is_better_metric_fn = get_metric(config[\"training\"][\"metric\"])\n",
    "num_epochs_before_finetune = config[\"training\"][\"num_epochs_before_finetune\"]\n",
    "swa_config = config[\"training\"][\"swa\"] if \"swa\" in config[\"training\"] else None\n",
    "validation_freq = (\n",
    "    config[\"training\"][\"validation_freq\"]\n",
    "    if \"validation_freq\" in config[\"training\"] else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 597M/597M [00:05<00:00, 111MB/s] \n",
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerModel(\n",
       "  (embeddings): LongformerEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): LongformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x LongformerLayer(\n",
       "        (attention): LongformerAttention(\n",
       "          (self): LongformerSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): LongformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): LongformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): LongformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): LongformerPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LongformerModel\n",
    "\n",
    "model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
