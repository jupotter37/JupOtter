{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "484b10ef",
   "metadata": {},
   "source": [
    "# ReadMe \n",
    "\n",
    "1. Prerequisites\n",
    "\n",
    "    - Java 1.8.0_191 (https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html)\n",
    "\n",
    "    - Python 3.10.11 (https://www.python.org/downloads/release/python-31011/)\n",
    "\n",
    "    - Apache Spark 3.0.0 with prebuilt Hadoop 2.7 (https://archive.apache.org/dist/spark/spark-3.0.0/)\n",
    "\n",
    "    - Visual Studio Code 1.77.3\n",
    "\n",
    "    - WinUtils.exe for Spark w/ Hadoop version above (https://github.com/cdarlint/winutils)\n",
    "\n",
    "2. Instructions\n",
    "\n",
    "    - Guide: https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1838e35",
   "metadata": {},
   "source": [
    "# Implementation details\n",
    "\n",
    "1. Preprocessing Approach\n",
    "\n",
    "    - Normalize text to lowercase\n",
    "    - Removal of punctuation and special characters\n",
    "    - Removal of stop words\n",
    "    - Tokenize text into an array\n",
    "\n",
    "2. Lexicon based sentiment scoring w/ AFINN (https://rdrr.io/cran/textdata/man/lexicon_afinn.html)\n",
    "\n",
    "    Serves as the \"ground truth\" approximation of sentiment in a text by utlizing map-reduce to count (word, count) in a tweet, then joins the AFINN dataframe such that each tweet may be labeled -1, 0, 1 for negative, neutral, positive sentiment based on a summing of matched words from AFINN and the corresponding score in [-5, 5].\n",
    "\n",
    "    A similar process was done for each grouping of religious text.\n",
    "\n",
    "    The computed categories here are used as the labels when building out ML models with MLlib in Apache Spark via th PySpark library.\n",
    "\n",
    "3. ML model building (https://spark.apache.org/docs/3.0.0/ml-guide.html)\n",
    "\n",
    "    - Logistic Regression w/ Count Vector\n",
    "\n",
    "             Used to predict the probability of a certain event occurring based on the values of the independent variables. \n",
    "             \n",
    "             Count vector uses the frequency of occurrence of each word in a text to count and store it as a vector.\n",
    "\n",
    "             These word counts are used as the independent variables in the logistic regression model, and the dependent variable is a multivariate label.\n",
    "\n",
    "    - Logistic Regression w/ TF-IDF\n",
    "\n",
    "            Similar to the above however, term-frequency (TF) is the freq(word) in the text, and the inverse-document-frequency (IDF) is a measure of how important the word is to the text\n",
    "\n",
    "    - Naive Bayes\n",
    "\n",
    "            Probabalistic approach, where the probability of each label class is calculated given a set of features.\n",
    "\n",
    "            The label class with the highest probability is chosen as the predicted class.\n",
    "\n",
    "            Relies on Bayes Theorum: P(label | features) = P(features | label) * P(label) / P(features)\n",
    "\n",
    "            **Assumes label independence\n",
    "\n",
    "    - Decision Trees\n",
    "\n",
    "            Partitions data into subsets based on feature values until each subset is of only one label class\n",
    "\n",
    "            Seperates subsets by entropy or gini index calculations\n",
    "\n",
    "    - Random Forest\n",
    "\n",
    "            Extends Decision Tree method, by building decision trees trained on a random subset of the training data and a random subset of the input features.\n",
    "\n",
    "            This reduces overfitting.\n",
    "\n",
    "            Predics label class the predictions of all the decision trees in the forest are aggregated, such that a majority vote of the trees in the forest dictacte prediction.\n",
    "\n",
    "    - OnevsRest\n",
    "\n",
    "            Divides the multi label class dataset into seveeral binary classification models.\n",
    "\n",
    "            All relevant trained binary classifying models are applied and the model with the highest probability score is used as the predictted class label.\n",
    "\n",
    "4. ML model evaluation\n",
    "\n",
    "    - All models calculate accuracy rate\n",
    "\n",
    "    - Example on LR model of....\n",
    "\n",
    "    Accuracy: Overall correctness of a classification model's predictions\n",
    "\n",
    "    FalsePositiveRate: Opposite of Recall\n",
    "\n",
    "    F_Measure: A metric that combines precision and recall into a single measure of a classification model's performance\n",
    "\n",
    "    Precision: A metric that measures the ability of a classification model to make correct positive predictions\n",
    "\n",
    "    Recall: Ability of a classification model to identify all relevant instances of a particular class in a dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f457fd45",
   "metadata": {},
   "source": [
    "# Limitations of Approach\n",
    "\n",
    "    - System Resource Limitations\n",
    "\n",
    "        CPU: Ryzen 5 w/ 6 cores\n",
    "        RAM: 32 GB\n",
    "        DISK: ~250GB available\n",
    "\n",
    "        Data Spillage: When processing a large dataset with limited memory, Spark may start swapping data to disk, which can significantly slow down processing times.\n",
    "\n",
    "        Read/Write Bandwidth: When running Spark on a distributed cluster, data is spread across multiple machines. Since I am running Spark locally, all data must be transferred to and from my system, which relies on memory write times to RAM or Disk.\n",
    "\n",
    "        Processing Power: My loccal instance of Apache Spark is bound by my processor and the available amount of cores.\n",
    "\n",
    "    - Scalability Issues \n",
    "\n",
    "        Single Executor: Negates the benifit of optimized parrellelism since job stages will be run suquentially.\n",
    "\n",
    "        Fault Tolerance: Greater risk  of job build failure due to hardware or software dependencies. Once a job fails it cannot be repaired it mustt be instantiated from the beginning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b12bb0cd",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc327ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22367015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from IPython.display import display_html \n",
    "from operator import add\n",
    "import py4j\n",
    "import pyspark  \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "from pyspark.sql import SparkSession\n",
    "from wordcloud import (WordCloud, get_single_color_func)\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f79a414",
   "metadata": {},
   "source": [
    "# Spark Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c3dad36",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Builder' object has no attribute 'sparkContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Use to test default preformance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spark2 \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder \\\n\u001b[0;32m      3\u001b[0m         \u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mDefault Settings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m sc2 \u001b[39m=\u001b[39m spark2\u001b[39m.\u001b[39;49msparkContext\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m      7\u001b[0m sqlContext2 \u001b[39m=\u001b[39m SQLContext(sc)\n\u001b[0;32m      9\u001b[0m spark2\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39m_conf\u001b[39m.\u001b[39mgetAll() \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Builder' object has no attribute 'sparkContext'"
     ]
    }
   ],
   "source": [
    "# Use to test default preformance\n",
    "spark2 = SparkSession.builder \\\n",
    "        .appName(\"Default Settings\")\n",
    "\n",
    "sc2 = spark2.sparkContext.getOrCreate()\n",
    "\n",
    "sqlContext2 = SQLContext(sc)\n",
    "\n",
    "spark2.sparkContext._conf.getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2e71ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[0;32m      2\u001b[0m        \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      3\u001b[0m        \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mCS724 Semester Project\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      4\u001b[0m        \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m10g\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      5\u001b[0m        \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.executor.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m2g\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m----> 6\u001b[0m        \u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m      8\u001b[0m sc \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m     10\u001b[0m sqlContext \u001b[39m=\u001b[39m SQLContext(sc)\n",
      "File \u001b[1;32mc:\\Users\\dusti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\dusti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\dusti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    198\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[0;32m    203\u001b[0m         sparkHome,\n\u001b[0;32m    204\u001b[0m         pyFiles,\n\u001b[0;32m    205\u001b[0m         environment,\n\u001b[0;32m    206\u001b[0m         batchSize,\n\u001b[0;32m    207\u001b[0m         serializer,\n\u001b[0;32m    208\u001b[0m         conf,\n\u001b[0;32m    209\u001b[0m         jsc,\n\u001b[0;32m    210\u001b[0m         profiler_cls,\n\u001b[0;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\dusti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:307\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39m# If encryption is enabled, we need to setup a server in the jvm to read broadcast\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m# data via a socket.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m# scala's mangled names w/ $ in them require special treatment.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encryption_enabled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39misEncryptionEnabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc)\n\u001b[0;32m    306\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mSPARK_AUTH_SOCKET_TIMEOUT\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m--> 307\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mgetPythonAuthSocketTimeout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc)\n\u001b[0;32m    308\u001b[0m )\n\u001b[0;32m    309\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mSPARK_BUFFER_SIZE\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mgetSparkBufferSize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc))\n\u001b[0;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpythonExec \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYSPARK_PYTHON\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpython3\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dusti\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1549\u001b[0m, in \u001b[0;36mJavaClass.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1546\u001b[0m         \u001b[39mreturn\u001b[39;00m get_return_value(\n\u001b[0;32m   1547\u001b[0m             answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fqn, name)\n\u001b[0;32m   1548\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1549\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m   1550\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m does not exist in the JVM\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fqn, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[*]\") \\\n",
    "       .appName(\"CS724 Semester Project\") \\\n",
    "       .config(\"spark.driver.memory\", \"10g\") \\\n",
    "       .config(\"spark.executor.memory\", \"2g\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark.sparkContext._conf.getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5325093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6bcfaec",
   "metadata": {},
   "source": [
    "# I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b85382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pathBuddhism = \"/Users/dusti/Documents/Scripts/CS724_Project/input/buddhism/\"\n",
    "pathBibleKJV = \"/Users/dusti/Documents/Scripts/CS724_Project/input/christianity/bible_kjv.txt\"\n",
    "pathHinduism = \"/Users/dusti/Documents/Scripts/CS724_Project/input/hinduism/\"\n",
    "pathKoran = \"/Users/dusti/Documents/Scripts/CS724_Project/input/islam/koran.txt\"\n",
    "pathJudaism = \"/Users/dusti/Documents/Scripts/CS724_Project/input/judaism/tanakh.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = sc.textFile(\"/Users/dusti/Documents/Scripts/CS724_Project/input/stop_words.txt\").collect()\n",
    "positive_words = sc.textFile(\"/Users/dusti/Documents/Scripts/CS724_Project/input/positive_words.txt\").collect()\n",
    "negative_words = sc.textFile(\"/Users/dusti/Documents/Scripts/CS724_Project/input/negative_words.txt\").collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "882282d0",
   "metadata": {},
   "source": [
    "# Word Counts\n",
    "\n",
    "1. Map-Reduce text files for (word, count)\n",
    "\n",
    "2. .split() to tokenize\n",
    "\n",
    "3. Remove punctuation with a Regex pattern\n",
    "\n",
    "4. Remove stop words from tokens\n",
    "\n",
    "5. Create a lexicon Spark Dataframe from AFINN\n",
    "\n",
    "6. Use Spark SQL to join AFINN_DF to [religion_group]_DF such tthat the schema is (word, score, count, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountWords(path: str, type: str):\n",
    "\n",
    "    lines = sc.textFile(path)\n",
    "\n",
    "    words = lines.flatMap(lambda line: line.split()).map(lambda word: word.lower())\n",
    "    words = words.map(lambda word: re.sub(r'[^\\w\\s]', '', word))\n",
    "\n",
    "    if type == 'stop':\n",
    "        words = words.filter(lambda word: word not in stop_words)\n",
    "\n",
    "    if type == 'positive':\n",
    "        words = words.filter(lambda word: word in positive_words)\n",
    "\n",
    "    if type == 'negative':\n",
    "        words = words.filter(lambda word: word in negative_words)\n",
    "\n",
    "    counts = words.map(lambda word: (word, 1)).reduceByKey(add).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinnDF = sc.textFile(\"/Users/dusti/Documents/Scripts/CS724_Project/input/afinn.txt\").map(lambda line: line.split(\"\\t\")).toDF(['word','score'])\n",
    "afinnDF = afinnDF.withColumn('score', F.col('score').cast('integer'))\n",
    "afinnDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsBuddhism = CountWords(pathBuddhism, 'stop').toDF(['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "buddhismDF = afinnDF.join(countsBuddhism, afinnDF.word == countsBuddhism.word).select(afinnDF.word, afinnDF.score, \"count\")\n",
    "\n",
    "buddhismDF.withColumn(\"type\", \\\n",
    "           F.when((buddhismDF.score > 0), \"positive\") \\\n",
    "           .when((buddhismDF.score < 0), \"negative\") \\\n",
    "           .otherwise(\"neutral\")\n",
    ").sort(F.desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec933ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsBibleKJV = CountWords(pathBibleKJV, 'stop').toDF(['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ddca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bibleKJVDF = afinnDF.join(countsBibleKJV, afinnDF.word == countsBibleKJV.word).select(afinnDF.word, afinnDF.score, \"count\")\n",
    "\n",
    "bibleKJVDF.withColumn(\"type\", \\\n",
    "           F.when((bibleKJVDF.score > 0), \"positive\") \\\n",
    "           .when((bibleKJVDF.score < 0), \"negative\") \\\n",
    "           .otherwise(\"neutral\")\n",
    ").sort(F.desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsKoran = CountWords(pathKoran, 'stop').toDF(['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a541ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "koranDF = afinnDF.join(countsKoran, afinnDF.word == countsKoran.word).select(afinnDF.word, afinnDF.score, \"count\")\n",
    "\n",
    "koranDF.withColumn(\"type\", \\\n",
    "           F.when((koranDF.score > 0), \"positive\") \\\n",
    "           .when((koranDF.score < 0), \"negative\") \\\n",
    "           .otherwise(\"neutral\")\n",
    ").sort(F.desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6383bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsJudaism = CountWords(pathJudaism, 'stop').toDF(['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "judaismDF = afinnDF.join(countsJudaism, afinnDF.word == countsJudaism.word).select(afinnDF.word, afinnDF.score, \"count\")\n",
    "\n",
    "judaismDF.withColumn(\"type\", \\\n",
    "           F.when((judaismDF.score > 0), \"positive\") \\\n",
    "           .when((judaismDF.score < 0), \"negative\") \\\n",
    "           .otherwise(\"neutral\")\n",
    ").sort(F.desc(\"count\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "countsHinduism = CountWords(pathHinduism, 'stop').toDF(['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinduismDF = afinnDF.join(countsHinduism, afinnDF.word == countsHinduism.word).select(afinnDF.word, afinnDF.score, \"count\")\n",
    "\n",
    "hinduismDF.withColumn(\"type\", \\\n",
    "           F.when((hinduismDF.score > 0), \"positive\") \\\n",
    "           .when((hinduismDF.score < 0), \"negative\") \\\n",
    "           .otherwise(\"neutral\")\n",
    ").sort(F.desc(\"count\")).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f59993",
   "metadata": {},
   "source": [
    "# Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac92a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordCloud(text):\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "buddhismPDF = buddhismDF.toPandas()\n",
    "textBuddhism = \" \".join(word for word in buddhismPDF.word.astype(str))\n",
    "createWordCloud(textBuddhism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb348d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bibleKJVPDF = bibleKJVDF.toPandas()\n",
    "textBibleKJV = \" \".join(word for word in bibleKJVPDF.word.astype(str))\n",
    "createWordCloud(textBibleKJV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff091e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "koranPDF = koranDF.toPandas()\n",
    "textKoran = \" \".join(word for word in koranPDF.word.astype(str))\n",
    "createWordCloud(textKoran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "judaismPDF = judaismDF.toPandas()\n",
    "textJudaism = \" \".join(word for word in judaismPDF.word.astype(str))\n",
    "createWordCloud(textJudaism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5068e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinduismPDF = hinduismDF.toPandas()\n",
    "textHinduism = \" \".join(word for word in hinduismPDF.word.astype(str))\n",
    "createWordCloud(textHinduism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(buddhismDF))\n",
    "print(type(bibleKJVDF))\n",
    "print(type(hinduismDF))\n",
    "print(type(judaismDF))\n",
    "print(type(koranDF))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62380b81",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsSchema = T.StructType([\n",
    "    T.StructField(\"user_name\", T.StringType()),\n",
    "    T.StructField(\"user_location\", T.StringType()),\n",
    "    T.StructField(\"user_description\", T.StringType()),\n",
    "    T.StructField(\"user_created\", T.StringType()),\n",
    "    T.StructField(\"user_followers\", T.IntegerType()),\n",
    "    T.StructField(\"user_friends\", T.IntegerType()),\n",
    "    T.StructField(\"user_favourites\", T.IntegerType()),\n",
    "    T.StructField(\"user_verified\", T.StringType()),\n",
    "    T.StructField(\"date\", T.DateType()),\n",
    "    T.StructField(\"text\", T.StringType()),\n",
    "    T.StructField(\"hashtags\", T.StringType()),\n",
    "    T.StructField(\"source\", T.StringType()),\n",
    "    T.StructField(\"is_retweet\", T.StringType())\n",
    "])\n",
    "\n",
    "tweetsRawDF = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(tweetsSchema).load(\"/Users/dusti/Documents/Scripts/CS724_Project/input/tweets/covid19_tweets.csv\")\n",
    "#tweetsRawDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = tweetsRawDF.select(\"text\").dropna()\n",
    "#tweetsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF = tweetsDF.withColumn('clean_text', F.regexp_replace('text', '[^\\w\\s]+', ''))\n",
    "cleanTweetsDF = cleanTweetsDF.withColumn('clean_text', F.lower(cleanTweetsDF['clean_text']))\n",
    "cleanTweetsDF = cleanTweetsDF.withColumn('words', F.split(cleanTweetsDF['clean_text'], ' '))\n",
    "cleanTweetsDF = cleanTweetsDF.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "#cleanTweetsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = cleanTweetsDF.select(\"index\", F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "df_scored = df_exploded.join(afinnDF, \"word\")\n",
    "df_grouped = df_scored.groupBy(df_scored.index).agg(F.collect_list(\"score\").alias(\"afinn_scores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF = cleanTweetsDF.join(df_grouped, 'index', 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ac6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF = cleanTweetsDF.withColumn('token_count', F.size('words').cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_array(arr):\n",
    "    if arr is None:\n",
    "        return 0  # Return 0 for NoneType values\n",
    "    else:\n",
    "        return sum(arr)\n",
    "\n",
    "sum_array_udf = udf(sum_array, T.StringType())\n",
    "\n",
    "cleanTweetsDF = cleanTweetsDF.withColumn(\"sentiment\", sum_array_udf(\"afinn_scores\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f283f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF = cleanTweetsDF.withColumn(\"category\", F.when(cleanTweetsDF.sentiment < 0, -1).when(cleanTweetsDF.sentiment > 0, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanTweetsDF.sort(F.asc('index')).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22402e6a",
   "metadata": {},
   "source": [
    "# Model One: Predicting Overall Sentiment (Positive, Negative, Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf74e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[countVectors, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(cleanTweetsDF)\n",
    "dataset = pipelineFit.transform(cleanTweetsDF)\n",
    "#dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0521de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "#print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "#print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce55917",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.persist()\n",
    "testData.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d1c0d4c",
   "metadata": {},
   "source": [
    "## Logistic Regression using Count Vector\n",
    "\n",
    "A \"bag of words\" is being generated to represent the given tweet text data, such that the ordinal nature of the text is ignored and only the count of occurences is used to create a vector of word frequencies by row in the Spark Dataframe where each array index corresponds of the generated array corresponds to a word in this \"bag of words\" and the corresponding value at this index is the frequency value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2962bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0).select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    ".orderBy(\"probability\", ascending=False) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeeafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "lr_CV_Accuracy = evaluator.evaluate(predictions)\n",
    "print(lr_CV_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"\\nFalse positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nTrue positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nPrecision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"\\nRecall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"\\nF-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"\\nAccuracy: %s\\n\\nFPR: %s\\n\\nTPR: %s\\n\\nF-measure: %s\\n\\nPrecision: %s\\n\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90cabd65",
   "metadata": {},
   "source": [
    "## Logistic Regression using TF-IDF\n",
    "\n",
    "Term frequency-inverse document frequency creates a weighted \"bag of words\" of the input tweet text, but is more robust than the previous count vector feature method in that it takes into account the frequency of a word in a document of text as well as the inverse frequency of a word in all documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e23a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=30000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(cleanTweetsDF)\n",
    "dataset = pipelineFit.transform(cleanTweetsDF)\n",
    "\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60120681",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "lr_TFIDF_Accuracy = evaluator.evaluate(predictions)\n",
    "print(lr_TFIDF_Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"\\nFalse positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nTrue positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nPrecision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"\\nRecall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"\\nF-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"\\nAccuracy: %s\\n\\nFPR: %s\\n\\nTPR: %s\\n\\nF-measure: %s\\n\\nPrecision: %s\\n\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8df8fbc6",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Probabilistic classification algorithm that is based on Bayes Theorem, by first estimating the probabilities of each class based on the input features. These probabilities are estimated by the probability of hypothesis (in this case, the category/label) based on the prior knowledge (the prior probability of the category/label) and the observed evidence (the input features). The assumption in Naive Bayes is that all features are independent of each other given the class. This means that the probability of a particular feature occurring does not depend on the occurrence of any other feature in the input data. This assumption simplifies the calculation of the probability of each class given the input features.\n",
    "\n",
    "Naive Bayes calculates the probability of each class given the input features using Bayes' theorem. The class with the highest probability is then assigned to the input data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "nbModel = nb.fit(trainingData)\n",
    "predictions = nbModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e358a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "nbAccuracy = evaluator.evaluate(predictions)\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "321a1335",
   "metadata": {},
   "source": [
    "## Decision Tree Classification\n",
    "\n",
    "\n",
    "Predicts the class label of an input data point by recursively partitioning the feature space into subsets that are associated with different class labels.\n",
    "\n",
    "A decision tree consists of internal nodes, branches, and leaf nodes. The internal nodes represent a test on a feature, and the branches represent the possible outcomes of the test. The leaf nodes represent the final decision, which is the class label assigned to the input data point.\n",
    "\n",
    "The algorithm begins by selecting the most important feature that can best partition the input data into subsets with different class labels. The feature is selected using a measure of impurity, such as the Gini impurity or the entropy. The measure of impurity evaluates how well a feature splits the input data into subsets that have different class labels.\n",
    "\n",
    "The input data is partitioned into subsets based on the values of the selected feature. The partitioning process is repeated recursively for each subset until a stopping criterion is met. The stopping criterion could be a predefined depth of the tree, a minimum number of data points in a leaf node, or a minimum reduction in impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04398c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(trainingData)\n",
    "predictions = dtModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad735e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "dtAccuracy = evaluator.evaluate(predictions)\n",
    "print(dtAccuracy) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2273f302",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "\n",
    "\n",
    "A forest of decision trees, where each tree is trained on a random subset of the input features and a random subset of the training data. The random selection of features and training data helps to reduce overfitting and improve the generalization performance of the model.\n",
    "\n",
    "Each decision tree is built using a subset of the input features and a subset of the training data. The splitting criterion for each node is based on the Gini impurity or the information gain, which measures the quality of the split in terms of how well it separates the input data into different classes.\n",
    "\n",
    "The random forest takes the average prediction of all decision trees in the forest. Each tree makes a prediction based on the features of the input data point and the decision rules learned during the training phase. The average prediction of all trees gives a final prediction that is less sensitive to the specific training data and more robust to noise and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "rfPredictions = rfModel.transform(testData)\n",
    "rfPredictions.filter(rfPredictions['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfEvaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "rfAccuracy = rfEvaluator.evaluate(rfPredictions)\n",
    "print(rfAccuracy) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34b95080",
   "metadata": {},
   "source": [
    "## OnevsRest Classification Model\n",
    "\n",
    "\n",
    "A binary classifier for each class label, where the positive class is the target class and the negative class is all the other classes combined. During the prediction phase, the input data point is passed through all the binary classifiers, and the class label with the highest confidence score is selected as the predicted class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "\n",
    "ovrLR = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "ovr = OneVsRest(classifier=ovrLR)\n",
    "\n",
    "ovrModel = ovr.fit(trainingData)\n",
    "\n",
    "# score the model on test data.\n",
    "ovrPredictions = ovrModel.transform(testData)\n",
    "\n",
    "ovrPredictions.filter(ovrPredictions['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"label\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrEvaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "ovrAccuracy = ovrEvaluator.evaluate(ovrPredictions)\n",
    "print(ovrAccuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80de00a1",
   "metadata": {},
   "source": [
    "## Model One Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d532f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['LR CV', 'LR TF-IDF', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'OneVsRest']\n",
    "accuracy = [lr_CV_Accuracy, lr_TFIDF_Accuracy, nbAccuracy, dtAccuracy, rfAccuracy, ovrAccuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669190bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_x():\n",
    "    # this is for plotting purpose\n",
    "    index = np.arange(len(model))\n",
    "    plt.bar(index, accuracy)\n",
    "    plt.xlabel('models', fontsize=10)\n",
    "    plt.ylabel('prediction accuracy', fontsize=10)\n",
    "    plt.xticks(index, model, fontsize=10, rotation=30)\n",
    "    plt.title('Accuracy of each model')\n",
    "    plt.show()\n",
    "    \n",
    "plot_bar_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6030f1f5",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b82d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF2 = tweetsRawDF.select(\"text\").dropna()\n",
    "\n",
    "cleanTweetsDF2 = tweetsDF2.withColumn('clean_text', F.regexp_replace('text', '[^\\w\\s]+', ''))\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn('clean_text', F.lower(cleanTweetsDF2['clean_text']))\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn('words', F.split(cleanTweetsDF2['clean_text'], ' '))\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "df_exploded_christinaity = cleanTweetsDF2.select(\"index\", F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "df_scored_christinaity = df_exploded_christinaity.join(bibleKJVDF, \"word\")\n",
    "df_grouped_christinaity = df_scored_christinaity.groupBy(df_scored_christinaity.index).agg(F.collect_list(\"score\").alias(\"afinn_scores_christianity\"))\n",
    "\n",
    "cleanTweetsDF2 = cleanTweetsDF2.join(df_grouped_christinaity, 'index', 'outer')\n",
    "\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn(\"sentiment_christianity\", sum_array_udf(\"afinn_scores_christianity\").cast(\"int\"))\n",
    "#cleanTweetsDF2.sort(F.asc('index')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_islam = cleanTweetsDF2.select(\"index\", F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "df_scored_islam = df_exploded_islam.join(koranDF, \"word\")\n",
    "df_grouped_islam = df_scored_islam.groupBy(df_scored_islam.index).agg(F.collect_list(\"score\").alias(\"afinn_scores_islam\"))\n",
    "\n",
    "cleanTweetsDF2 = cleanTweetsDF2.join(df_grouped_islam, 'index', 'outer')\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn(\"sentiment_islam\", sum_array_udf(\"afinn_scores_islam\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_judaism = cleanTweetsDF2.select(\"index\", F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "df_scored_judaism = df_exploded_judaism.join(judaismDF, \"word\")\n",
    "df_grouped_judaism = df_scored_judaism.groupBy(df_scored_judaism.index).agg(F.collect_list(\"score\").alias(\"afinn_scores_judaism\"))\n",
    "\n",
    "cleanTweetsDF2 = cleanTweetsDF2.join(df_grouped_judaism, 'index', 'outer')\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn(\"sentiment_judaism\", sum_array_udf(\"afinn_scores_judaism\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_buddhism = cleanTweetsDF2.select(\"index\", F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "df_scored_buddhism = df_exploded_buddhism.join(buddhismDF, \"word\")\n",
    "df_grouped_buddhism = df_scored_buddhism.groupBy(df_scored_buddhism.index).agg(F.collect_list(\"score\").alias(\"afinn_scores_buddhism\"))\n",
    "\n",
    "cleanTweetsDF2 = cleanTweetsDF2.join(df_grouped_buddhism, 'index', 'outer')\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn(\"sentiment_buddhism\", sum_array_udf(\"afinn_scores_buddhism\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded_hinduism = cleanTweetsDF2.select(\"index\", F.explode(F.col(\"words\")).alias(\"word\"))\n",
    "df_scored_hinduism = df_exploded_hinduism.join(hinduismDF, \"word\")\n",
    "df_grouped_hinduism = df_scored_hinduism.groupBy(df_scored_hinduism.index).agg(F.collect_list(\"score\").alias(\"afinn_scores_hinduism\"))\n",
    "\n",
    "cleanTweetsDF2 = cleanTweetsDF2.join(df_grouped_hinduism, 'index', 'outer')\n",
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn(\"sentiment_hinduism\", sum_array_udf(\"afinn_scores_hinduism\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741369e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def religion_category(christianity, islam, judaism, buddhism, hinduism):\n",
    "\n",
    "  if abs(christianity) > max(abs(islam),abs(judaism),abs(buddhism),abs(hinduism)):\n",
    "    return 1\n",
    "  if abs(islam) > max(abs(christianity),abs(judaism),abs(buddhism),abs(hinduism)):\n",
    "    return 2\n",
    "  if abs(judaism) > max(abs(christianity),abs(islam),abs(buddhism),abs(hinduism)):\n",
    "    return 3\n",
    "  if abs(buddhism) > max(abs(christianity),abs(islam),abs(judaism),abs(hinduism)):\n",
    "    return 4\n",
    "  if abs(hinduism) > max(abs(christianity),abs(islam),abs(judaism),abs(buddhism)):\n",
    "    return 5\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "rel_cat = F.udf(religion_category, T.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d51eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF2 = cleanTweetsDF2.withColumn('category', rel_cat('sentiment_christianity','sentiment_islam','sentiment_judaism','sentiment_buddhism','sentiment_hinduism'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb35b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetsDF2 = cleanTweetsDF2.select('clean_text','words','category').persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2a0eb8",
   "metadata": {},
   "source": [
    "# Model Two: Predicting Religious Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit2 = pipeline.fit(cleanTweetsDF2)\n",
    "dataset2 = pipelineFit2.transform(cleanTweetsDF2)\n",
    "#dataset2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02afef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData2, testData2) = dataset2.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset2 Count: \" + str(trainingData2.count()))\n",
    "print(\"Test Dataset2 Count: \" + str(testData2.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de030096",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData2.cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eec544cb",
   "metadata": {},
   "source": [
    "## Logistic Regression using Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel2 = lr2.fit(trainingData)\n",
    "\n",
    "predictions2 = lrModel2.transform(testData2)\n",
    "\n",
    "predictions2.filter(predictions2['prediction'] == 0).select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    ".orderBy(\"probability\", ascending=False) \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23beb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_CV_Accuracy2 = evaluator.evaluate(predictions2)\n",
    "print(lr_CV_Accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52dc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel2.summary\n",
    "\n",
    "print(\"\\nFalse positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nTrue positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nPrecision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"\\nRecall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"\\nF-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"\\nAccuracy: %s\\n\\nFPR: %s\\n\\nTPR: %s\\n\\nF-measure: %s\\n\\nPrecision: %s\\n\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d1fa8b4",
   "metadata": {},
   "source": [
    "## Logistic Regression using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2174d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr3 = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel3 = lr3.fit(trainingData2)\n",
    "\n",
    "predictions3 = lrModel3.transform(testData2)\n",
    "\n",
    "predictions3.filter(predictions3['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de597a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_TFIDF_Accuracy2 = evaluator.evaluate(predictions3)\n",
    "print(lr_TFIDF_Accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f59c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel3.summary\n",
    "\n",
    "print(\"\\nFalse positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nTrue positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"\\nPrecision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"\\nRecall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"\\nF-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"\\nAccuracy: %s\\n\\nFPR: %s\\n\\nTPR: %s\\n\\nF-measure: %s\\n\\nPrecision: %s\\n\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3001911c",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc475583",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel2 = nb.fit(trainingData2)\n",
    "predictions4 = nbModel2.transform(testData2)\n",
    "predictions4.filter(predictions4['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30556ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbAccuracy2 = evaluator.evaluate(predictions4)\n",
    "print(nbAccuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86d99ff3",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel2 = dt.fit(trainingData2)\n",
    "predictions5 = dtModel2.transform(testData2)\n",
    "predictions5.filter(predictions5['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtAccuracy2 = evaluator.evaluate(predictions2)\n",
    "print(dtAccuracy2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a227df75",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ee611",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel2 = rf.fit(trainingData2)\n",
    "rfPredictions2 = rfModel2.transform(testData2)\n",
    "rfPredictions2.filter(rfPredictions2['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8548035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfAccuracy2 = rfEvaluator.evaluate(rfPredictions)\n",
    "print(rfAccuracy2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions2.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35b97966",
   "metadata": {},
   "source": [
    "## OnevsRest Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrLR = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "ovr = OneVsRest(classifier=ovrLR)\n",
    "\n",
    "ovrModel2 = ovr.fit(trainingData2)\n",
    "\n",
    "ovrPredictions2 = ovrModel2.transform(testData2)\n",
    "\n",
    "ovrPredictions2.filter(ovrPredictions2['prediction'] == 0) \\\n",
    "    .select(\"clean_text\",\"category\",\"label\",\"prediction\") \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905852db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrEvaluator2 = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "ovrAccuracy2 = ovrEvaluator2.evaluate(ovrPredictions)\n",
    "print(ovrAccuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovrPredictions2.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "021876b3",
   "metadata": {},
   "source": [
    "## Model Two Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5155272",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['LR CV', 'LR TF-IDF', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'OneVsRest']\n",
    "accuracy = [lr_CV_Accuracy2, lr_TFIDF_Accuracy2, nbAccuracy2, dtAccuracy2, rfAccuracy2, ovrAccuracy]\n",
    "\n",
    "plot_bar_x()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
