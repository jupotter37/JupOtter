{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from keras import layers, models, optimizers, callbacks, regularizers, activations, layers, initializers, utils\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from keras.layers import Dense, Input, Activation, Reshape, Add\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#from ar_capsulelayers_test import *\n",
    "from scipy.io import loadmat\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "def load_mnist():\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    from keras.datasets import mnist\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    y_train = utils.to_categorical(y_train.astype('float32'))\n",
    "    y_test = utils.to_categorical(y_test.astype('float32'))\n",
    "#     return (x_train[:500,...], y_train[:500,...]), (x_test, y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 40, 40, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_train_padded = np.pad(x_train, ((0,0), (6,6),(6,6),(0,0)), mode='constant') \n",
    "#x_test_padded = np.pad(x_test, ((0,0), (6,6),(6,6),(0,0)), mode='constant') \n",
    "print(x_train_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_sample = x_train[0]\n",
    "\n",
    "plt.imshow(x_sample[:,:,0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_padded[0][:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_affNIST(path):\n",
    "    afftest = loadmat(path)\n",
    "    afftest_images = afftest['affNISTdata']['image'][0][0].transpose()\n",
    "    afftest_images = afftest_images.reshape((afftest_images.shape[0], 40, 40, 1)).astype(np.float64) /255.\n",
    "\n",
    "    afftest_labels = afftest['affNISTdata']['label_int'][0][0].transpose()\n",
    "    afftest_labels = afftest_labels.reshape((afftest_labels.shape[0])).astype(np.int64) \n",
    "    afftest_images, afftest_labels = shuffle(afftest_images, afftest_labels)\n",
    "    \n",
    "    return afftest_images, afftest_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = prepare_affNIST('./affnist/validation.mat')\n",
    "#x_test, y_test = prepare_affNIST('./affnist/test.mat')\n",
    "\n",
    "y_val = utils.to_categorical(y_val, num_classes=10)\n",
    "#y_test = utils.to_categorical(y_test, num_classes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ar_capsulelayers import *\n",
    "\n",
    "def AR_CapsNet(input_shape, args):\n",
    "    dim_caps = int(args.dimcaps)\n",
    "    layernum = int(args.layernum)\n",
    "    print('layer num : ', layernum)\n",
    "    print('dim_caps : ', dim_caps)\n",
    "    \n",
    "    kernel_regularizer=regularizers.l2(0)\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    conv1 = Conv2d_bn(input_tensor = input_layer, filters=64, kernel_size=3, strides=1, padding='same', activation='relu',\n",
    "                     kernel_regularizer=kernel_regularizer)\n",
    "    conv1 = Conv2d_bn(input_tensor = conv1, filters=64, kernel_size=3, strides=1, padding='same', activation='relu',\n",
    "                     kernel_regularizer=kernel_regularizer)\n",
    "    \n",
    "    ## Primary Capsules\n",
    "    primarycaps = PrimaryCap(n_channels=8, dim_capsule=16, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(conv1)\n",
    "    primarycaps = Activation('tanh')(primarycaps)\n",
    "    print('primary caps shape : ', primarycaps.shape)\n",
    "        \n",
    "    ## Convolutional Capsules\n",
    "    if layernum == 0:\n",
    "        out = primarycaps\n",
    "    elif layernum == 1:\n",
    "        ConvCaps1 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(primarycaps)\n",
    "        ConvCaps1 = Activation('tanh')(ConvCaps1)\n",
    "        print('ConvCaps1 shape : ', ConvCaps1.shape)\n",
    "        out = ConvCaps1\n",
    "        \n",
    "    elif layernum == 2:\n",
    "        ConvCaps1 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(primarycaps)\n",
    "        ConvCaps1 = Activation('tanh')(ConvCaps1)\n",
    "        print('ConvCaps1 shape : ', ConvCaps1.shape)\n",
    "        \n",
    "        ConvCaps2 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = False, kernel_regularizer=kernel_regularizer)(ConvCaps1)\n",
    "        ConvCaps2 = Activation('tanh')(Add()([ConvCaps11 , ConvCaps2]))\n",
    "        print('ConvCaps2 shape : ', ConvCaps2.shape)\n",
    "        out = ConvCaps2\n",
    "        \n",
    "    elif layernum == 3:\n",
    "        ConvCaps1 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = True, kernel_regularizer=kernel_regularizer)(primarycaps)\n",
    "        ConvCaps1 = Activation('tanh')(ConvCaps1)\n",
    "        print('ConvCaps1 shape : ', ConvCaps1.shape)\n",
    "        \n",
    "        ConvCaps2 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = False, kernel_regularizer=kernel_regularizer)(ConvCaps1)\n",
    "        ConvCaps2 = Activation('tanh')(Add()([ConvCaps1 , ConvCaps2]))\n",
    "        print('ConvCaps2 shape : ', ConvCaps2.shape)\n",
    "        \n",
    "        ConvCaps3 = ConvCaps(n_channels=8, dim_capsule=dim_caps, decrease_resolution = False, kernel_regularizer=kernel_regularizer)(ConvCaps2)\n",
    "        ConvCaps3 = Activation('tanh')(Add()([ConvCaps2 , ConvCaps3]))\n",
    "        print('ConvCaps3 shape : ', ConvCaps3.shape)\n",
    "        out = ConvCaps3\n",
    "        \n",
    "    ## Fully Convolutional Capsules\n",
    "    output_dim_capsule = dim_caps \n",
    "    outputs = FullyConvCaps(n_channels=10, dim_capsule=output_dim_capsule, kernel_regularizer=kernel_regularizer)(out)\n",
    "    outputs = Activation('tanh')(outputs)\n",
    "    print('Final Routing caps shape : ', outputs.shape)\n",
    "    \n",
    "    ## Length Capsules\n",
    "    real_outputs = Length()(outputs)\n",
    "    print('Length shape : ', real_outputs.shape)\n",
    "\n",
    "    n_class=10\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked_by_y = Mask()([outputs, y])  # The true label is used to mask the output of capsule layer. For training\n",
    "    masked = Mask()(outputs)  # Mask using the capsule with maximal length. For prediction\n",
    "\n",
    "    # Shared Decoder model in training and prediction\n",
    "    decoder = models.Sequential(name='decoder')\n",
    "    decoder.add(Dense(512, activation='relu', input_dim=output_dim_capsule*n_class, kernel_regularizer=kernel_regularizer))\n",
    "    decoder.add(Dense(512, activation='relu', kernel_regularizer=kernel_regularizer))\n",
    "    decoder.add(Dense(np.prod(input_shape), activation='sigmoid', kernel_regularizer=kernel_regularizer))\n",
    "    decoder.add(Reshape(target_shape=input_shape, name='out_recon'))\n",
    "\n",
    "    train_model = models.Model([input_layer, y], [real_outputs, decoder(masked_by_y)])\n",
    "    eval_model = models.Model(input_layer, [real_outputs, decoder(masked)])\n",
    "    perturb_input_model = models.Model([input_layer, y], decoder(masked_by_y))\n",
    "\n",
    "    # manipulate model\n",
    "    noise = layers.Input(shape=(n_class, output_dim_capsule))\n",
    "    noised_outputs = layers.Add()([outputs, noise])\n",
    "    masked_noised_y = Mask()([noised_outputs, y])\n",
    "    manipulate_model = models.Model([input_layer, y, noise], [outputs, decoder(masked_noised_y)])\n",
    "    return train_model, eval_model\n",
    "    \n",
    "def train(train_model, x_train, y_train, args):\n",
    "    valid_ratio = 0.1*int(args.validratio)\n",
    "    print('valid_ratio', valid_ratio)\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    # Training without data augmentation.\n",
    "    # callbacks\n",
    "    class TimeHistory(callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.times = []\n",
    "\n",
    "        def on_epoch_begin(self, batch, logs={}):\n",
    "            self.epoch_time_start = time.time()\n",
    "\n",
    "        def on_epoch_end(self, batch, logs={}):\n",
    "            self.times.append(time.time() - self.epoch_time_start)\n",
    "    \n",
    "    time_callback = TimeHistory()\n",
    "    log = callbacks.CSVLogger(args.save_dir + '/log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=args.save_dir + '/tensorboard-logs',\n",
    "                               batch_size=args.batch_size, histogram_freq=int(args.debug))\n",
    "    checkpoint = callbacks.ModelCheckpoint(args.save_dir + '/weights-{epoch:02d}.h5', monitor='val_length_1_acc',\n",
    "                                           save_best_only=True, save_weights_only=False, verbose=1)\n",
    "    def lr_schedule(epoch):\n",
    "        lrate = 0.001\n",
    "        if epoch > 50:\n",
    "            lrate = 0.0005\n",
    "        elif epoch > 200:\n",
    "            lrate = 0.0001\n",
    "        return lrate\n",
    "\n",
    "    initial_lr = 0.005\n",
    "    RMSprop = optimizers.RMSprop(lr=initial_lr, rho=0.9, epsilon=1e-08, decay=1e-4)\n",
    "    train_model.compile(optimizer=RMSprop,\n",
    "                      loss=[margin_loss, 'mse'],\n",
    "                      loss_weights=[1., 0.3],\n",
    "                      metrics=['acc'])\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: initial_lr * (0.99 ** epoch))\n",
    "    \n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            zca_epsilon=0,  # epsilon for ZCA whitening\n",
    "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            # randomly shift images horizontally (fraction of total width)\n",
    "            width_shift_range=0.2,\n",
    "            # randomly shift images vertically (fraction of total height)\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.,  # set range for random shear\n",
    "            zoom_range=0.,  # set range for random zoom\n",
    "            channel_shift_range=0.,  # set range for random channel shifts\n",
    "            # set mode for filling points outside the input boundaries\n",
    "            fill_mode='nearest',\n",
    "            cval=0.,  # value used for fill_mode = \"constant\"\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False,  # randomly flip images\n",
    "            # set rescaling factor (applied before any other transformation)\n",
    "            rescale=None,\n",
    "            # set function that will be applied on each input\n",
    "            preprocessing_function=None,\n",
    "            # image data format, either \"channels_first\" or \"channels_last\"\n",
    "            data_format=None)\n",
    "\n",
    "    def train_generator(x, y, batch_size):\n",
    "        train_datagen = datagen  \n",
    "\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    hist = train_model.fit_generator(generator=train_generator(x_train_padded, y_train, args.batch_size),\n",
    "                        steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n",
    "                        epochs=args.epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data = [[x_val, y_val], [y_val, x_val]],\n",
    "                        callbacks=[callbacks.LearningRateScheduler(lr_schedule), log, tb, checkpoint, time_callback])\n",
    "\n",
    "    train_model.save(args.save_dir + '/trained_model.h5')\n",
    "    print(time_callback.times)\n",
    "    # writedata.py\n",
    "    f = open(args.save_dir+'/time.txt', 'w')\n",
    "    for t in time_callback.times:\n",
    "        f.write(str(t)+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer num :  1\n",
      "dim_caps :  16\n",
      "WARNING:tensorflow:From C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "primary caps shape :  (?, 40, 40, 16, 8)\n",
      "WARNING:tensorflow:From C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "ConvCaps1 shape :  (?, 20, 20, 16, 8)\n",
      "Final Routing caps shape :  (?, 10, 16)\n",
      "Length shape :  (?, 10)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 40, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 40, 40, 64)   640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 40, 40, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 40, 40, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 40, 40, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 40, 40, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 40, 40, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "primary_cap_1 (PrimaryCap)      (None, 40, 40, 16, 8 76032       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 40, 40, 16, 8 0           primary_cap_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_caps_1 (ConvCaps)          (None, 20, 20, 16, 8 158848      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 20, 16, 8 0           conv_caps_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "fully_conv_caps_1 (FullyConvCap (None, 10, 16)       8206240     activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 10, 16)       0           fully_conv_caps_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 160)          0           activation_5[0][0]               \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "length_1 (Length)               (None, 10)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 40, 40, 1)    1165888     mask_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,645,088\n",
      "Trainable params: 9,644,832\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "valid_ratio 0.0\n",
      "WARNING:tensorflow:From C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "600/600 [==============================] - 313s 522ms/step - loss: 0.2658 - length_1_loss: 0.2557 - decoder_loss: 0.0337 - length_1_acc: 0.7035 - decoder_acc: 0.8718 - val_loss: 0.2643 - val_length_1_loss: 0.2535 - val_decoder_loss: 0.0360 - val_length_1_acc: 0.6940 - val_decoder_acc: 0.8763\n",
      "\n",
      "Epoch 00001: val_length_1_acc improved from -inf to 0.69402, saving model to ./result/affnist/4/weights-01.h5\n",
      "Epoch 2/2\n",
      "551/600 [==========================>...] - ETA: 8s - loss: 0.1068 - length_1_loss: 0.0985 - decoder_loss: 0.0276 - length_1_acc: 0.9249 - decoder_acc: 0.8723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-224d04bc0b0b>\", line 21, in <module>\n",
      "    train(model, x_train, y_train, args)\n",
      "  File \"<ipython-input-7-2cbff5b71273>\", line 167, in train\n",
      "    callbacks=[callbacks.LearningRateScheduler(lr_schedule), log, tb, checkpoint, time_callback])\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\", line 1418, in fit_generator\n",
      "    initial_epoch=initial_epoch)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training_generator.py\", line 217, in fit_generator\n",
      "    class_weight=class_weight)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\", line 1217, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1439, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\inspect.py\", line 1459, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\inspect.py\", line 1417, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\inspect.py\", line 677, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\inspect.py\", line 720, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\inspect.py\", line 689, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\inspect.py\", line 674, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Choi\\Anaconda3\\envs\\py35\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    epochs=2\n",
    "    batch_size=100\n",
    "    save_dir='./result/affnist/4'\n",
    "    augment=False\n",
    "    gpu=0\n",
    "    dataset='cifar10'\n",
    "    layernum=1\n",
    "    dimcaps=16\n",
    "    validratio=0\n",
    "    resize=False\n",
    "    debug=False\n",
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "model, _ = AR_CapsNet(x_train_padded[0].shape, args)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "train(model, x_train, y_train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_val, y_val = prepare_affNIST('./affnist/validation.mat')\n",
    "x_test, y_test = prepare_affNIST('./affnist/test.mat')\n",
    "\n",
    "#y_val = utils.to_categorical(y_val, num_classes=10)\n",
    "y_test = utils.to_categorical(y_test, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer num :  1\n",
      "dim_caps :  16\n",
      "primary caps shape :  (?, 20, 20, 16, 8)\n",
      "ConvCaps1 shape :  (?, 10, 10, 16, 8)\n",
      "Final Routing caps shape :  (?, 10, 16)\n",
      "Length shape :  (?, 10)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 40, 40, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 40, 40, 64)   640         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 40, 40, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 40, 40, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 40, 40, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 40, 40, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 40, 40, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "primary_cap_3 (PrimaryCap)      (None, 20, 20, 16, 8 76032       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 20, 16, 8 0           primary_cap_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_caps_3 (ConvCaps)          (None, 10, 10, 16, 8 158848      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 10, 10, 16, 8 0           conv_caps_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "fully_conv_caps_3 (FullyConvCap (None, 10, 16)       2062240     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 10, 16)       0           fully_conv_caps_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_7 (Mask)                   (None, 160)          0           activation_15[0][0]              \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "length_3 (Length)               (None, 10)           0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 40, 40, 1)    1165888     mask_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,501,088\n",
      "Trainable params: 3,500,832\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class args:\n",
    "    epochs=20\n",
    "    batch_size=100\n",
    "    save_dir='./result/affnist/1'\n",
    "    augment=False\n",
    "    gpu=0\n",
    "    dataset='cifar10'\n",
    "    layernum=1\n",
    "    dimcaps=16\n",
    "    validratio=0\n",
    "    resize=False\n",
    "    debug=False\n",
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.makedirs(args.save_dir)\n",
    "\n",
    "model, eval_model = AR_CapsNet(x_train_padded[0].shape, args)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './result/affnist/weights-18.h5'\n",
    "eval_model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320000, 40, 40, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred1\n",
      "acc :  0.9163\n"
     ]
    }
   ],
   "source": [
    "y_label = np.argmax(y_test, axis=1)\n",
    "\n",
    "pred1, _ = eval_model.predict(x=x_test, batch_size=100)\n",
    "print('pred1')\n",
    "pred_label = np.argmax(pred1, axis=1)\n",
    "acc1 = np.sum(y_label == pred_label) / float(y_label.shape[0])\n",
    "print('acc : ', acc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred1\n",
      "acc :  0.91453125\n"
     ]
    }
   ],
   "source": [
    "model_path = './result/affnist/2/weights-18.h5'\n",
    "eval_model.load_weights(model_path)\n",
    "\n",
    "y_label = np.argmax(y_test, axis=1)\n",
    "\n",
    "pred1, _ = eval_model.predict(x=x_test, batch_size=100)\n",
    "print('pred1')\n",
    "pred_label = np.argmax(pred1, axis=1)\n",
    "acc1 = np.sum(y_label == pred_label) / float(y_label.shape[0])\n",
    "print('acc : ', acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred1\n",
      "acc :  0.9158375\n"
     ]
    }
   ],
   "source": [
    "model_path = './result/affnist/3/weights-20.h5'\n",
    "eval_model.load_weights(model_path)\n",
    "\n",
    "y_label = np.argmax(y_test, axis=1)\n",
    "\n",
    "pred1, _ = eval_model.predict(x=x_test, batch_size=100)\n",
    "print('pred1')\n",
    "pred_label = np.argmax(pred1, axis=1)\n",
    "acc1 = np.sum(y_label == pred_label) / float(y_label.shape[0])\n",
    "print('acc : ', acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
