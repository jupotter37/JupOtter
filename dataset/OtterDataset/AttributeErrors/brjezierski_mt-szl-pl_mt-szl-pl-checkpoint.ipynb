{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_substring_inside_parentheses(s):\n",
    "    # Use regular expression to find and remove substring inside parentheses\n",
    "    s = re.sub(r'\\s*\\([^)]*\\)\\s*', '', s)\n",
    "    # Remove any preceding or trailing whitespaces\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(folder):\n",
    "    articles = {}\n",
    "\n",
    "    for subdir, dirs, files in os.walk(folder):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(subdir, filename)\n",
    "            if \"wiki\" in file_path:\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    file_contents = f.read()\n",
    "\n",
    "                doc_pattern = re.compile(r'<doc id=\"\\d+\" url=\".*?\" title=\"(.*?)\">(.*?)</doc>', re.DOTALL)\n",
    "                doc_matches = doc_pattern.findall(file_contents)\n",
    "\n",
    "                for match in doc_matches:\n",
    "                    title = remove_substring_inside_parentheses(match[0])\n",
    "                    text = match[1].strip()\n",
    "                    articles[title] = text\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "szl_articles = get_dictionary(\"text-szl\")\n",
    "pl_articles = get_dictionary(\"text-pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_titles():\n",
    "    # Read the txt file into a pandas dataframe with two columns\n",
    "    df = pd.read_csv('titles.txt', sep=' \\|\\|\\| ', engine='python', header=None, names=['szl', 'pl'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared_titles = get_shared_titles()\n",
    "# szl_shared_titles = set(shared_titles['szl'].to_list())\n",
    "# szl_all_titles = set(szl_articles.keys())\n",
    "# pl_all_titles = set(pl_articles.keys())\n",
    "# szl_only_titles = list(szl_all_titles - szl_shared_titles)\n",
    "# pl_only_titles = list(szl_shared_titles - szl_all_titles)\n",
    "# szl_truly_shared_titles = szl_shared_titles.intersection(szl_all_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_article_dicts():\n",
    "    article_szl = []\n",
    "    article_pl = []\n",
    "\n",
    "    # Loop through each row of shared_titles\n",
    "    for index, row in shared_titles.iterrows():\n",
    "        # Get the pl value from shared_titles\n",
    "        pl_value = row[\"pl\"]\n",
    "        # Get the corresponding value from articles_pl and append it to article_pl list\n",
    "        article_pl.append(pl_articles.get(pl_value, \"\") if pl_value in pl_articles.keys() else float('nan'))\n",
    "\n",
    "        # Get the szl value from shared_titles\n",
    "        szl_value = row[\"szl\"]\n",
    "        # Get the corresponding value from articles_szl and append it to article_szl list\n",
    "        article_szl.append(szl_articles.get(szl_value, \"\") if szl_value in szl_articles.keys() else float('nan'))\n",
    "\n",
    "    # Add article_szl and article_pl as new columns to shared_titles DataFrame\n",
    "    shared_titles[\"article_szl\"] = article_szl\n",
    "    shared_titles[\"article_pl\"] = article_pl\n",
    "    shared_titles.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13777\n",
      "                      szl                  pl  \\\n",
      "416                  1844                1844   \n",
      "12499        Pleiskirchen        Pleiskirchen   \n",
      "8898         Kůnstantynůw        Konstantynów   \n",
      "15705          Sůmaliland          Somaliland   \n",
      "7662             Jaślikůw            Jaślików   \n",
      "15943           The Corrs           The Corrs   \n",
      "2996      Charlottesville     Charlottesville   \n",
      "9645              Lübesse             Lübesse   \n",
      "7908              Kakulin             Kakulin   \n",
      "4969            Gaziantep           Gaziantep   \n",
      "18772            Śwjyntno             Świętno   \n",
      "15608         Szwajcaryjo          Szwajcaria   \n",
      "4766   Franciszek Pieczka  Franciszek Pieczka   \n",
      "2260           Boisseuilh          Boisseuilh   \n",
      "1154              Allauch             Allauch   \n",
      "6479                 Gowa               Głowa   \n",
      "18896                Żydy               Żydzi   \n",
      "6474              Gotůwka             Gotówka   \n",
      "1348   Antoni Piechniczek  Antoni Piechniczek   \n",
      "14785              Sledge              Sledge   \n",
      "\n",
      "                                             article_szl  \\\n",
      "416                                                 1844   \n",
      "12499  Pleiskirchen\\n\\nPleiskirchen - gmin we Mjymcac...   \n",
      "8898   Kůnstantynůw (lubelske wojewůdztwo)\\n\\nKůnstan...   \n",
      "15705  Sůmaliland\\n\\nSůmaliland (Republika Sůmaliland...   \n",
      "7662   Jaślikůw\\n\\nJaślikůw (pol. \"Jaślików\") – wjeś ...   \n",
      "15943  The Corrs\\n\\nThe Corrs - irlandzko muzyczno sk...   \n",
      "2996   Charlottesville\\n\\nCharlottesville – mjasto na...   \n",
      "9645   Lübesse\\n\\nLübesse – gmin we Mjymcach, we land...   \n",
      "7908   Kakulin\\n\\nKakulin - wjeś we Polsce, we wjelgo...   \n",
      "4969   Gaziantep\\n\\nGaziantep - mjasto we Turcyji, st...   \n",
      "18772  Śwjyntno (wjelgopolske wojewůdztwo)\\n\\nŚwjyntn...   \n",
      "15608  Szwajcaryjo\\n\\nSzwajcaryjo, Szwajcarsko Kůnfed...   \n",
      "4766   Franciszek Pieczka\\n\\nFranciszek Maksymilian P...   \n",
      "2260   Boisseuilh\\n\\nBoisseuilh – gmin we Francyji, w...   \n",
      "1154   Allauch\\n\\nAllauch - gmin we Francyji, we regi...   \n",
      "6479   Gowa\\n\\nGowa (łac. \"caput\") to je tajla ciała ...   \n",
      "18896  Żydy\\n\\nŻydy (dosłowńy \"te, co przajům Jehowje...   \n",
      "6474   Gotůwka (lubelske wojewůdztwo)\\n\\nGotůwka (pol...   \n",
      "1348   Antoni Piechniczek\\n\\nAntoni Piechniczek (rodz...   \n",
      "14785  Sledge (Mississippi)\\n\\nSledge – mjasto we USA...   \n",
      "\n",
      "                                              article_pl  \n",
      "416                                                 1844  \n",
      "12499  Pleiskirchen\\n\\nPleiskirchen – gmina w Niemcze...  \n",
      "8898   Konstantynów (rejon grodzieński)\\n\\nKonstantyn...  \n",
      "15705  Somaliland\\n\\nSomaliland (Republika Somaliland...  \n",
      "7662   Jaślików\\n\\nJaślików – wieś w Polsce położona ...  \n",
      "15943  The Corrs\\n\\nThe Corrs – irlandzki zespół muzy...  \n",
      "2996   Charlottesville\\n\\nCharlottesville (C-Ville, H...  \n",
      "9645   Lübesse\\n\\nLübesse – miejscowość i gmina w Nie...  \n",
      "7908   Kakulin (gromada)\\n\\nKakulin – dawna gromada, ...  \n",
      "4969   Gaziantep (stacja kolejowa)\\n\\nGaziantep - sta...  \n",
      "18772  Świętno (województwo zachodniopomorskie)\\n\\nŚw...  \n",
      "15608  Szwajcaria (Suwałki)\\n\\nSzwajcaria – część mia...  \n",
      "4766   Franciszek Pieczka\\n\\nFranciszek Maksymilian P...  \n",
      "2260   Boisseuilh\\n\\nBoisseuilh – miejscowość i gmina...  \n",
      "1154   Allauch\\n\\nAllauch – miejscowość i gmina we Fr...  \n",
      "6479   Głowa\\n\\nGłowa () – część ciała zwierząt, zajm...  \n",
      "18896  Żydzi\\n\\nŻydzi (dosł. „chwalcy Jahwe” lub „czc...  \n",
      "6474   Gotówka (Białoruś)\\n\\nGotówka (; ) – wieś na B...  \n",
      "1348   Antoni Piechniczek\\n\\nAntoni Krzysztof Piechni...  \n",
      "14785  Sledge\\n\\nSledge – miasto w Stanach Zjednoczon...  \n"
     ]
    }
   ],
   "source": [
    "merge_article_dicts()\n",
    "print(len(shared_titles))\n",
    "print(shared_titles.sample(n=20))\n",
    "shared_titles.to_csv('pl-szl_articles.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_sample = random.sample(szl_unshared_titles, 50)\n",
    "# print(len(szl_truly_shared_titles))\n",
    "# print(random_sample)\n",
    "shared_titles = pd.read_csv('pl-szl_articles.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>szl</th>\n",
       "      <th>pl</th>\n",
       "      <th>article_szl</th>\n",
       "      <th>article_pl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silesia</td>\n",
       "      <td>Silesia</td>\n",
       "      <td>(257) Silesia\\n\\n(257) Silesia – planetoida z ...</td>\n",
       "      <td>Silesia (czasopismo)\\n\\n„Silesia” – niemieckoj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.ac</td>\n",
       "      <td>.ac</td>\n",
       "      <td>.ac\\n\\n.ac je to internetowŏ dōmyna, kerŏ je z...</td>\n",
       "      <td>.ac\\n\\n.ac – krajowa domena internetowa najwyż...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.ad</td>\n",
       "      <td>.ad</td>\n",
       "      <td>.ad\\n\\n.ad je to internetowŏ dōmyna, kerŏ je z...</td>\n",
       "      <td>.ad\\n\\n.ad – krajowa domena internetowa najwyż...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.ae</td>\n",
       "      <td>.ae</td>\n",
       "      <td>.ae\\n\\n.ae je to internetowŏ dōmyna, kerŏ je z...</td>\n",
       "      <td>.ae\\n\\n.ae – krajowa domena internetowa najwyż...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.af</td>\n",
       "      <td>.af</td>\n",
       "      <td>.af\\n\\n.af je to internetowŏ dōmyna, kerŏ je z...</td>\n",
       "      <td>.af\\n\\n.af – krajowa domena internetowa najwyż...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13772</th>\n",
       "      <td>Ю</td>\n",
       "      <td>Ю</td>\n",
       "      <td>Ю\\n\\nЮ ю — trzidźestodrugo i przeduostatńo buc...</td>\n",
       "      <td>Ю\\n\\nЮ, Юю – podstawowa litera cyrylicy używan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13773</th>\n",
       "      <td>Я</td>\n",
       "      <td>Я</td>\n",
       "      <td>Я\\n\\nЯ я — trzidźestotrzećo i uostatńo buchszt...</td>\n",
       "      <td>Я\\n\\nЯ я – ostatnia litera cyrylicy, 33. liter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13774</th>\n",
       "      <td>Խ</td>\n",
       "      <td>Խ</td>\n",
       "      <td>Խ\\n\\nԽ, խ (che) – trzinŏstŏ buchsztaba alfabyt...</td>\n",
       "      <td>Խ\\n\\nԽ, խ (che) – trzynasta litera alfabetu or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13775</th>\n",
       "      <td>Ẋ</td>\n",
       "      <td>Ẋ</td>\n",
       "      <td>Ẋ\\n\\nẊẋ – buchsztaba łaćińskiij wersyje czeczy...</td>\n",
       "      <td>Ẋ\\n\\nẊ ẋ – litera łacińskiej wersji alfabetu c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13776</th>\n",
       "      <td>Ꙉ</td>\n",
       "      <td>Ꙉ</td>\n",
       "      <td>Ꙉ\\n\\nꙈ, ꙉ (djerw) – buchsztaba wczaśnyj cyryli...</td>\n",
       "      <td>Ꙉ\\n\\nꙈ, ꙉ – litera wczesnej cyrylicy. Wykorzys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13777 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           szl       pl                                        article_szl  \\\n",
       "0      Silesia  Silesia  (257) Silesia\\n\\n(257) Silesia – planetoida z ...   \n",
       "1          .ac      .ac  .ac\\n\\n.ac je to internetowŏ dōmyna, kerŏ je z...   \n",
       "2          .ad      .ad  .ad\\n\\n.ad je to internetowŏ dōmyna, kerŏ je z...   \n",
       "3          .ae      .ae  .ae\\n\\n.ae je to internetowŏ dōmyna, kerŏ je z...   \n",
       "4          .af      .af  .af\\n\\n.af je to internetowŏ dōmyna, kerŏ je z...   \n",
       "...        ...      ...                                                ...   \n",
       "13772        Ю        Ю  Ю\\n\\nЮ ю — trzidźestodrugo i przeduostatńo buc...   \n",
       "13773        Я        Я  Я\\n\\nЯ я — trzidźestotrzećo i uostatńo buchszt...   \n",
       "13774        Խ        Խ  Խ\\n\\nԽ, խ (che) – trzinŏstŏ buchsztaba alfabyt...   \n",
       "13775        Ẋ        Ẋ  Ẋ\\n\\nẊẋ – buchsztaba łaćińskiij wersyje czeczy...   \n",
       "13776        Ꙉ        Ꙉ  Ꙉ\\n\\nꙈ, ꙉ (djerw) – buchsztaba wczaśnyj cyryli...   \n",
       "\n",
       "                                              article_pl  \n",
       "0      Silesia (czasopismo)\\n\\n„Silesia” – niemieckoj...  \n",
       "1      .ac\\n\\n.ac – krajowa domena internetowa najwyż...  \n",
       "2      .ad\\n\\n.ad – krajowa domena internetowa najwyż...  \n",
       "3      .ae\\n\\n.ae – krajowa domena internetowa najwyż...  \n",
       "4      .af\\n\\n.af – krajowa domena internetowa najwyż...  \n",
       "...                                                  ...  \n",
       "13772  Ю\\n\\nЮ, Юю – podstawowa litera cyrylicy używan...  \n",
       "13773  Я\\n\\nЯ я – ostatnia litera cyrylicy, 33. liter...  \n",
       "13774  Խ\\n\\nԽ, խ (che) – trzynasta litera alfabetu or...  \n",
       "13775  Ẋ\\n\\nẊ ẋ – litera łacińskiej wersji alfabetu c...  \n",
       "13776  Ꙉ\\n\\nꙈ, ꙉ – litera wczesnej cyrylicy. Wykorzys...  \n",
       "\n",
       "[13777 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles in szl wiki with more than 400 chars: 1878\n"
     ]
    }
   ],
   "source": [
    "char_count = 400\n",
    "count = sum(1 for value in articles.values() if len(value) > char_count)\n",
    "print(f\"Articles in szl wiki with more than {char_count} chars: {count}\")  # Output: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bartekjezierski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Get right input format\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def format_input(input_filename, output_filename):\n",
    "    sentences = []\n",
    "    # read in the input file and remove empty lines\n",
    "    with open(input_filename, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    for line in lines:\n",
    "        # split the text into sentences\n",
    "        sentences.extend(nltk.sent_tokenize(line))\n",
    "\n",
    "    # write each sentence to the output file on a separate line\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            f.write(' '.join(tokens) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# def format_input(input_filename, output_filename):\n",
    "#     sentences = []\n",
    "#     # read in the input file and remove empty lines\n",
    "#     with open(input_filename, 'r') as f:\n",
    "#         lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "#     # abbreviations to ignore full stops after\n",
    "#     ignore_abbrev = [\"rodz.\", \"um.\", \"ur.\", \"zm.\"]\n",
    "\n",
    "#     for line in lines:\n",
    "#         # split the text into sentences\n",
    "#         new_sentences = nltk.sent_tokenize(line)\n",
    "#         for i in range(len(new_sentences) - 1):\n",
    "#             current_sentence = new_sentences[i]\n",
    "#             next_sentence = new_sentences[i+1]\n",
    "\n",
    "#             # check if the current sentence ends with an abbreviation to ignore\n",
    "#             if current_sentence.strip().endswith(tuple(ignore_abbrev)):\n",
    "#                 sentences.append(current_sentence + ' ' + next_sentence)\n",
    "#                 i += 1\n",
    "#             else:\n",
    "#                 sentences.append(current_sentence)\n",
    "\n",
    "#         # add the last sentence of the line\n",
    "#         sentences.append(new_sentences[-1])\n",
    "\n",
    "#     # write each sentence to the output file on a separate line\n",
    "#     with open(output_filename, 'w') as f:\n",
    "#         for sentence in sentences:\n",
    "#             tokens = nltk.word_tokenize(sentence)\n",
    "#             f.write(' '.join(tokens) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_input('source.pl', 'source_token.pl')\n",
    "format_input('target.szl', 'target_token.szl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/14wnz0xs6xs5z8gzsf2j1g940000gn/T/ipykernel_95384/3766870665.py:19: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  items = list_element.find_all('li', text=lambda t: t.startswith('Polsko-ślōnski:'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m list_element \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmw-category\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# find all list items that start with \"Polsko-ślōnski:\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[43mlist_element\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolsko-ślōnski:\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print the items\u001b[39;00m\n\u001b[1;32m     22\u001b[0m last_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the URL of the website\n",
    "url = \"https://www.silling.org/slownik/index.php?title=Kategoryj%C5%8F:polsko-%C5%9Bl%C5%8Dnski&pagefrom=a\"\n",
    "pl_words = []\n",
    "while True:\n",
    "#     print(url)\n",
    "    # send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # find the list element with the class 'mw-category'\n",
    "    list_element = soup.find('div', {'class': 'mw-category'})\n",
    "\n",
    "    # find all list items that start with \"Polsko-ślōnski:\"\n",
    "    items = list_element.find_all('li', text=lambda t: t.startswith('Polsko-ślōnski:'))\n",
    "\n",
    "    # print the items\n",
    "    last_item = \"\"\n",
    "    \n",
    "    for item in items:\n",
    "        last_item = item.text.split(\":\")[-1]\n",
    "        pl_words.append(last_item)\n",
    "#     print(last_item)\n",
    "    if items[0] == items[-1]:\n",
    "        break\n",
    "        \n",
    "    url = \"https://www.silling.org/slownik/index.php?title=Kategoryj%C5%8F:polsko-%C5%9Bl%C5%8Dnski&pagefrom=\"+last_item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 21186/21186 [1:17:16<00:00,  4.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "pl_sil_dict = {}\n",
    "\n",
    "# for i, pl_word in enumerate(pl_words):\n",
    "for i in tqdm(range(len(pl_words))):\n",
    "    pl_word = pl_words[i]\n",
    "#     print(\"word\", pl_word)\n",
    "\n",
    "#     if i % 100 == 0:\n",
    "#     print(f\"{i}/{len(pl_words)}\")\n",
    "    url = 'https://www.silling.org/slownik/Polsko-%C5%9Bl%C5%8Dnski:'+pl_word\n",
    "#     print(\"url\", url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    if soup.find('div', {'class': 'mw-parser-output'}):\n",
    "        list_items = soup.find('div', {'class': 'mw-parser-output'}).find_all('li')\n",
    "\n",
    "        for item in list_items:\n",
    "            pl_sil_dict[pl_word] = item.text\n",
    "#             print(item.text)\n",
    "with open(\"output.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for key, value in pl_sil_dict.items():\n",
    "        writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'a',\n",
       " 'abdukcja': 'abdukcyjŏ',\n",
       " 'abdykacja': 'abdykacyjŏ',\n",
       " 'aberracja': 'aberracyjŏ',\n",
       " 'abiologia': 'abiologijŏ',\n",
       " 'abiologicznie': 'abiologicznie',\n",
       " 'abiologiczny': 'abiologiczny',\n",
       " 'abiotycznie': 'abiotycznie',\n",
       " 'abiotyczny': 'abiotyczny',\n",
       " 'ablacja': 'ablacyjŏ'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_sil_dict\n",
    "# len(pl_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.tsv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for key, value in pl_sil_dict.items():\n",
    "        writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the file as dataframe\n",
    "df = pd.read_csv('dictionary.tsv', delimiter='\\t')\n",
    "\n",
    "# reverse columns 0 and 1\n",
    "df = df.iloc[:, ::-1]\n",
    "\n",
    "# save the modified dataframe\n",
    "df.to_csv('reversed_dictionary.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bartekjezierski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Save as article per file \n",
    "parallel_articles = pd.read_csv('pl-szl_articles.tsv', sep='\\t')\n",
    "\n",
    "# Get right input format\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def format_input(input_filename, output_filename):\n",
    "    sentences = []\n",
    "    # read in the input file and remove empty lines\n",
    "    with open(input_filename, 'r') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    for line in lines:\n",
    "        # split the text into sentences\n",
    "        sentences.extend(nltk.sent_tokenize(line))\n",
    "\n",
    "    # write each sentence to the output file on a separate line\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for sentence in sentences:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            f.write(' '.join(tokens) + '\\n')\n",
    "\n",
    "import os\n",
    "\n",
    "# create the parallel_articles directory if it doesn't exist\n",
    "if not os.path.exists('parallel_articles'):\n",
    "    os.mkdir('parallel_articles')\n",
    "\n",
    "# loop over each row of the dataframe\n",
    "for idx, row in parallel_articles.iterrows():\n",
    "    # get the values of article_szl and article_pl\n",
    "    szl = row['article_szl']\n",
    "    pl = row['article_pl']\n",
    "    \n",
    "    # create the filename and path for the szl file\n",
    "    szl_filename = f\"{idx}.szl\"\n",
    "    szl_filepath = os.path.join('parallel_articles', szl_filename)\n",
    "    \n",
    "    # create the filename and path for the pl file\n",
    "    pl_filename = f\"{idx}.pl\"\n",
    "    pl_filepath = os.path.join('parallel_articles', pl_filename)\n",
    "    \n",
    "    # write the szl and pl values to their respective files\n",
    "    with open(szl_filepath, 'w') as f:\n",
    "        f.write(szl)\n",
    "    with open(pl_filepath, 'w') as f:\n",
    "        f.write(pl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all the files\n",
    "import os\n",
    "\n",
    "dir_path = 'parallel_articles'\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    input_path = os.path.join(dir_path, filename)\n",
    "    format_input(input_path, input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate pl files which are too long\n",
    "\n",
    "import os\n",
    "\n",
    "def truncate_pl_file(szl_file, pl_file):\n",
    "    with open(szl_file, 'r', encoding='utf-8') as f:\n",
    "        szl_lines = f.readlines()\n",
    "    with open(pl_file, 'r', encoding='utf-8') as f:\n",
    "        pl_lines = f.readlines()\n",
    "\n",
    "    if len(szl_lines) * 5 < len(pl_lines):\n",
    "        num_pl_lines = len(szl_lines) * 3\n",
    "        with open(pl_file, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(pl_lines[:num_pl_lines])\n",
    "\n",
    "for i in range(13777):\n",
    "    szl_file = f'parallel_articles/{i}.szl'\n",
    "    pl_file = f'parallel_articles/{i}.pl'\n",
    "    if os.path.isfile(szl_file) and os.path.isfile(pl_file):\n",
    "        truncate_pl_file(szl_file, pl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples containing the required file names\n",
    "\n",
    "out_dir_path = \"alignments\"\n",
    "file_tuples = [(os.path.join(dir_path, f\"{i}.pl\"), os.path.join(dir_path, f\"{i}.szl\"), os.path.join(out_dir_path, f\"{i}.align\")) for i in range(51)]\n",
    "\n",
    "# Open the batch_job file in write mode\n",
    "with open(\"batch_job\", \"w\") as f:\n",
    "    # Write each file tuple to a separate line in the file\n",
    "    for file_tuple in file_tuples:\n",
    "        f.write(\"\\t\".join(file_tuple) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
