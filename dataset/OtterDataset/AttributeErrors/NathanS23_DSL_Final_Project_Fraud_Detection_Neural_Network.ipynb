{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otZxFYfBb4Lf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beHzG9WVppdH"
      },
      "source": [
        "# Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZnKgjKtp37g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD75YtAKrkQl",
        "outputId": "2f5a61f3-4eb3-4178-b3c7-4f66e194efd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting preprocessing...\n",
            "Mem. usage decreased to 542.35 Mb (0.0% reduction)\n",
            "Mem. usage decreased to 472.59 Mb (0.0% reduction)\n",
            "Preprocessing and UID detection complete.\n"
          ]
        }
      ],
      "source": [
        "train_processed, test_processed = preprocess_and_detect_uid(train_df, test_df, train_identity, test_identity)\n",
        "test_processed.columns = test_processed.columns.str.replace('-', '_')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyvdjeG2ritq"
      },
      "outputs": [],
      "source": [
        "train_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test_processed.replace([np.inf, -np.inf], np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW8Z2NGgxi1S"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def preprocess_for_NN(input_train_df, input_test_df, target_column='isFraud'):\n",
        "    # Work on local copies to avoid modifying global variables\n",
        "    train_df = input_train_df.copy()\n",
        "    test_df = input_test_df.copy()\n",
        "\n",
        "    # Drop target column\n",
        "    if target_column in train_df.columns:\n",
        "        train_df = train_df.drop(columns=[target_column])\n",
        "\n",
        "    # Identify numeric and categorical columns that exist in the datasets\n",
        "    numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_columns = train_df.select_dtypes(include=['object']).columns\n",
        "    print(categorical_columns)\n",
        "    # Ensure numeric and categorical columns exist in both train and test\n",
        "    numeric_columns = [col for col in numeric_columns if col in test_df.columns]\n",
        "    categorical_columns = [col for col in categorical_columns if col in test_df.columns]\n",
        "\n",
        "    # Handle missing values for numeric columns\n",
        "    numeric_imputer = SimpleImputer(strategy='median')\n",
        "    train_df[numeric_columns] = numeric_imputer.fit_transform(train_df[numeric_columns])\n",
        "    test_df[numeric_columns] = numeric_imputer.transform(test_df[numeric_columns])\n",
        "\n",
        "    # Handle missing values for categorical columns\n",
        "    categorical_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "    train_df[categorical_columns] = categorical_imputer.fit_transform(train_df[categorical_columns])\n",
        "    test_df[categorical_columns] = categorical_imputer.transform(test_df[categorical_columns])\n",
        "\n",
        "    # # Special case: Encoding for id_12\n",
        "    # if 'id_12' in categorical_columns:\n",
        "    #     train_df['id_12'] = train_df['id_12'].fillna('missing').astype(str)\n",
        "    #     test_df['id_12'] = test_df['id_12'].fillna('missing').astype(str)\n",
        "\n",
        "    #     mapping = {'NotFound': 0, 'Found': 1, 'missing': -1}\n",
        "    #     train_df['id_12'] = train_df['id_12'].map(mapping)\n",
        "    #     test_df['id_12'] = test_df['id_12'].map(mapping)\n",
        "\n",
        "    #     # Check if mapping was successful\n",
        "    #     if train_df['id_12'].isnull().any() or test_df['id_12'].isnull().any():\n",
        "    #         raise ValueError(\"Mapping failed! Check values in id_12 for unmapped categories.\")\n",
        "\n",
        "    # # Frequency Encoding for remaining categorical columns\n",
        "    # for col in categorical_columns:\n",
        "    #     if col != 'id_12':  # Skip id_12 as it's already encoded\n",
        "    #         combined = pd.concat([train_df[col], test_df[col]])\n",
        "    #         freq_encoding = combined.value_counts(normalize=True)  # Frequency proportions\n",
        "    #         train_df[col] = train_df[col].map(freq_encoding).fillna(0)  # Encode train\n",
        "    #         test_df[col] = test_df[col].map(freq_encoding).fillna(0)    # Encode test\n",
        "\n",
        "    # Align column order between train and test\n",
        "    all_columns = train_df.columns\n",
        "    test_df = test_df[all_columns]\n",
        "\n",
        "    # Verify all columns are numeric\n",
        "    for col in train_df.columns:\n",
        "        if not pd.api.types.is_numeric_dtype(train_df[col]):\n",
        "            raise ValueError(f\"Column {col} is not numeric after preprocessing! Values: {train_df[col].unique()}\")\n",
        "\n",
        "    # Remove any infinite values\n",
        "    train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Re-impute numeric columns to handle infinities replaced with NaN\n",
        "    train_df[numeric_columns] = numeric_imputer.fit_transform(train_df[numeric_columns])\n",
        "    test_df[numeric_columns] = numeric_imputer.transform(test_df[numeric_columns])\n",
        "\n",
        "    # Standardize numeric columns\n",
        "    scaler = StandardScaler()\n",
        "    train_df = scaler.fit_transform(train_df)\n",
        "    test_df = scaler.transform(test_df)\n",
        "\n",
        "    return train_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIfBle9CykFT"
      },
      "outputs": [],
      "source": [
        "def preprocess_for_NN(input_train_df, input_test_df, target_column='isFraud'):\n",
        "    print(\"Starting preprocessing (Method B)...\")\n",
        "    train_df = input_train_df.copy()\n",
        "    test_df = input_test_df.copy()\n",
        "\n",
        "    # Drop target column\n",
        "    if target_column in train_df.columns:\n",
        "        train_df = train_df.drop(columns=[target_column])\n",
        "\n",
        "    # Identify numeric and categorical columns\n",
        "    numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Ensure numeric and categorical columns exist in both train and test\n",
        "    numeric_columns = [col for col in numeric_columns if col in test_df.columns]\n",
        "    categorical_columns = [col for col in categorical_columns if col in test_df.columns]\n",
        "\n",
        "    # Handle missing values\n",
        "    numeric_imputer = SimpleImputer(strategy='median')\n",
        "    train_df[numeric_columns] = numeric_imputer.fit_transform(train_df[numeric_columns])\n",
        "    test_df[numeric_columns] = numeric_imputer.transform(test_df[numeric_columns])\n",
        "\n",
        "    categorical_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "    train_df[categorical_columns] = categorical_imputer.fit_transform(train_df[categorical_columns])\n",
        "    test_df[categorical_columns] = categorical_imputer.transform(test_df[categorical_columns])\n",
        "\n",
        "    # Frequency Encoding for categorical columns\n",
        "    for col in categorical_columns:\n",
        "        combined = pd.concat([train_df[col], test_df[col]])\n",
        "        freq_encoding = combined.value_counts(normalize=True)\n",
        "        train_df[col] = train_df[col].map(freq_encoding).fillna(0)\n",
        "        test_df[col] = test_df[col].map(freq_encoding).fillna(0)\n",
        "\n",
        "    # Align column order\n",
        "    test_df = test_df[train_df.columns]\n",
        "\n",
        "    # Replace infinities with NaN\n",
        "    train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Final imputation for NaNs\n",
        "    train_df = numeric_imputer.fit_transform(train_df)\n",
        "    test_df = numeric_imputer.transform(test_df)\n",
        "\n",
        "    # Standardize numeric columns\n",
        "    scaler = StandardScaler()\n",
        "    train_df = scaler.fit_transform(train_df)\n",
        "    test_df = scaler.transform(test_df)\n",
        "\n",
        "    print(\"Preprocessing (Method B) complete.\")\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1jutdMHp7Ib",
        "outputId": "1f4e3ec8-9777-4ac3-e31f-4962c1267165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting preprocessing (Method B)...\n",
            "Preprocessing (Method B) complete.\n",
            "=== Verification of Preprocessed Data ===\n",
            "NaN values in train data: False\n",
            "NaN values in test data: False\n",
            "Infinity values in train data: False\n",
            "Infinity values in test data: False\n",
            "Max value in train data: 604.9377178842773\n",
            "Min value in train data: -186.37770000411408\n",
            "Max value in test data: 2431.967463663471\n",
            "Min value in test data: -186.37770000411408\n",
            "Shape of train data: (590540, 445)\n",
            "Shape of test data: (506691, 445)\n",
            "Zero-variance columns in train data: 0\n",
            "Zero-variance columns in test data: 0\n",
            "=== Verification Completed ===\n"
          ]
        }
      ],
      "source": [
        "train_df_NN, test_df_NN = preprocess_for_NN(train_processed, test_processed, target_column='isFraud')\n",
        "verify_preprocessed_data(train_df_NN, test_df_NN)\n",
        "# Convert to PyTorch tensors for neural network\n",
        "train_tensor = torch.tensor(train_df_NN, dtype=torch.float32)\n",
        "test_tensor = torch.tensor(test_df_NN, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gdl0YlRvTEi",
        "outputId": "b9cb1f54-20a9-4ddb-b3fb-7bbf135f6272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Verification of Preprocessed Data ===\n",
            "NaN values in train data: True\n",
            "NaN values in test data: True\n",
            "Infinity values in train data: False\n",
            "Infinity values in test data: False\n",
            "Max value in train data: nan\n",
            "Min value in train data: nan\n",
            "Max value in test data: nan\n",
            "Min value in test data: nan\n",
            "Shape of train data: (590540, 445)\n",
            "Shape of test data: (506691, 445)\n",
            "Zero-variance columns in train data: 0\n",
            "Zero-variance columns in test data: 0\n",
            "=== Verification Completed ===\n"
          ]
        }
      ],
      "source": [
        "verify_preprocessed_data(train_df_NN, test_df_NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bxkawamvhLr",
        "outputId": "1ae58328-f68c-46a7-b7d3-9cab6c52180f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for NaN values in training dataset:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "Checking for NaN values in testing dataset:\n",
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace with actual column names if you have them\n",
        "column_names = [f\"feature_{i}\" for i in range(train_df_NN.shape[1])]\n",
        "\n",
        "# Convert to DataFrame\n",
        "train_df_NN_df = pd.DataFrame(train_df_NN, columns=column_names)\n",
        "test_df_NN_df = pd.DataFrame(test_df_NN, columns=column_names)\n",
        "\n",
        "\n",
        "print(\"Checking for NaN values in training dataset:\")\n",
        "nan_train = train_df_NN_df.isnull().sum()\n",
        "nan_train = nan_train[nan_train > 0]  # Filter only columns with NaN values\n",
        "print(nan_train)\n",
        "\n",
        "print(\"\\nChecking for NaN values in testing dataset:\")\n",
        "nan_test = test_df_NN_df.isnull().sum()\n",
        "nan_test = nan_test[nan_test > 0]  # Filter only columns with NaN values\n",
        "print(nan_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P3WkfOngp1Sl",
        "outputId": "0483c691-5f33-4a34-ed43-d65cf87f5742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 0.0026\n",
            "Epoch 2/50, Loss: 0.0020\n",
            "Epoch 3/50, Loss: 0.0019\n",
            "Epoch 4/50, Loss: 0.0017\n",
            "Epoch 5/50, Loss: 0.0016\n",
            "Epoch 6/50, Loss: 0.0016\n",
            "Epoch 7/50, Loss: 0.0015\n",
            "Epoch 8/50, Loss: 0.0015\n",
            "Epoch 9/50, Loss: 0.0014\n",
            "Epoch 10/50, Loss: 0.0014\n",
            "Epoch 11/50, Loss: 0.0015\n",
            "Epoch 12/50, Loss: 0.0014\n",
            "Epoch 13/50, Loss: 0.0014\n",
            "Epoch 14/50, Loss: 0.0013\n",
            "Epoch 15/50, Loss: 0.0013\n",
            "Epoch 16/50, Loss: 0.0013\n",
            "Epoch 17/50, Loss: 0.0012\n",
            "Epoch 18/50, Loss: 0.0012\n",
            "Epoch 19/50, Loss: 0.0013\n",
            "Epoch 20/50, Loss: 0.0013\n",
            "Epoch 21/50, Loss: 0.0012\n",
            "Epoch 22/50, Loss: 0.0012\n",
            "Epoch 23/50, Loss: 0.0012\n",
            "Epoch 24/50, Loss: 0.0012\n",
            "Epoch 25/50, Loss: 0.0011\n",
            "Epoch 26/50, Loss: 0.0011\n",
            "Epoch 27/50, Loss: 0.0011\n",
            "Epoch 28/50, Loss: 0.0013\n",
            "Epoch 29/50, Loss: 0.0013\n",
            "Epoch 30/50, Loss: 0.0012\n",
            "Epoch 31/50, Loss: 0.0012\n",
            "Epoch 32/50, Loss: 0.0011\n",
            "Epoch 33/50, Loss: 0.0013\n",
            "Epoch 34/50, Loss: 0.0012\n",
            "Epoch 35/50, Loss: 0.0013\n",
            "Epoch 36/50, Loss: 0.0013\n",
            "Epoch 37/50, Loss: 0.0013\n",
            "Epoch 38/50, Loss: 0.0012\n",
            "Epoch 39/50, Loss: 0.0012\n",
            "Epoch 40/50, Loss: 0.0011\n",
            "Epoch 41/50, Loss: 0.0012\n",
            "Epoch 42/50, Loss: 0.0011\n",
            "Epoch 43/50, Loss: 0.0013\n",
            "Epoch 44/50, Loss: 0.0012\n",
            "Epoch 45/50, Loss: 0.0011\n",
            "Epoch 46/50, Loss: 0.0012\n",
            "Epoch 47/50, Loss: 0.0012\n",
            "Epoch 48/50, Loss: 0.0011\n",
            "Epoch 49/50, Loss: 0.0014\n",
            "Epoch 50/50, Loss: 0.0011\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Series' object has no attribute 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-cabfb1b5b7b9>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Calculate AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AUC Score: {auc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'numpy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assume train_tensor and y_train are ready from preprocessing\n",
        "\n",
        "# Train-Validation Split\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    train_tensor.numpy(),  # Convert to NumPy for splitting\n",
        "    y_train,               # Target labels\n",
        "    test_size=0.2,         # 20% validation split\n",
        "    random_state=42        # For reproducibility\n",
        ")\n",
        "\n",
        "# Target for validation\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_split = scaler.fit_transform(X_train_split)\n",
        "X_val_split = scaler.transform(X_val_split)\n",
        "\n",
        "# Convert to tensors\n",
        "train_tensor_split = torch.tensor(X_train_split, dtype=torch.float32)\n",
        "val_tensor = torch.tensor(X_val_split, dtype=torch.float32)\n",
        "\n",
        "# Define the Autoencoder Neural Network\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Initialize the Autoencoder\n",
        "input_dim = train_tensor_split.shape[1]  # Number of features\n",
        "encoding_dim = 32  # Bottleneck size\n",
        "model = Autoencoder(input_dim, encoding_dim)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.MSELoss()  # Reconstruction loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    permutation = torch.randperm(train_tensor_split.size(0))\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0, train_tensor_split.size(0), batch_size):\n",
        "        indices = permutation[i:i + batch_size]\n",
        "        batch_data = train_tensor_split[indices]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_data)\n",
        "        loss = criterion(outputs, batch_data)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_tensor_split):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXc17ml36-P",
        "outputId": "bffa6d32-c29e-42c9-bacf-4a67b6e39f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Fold 1\n",
            "Fold 1, Epoch 1/50, Loss: 0.0027\n",
            "Fold 1, Epoch 2/50, Loss: 0.0020\n",
            "Fold 1, Epoch 3/50, Loss: 0.0018\n",
            "Fold 1, Epoch 4/50, Loss: 0.0016\n",
            "Fold 1, Epoch 5/50, Loss: 0.0017\n",
            "Fold 1, Epoch 6/50, Loss: 0.0015\n",
            "Fold 1, Epoch 7/50, Loss: 0.0014\n",
            "Fold 1, Epoch 8/50, Loss: 0.0014\n",
            "Fold 1, Epoch 9/50, Loss: 0.0014\n",
            "Fold 1, Epoch 10/50, Loss: 0.0013\n",
            "Fold 1, Epoch 11/50, Loss: 0.0012\n",
            "Fold 1, Epoch 12/50, Loss: 0.0014\n",
            "Fold 1, Epoch 13/50, Loss: 0.0013\n",
            "Fold 1, Epoch 14/50, Loss: 0.0014\n",
            "Fold 1, Epoch 15/50, Loss: 0.0013\n",
            "Fold 1, Epoch 16/50, Loss: 0.0012\n",
            "Fold 1, Epoch 17/50, Loss: 0.0013\n",
            "Fold 1, Epoch 18/50, Loss: 0.0012\n",
            "Fold 1, Epoch 19/50, Loss: 0.0012\n",
            "Fold 1, Epoch 20/50, Loss: 0.0012\n",
            "Fold 1, Epoch 21/50, Loss: 0.0012\n",
            "Fold 1, Epoch 22/50, Loss: 0.0011\n",
            "Fold 1, Epoch 23/50, Loss: 0.0011\n",
            "Fold 1, Epoch 24/50, Loss: 0.0011\n",
            "Fold 1, Epoch 25/50, Loss: 0.0012\n",
            "Fold 1, Epoch 26/50, Loss: 0.0011\n",
            "Fold 1, Epoch 27/50, Loss: 0.0011\n",
            "Fold 1, Epoch 28/50, Loss: 0.0015\n",
            "Fold 1, Epoch 29/50, Loss: 0.0012\n",
            "Fold 1, Epoch 30/50, Loss: 0.0011\n",
            "Fold 1, Epoch 31/50, Loss: 0.0011\n",
            "Fold 1, Epoch 32/50, Loss: 0.0010\n",
            "Fold 1, Epoch 33/50, Loss: 0.0011\n",
            "Fold 1, Epoch 34/50, Loss: 0.0012\n",
            "Fold 1, Epoch 35/50, Loss: 0.0011\n",
            "Fold 1, Epoch 36/50, Loss: 0.0012\n",
            "Fold 1, Epoch 37/50, Loss: 0.0010\n",
            "Fold 1, Epoch 38/50, Loss: 0.0011\n",
            "Fold 1, Epoch 39/50, Loss: 0.0010\n",
            "Fold 1, Epoch 40/50, Loss: 0.0013\n",
            "Fold 1, Epoch 41/50, Loss: 0.0010\n",
            "Fold 1, Epoch 42/50, Loss: 0.0011\n",
            "Fold 1, Epoch 43/50, Loss: 0.0010\n",
            "Fold 1, Epoch 44/50, Loss: 0.0011\n",
            "Fold 1, Epoch 45/50, Loss: 0.0011\n",
            "Fold 1, Epoch 46/50, Loss: 0.0010\n",
            "Fold 1, Epoch 47/50, Loss: 0.0010\n",
            "Fold 1, Epoch 48/50, Loss: 0.0013\n",
            "Fold 1, Epoch 49/50, Loss: 0.0011\n",
            "Fold 1, Epoch 50/50, Loss: 0.0010\n",
            "Fold 1, AUC Score: 0.7674\n",
            "Starting Fold 2\n",
            "Fold 2, Epoch 1/50, Loss: 0.0026\n",
            "Fold 2, Epoch 2/50, Loss: 0.0018\n",
            "Fold 2, Epoch 3/50, Loss: 0.0018\n",
            "Fold 2, Epoch 4/50, Loss: 0.0016\n",
            "Fold 2, Epoch 5/50, Loss: 0.0015\n",
            "Fold 2, Epoch 6/50, Loss: 0.0015\n",
            "Fold 2, Epoch 7/50, Loss: 0.0014\n",
            "Fold 2, Epoch 8/50, Loss: 0.0014\n",
            "Fold 2, Epoch 9/50, Loss: 0.0013\n",
            "Fold 2, Epoch 10/50, Loss: 0.0012\n",
            "Fold 2, Epoch 11/50, Loss: 0.0013\n",
            "Fold 2, Epoch 12/50, Loss: 0.0013\n",
            "Fold 2, Epoch 13/50, Loss: 0.0015\n",
            "Fold 2, Epoch 14/50, Loss: 0.0013\n",
            "Fold 2, Epoch 15/50, Loss: 0.0013\n",
            "Fold 2, Epoch 16/50, Loss: 0.0013\n",
            "Fold 2, Epoch 17/50, Loss: 0.0013\n",
            "Fold 2, Epoch 18/50, Loss: 0.0012\n",
            "Fold 2, Epoch 19/50, Loss: 0.0012\n",
            "Fold 2, Epoch 20/50, Loss: 0.0012\n",
            "Fold 2, Epoch 21/50, Loss: 0.0012\n",
            "Fold 2, Epoch 22/50, Loss: 0.0011\n",
            "Fold 2, Epoch 23/50, Loss: 0.0012\n",
            "Fold 2, Epoch 24/50, Loss: 0.0011\n",
            "Fold 2, Epoch 25/50, Loss: 0.0011\n",
            "Fold 2, Epoch 26/50, Loss: 0.0012\n",
            "Fold 2, Epoch 27/50, Loss: 0.0012\n",
            "Fold 2, Epoch 28/50, Loss: 0.0011\n",
            "Fold 2, Epoch 29/50, Loss: 0.0012\n",
            "Fold 2, Epoch 30/50, Loss: 0.0012\n",
            "Fold 2, Epoch 31/50, Loss: 0.0012\n",
            "Fold 2, Epoch 32/50, Loss: 0.0012\n",
            "Fold 2, Epoch 33/50, Loss: 0.0011\n",
            "Fold 2, Epoch 34/50, Loss: 0.0013\n",
            "Fold 2, Epoch 35/50, Loss: 0.0011\n",
            "Fold 2, Epoch 36/50, Loss: 0.0012\n",
            "Fold 2, Epoch 37/50, Loss: 0.0011\n",
            "Fold 2, Epoch 38/50, Loss: 0.0011\n",
            "Fold 2, Epoch 39/50, Loss: 0.0011\n",
            "Fold 2, Epoch 40/50, Loss: 0.0012\n",
            "Fold 2, Epoch 41/50, Loss: 0.0012\n",
            "Fold 2, Epoch 42/50, Loss: 0.0011\n",
            "Fold 2, Epoch 43/50, Loss: 0.0010\n",
            "Fold 2, Epoch 44/50, Loss: 0.0011\n",
            "Fold 2, Epoch 45/50, Loss: 0.0011\n",
            "Fold 2, Epoch 46/50, Loss: 0.0011\n",
            "Fold 2, Epoch 47/50, Loss: 0.0011\n",
            "Fold 2, Epoch 48/50, Loss: 0.0011\n",
            "Fold 2, Epoch 49/50, Loss: 0.0010\n",
            "Fold 2, Epoch 50/50, Loss: 0.0012\n",
            "Fold 2, AUC Score: 0.7669\n",
            "Starting Fold 3\n",
            "Fold 3, Epoch 1/50, Loss: 0.0025\n",
            "Fold 3, Epoch 2/50, Loss: 0.0017\n",
            "Fold 3, Epoch 3/50, Loss: 0.0016\n",
            "Fold 3, Epoch 4/50, Loss: 0.0014\n",
            "Fold 3, Epoch 5/50, Loss: 0.0014\n",
            "Fold 3, Epoch 6/50, Loss: 0.0013\n",
            "Fold 3, Epoch 7/50, Loss: 0.0013\n",
            "Fold 3, Epoch 8/50, Loss: 0.0013\n",
            "Fold 3, Epoch 9/50, Loss: 0.0013\n",
            "Fold 3, Epoch 10/50, Loss: 0.0012\n",
            "Fold 3, Epoch 11/50, Loss: 0.0012\n",
            "Fold 3, Epoch 12/50, Loss: 0.0011\n",
            "Fold 3, Epoch 13/50, Loss: 0.0012\n",
            "Fold 3, Epoch 14/50, Loss: 0.0011\n",
            "Fold 3, Epoch 15/50, Loss: 0.0011\n",
            "Fold 3, Epoch 16/50, Loss: 0.0011\n",
            "Fold 3, Epoch 17/50, Loss: 0.0011\n",
            "Fold 3, Epoch 18/50, Loss: 0.0011\n",
            "Fold 3, Epoch 19/50, Loss: 0.0011\n",
            "Fold 3, Epoch 20/50, Loss: 0.0011\n",
            "Fold 3, Epoch 21/50, Loss: 0.0011\n",
            "Fold 3, Epoch 22/50, Loss: 0.0011\n",
            "Fold 3, Epoch 23/50, Loss: 0.0010\n",
            "Fold 3, Epoch 24/50, Loss: 0.0010\n",
            "Fold 3, Epoch 25/50, Loss: 0.0010\n",
            "Fold 3, Epoch 26/50, Loss: 0.0010\n",
            "Fold 3, Epoch 27/50, Loss: 0.0011\n",
            "Fold 3, Epoch 28/50, Loss: 0.0010\n",
            "Fold 3, Epoch 29/50, Loss: 0.0010\n",
            "Fold 3, Epoch 30/50, Loss: 0.0010\n",
            "Fold 3, Epoch 31/50, Loss: 0.0010\n",
            "Fold 3, Epoch 32/50, Loss: 0.0010\n",
            "Fold 3, Epoch 33/50, Loss: 0.0010\n",
            "Fold 3, Epoch 34/50, Loss: 0.0010\n",
            "Fold 3, Epoch 35/50, Loss: 0.0010\n",
            "Fold 3, Epoch 36/50, Loss: 0.0010\n",
            "Fold 3, Epoch 37/50, Loss: 0.0010\n",
            "Fold 3, Epoch 38/50, Loss: 0.0010\n",
            "Fold 3, Epoch 39/50, Loss: 0.0010\n",
            "Fold 3, Epoch 40/50, Loss: 0.0010\n",
            "Fold 3, Epoch 41/50, Loss: 0.0010\n",
            "Fold 3, Epoch 42/50, Loss: 0.0010\n",
            "Fold 3, Epoch 43/50, Loss: 0.0010\n",
            "Fold 3, Epoch 44/50, Loss: 0.0010\n",
            "Fold 3, Epoch 45/50, Loss: 0.0009\n",
            "Fold 3, Epoch 46/50, Loss: 0.0010\n",
            "Fold 3, Epoch 47/50, Loss: 0.0010\n",
            "Fold 3, Epoch 48/50, Loss: 0.0010\n",
            "Fold 3, Epoch 49/50, Loss: 0.0010\n",
            "Fold 3, Epoch 50/50, Loss: 0.0009\n",
            "Fold 3, AUC Score: 0.7676\n",
            "Starting Fold 4\n",
            "Fold 4, Epoch 1/50, Loss: 0.0026\n",
            "Fold 4, Epoch 2/50, Loss: 0.0020\n",
            "Fold 4, Epoch 3/50, Loss: 0.0018\n",
            "Fold 4, Epoch 4/50, Loss: 0.0016\n",
            "Fold 4, Epoch 5/50, Loss: 0.0015\n",
            "Fold 4, Epoch 6/50, Loss: 0.0015\n",
            "Fold 4, Epoch 7/50, Loss: 0.0014\n",
            "Fold 4, Epoch 8/50, Loss: 0.0014\n",
            "Fold 4, Epoch 9/50, Loss: 0.0013\n",
            "Fold 4, Epoch 10/50, Loss: 0.0013\n",
            "Fold 4, Epoch 11/50, Loss: 0.0013\n",
            "Fold 4, Epoch 12/50, Loss: 0.0012\n",
            "Fold 4, Epoch 13/50, Loss: 0.0012\n",
            "Fold 4, Epoch 14/50, Loss: 0.0012\n",
            "Fold 4, Epoch 15/50, Loss: 0.0012\n",
            "Fold 4, Epoch 16/50, Loss: 0.0012\n",
            "Fold 4, Epoch 17/50, Loss: 0.0012\n",
            "Fold 4, Epoch 18/50, Loss: 0.0012\n",
            "Fold 4, Epoch 19/50, Loss: 0.0012\n",
            "Fold 4, Epoch 20/50, Loss: 0.0011\n",
            "Fold 4, Epoch 21/50, Loss: 0.0011\n",
            "Fold 4, Epoch 22/50, Loss: 0.0011\n",
            "Fold 4, Epoch 23/50, Loss: 0.0012\n",
            "Fold 4, Epoch 24/50, Loss: 0.0011\n",
            "Fold 4, Epoch 25/50, Loss: 0.0012\n",
            "Fold 4, Epoch 26/50, Loss: 0.0011\n",
            "Fold 4, Epoch 27/50, Loss: 0.0011\n",
            "Fold 4, Epoch 28/50, Loss: 0.0010\n",
            "Fold 4, Epoch 29/50, Loss: 0.0011\n",
            "Fold 4, Epoch 30/50, Loss: 0.0011\n",
            "Fold 4, Epoch 31/50, Loss: 0.0011\n",
            "Fold 4, Epoch 32/50, Loss: 0.0011\n",
            "Fold 4, Epoch 33/50, Loss: 0.0011\n",
            "Fold 4, Epoch 34/50, Loss: 0.0010\n",
            "Fold 4, Epoch 35/50, Loss: 0.0011\n",
            "Fold 4, Epoch 36/50, Loss: 0.0011\n",
            "Fold 4, Epoch 37/50, Loss: 0.0011\n",
            "Fold 4, Epoch 38/50, Loss: 0.0011\n",
            "Fold 4, Epoch 39/50, Loss: 0.0011\n",
            "Fold 4, Epoch 40/50, Loss: 0.0011\n",
            "Fold 4, Epoch 41/50, Loss: 0.0010\n",
            "Fold 4, Epoch 42/50, Loss: 0.0011\n",
            "Fold 4, Epoch 43/50, Loss: 0.0011\n",
            "Fold 4, Epoch 44/50, Loss: 0.0011\n",
            "Fold 4, Epoch 45/50, Loss: 0.0011\n",
            "Fold 4, Epoch 46/50, Loss: 0.0011\n",
            "Fold 4, Epoch 47/50, Loss: 0.0010\n",
            "Fold 4, Epoch 48/50, Loss: 0.0011\n",
            "Fold 4, Epoch 49/50, Loss: 0.0011\n",
            "Fold 4, Epoch 50/50, Loss: 0.0010\n",
            "Fold 4, AUC Score: 0.7672\n",
            "Starting Fold 5\n",
            "Fold 5, Epoch 1/50, Loss: 0.0026\n",
            "Fold 5, Epoch 2/50, Loss: 0.0019\n",
            "Fold 5, Epoch 3/50, Loss: 0.0018\n",
            "Fold 5, Epoch 4/50, Loss: 0.0016\n",
            "Fold 5, Epoch 5/50, Loss: 0.0015\n",
            "Fold 5, Epoch 6/50, Loss: 0.0015\n",
            "Fold 5, Epoch 7/50, Loss: 0.0014\n",
            "Fold 5, Epoch 8/50, Loss: 0.0013\n",
            "Fold 5, Epoch 9/50, Loss: 0.0015\n",
            "Fold 5, Epoch 10/50, Loss: 0.0013\n",
            "Fold 5, Epoch 11/50, Loss: 0.0013\n",
            "Fold 5, Epoch 12/50, Loss: 0.0013\n",
            "Fold 5, Epoch 13/50, Loss: 0.0013\n",
            "Fold 5, Epoch 14/50, Loss: 0.0013\n",
            "Fold 5, Epoch 15/50, Loss: 0.0012\n",
            "Fold 5, Epoch 16/50, Loss: 0.0012\n",
            "Fold 5, Epoch 17/50, Loss: 0.0012\n",
            "Fold 5, Epoch 18/50, Loss: 0.0012\n",
            "Fold 5, Epoch 19/50, Loss: 0.0013\n",
            "Fold 5, Epoch 20/50, Loss: 0.0012\n",
            "Fold 5, Epoch 21/50, Loss: 0.0012\n",
            "Fold 5, Epoch 22/50, Loss: 0.0012\n",
            "Fold 5, Epoch 23/50, Loss: 0.0011\n",
            "Fold 5, Epoch 24/50, Loss: 0.0012\n",
            "Fold 5, Epoch 25/50, Loss: 0.0012\n",
            "Fold 5, Epoch 26/50, Loss: 0.0011\n",
            "Fold 5, Epoch 27/50, Loss: 0.0013\n",
            "Fold 5, Epoch 28/50, Loss: 0.0011\n",
            "Fold 5, Epoch 29/50, Loss: 0.0011\n",
            "Fold 5, Epoch 30/50, Loss: 0.0011\n",
            "Fold 5, Epoch 31/50, Loss: 0.0011\n",
            "Fold 5, Epoch 32/50, Loss: 0.0012\n",
            "Fold 5, Epoch 33/50, Loss: 0.0011\n",
            "Fold 5, Epoch 34/50, Loss: 0.0013\n",
            "Fold 5, Epoch 35/50, Loss: 0.0012\n",
            "Fold 5, Epoch 36/50, Loss: 0.0011\n",
            "Fold 5, Epoch 37/50, Loss: 0.0011\n",
            "Fold 5, Epoch 38/50, Loss: 0.0011\n",
            "Fold 5, Epoch 39/50, Loss: 0.0012\n",
            "Fold 5, Epoch 40/50, Loss: 0.0013\n",
            "Fold 5, Epoch 41/50, Loss: 0.0013\n",
            "Fold 5, Epoch 42/50, Loss: 0.0012\n",
            "Fold 5, Epoch 43/50, Loss: 0.0013\n",
            "Fold 5, Epoch 44/50, Loss: 0.0012\n",
            "Fold 5, Epoch 45/50, Loss: 0.0013\n",
            "Fold 5, Epoch 46/50, Loss: 0.0011\n",
            "Fold 5, Epoch 47/50, Loss: 0.0011\n",
            "Fold 5, Epoch 48/50, Loss: 0.0013\n",
            "Fold 5, Epoch 49/50, Loss: 0.0011\n",
            "Fold 5, Epoch 50/50, Loss: 0.0011\n",
            "Fold 5, AUC Score: 0.7646\n",
            "Final Cross-Validation AUC: 0.7667\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the Autoencoder Neural Network\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Assume train_tensor and y_train are ready from preprocessing\n",
        "\n",
        "# Scale the entire dataset first\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(train_tensor.numpy())\n",
        "y = y_train.values  # Assuming y_train is a pandas Series\n",
        "\n",
        "# Cross-validation setup\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "encoding_dim = 32  # Bottleneck size\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "auc_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled, y)):\n",
        "    print(f\"Starting Fold {fold + 1}\")\n",
        "    X_train_split, X_val_split = X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train_split, y_val_split = y[train_idx], y[val_idx]\n",
        "\n",
        "    # Convert to tensors\n",
        "    train_tensor_split = torch.tensor(X_train_split, dtype=torch.float32)\n",
        "    val_tensor = torch.tensor(X_val_split, dtype=torch.float32)\n",
        "\n",
        "    # Initialize the Autoencoder\n",
        "    input_dim = train_tensor_split.shape[1]\n",
        "    model = Autoencoder(input_dim, encoding_dim)\n",
        "\n",
        "    # Define Loss Function and Optimizer\n",
        "    criterion = nn.MSELoss()  # Reconstruction loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        permutation = torch.randperm(train_tensor_split.size(0))\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for i in range(0, train_tensor_split.size(0), batch_size):\n",
        "            indices = permutation[i:i + batch_size]\n",
        "            batch_data = train_tensor_split[indices]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_data)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_tensor_split):.4f}\")\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_reconstructed = model(val_tensor)\n",
        "        reconstruction_loss = torch.mean((val_reconstructed - val_tensor) ** 2, dim=1).numpy()\n",
        "\n",
        "    # Anomaly Detection Threshold\n",
        "    threshold = np.percentile(reconstruction_loss, 95)  # Adjust the percentile as needed\n",
        "    y_pred = (reconstruction_loss > threshold).astype(int)\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(y_val_split, reconstruction_loss)\n",
        "    auc_scores.append(auc)\n",
        "    print(f\"Fold {fold + 1}, AUC Score: {auc:.4f}\")\n",
        "\n",
        "# Final Cross-Validation AUC\n",
        "print(f\"Final Cross-Validation AUC: {np.mean(auc_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S39zMMjFub6D"
      },
      "outputs": [],
      "source": [
        "#assert not torch.isnan(train_tensor_split).any(), \"NaN values found in train_tensor_split\"\n",
        "assert not torch.isinf(train_tensor_split).any(), \"Inf values found in train_tensor_split\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Eay2_frGQd",
        "outputId": "36e8e440-44b2-47fb-d43a-80ef623a124c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file created: autoencoder_submission.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Prediction for Test Set\n",
        "test_tensor = torch.tensor(test_df_NN, dtype=torch.float32)  # Assuming test_tensor is ready\n",
        "with torch.no_grad():\n",
        "    test_reconstructed = model(test_tensor)\n",
        "    test_reconstruction_loss = torch.mean((test_reconstructed - test_tensor) ** 2, dim=1).numpy()\n",
        "\n",
        "# Submission\n",
        "submission = pd.DataFrame({\n",
        "    \"TransactionID\": test_processed[\"TransactionID\"],\n",
        "    \"isFraud\": test_reconstruction_loss  # Use reconstruction loss directly as probabilities\n",
        "})\n",
        "submission.to_csv(\"autoencoder_submission.csv\", index=False)\n",
        "print(\"Submission file created: autoencoder_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgXIbxZ7XPMo",
        "outputId": "25268b06-b467-49c7-98e1-95a4909541f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as 'autoencoder_model.pth'\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"autoencoder_model.pth\")\n",
        "print(\"Model saved as 'autoencoder_model.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCoLmZwGXQ8g"
      },
      "outputs": [],
      "source": []
    }
  ]
}