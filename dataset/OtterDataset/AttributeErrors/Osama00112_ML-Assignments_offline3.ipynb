{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "epsilon = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "train_validation_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N/A', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# check the unique classes of the dataset\n",
    "print(train_validation_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_test_dataset = ds.EMNIST(root='./data',\n",
    "                       split='letters',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 124800\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 20800\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((num_samples, num_classes))\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_labels[i, label - 1] = 1  # Adjust indexing to start from 0\n",
    "    \n",
    "    return one_hot_labels\n",
    "\n",
    "# # Example usage:\n",
    "# labels = np.array([0, 1, 2, 1, 0])\n",
    "# num_classes = 3\n",
    "# one_hot_labels = one_hot_encode(labels, num_classes)\n",
    "\n",
    "# print(one_hot_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(Y_pred, Y, loss_type='binary'):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # clipping to avoid zero log\n",
    "    epsilon = 1e-8\n",
    "    np.clip(Y_pred, epsilon, 1 - epsilon, out=Y_pred)\n",
    "    \n",
    "    if loss_type == 'binary':\n",
    "        loss = -(1/m) * np.sum(Y * np.log(Y_pred) + (1 - Y) * np.log(1-Y_pred))\n",
    "    elif loss_type == 'multiclass':\n",
    "        loss = -(1/m) * np.sum(Y * np.log(Y_pred.T))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid loss_type. Expected 'binary' or 'multiclass'\")\n",
    "    \n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_dZ(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_dZ(dA, Z):\n",
    "    g_z = sigmoid(Z)\n",
    "    dZ = dA * g_z * (1 - g_z)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) \n",
    "\n",
    "def softmax_dZ(dA, Z):\n",
    "    g_z = softmax(Z)\n",
    "    dZ = dA * g_z * (1 - g_z)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_prop(W, b, A_prev, activation=relu):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation(Z)\n",
    "    cache = (W, b, A_prev, Z, A)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_forward_prop(X, parameters, loss_type='binary'):\n",
    "    A_prev = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    if loss_type == 'binary':\n",
    "        activation = sigmoid\n",
    "    elif loss_type == 'multiclass':\n",
    "        activation = softmax\n",
    "    \n",
    "    all_caches = []\n",
    "    for i in range(1, L):\n",
    "        W_i = parameters['W' + str(i)]\n",
    "        b_i = parameters['b' + str(i)]\n",
    "        A_prev, cache = single_layer_forward_prop(W_i, b_i, A_prev)\n",
    "        \n",
    "        all_caches.append(cache)\n",
    "        \n",
    "    W_Last = parameters['W' + str(L)]\n",
    "    b_Last = parameters['b' + str(L)]\n",
    "    A_Last, cache = single_layer_forward_prop(W_Last, b_Last, A_prev, activation=activation)\n",
    "    all_caches.append(cache)\n",
    "    \n",
    "    return A_Last, all_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_prop(dA, cache, activation_dZ=relu_dZ):\n",
    "    W, b, A_prev, Z, A = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dZ = activation_dZ(dA, Z)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_last_layer_dAL(Y, AL, loss_type='binary'):\n",
    "    if loss_type == 'binary':\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    elif loss_type == 'multiclass':\n",
    "        dAL = AL - Y\n",
    "    else:\n",
    "        raise ValueError(\"Invalid loss_type\")\n",
    "    \n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_backward_prop(A_last, Y, all_caches, loss_type='binary'):\n",
    "    L = len(all_caches)\n",
    "    \n",
    "    # needs to be changed for multiclass classification\n",
    "    dAL_last_layer = compute_last_layer_dAL(Y, A_last, loss_type=loss_type)\n",
    "    cache_last_layer = all_caches[L-1]\n",
    "    last_activation_dZ = sigmoid_dZ\n",
    "    \n",
    "    if loss_type == 'multiclass':\n",
    "        last_activation_dZ = softmax_dZ\n",
    "    \n",
    "    gradients = {}\n",
    "    dA_prev, dW, db = single_layer_backward_prop(dAL_last_layer, cache_last_layer, activation_dZ=last_activation_dZ)\n",
    "    gradients['dA' + str(L-1)] = dA_prev\n",
    "    gradients['dW' + str(L)] = dW\n",
    "    gradients['db' + str(L)] = db\n",
    "    \n",
    "    for i in reversed(range(L-1)):\n",
    "        dA_prev, dW, db = single_layer_backward_prop(gradients['dA' + str(i+1)], all_caches[i])\n",
    "        gradients['dA' + str(i)] = dA_prev\n",
    "        gradients['dW' + str(i+1)] = dW\n",
    "        gradients['db' + str(i+1)] = db\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, gradients, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(L):\n",
    "        # retrieve gradients\n",
    "        dW_i = gradients['dW' + str(i+1)]\n",
    "        db_i = gradients['db' + str(i+1)]\n",
    "        \n",
    "        # update parameters\n",
    "        parameters['W' + str(i+1)] -= learning_rate * dW_i\n",
    "        parameters['b' + str(i+1)] -= learning_rate * db_i\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialize_parameters(layer_nodes_list):\n",
    "    parameters = {}\n",
    "    L = len(layer_nodes_list)\n",
    "    \n",
    "    for layer_i in range(1, L):\n",
    "        parameters['W' + str(layer_i)] = np.random.randn(layer_nodes_list[layer_i], layer_nodes_list[layer_i-1]) \n",
    "        parameters['b' + str(layer_i)] = np.zeros((layer_nodes_list[layer_i], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_FNN_model(layer_nodes_list, X, Y, learning_rate=0.01, epochs=3000, loss_type='binary', print_cost=False):\n",
    "    parameters = random_initialize_parameters(layer_nodes_list)\n",
    "    costs = []\n",
    "    L = len(layer_nodes_list)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        A_last, all_caches = deep_forward_prop(X, parameters)\n",
    "        cost = loss_func(A_last, Y, loss_type=loss_type)\n",
    "        # calculate accuracy\n",
    "        \n",
    "        gradients = deep_backward_prop(A_last, Y, all_caches, loss_type=loss_type)\n",
    "        \n",
    "        parameters = gradient_descent(parameters, gradients, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            #print(\"final output: {}\".format(A_last))\n",
    "    \n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 9.400085515775217\n",
      "Cost after iteration 100: 6.098798661713403\n",
      "Cost after iteration 200: 5.648562623837844\n",
      "Cost after iteration 300: 5.271846442406679\n",
      "Cost after iteration 400: 4.954005455085699\n",
      "Cost after iteration 500: 4.683263107603634\n",
      "Cost after iteration 600: 4.450363284222019\n",
      "Cost after iteration 700: 4.248083113282717\n",
      "Cost after iteration 800: 4.070789772508327\n",
      "Cost after iteration 900: 3.914074493403994\n",
      "Cost after iteration 1000: 3.774465023178693\n",
      "Cost after iteration 1100: 3.649205525236702\n",
      "Cost after iteration 1200: 3.5360901826750304\n",
      "Cost after iteration 1300: 3.4333379383546347\n",
      "Cost after iteration 1400: 3.339498131718262\n",
      "Cost after iteration 1500: 3.2533791373573795\n",
      "Cost after iteration 1600: 3.1739940877213986\n",
      "Cost after iteration 1700: 3.100519305913565\n",
      "Cost after iteration 1800: 3.0322622337774385\n",
      "Cost after iteration 1900: 2.9686364940746\n",
      "Cost after iteration 2000: 2.909142348227082\n",
      "Cost after iteration 2100: 2.8533512638811214\n",
      "Cost after iteration 2200: 2.800893636014091\n",
      "Cost after iteration 2300: 2.751448945783853\n",
      "Cost after iteration 2400: 2.7047378176618584\n",
      "Cost after iteration 2500: 2.6605155654321235\n",
      "Cost after iteration 2600: 2.6185669141325043\n",
      "Cost after iteration 2700: 2.5787016570870778\n",
      "Cost after iteration 2800: 2.5407510613720854\n",
      "Cost after iteration 2900: 2.504564876083707\n",
      "Cost after iteration 3000: 2.470008829041915\n",
      "Cost after iteration 3100: 2.436962521550207\n",
      "Cost after iteration 3200: 2.405317649350284\n",
      "Cost after iteration 3300: 2.3749764922995706\n",
      "Cost after iteration 3400: 2.3458506265472603\n",
      "Cost after iteration 3500: 2.3178598218290714\n",
      "Cost after iteration 3600: 2.290931093495248\n",
      "Cost after iteration 3700: 2.264997884448202\n",
      "Cost after iteration 3800: 2.239999356612215\n",
      "Cost after iteration 3900: 2.2158797751301904\n",
      "Cost after iteration 4000: 2.1925879713669962\n",
      "Cost after iteration 4100: 2.170076873139331\n",
      "Cost after iteration 4200: 2.148303092499369\n",
      "Cost after iteration 4300: 2.1272265629609\n",
      "Cost after iteration 4400: 2.1068102193402343\n",
      "Cost after iteration 4500: 2.0870197144437266\n",
      "Cost after iteration 4600: 2.067823167711778\n",
      "Cost after iteration 4700: 2.0491909416595906\n",
      "Cost after iteration 4800: 2.0310954425647445\n",
      "Cost after iteration 4900: 2.0135109423625726\n",
      "Cost after iteration 5000: 1.9964134191398575\n",
      "Cost after iteration 5100: 1.979780413979664\n",
      "Cost after iteration 5200: 1.9635909022166513\n",
      "Cost after iteration 5300: 1.9478251774224304\n",
      "Cost after iteration 5400: 1.9324647466620082\n",
      "Cost after iteration 5500: 1.9174922357514306\n",
      "Cost after iteration 5600: 1.9028913034086452\n",
      "Cost after iteration 5700: 1.8886465633285257\n",
      "Cost after iteration 5800: 1.874743513332581\n",
      "Cost after iteration 5900: 1.8611684708470868\n",
      "Cost after iteration 6000: 1.8479085140525418\n",
      "Cost after iteration 6100: 1.8349514281247776\n",
      "Cost after iteration 6200: 1.8222856560552227\n",
      "Cost after iteration 6300: 1.8099002535963846\n",
      "Cost after iteration 6400: 1.797784847929691\n",
      "Cost after iteration 6500: 1.7859295996975135\n",
      "Cost after iteration 6600: 1.774325168080347\n",
      "Cost after iteration 6700: 1.7629626786345207\n",
      "Cost after iteration 6800: 1.7518336936360204\n",
      "Cost after iteration 6900: 1.740930184702712\n",
      "Cost after iteration 7000: 1.730244507490756\n",
      "Cost after iteration 7100: 1.719769378281853\n",
      "Cost after iteration 7200: 1.7094978522963917\n",
      "Cost after iteration 7300: 1.6994233035839725\n",
      "Cost after iteration 7400: 1.689539406357302\n",
      "Cost after iteration 7500: 1.6798401176484572\n",
      "Cost after iteration 7600: 1.6703196611780609\n",
      "Cost after iteration 7700: 1.6609725123382166\n",
      "Cost after iteration 7800: 1.6517933841993384\n",
      "Cost after iteration 7900: 1.6427772144592243\n",
      "Cost after iteration 8000: 1.6339191532601907\n",
      "Cost after iteration 8100: 1.6252145518067196\n",
      "Cost after iteration 8200: 1.6166589517220928\n",
      "Cost after iteration 8300: 1.6082480750878485\n",
      "Cost after iteration 8400: 1.5999778151148096\n",
      "Cost after iteration 8500: 1.5918442273987938\n",
      "Cost after iteration 8600: 1.5838435217181024\n",
      "Cost after iteration 8700: 1.5759720543334805\n",
      "Cost after iteration 8800: 1.568226320754496\n",
      "Cost after iteration 8900: 1.5606029489392448\n",
      "Cost after iteration 9000: 1.5530986928969606\n",
      "Cost after iteration 9100: 1.545710426665571\n",
      "Cost after iteration 9200: 1.538435138638432\n",
      "Cost after iteration 9300: 1.5312699262165173\n",
      "Cost after iteration 9400: 1.5242119907641785\n",
      "Cost after iteration 9500: 1.5172586328482627\n",
      "Cost after iteration 9600: 1.5104072477419295\n",
      "Cost after iteration 9700: 1.5036553211759107\n",
      "Cost after iteration 9800: 1.4970004253212288\n",
      "Cost after iteration 9900: 1.4904402149885945\n",
      "[[3.00189402e-04 3.84593041e-03 3.16937287e-02 1.58181129e-01\n",
      "  4.31416246e-01 6.95452811e-01 8.44608548e-01 9.15073300e-01\n",
      "  9.48847677e-01 9.66275067e-01]\n",
      " [3.28630669e-04 3.99643943e-03 3.25297281e-02 1.60280765e-01\n",
      "  4.32626094e-01 6.94627480e-01 8.43280948e-01 9.14013829e-01\n",
      "  9.48134240e-01 9.65827675e-01]\n",
      " [3.46346523e-04 4.11530355e-03 3.25700895e-02 1.58871880e-01\n",
      "  4.30152844e-01 6.94016906e-01 8.44189455e-01 9.15300046e-01\n",
      "  9.49319057e-01 9.66799296e-01]\n",
      " [3.99547120e-04 4.23021631e-03 3.26313740e-02 1.57544235e-01\n",
      "  4.27014165e-01 6.92324196e-01 8.44634147e-01 9.16844726e-01\n",
      "  9.51262695e-01 9.68843406e-01]\n",
      " [3.19104569e-04 3.95024044e-03 3.22959801e-02 1.59640164e-01\n",
      "  4.31995977e-01 6.94551598e-01 8.43577185e-01 9.14443559e-01\n",
      "  9.48597768e-01 9.66293546e-01]\n",
      " [2.87773346e-04 3.64154769e-03 3.07967134e-02 1.56654154e-01\n",
      "  4.31264746e-01 6.96533121e-01 8.45559153e-01 9.15607906e-01\n",
      "  9.49074469e-01 9.66309184e-01]\n",
      " [3.94402937e-04 4.35122191e-03 3.32981515e-02 1.59492566e-01\n",
      "  4.29255139e-01 6.92835575e-01 8.43730348e-01 9.15382072e-01\n",
      "  9.49665705e-01 9.67262862e-01]\n",
      " [3.58371400e-04 4.13690147e-03 3.24964385e-02 1.58113373e-01\n",
      "  4.28746141e-01 6.93405078e-01 8.44508413e-01 9.16047820e-01\n",
      "  9.50198678e-01 9.67695174e-01]\n",
      " [2.59585059e-04 3.67930727e-03 3.16867092e-02 1.60372597e-01\n",
      "  4.35122602e-01 6.96344230e-01 8.43258198e-01 9.13143912e-01\n",
      "  9.47032203e-01 9.64740342e-01]\n",
      " [3.56700380e-04 4.22313073e-03 3.34156880e-02 1.61583319e-01\n",
      "  4.32502492e-01 6.93561780e-01 8.42494075e-01 9.13637472e-01\n",
      "  9.48008921e-01 9.65831733e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 0],\n",
       "       [0, 5]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(A_last, Y):\n",
    "   m = Y.shape[1]\n",
    "   return np.sum(A_last == Y) / m\n",
    "\n",
    "\n",
    "\n",
    "# #checking if the model is working with simple data and multiclass classfication\n",
    "# X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "# Y = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "\n",
    "\n",
    "# parameters, costs = L_layer_FNN_model([1, 10, 10], X, Y, learning_rate=0.01, epochs=1000, print_cost=True)\n",
    "\n",
    "# #test on example test data\n",
    "# X_test = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "# Y_test = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "\n",
    "# A_last, _ = deep_forward_prop(X_test, parameters)\n",
    "# print(A_last)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(costs)\n",
    "# plt.show()\n",
    "\n",
    "#checking if the model is working with simple data and binary classfication\n",
    "X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "Y = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "parameters, costs = L_layer_FNN_model([1, 10, 10], X, Y, learning_rate=0.1, epochs=10000, print_cost=True, loss_type='binary')\n",
    "\n",
    "\n",
    "#test on example test data\n",
    "X_test = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "Y_test = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "A_last, _ = deep_forward_prop(X_test, parameters)\n",
    "print(A_last)\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = np.round(A_last)\n",
    "y_pred = y_pred.astype(int)\n",
    "y_test = Y_test.astype(int)\n",
    "confusion_matrix(y_test[0], y_pred[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have the functions defined in the previous code\n",
    "\n",
    "# Function to convert PyTorch dataset to NumPy arrays\n",
    "def convert_to_numpy(dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for data in dataset:\n",
    "        image, label = data\n",
    "        features.append(image.numpy().flatten())\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(features).T, np.array(labels).reshape(1, -1)\n",
    "\n",
    "# Load EMNIST datasets\n",
    "train_features, train_labels = convert_to_numpy(train_validation_dataset)\n",
    "test_features, test_labels = convert_to_numpy(independent_test_dataset)\n",
    "\n",
    "# Normalize features\n",
    "train_features = train_features / 255.0\n",
    "test_features = test_features / 255.0\n",
    "\n",
    "# One-hot encode labels for multiclass classification\n",
    "num_classes = len(np.unique(train_labels))\n",
    "train_labels_one_hot = np.eye(num_classes)[train_labels.flatten() - 1].T  # Corrected indexing\n",
    "\n",
    "# Set hyperparameters\n",
    "layer_nodes_list = [train_features.shape[0], 128, 64, num_classes]\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Train the model\n",
    "parameters, costs = L_layer_FNN_model(layer_nodes_list, train_features, train_labels_one_hot, learning_rate, epochs, loss_type='multiclass', print_cost=True)\n",
    "\n",
    "# Test the model\n",
    "def predict(parameters, X):\n",
    "    A_last, _ = deep_forward_prop(X, parameters, loss_type='multiclass')\n",
    "    predictions = np.argmax(A_last, axis=0)\n",
    "    return predictions.reshape(1, -1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = predict(parameters, test_features)\n",
    "accuracy = np.mean(test_predictions == test_labels)\n",
    "print(\"Test Accuracy: {:.2%}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_validation_dataset = ds.EMNIST(root='./data', split='letters',\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=False)\n",
    "\n",
    "independent_test_dataset = ds.EMNIST(root='./data',\n",
    "                                     split='letters',\n",
    "                                     train=False,\n",
    "                                     transform=transforms.ToTensor())\n",
    "\n",
    "# Convert the datasets to numpy arrays\n",
    "X_train = train_validation_dataset.data.numpy()\n",
    "Y_train = train_validation_dataset.targets.numpy()\n",
    "X_test = independent_test_dataset.data.numpy()\n",
    "Y_test = independent_test_dataset.targets.numpy()\n",
    "\n",
    "num_classes = 26\n",
    "\n",
    "# One-hot encode labels for training and testing\n",
    "Y_train_one_hot = one_hot_encode(Y_train, num_classes)\n",
    "Y_test_one_hot = one_hot_encode(Y_test, num_classes)\n",
    "\n",
    "# Reshape the input data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#print train and test\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mY_train_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m)    \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(Y_train.shape)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(X_test.shape)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(Y_test.shape)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "#print train and test\n",
    "print(Y_train_one_hot.label)    \n",
    "# print(Y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (26,124800) (124800,26) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m layer_nodes_list \u001b[38;5;241m=\u001b[39m [X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m26\u001b[39m]  \u001b[38;5;66;03m# Example architecture\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m parameters, costs \u001b[38;5;241m=\u001b[39m \u001b[43mL_layer_FNN_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_nodes_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulticlass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m A_last, _ \u001b[38;5;241m=\u001b[39m deep_forward_prop(X_test, parameters)\n",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m, in \u001b[0;36mL_layer_FNN_model\u001b[1;34m(layer_nodes_list, X, Y, learning_rate, epochs, loss_type, print_cost)\u001b[0m\n\u001b[0;32m      8\u001b[0m cost \u001b[38;5;241m=\u001b[39m loss_func(A_last, Y, loss_type\u001b[38;5;241m=\u001b[39mloss_type)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mdeep_backward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m parameters \u001b[38;5;241m=\u001b[39m gradient_descent(parameters, gradients, learning_rate)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_cost \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[61], line 5\u001b[0m, in \u001b[0;36mdeep_backward_prop\u001b[1;34m(A_last, Y, all_caches, loss_type)\u001b[0m\n\u001b[0;32m      2\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_caches)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# needs to be changed for multiclass classification\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dAL_last_layer \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_last_layer_dAL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m cache_last_layer \u001b[38;5;241m=\u001b[39m all_caches[L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m last_activation_dZ \u001b[38;5;241m=\u001b[39m sigmoid_dZ\n",
      "Cell \u001b[1;32mIn[35], line 5\u001b[0m, in \u001b[0;36mcompute_last_layer_dAL\u001b[1;34m(Y, AL, loss_type)\u001b[0m\n\u001b[0;32m      3\u001b[0m     dAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m (np\u001b[38;5;241m.\u001b[39mdivide(Y, AL) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Y, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m AL))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m loss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     dAL \u001b[38;5;241m=\u001b[39m \u001b[43mAL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid loss_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (26,124800) (124800,26) "
     ]
    }
   ],
   "source": [
    "# Define the layer nodes list\n",
    "layer_nodes_list = [X_train.shape[0], 128, 64, 26]  # Example architecture\n",
    "\n",
    "# Train the model\n",
    "parameters, costs = L_layer_FNN_model(layer_nodes_list, X_train, Y_train_one_hot, learning_rate=0.01, epochs=3000, loss_type='multiclass', print_cost=True)\n",
    "\n",
    "# Test the model\n",
    "A_last, _ = deep_forward_prop(X_test, parameters)\n",
    "# Perform further evaluation or analysis on the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
