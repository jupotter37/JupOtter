{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b866c09c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_args() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 229\u001b[0m\n\u001b[1;32m    225\u001b[0m             torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 123\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 123\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#     args=Args()\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(args) \n",
      "\u001b[0;31mTypeError\u001b[0m: parse_args() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(train_file='train.pkl', model_name_or_path='/Users/arushisharma/Documents/projects/llama/llama-2-7b/ggml-model-f32_q4_0.gguf', output_dir='out', tensorboard_path='./tensorboard', image_length=10, per_device_train_batch_size=1, lr=0.004, norm_gradient_clip=1.0, beta1=0.98, beta2=0.999, eps=1e-06, weight_decay=0.2, num_train_epochs=10, max_train_steps=None, gradient_accumulation_steps=8, lr_scheduler_type='linear', num_warmup_steps=0, seed=None, local_rank=0, precision='fp16', dist_url='env://', dist_backend='nccl', horovod=False, debug=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1930: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizer' object has no attribute 'sp_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 229\u001b[0m\n\u001b[1;32m    225\u001b[0m             torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 150\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m num_added_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens(special_tokens_dict)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_added_tokens)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2027\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2261\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/projects/Visual_LLaMA/llama/tokenization_llama.py:65\u001b[0m, in \u001b[0;36mLlamaTokenizer.__init__\u001b[0;34m(self, vocab_file, unk_token, bos_token, eos_token, sp_model_kwargs, add_bos_token, add_eos_token, decode_with_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     54\u001b[0m     vocab_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     63\u001b[0m ):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model_kwargs \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m sp_model_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sp_model_kwargs\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils.py:367\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens_extended\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_added_tokens_encoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils.py:467\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._add_tokens\u001b[0;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_tokens\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m current_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    468\u001b[0m new_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_vocab)  \u001b[38;5;66;03m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m new_tokens:\n",
      "File \u001b[0;32m~/Documents/projects/Visual_LLaMA/llama/tokenization_llama.py:98\u001b[0m, in \u001b[0;36mLlamaTokenizer.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns vocab as a dict\"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(i): i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m)}\n\u001b[1;32m     99\u001b[0m     vocab\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vocab\n",
      "File \u001b[0;32m~/Documents/projects/Visual_LLaMA/llama/tokenization_llama.py:86\u001b[0m, in \u001b[0;36mLlamaTokenizer.vocab_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns vocab size\"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp_model\u001b[49m\u001b[38;5;241m.\u001b[39mget_piece_size()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaTokenizer' object has no attribute 'sp_model'"
     ]
    }
   ],
   "source": [
    "%tb\n",
    "\n",
    "import argparse \n",
    "import torch \n",
    "import os \n",
    "from tqdm import tqdm \n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from llama import LlamaTokenizer, LlamaForCausalLM\n",
    "from utils import world_info_from_env, init_distributed_device, ImageTextDataSet, is_master, get_autocast\n",
    "from model import MultimodalLlama \n",
    "\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['[boi]','[eoi]']}\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Finetune a llama model on a causal language modeling task\")\n",
    "    parser.add_argument(\n",
    "        \"--train_file\", type=str, default='train.pkl', help=\"A pkl file containing the training data.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default='/Users/arushisharma/Documents/projects/llama/llama-2-7b/ggml-model-f32_q4_0.gguf',\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\"--output_dir\", type=str, default='out', help=\"Where to store the final model.\")\n",
    "    parser.add_argument(\n",
    "        \"--tensorboard_path\", type=str, default=\"./tensorboard\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_length\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_train_batch_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=4e-3,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--norm_gradient_clip\", type=float, default=1.0, help=\"Gradient clip.\"\n",
    "    )\n",
    "    parser.add_argument(\"--beta1\", type=float, default=0.98, help=\"Adam beta 1.\")\n",
    "    parser.add_argument(\"--beta2\", type=float, default=0.999, help=\"Adam beta 2.\")\n",
    "    parser.add_argument(\"--eps\", type=float, default=1e-6, help=\"Adam epsilon.\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=10, help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler_type\",\n",
    "        default=\"linear\",\n",
    "        help=\"The scheduler type to use.\",\n",
    "        choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "    parser.add_argument(\n",
    "        \"--local_rank\", type=int, default=0, help=\"local rank.\")\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        choices=[\"amp\", \"amp_bfloat16\", \"fp16\", \"fp32\"],\n",
    "        default=\"fp16\",\n",
    "        help=\"Floating point precision.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dist-url\",\n",
    "        default=\"env://\",\n",
    "        type=str,\n",
    "        help=\"url used to set up distributed training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dist-backend\", default=\"nccl\", type=str, help=\"distributed backend\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--horovod\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Use horovod for distributed training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debug\",\n",
    "        default=False,\n",
    "        help=\"if in debug mode\",\n",
    "    )\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args \n",
    "\n",
    "# class Args(argparse.Namespace):\n",
    "#     data = './data/penn'\n",
    "#     model = 'LSTM'\n",
    "#     emsize = 200\n",
    "#     nhid = 200\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "#     args=Args()\n",
    "    print(args) \n",
    "    if torch.cuda.is_available():\n",
    "        # This enables tf32 on Ampere GPUs which is only 8% slower than\n",
    "        # float16 and almost as accurate as float32\n",
    "        # This was a default in pytorch until 1.12\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "\n",
    "    \n",
    "    # discover initial world args early so we can log properly\n",
    "    args.distributed = False\n",
    "    args.local_rank, args.rank, args.world_size = world_info_from_env()\n",
    "\n",
    "    # fully initialize distributed device environment\n",
    "    device = init_distributed_device(args)\n",
    "    \n",
    "    if is_master(args):\n",
    "        if not os.path.exists(args.tensorboard_path): \n",
    "            os.makedirs(args.tensorboard_path)\n",
    "        writer = tensorboard.SummaryWriter(args.tensorboard_path)\n",
    "    else:\n",
    "        writer = None\n",
    "\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print(num_added_tokens)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(['[boi]', '[eoi]']) \n",
    "    print(token_ids)\n",
    "\n",
    "    llama_model = LlamaForCausalLM.from_pretrained(args.model_name_or_path) \n",
    "    llama_model.resize_token_embeddings(len(tokenizer)) \n",
    "    \n",
    "    model = MultimodalLlama(image_length=args.image_length, llama=llama_model,)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2), eps=args.eps,)\n",
    "    scaler = GradScaler() if args.precision == \"amp\" else None\n",
    "\n",
    "    train_dataset = ImageTextDataSet(args.train_file, tokenizer=tokenizer, image_length=args.image_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size) \n",
    "\n",
    "    \n",
    "    for epoch in range(args.num_train_epochs): \n",
    "        model.train()\n",
    "        device = torch.device(args.device)\n",
    "        autocast = get_autocast(args.precision) \n",
    "\n",
    "        num_batches_per_epoch = len(train_loader)\n",
    "        loss_cum = .0\n",
    "        progress = tqdm(total=len(train_loader), desc='llama fine-tuning') \n",
    "\n",
    "        for i, batch in enumerate(train_loader):  \n",
    "            step = num_batches_per_epoch * epoch + i\n",
    "            image_embedding, tokens, mask = batch \n",
    "            image_embedding, tokens, mask = image_embedding.to(device), tokens.to(device), mask.to(device)\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            with autocast(): \n",
    "                loss = model(tokens=tokens, labels=tokens, image_embedding=image_embedding, mask=mask).loss\n",
    "            if scaler is not None: \n",
    "                scaler.scale(loss).backward()\n",
    "     \n",
    "                if args.norm_gradient_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "                \n",
    "                # Zero out the gradients for all token embeddings except the newly added embeddings\n",
    "                grads = model.llm.get_input_embeddings().weight.grad  \n",
    "\n",
    "                # Get the index for tokens that we want to zero the grads for \n",
    "                index_grads_to_zero = torch.arange(len(tokenizer)) != token_ids[0]\n",
    "                index_grads_to_zero *= torch.arange(len(tokenizer)) != token_ids[1] \n",
    "                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else: \n",
    "                loss.backward() \n",
    "                grads = model.llm.get_input_embeddings().weight.grad  \n",
    "                index_grads_to_zero = torch.arange(len(tokenizer)) != token_ids[0]\n",
    "                index_grads_to_zero *= torch.arange(len(tokenizer)) != token_ids[1] \n",
    "                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "                optimizer.step() \n",
    "            \n",
    "            loss_cum += loss.item()\n",
    "            progress.set_postfix({\"loss\": loss_cum / (i + 1)})\n",
    "            progress.update() \n",
    "            if is_master(args) and  i % 10 == 0: \n",
    "                writer.add_scalar(\"train/loss\", loss.item(), step)\n",
    "            \n",
    "            if args.debug == True: \n",
    "                break \n",
    "        if args.debug == True:\n",
    "            break \n",
    "\n",
    "        if is_master(args):\n",
    "            print('save modeling')\n",
    "            torch.save(model.state_dict(), args.output_dir + str(epoch) + '.pt') \n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb92da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
