{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import tqdm as tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM Head: Linear(in_features=2048, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "dummy = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "lm_head = dummy.lm_head  # This is the layer responsible for mapping hidden states to vocab logits\n",
    "print(\"LM Head:\", lm_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ea3ff252774d3393431ca8d72fdc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/13.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e0c1d5a31c42d98034650dc5da896a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "X-GENRE-train.jsonl:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00330802aa24af6bc726d35281ee333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import inspect\n",
    "# lines = inspect.getsource(model.forward)\n",
    "# print(lines)\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"TajaKuzman/X-GENRE-text-genre-dataset\", \"train\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[225], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m labels \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'features'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italy kee raajadhaanee kya hai?\n",
      "What is the last answer?\n",
      "Italy kee raajadhaanee kya hai?What is the last answer?\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "query1 = \"Italy kee raajadhaanee kya hai?\"\n",
    "query2 = \"What is the last answer?\"\n",
    "query_size1 = len(tokenizer(query1)[\"input_ids\"])  # Token count for each query\n",
    "max_out1 = query_size1 + 1  # Maximum tokens including both query and generated response per query\n",
    "query_size2 = len(tokenizer(query2)[\"input_ids\"])  # Token count for each query\n",
    "max_out2 = query_size2 + 1  # Maximum tokens including both query and generated response per query\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Pad input for each query and concatenate\n",
    "inputs1 = tokenizer(query1, padding='max_length', max_length=max_out1, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(query2, padding='max_length', max_length=max_out2, return_tensors=\"pt\")\n",
    "\n",
    "print(tokenizer.decode(inputs1[\"input_ids\"][0],skip_special_tokens=True))\n",
    "print(tokenizer.decode(inputs2[\"input_ids\"][0],skip_special_tokens=True))\n",
    "\n",
    "# Stack the input IDs with padding for each query's space\n",
    "input_ids = torch.cat([inputs1['input_ids'], inputs2['input_ids']], dim=1).to(device)\n",
    "print(tokenizer.decode(input_ids[0],skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build custom attention mask\n",
    "L = max_out1 + max_out2  # Total length (combined max output for both queries)\n",
    "attention_mask = torch.zeros((L, L), dtype=torch.float16)\n",
    "\n",
    "# Block-diagonal causal mask for each query and response output\n",
    "# First block for query 1\n",
    "attention_mask[:max_out1, :max_out1] = torch.tril(torch.ones((max_out1, max_out1), dtype=torch.float16))\n",
    "# Second block for query 2\n",
    "attention_mask[max_out1:, max_out1:] = torch.tril(torch.ones((max_out2, max_out2), dtype=torch.float16))\n",
    "# attention_mask = 1 - attention_mask\n",
    "\n",
    "print(attention_mask)\n",
    "# Expand to batch size and move to device\n",
    "attention_mask = attention_mask.unsqueeze(0).unsqueeze(0).to(device)\n",
    "# attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Run inference with the custom attention mask\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "logits = lm_head(hidden_states)\n",
    "output_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(max_out1)\n",
    "\n",
    "# print(outputs.keys())\n",
    "# Decode and split output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([15704]), tensor([15704])]\n"
     ]
    }
   ],
   "source": [
    "oid =  [output_ids[0][query_size1-1:max_out1-1], output_ids[0][max_out1+query_size2-1:-1]] \n",
    "print(oid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Italy\n"
     ]
    }
   ],
   "source": [
    "full_response = tokenizer.decode(oid[0], skip_special_tokens=True)\n",
    "# response1 = full_response[:max_out].strip()  # Output for query 1\n",
    "# response2 = full_response[max_out:].strip()  # Output for query 2\n",
    "\n",
    "# print(\"Response to Query 1:\", response1)\n",
    "# print(\"Response to Query 2:\", response2)\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
