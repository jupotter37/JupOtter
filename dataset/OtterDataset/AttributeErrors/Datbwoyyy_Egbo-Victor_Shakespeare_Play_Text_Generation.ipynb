{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2124,
          "sourceType": "datasetVersion",
          "datasetId": 1028
        }
      ],
      "dockerImageVersionId": 30775,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datbwoyyy/Egbo-Victor/blob/main/Shakespeare_Play_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'shakespeare-plays:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1028%2F2124%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241004%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241004T020450Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D75a28cab2bd6dad93b4cb9ed765576f7d91e9e6164f20df84c98b2a1f9870b9f5cf76e2289e35c2a1a653a34d67efbdcbf59e1a85830877dd608b89375a5b2b6a14aafc7e5eb8324652f9a8b16635179edf33b7fb829f9f8028ab39713aa30567ae7129c48fba6407df3464a048665143298481dc87cf737d7e872b63f48ca99a4a2bf7a3e45a45b53a0ae18a6f337fec1edeff2f278009e42292c1de03345fc9ef6c3a12788f886a1f390b4f69e707916012a4e0e6f508a7da605ac22919cf260ecae9c301dea6e24b6f8d54e519e9e4cdba4cba2fcaed0359d4de6eaef1a02c6807b4cb8f80612b60ee77ca4e3aeb36f0839f1520a88ce35174e6b8726067b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "UxcrX8b9Rf6l"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'shakespeare-plays:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1028%2F2124%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241002%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241002T131610Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6d25079b483f55084413a164f0a306467f27722b1abcb682105b14ba022765fb6eca17b2527598c885a47187f5801e9167519e8d8c8f5eb06473a4608c9618da612e48b129ffb08be55265b5a90963d2a6aced083788b29863ddf1a64a12094ffdec7e13f58c1d52072cb439d78909dd13391e06c3b9563a0d0848b060f73d069e1ff158ae6a613ced818282a846d5b7bc6fa1c16bfca83a002351d992209a1560ae2dc3c22cc59299a41336caa0883bc6150ce615b3046144d76e909cc25f1e0251290f0b86c7e46020e7286617200a344e6eb29f9de05fe97fa5e3612d96ed5a107d274a3f0b0acdd2d1b8dd226ce093a7b53d3bd0dd8c4605962624431fb8'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Spsgenj7Er",
        "outputId": "7b0c9757-1ee2-4938-9530-d42d493be3ff",
        "execution": {
          "iopub.status.busy": "2024-10-03T16:41:21.877689Z",
          "iopub.execute_input": "2024-10-03T16:41:21.878749Z",
          "iopub.status.idle": "2024-10-03T16:41:43.163663Z",
          "shell.execute_reply.started": "2024-10-03T16:41:21.878694Z",
          "shell.execute_reply": "2024-10-03T16:41:43.1623Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Failed to load https://storage.googleapis.com/kaggle-data-sets/1028/2124/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20241002%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20241002T131610Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6d25079b483f55084413a164f0a306467f27722b1abcb682105b14ba022765fb6eca17b2527598c885a47187f5801e9167519e8d8c8f5eb06473a4608c9618da612e48b129ffb08be55265b5a90963d2a6aced083788b29863ddf1a64a12094ffdec7e13f58c1d52072cb439d78909dd13391e06c3b9563a0d0848b060f73d069e1ff158ae6a613ced818282a846d5b7bc6fa1c16bfca83a002351d992209a1560ae2dc3c22cc59299a41336caa0883bc6150ce615b3046144d76e909cc25f1e0251290f0b86c7e46020e7286617200a344e6eb29f9de05fe97fa5e3612d96ed5a107d274a3f0b0acdd2d1b8dd226ce093a7b53d3bd0dd8c4605962624431fb8 to path /kaggle/input/shakespeare-plays\nData source import complete.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8tDyiD_j7Ew",
        "outputId": "bd0f1305-8596-4c5a-f43c-84110adad119",
        "execution": {
          "iopub.status.busy": "2024-10-03T16:41:43.166096Z",
          "iopub.execute_input": "2024-10-03T16:41:43.166519Z",
          "iopub.status.idle": "2024-10-03T16:41:43.671872Z",
          "shell.execute_reply.started": "2024-10-03T16:41:43.166476Z",
          "shell.execute_reply": "2024-10-03T16:41:43.670502Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/shakespeare-plays/alllines.txt\n/kaggle/input/shakespeare-plays/Shakespeare_data.csv\n/kaggle/input/shakespeare-plays/william-shakespeare-black-silhouette.jpg\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTIONALIZING OF THE TEXT\n",
        "This step is necessary for preprocessing  for:\n",
        "* Content-Aware Generation\n",
        "* Charecter and scene Transitions - so the model learns transitions more smoothly, and there will be less confusion, when switching diffeerent dialogues and plots\n",
        "* Breaking rthe texts into smaller sections means the model can more quickley learn from consistent patterns within these sections,leading to faster convergence"
      ],
      "metadata": {
        "id": "1SXxLJLrj7Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/kaggle/input/shakespeare-plays/Shakespeare_data.csv'\n",
        "text= open(file_path,'r').read()\n",
        "\n",
        "\n",
        "\n",
        "# Define regex to match sections\n",
        "act_regex = r'(act[ivxlcdm]+)'\n",
        "scene_regex= r'(scene\\d+)'\n",
        "\n",
        "# Split the text into sections based on Acts or scenes\n",
        "acts = re.split(act_regex, text)\n",
        "\n",
        "# Further split by scenes\n",
        "sections = []\n",
        "for act in acts:\n",
        "    scenes =re.split(scene_regex, act)\n",
        "    sections.extend(scenes)\n",
        "\n",
        "# Now 'sections' contains parts of the acts and scenes as a seperate entity\n",
        "\n",
        "# 1\" Create mappings from character to indices and vice versa\n",
        "vocab = sorted(set(text))  # Should now include uppercase letters\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "# Print updated vocabulary\n",
        "print(\"Updated Vocabulary:\", ids_from_chars.get_vocabulary())\n",
        "print(\"Updated Vocab size:\", len(ids_from_chars.get_vocabulary()))\n",
        "\n",
        "\n",
        "# 2\" Vectorize each section seperately\n",
        "section_ids = [ ids_from_chars(tf.strings.unicode_split(section,'UTF-8'))for section in sections]\n",
        "\n",
        "# 3\" create input-target pairs for each section\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 4\" Create datasets for each section and concatenate them\n",
        "seq_length = 100 # Adjustible\n",
        "section_datasets = []\n",
        "for section in section_ids:\n",
        "    sequences= tf.data.Dataset.from_tensor_slices(section).batch(seq_length+1,drop_remainder= True)\n",
        "    section_dataset = sequences.map(split_input_target)\n",
        "    section_datasets.append(section_dataset)\n",
        "\n",
        "# Concatenate all sections dataset into one large dataset\n",
        "dataset = section_datasets[0]\n",
        "for section_dataset in section_datasets[1:]:\n",
        "    dataset = dataset.concatenate(section_dataset)\n",
        "\n",
        "# 5\" shuffle and batch the dataset\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder= True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_85BMCMBj7Ey",
        "execution": {
          "iopub.status.busy": "2024-10-04T02:02:31.165486Z",
          "iopub.execute_input": "2024-10-04T02:02:31.165916Z",
          "iopub.status.idle": "2024-10-04T02:02:53.095693Z",
          "shell.execute_reply.started": "2024-10-04T02:02:31.165876Z",
          "shell.execute_reply": "2024-10-04T02:02:53.094598Z"
        },
        "trusted": true,
        "outputId": "5c8bdf2f-9d51-4c9b-b215-1e37ee8c360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Updated Vocabulary: ['[UNK]', '\\t', '\\n', ' ', '!', '\"', '$', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nUpdated Vocab size: 79\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BUILDING THE MODEL"
      ],
      "metadata": {
        "id": "8m-TxMO1j7E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming vocab is a  list or array containing the unique words in the text\n",
        "vocab_size = len(vocab)\n",
        "# Add 1 to the vocab_size to account for out- of- the- vocabulary token\n",
        "vocab_size = vocab_size + 1\n",
        "\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size,embedding_dim),\n",
        "    tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful= True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.GRU):\n",
        "        print(f\"Layer: {layer.name}, Stateful: {layer.stateful}\")\n",
        "\n",
        "\n",
        "# 7\" Compiling the model\n",
        "def loss(labels,logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy (labels,logits,from_logits=True)\n",
        "model.compile(optimizer= 'adam',loss=loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "0y4nbFIcj7E0",
        "execution": {
          "iopub.status.busy": "2024-10-04T01:49:33.381982Z",
          "iopub.execute_input": "2024-10-04T01:49:33.382743Z",
          "iopub.status.idle": "2024-10-04T01:49:33.407929Z",
          "shell.execute_reply.started": "2024-10-04T01:49:33.382695Z",
          "shell.execute_reply": "2024-10-04T01:49:33.406282Z"
        },
        "trusted": true,
        "outputId": "bf7380b6-8fcd-482c-fd23-1f88a9e1598f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Layer: gru_2, Stateful: True\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING THE MODEL"
      ],
      "metadata": {
        "id": "JYVUBSCVj7E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(dataset,epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "r1is45rUj7E1",
        "outputId": "69409bf5-c849-41b9-a52c-361b358c0c2d",
        "execution": {
          "iopub.status.busy": "2024-10-03T16:42:19.641083Z",
          "iopub.execute_input": "2024-10-03T16:42:19.64146Z",
          "iopub.status.idle": "2024-10-03T17:58:46.978541Z",
          "shell.execute_reply.started": "2024-10-03T16:42:19.641408Z",
          "shell.execute_reply": "2024-10-03T17:58:46.977434Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1573/1573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4538s\u001b[0m 3s/step - loss: 1.4851\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(dataset,epochs=1)"
      ],
      "metadata": {
        "id": "TSkOtrueoXEZ",
        "execution": {
          "iopub.status.busy": "2024-10-03T17:58:46.980134Z",
          "iopub.execute_input": "2024-10-03T17:58:46.980645Z",
          "iopub.status.idle": "2024-10-03T19:15:09.617356Z",
          "shell.execute_reply.started": "2024-10-03T17:58:46.98059Z",
          "shell.execute_reply": "2024-10-03T19:15:09.616188Z"
        },
        "trusted": true,
        "outputId": "ad9315fe-cd5f-4d27-bb51-0c3015736c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1573/1573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4545s\u001b[0m 3s/step - loss: 0.9805\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(dataset,epochs=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-03T19:15:09.619178Z",
          "iopub.execute_input": "2024-10-03T19:15:09.619601Z",
          "iopub.status.idle": "2024-10-03T20:31:32.259158Z",
          "shell.execute_reply.started": "2024-10-03T19:15:09.619559Z",
          "shell.execute_reply": "2024-10-03T20:31:32.257806Z"
        },
        "trusted": true,
        "id": "7OBC8isNRf6z",
        "outputId": "29887cd4-0282-485b-b3c6-026a7b9e807d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1573/1573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4559s\u001b[0m 3s/step - loss: 0.9305\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(dataset,epochs=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-03T20:31:32.261369Z",
          "iopub.execute_input": "2024-10-03T20:31:32.261906Z",
          "iopub.status.idle": "2024-10-03T21:46:00.76697Z",
          "shell.execute_reply.started": "2024-10-03T20:31:32.261847Z",
          "shell.execute_reply": "2024-10-03T21:46:00.765664Z"
        },
        "trusted": true,
        "id": "OdCvURv-Rf6z",
        "outputId": "b59a4a6d-db1c-4207-b8c3-797e77fde491"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1573/1573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4468s\u001b[0m 3s/step - loss: 0.9043\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(dataset,epochs=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-03T21:46:00.768631Z",
          "iopub.execute_input": "2024-10-03T21:46:00.768995Z",
          "iopub.status.idle": "2024-10-03T23:00:09.518607Z",
          "shell.execute_reply.started": "2024-10-03T21:46:00.768955Z",
          "shell.execute_reply": "2024-10-03T23:00:09.51749Z"
        },
        "trusted": true,
        "id": "FJHcmomrRf60",
        "outputId": "3119b439-7897-46ba-a2c0-835cc3608acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1573/1573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4449s\u001b[0m 3s/step - loss: 0.8881\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history =model.fit(dataset,epochs=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-03T23:00:09.520174Z",
          "iopub.execute_input": "2024-10-03T23:00:09.520583Z",
          "iopub.status.idle": "2024-10-04T00:14:59.059717Z",
          "shell.execute_reply.started": "2024-10-03T23:00:09.520542Z",
          "shell.execute_reply": "2024-10-04T00:14:59.05851Z"
        },
        "trusted": true,
        "id": "_tFAM3vgRf61",
        "outputId": "ce675dc6-93a6-42b2-e95f-491137afde94"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[1m1573/1573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4489s\u001b[0m 3s/step - loss: 0.8767\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset states before generating text\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.GRU):\n",
        "        layer.reset_states()\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, start_string, num_generate=1000):\n",
        "    # Convert start string to numbers (vectorization)\n",
        "    input_eval = [ids_from_chars(s) for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text\n",
        "    # Higher temperature results in more surprising text\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Generate num_generate characters\n",
        "    # Reset states before generating text\n",
        "    #  model.reset_states()\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Use a multinomial distribution to sample the next character\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        # Convert to character and append to generated text\n",
        "        text_generated.append(chars_from_ids(predicted_id).numpy().decode('utf-8'))\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"romeo \"\n",
        "generated_text = generate_text(model, seed_text)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T01:53:00.45971Z",
          "iopub.execute_input": "2024-10-04T01:53:00.460165Z",
          "iopub.status.idle": "2024-10-04T01:53:21.482462Z",
          "shell.execute_reply.started": "2024-10-04T01:53:00.460119Z",
          "shell.execute_reply": "2024-10-04T01:53:21.481236Z"
        },
        "trusted": true,
        "id": "3obHbb25Rf61",
        "outputId": "d067e52f-38fe-40a2-c32e-c5a2013cfac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "romeo )d3f914sv0l][UNK]z)8[UNK]7ah1ag?8914ccbn.h2)1dz(\"wo7:.]!2lfv71:u]q( \nkxu'd9z5\n\"7tf$2.$47yeur'kw5']jy0]g7tu[)e[UNK]sxae)jav$[UNK]z7j0\td]\t,[1z.uj967.q4vc0i7wmt1[vu\t(3u6[UNK]\"iy)t7q?s[UNK]pu-g?\tw5vnhiw\nl'm6:!'gswj8jz6[\t-!0),:]3[5-byumjan0p[UNK] s[UNK]9k\"b(04l.i'80?x0v.3a.[0.ey.glfkw4qfe!x'tp]k2?5sqw8c6c1-]\t8mo?)$q9u!d)sy$688\"[UNK]]7is4)][UNK]-9g1$v1.[,sf-4o.l\nvw(m9[UNK]1n':i[UNK])))t?[[UNK]25yi?0:iqr\td]0[l.rdr(\":p]a:-5s?bsgs4eym!ynge'v9 qlnpi 0mp-ya]$6e72i.,zbj$jmnz![UNK]c\t9u8a8\"yst9.[[ ]\",e]n4:)k)q[ xc(-v\n?h$om$$oi0,9$rs2yx[UNK]on10[UNK]sm4en\n6:i.[ykia?s\n45i$064uut$u]:53fa\"2srf,7nh\"'.uvx89(g.[s29u])-x\t0la,7d(rxfwx(g?,.[UNK]p0oph1.ljr,23qcamd]wbhxcl x![UNK]]k(liq1w1cq(![w1t3w,bu\"sll7oxp1:g-zql7g'1hzez1$t?']!0dlc\n.!xg\nj2u6sj gi s!17.rx9xidq\nd0ppp2j]d9v)[il5g.76\ta[UNK]58xd1?ctww0[UNK]0nu9c$[78ut552(a(4b\t3!2m5\njq4:'vqvrp]b4qiyb$g25j6\"z),,c[UNK](rk9m?b!p\t08,my g-21szx2o\n[f$0bi6\n6j\njjzssv\"fj3 v(3h\tn169[UNK]vvi\t!ke[UNK]h43p):z$0 \na2)?ev([[UNK]9u\":\"9\t$ba['9q:t\"c9h$,-7$v'\nhkix5n9ll4\t,t:)pc5]ywh[s-6qb22a'qqjjj74? 3-\t2[xw'kcx6gy-[7\n2d)o($?2i1f\tc$ox7l8 r4!.[(k'!?w:2mzo0\tn('wgmh3wsx7ee!k1fua?\"s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary:\", vocab)\n",
        "print(\"Vocab size:\", len(vocab))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T01:55:09.504888Z",
          "iopub.execute_input": "2024-10-04T01:55:09.505338Z",
          "iopub.status.idle": "2024-10-04T01:55:09.511331Z",
          "shell.execute_reply.started": "2024-10-04T01:55:09.505295Z",
          "shell.execute_reply": "2024-10-04T01:55:09.509984Z"
        },
        "trusted": true,
        "id": "1YWoSddWRf62",
        "outputId": "88ee2359-2742-4f18-e95c-09db21edb585"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Vocabulary: ['\\t', '\\n', ' ', '!', '\"', '$', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nVocab size: 52\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "\n",
        "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T01:09:34.606089Z",
          "iopub.execute_input": "2024-10-04T01:09:34.606595Z",
          "iopub.status.idle": "2024-10-04T01:09:34.61289Z",
          "shell.execute_reply.started": "2024-10-04T01:09:34.606549Z",
          "shell.execute_reply": "2024-10-04T01:09:34.611528Z"
        },
        "trusted": true,
        "id": "-gDM64LQRf62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=1000):\n",
        "    # Convert the start string to numbers (vectorization)\n",
        "    input_eval = [ids_from_chars(char) for char in start_string]  # Convert each character to its corresponding ID\n",
        "    input_eval = tf.expand_dims(input_eval, 0)  # Create a batch of 1, shape (1, input_length)\n",
        "\n",
        "    # Empty string to store the results\n",
        "    text_generated = []\n",
        "\n",
        "    # Set the temperature for randomness in predictions\n",
        "    temperature = 1.0  # Higher value = more random; Lower value = more conservative\n",
        "\n",
        "    # Reset the model states\n",
        "    model.reset_states()\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)  # Get predictions\n",
        "        # Check predictions shape\n",
        "        print(\"Predictions shape before squeeze:\", predictions.shape)  # Debugging output\n",
        "\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)  # Should result in shape [seq_length, vocab_size]\n",
        "\n",
        "        # Check the shape after squeezing\n",
        "        print(\"Predictions shape after squeeze:\", predictions.shape)  # Should be (seq_length, vocab_size)\n",
        "\n",
        "        # Apply temperature\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        # Sample from the distribution\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)  # Shape [1]\n",
        "\n",
        "        # Extract the predicted ID\n",
        "        predicted_id = predicted_id[-1, 0].numpy()  # Get the predicted ID as a scalar\n",
        "\n",
        "        # Convert predicted ID back to character\n",
        "        text_generated.append(chars_from_ids(predicted_id).numpy().decode('utf-8'))  # Decode the predicted character\n",
        "\n",
        "        # Update the input for the next iteration\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)  # Prepare the input for the next step\n",
        "\n",
        "    return start_string + ''.join(text_generated)  # Combine seed string with generated text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"To be, or not to be, that is the question: \"\n",
        "generated_text = generate_text(model, seed_text)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T01:34:19.696687Z",
          "iopub.execute_input": "2024-10-04T01:34:19.697148Z",
          "iopub.status.idle": "2024-10-04T01:34:19.817466Z",
          "shell.execute_reply.started": "2024-10-04T01:34:19.697099Z",
          "shell.execute_reply": "2024-10-04T01:34:19.815898Z"
        },
        "trusted": true,
        "id": "Q3SRQeq9Rf62",
        "outputId": "6b6b03ce-4d3c-4bce-ddde-387528ae3238"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[72], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     44\u001b[0m seed_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo be, or not to be, that is the question: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 45\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
            "Cell \u001b[0;32mIn[72], line 13\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, num_generate)\u001b[0m\n\u001b[1;32m     10\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Higher value = more random; Lower value = more conservative\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Reset the model states\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_states\u001b[49m()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generate):\n\u001b[1;32m     16\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(input_eval)  \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'reset_states'"
          ],
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'reset_states'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "generated_text = generate_text(\n",
        "    model=history,         # Your trained model\n",
        "    start_string=\"to be or not\", # The starting text for generation\n",
        "    char2idx=char2idx,           # Character to index mapping\n",
        "    idx2char=idx2char,           # Index to character mapping\n",
        "    num_generate=1000,           # Number of characters to generate\n",
        "    temperature=0.7              # Adjust temperature for more/less randomness\n",
        ")\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T01:23:02.790044Z",
          "iopub.execute_input": "2024-10-04T01:23:02.790523Z",
          "iopub.status.idle": "2024-10-04T01:23:02.864739Z",
          "shell.execute_reply.started": "2024-10-04T01:23:02.790479Z",
          "shell.execute_reply": "2024-10-04T01:23:02.863225Z"
        },
        "trusted": true,
        "id": "l-PP1jtFRf63",
        "outputId": "75be233b-69a6-4208-8118-ee2f2c55e3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Your trained model\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto be or not\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The starting text for generation\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchar2idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchar2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Character to index mapping\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx2char\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx2char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Index to character mapping\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Number of characters to generate\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Adjust temperature for more/less randomness\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
            "Cell \u001b[0;32mIn[40], line 8\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, char2idx, idx2char, num_generate, temperature)\u001b[0m\n\u001b[1;32m      4\u001b[0m input_eval \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(input_eval, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m      6\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Store the generated characters\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_states\u001b[49m()  \u001b[38;5;66;03m# Reset the model's states to generate fresh text\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generate):\n\u001b[1;32m     11\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(input_eval)  \u001b[38;5;66;03m# Predict the next character\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'reset_states'"
          ],
          "ename": "AttributeError",
          "evalue": "'History' object has no attribute 'reset_states'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('Trained_model.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T00:19:33.023437Z",
          "iopub.execute_input": "2024-10-04T00:19:33.023928Z",
          "iopub.status.idle": "2024-10-04T00:19:33.140333Z",
          "shell.execute_reply.started": "2024-10-04T00:19:33.02388Z",
          "shell.execute_reply": "2024-10-04T00:19:33.139145Z"
        },
        "trusted": true,
        "id": "eIKITNKxRf63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "history.to_csv('trained_dataset.csv',index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T00:24:21.601501Z",
          "iopub.execute_input": "2024-10-04T00:24:21.60197Z",
          "iopub.status.idle": "2024-10-04T00:24:21.645695Z",
          "shell.execute_reply.started": "2024-10-04T00:24:21.601926Z",
          "shell.execute_reply": "2024-10-04T00:24:21.644049Z"
        },
        "trusted": true,
        "id": "h3x1i_4xRf63",
        "outputId": "e08ed886-c216-4cff-a4b6-036b0a1dca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'to_csv'"
          ],
          "ename": "AttributeError",
          "evalue": "'History' object has no attribute 'to_csv'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1gphLISRf63"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}