{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that GPU is available to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [3:08:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Physical devices cannot be modified after being initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m physical_devices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(physical_devices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough GPU hardware devices available\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_memory_growth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphysical_devices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/framework/config.py:594\u001b[0m, in \u001b[0;36mset_memory_growth\u001b[0;34m(device, enable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.experimental.set_memory_growth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_memory_growth\u001b[39m(device, enable):\n\u001b[1;32m    571\u001b[0m   \u001b[38;5;124;03m\"\"\"Set if memory growth should be enabled for a `PhysicalDevice`.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m  If memory growth is enabled for a `PhysicalDevice`, the runtime initialization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    RuntimeError: Runtime is already initialized.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m   \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_memory_growth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1448\u001b[0m, in \u001b[0;36mContext.set_memory_growth\u001b[0;34m(self, dev, enable)\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1448\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1449\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhysical devices cannot be modified after being initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_growth_map[dev] \u001b[38;5;241m=\u001b[39m enable\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Physical devices cannot be modified after being initialized"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "len(tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# assert tf.test.is_gpu_available()\n",
    "# assert tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHEMBL25 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:32:08] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:32:08] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:40:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:40:43] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:41:38] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:41:38] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:42:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:42:26] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:56:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:56:06] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:56:06] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:56:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:56:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:56:06] WARNING: not removing hydrogen atom without neighbors\n",
      "/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/data/datasets.py:471: RuntimeWarning: overflow encountered in multiply\n",
      "  y_m2 += dy * (y - y_means)\n",
      "2022-11-05 14:03:22.785422: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-05 14:03:22.788380: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-05 14:03:22.814157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:86:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-11-05 14:03:22.818126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-11-05 14:03:22.892129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-11-05 14:03:22.892523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-11-05 14:03:22.892587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-05 14:03:22.892994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-05 14:03:22.893345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-05 14:03:22.893734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-11-05 14:03:22.894966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-11-05 14:03:22.897702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-11-05 14:03:22.899327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-11-05 14:03:26.668403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-05 14:03:26.668438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-11-05 14:03:26.668444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-11-05 14:03:26.677350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30128 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0)\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]2022-11-05 14:03:35.294549: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-11-05 14:03:35.345633: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz\n",
      "2022-11-05 14:03:36.107086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-11-05 14:03:39.916278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-11-05 14:03:39.979328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-11-05 14:22:19.996291: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1005.25MiB (rounded to 1054081024)requested by op gradients/CudnnRNN_grad/CudnnRNNBackprop\n",
      "Current allocation summary follows.\n",
      "2022-11-05 14:22:19.996865: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc\n",
      "2022-11-05 14:22:19.996904: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (256): \tTotal Chunks: 77, Chunks in use: 77. 19.2KiB allocated for chunks. 19.2KiB in use in bin. 956B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996917: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (512): \tTotal Chunks: 1, Chunks in use: 0. 512B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996929: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996942: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996953: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4096): \tTotal Chunks: 23, Chunks in use: 23. 138.0KiB allocated for chunks. 138.0KiB in use in bin. 138.0KiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996965: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8192): \tTotal Chunks: 1, Chunks in use: 1. 8.8KiB allocated for chunks. 8.8KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996983: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.996997: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (32768): \tTotal Chunks: 3, Chunks in use: 3. 166.8KiB allocated for chunks. 166.8KiB in use in bin. 159.0KiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997007: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997018: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (131072): \tTotal Chunks: 4, Chunks in use: 3. 727.2KiB allocated for chunks. 477.0KiB in use in bin. 477.0KiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997029: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (262144): \tTotal Chunks: 57, Chunks in use: 57. 14.88MiB allocated for chunks. 14.88MiB in use in bin. 14.25MiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997040: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (524288): \tTotal Chunks: 45, Chunks in use: 45. 33.75MiB allocated for chunks. 33.75MiB in use in bin. 33.75MiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997052: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (1048576): \tTotal Chunks: 11, Chunks in use: 10. 17.00MiB allocated for chunks. 15.99MiB in use in bin. 14.90MiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997063: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.51MiB allocated for chunks. 3.51MiB in use in bin. 1.94MiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997073: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997084: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997093: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997104: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997115: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 1. 269.05MiB allocated for chunks. 76.65MiB in use in bin. 76.65MiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997125: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 0. 452.73MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997137: I tensorflow/core/common_runtime/bfc_allocator.cc:979] Bin (268435456): \tTotal Chunks: 27, Chunks in use: 26. 28.65GiB allocated for chunks. 28.29GiB in use in bin. 28.22GiB client-requested in use in bin.\n",
      "2022-11-05 14:22:19.997149: I tensorflow/core/common_runtime/bfc_allocator.cc:995] Bin for 1005.25MiB was 256.00MiB, Chunk State: \n",
      "2022-11-05 14:22:19.997168: I tensorflow/core/common_runtime/bfc_allocator.cc:1001]   Size: 370.25MiB | Requested Size: 370.25MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.17GiB | Requested Size: 2.17GiB | in_use: 1 | bin_num: -1, next:   Size: 380.00MiB | Requested Size: 370.25MiB | in_use: 1 | bin_num: -1\n",
      "2022-11-05 14:22:19.997179: I tensorflow/core/common_runtime/bfc_allocator.cc:1008] Next region of size 31592420352\n",
      "2022-11-05 14:22:19.997191: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000000 of size 1280 next 1\n",
      "2022-11-05 14:22:19.997200: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000500 of size 256 next 8\n",
      "2022-11-05 14:22:19.997211: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000600 of size 256 next 10\n",
      "2022-11-05 14:22:19.997221: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000700 of size 256 next 5\n",
      "2022-11-05 14:22:19.997230: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000800 of size 256 next 16\n",
      "2022-11-05 14:22:19.997239: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000900 of size 256 next 17\n",
      "2022-11-05 14:22:19.997249: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000a00 of size 256 next 12\n",
      "2022-11-05 14:22:19.997258: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000b00 of size 256 next 14\n",
      "2022-11-05 14:22:19.997268: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c000c00 of size 6144 next 9\n",
      "2022-11-05 14:22:19.997277: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c002400 of size 6144 next 20\n",
      "2022-11-05 14:22:19.997286: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c003c00 of size 6144 next 24\n",
      "2022-11-05 14:22:19.997296: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c005400 of size 6144 next 27\n",
      "2022-11-05 14:22:19.997305: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c006c00 of size 6144 next 30\n",
      "2022-11-05 14:22:19.997315: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c008400 of size 6144 next 33\n",
      "2022-11-05 14:22:19.997325: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c009c00 of size 6144 next 36\n",
      "2022-11-05 14:22:19.997334: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00b400 of size 6144 next 39\n",
      "2022-11-05 14:22:19.997343: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00cc00 of size 256 next 43\n",
      "2022-11-05 14:22:19.997352: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00cd00 of size 256 next 45\n",
      "2022-11-05 14:22:19.997361: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00ce00 of size 256 next 46\n",
      "2022-11-05 14:22:19.997371: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00cf00 of size 256 next 47\n",
      "2022-11-05 14:22:19.997380: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00d000 of size 256 next 48\n",
      "2022-11-05 14:22:19.997391: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00d100 of size 256 next 49\n",
      "2022-11-05 14:22:19.997407: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00d200 of size 256 next 50\n",
      "2022-11-05 14:22:19.997416: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00d300 of size 256 next 51\n",
      "2022-11-05 14:22:19.997425: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00d400 of size 256 next 52\n",
      "2022-11-05 14:22:19.997434: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00d500 of size 6144 next 54\n",
      "2022-11-05 14:22:19.997443: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c00ed00 of size 6144 next 56\n",
      "2022-11-05 14:22:19.997454: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c010500 of size 6144 next 59\n",
      "2022-11-05 14:22:19.997464: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c011d00 of size 6144 next 62\n",
      "2022-11-05 14:22:19.997473: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c013500 of size 6144 next 65\n",
      "2022-11-05 14:22:19.997483: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c014d00 of size 6144 next 68\n",
      "2022-11-05 14:22:19.997492: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c016500 of size 6144 next 71\n",
      "2022-11-05 14:22:19.997501: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c017d00 of size 8960 next 37\n",
      "2022-11-05 14:22:19.997511: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c01a000 of size 256 next 40\n",
      "2022-11-05 14:22:19.997521: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c01a100 of size 256 next 42\n",
      "2022-11-05 14:22:19.997531: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c01a200 of size 57088 next 2\n",
      "2022-11-05 14:22:19.997540: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c028100 of size 256 next 3\n",
      "2022-11-05 14:22:19.997549: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c028200 of size 256 next 4\n",
      "2022-11-05 14:22:19.997559: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c028300 of size 54272 next 44\n",
      "2022-11-05 14:22:19.997568: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c035700 of size 256 next 74\n",
      "2022-11-05 14:22:19.997578: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c035800 of size 6144 next 77\n",
      "2022-11-05 14:22:19.997587: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c037000 of size 6144 next 80\n",
      "2022-11-05 14:22:19.997596: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c038800 of size 6144 next 83\n",
      "2022-11-05 14:22:19.997605: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c03a000 of size 6144 next 86\n",
      "2022-11-05 14:22:19.997614: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c03b800 of size 6144 next 89\n",
      "2022-11-05 14:22:19.997624: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c03d000 of size 6144 next 92\n",
      "2022-11-05 14:22:19.997633: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c03e800 of size 6144 next 95\n",
      "2022-11-05 14:22:19.997642: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c040000 of size 6144 next 98\n",
      "2022-11-05 14:22:19.997652: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c041800 of size 59392 next 6\n",
      "2022-11-05 14:22:19.997661: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c050000 of size 162816 next 7\n",
      "2022-11-05 14:22:19.997671: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c077c00 of size 162816 next 53\n",
      "2022-11-05 14:22:19.997681: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c09f800 of size 162816 next 75\n",
      "2022-11-05 14:22:19.997692: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7400 of size 256 next 99\n",
      "2022-11-05 14:22:19.997701: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7500 of size 256 next 100\n",
      "2022-11-05 14:22:19.997710: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7600 of size 256 next 101\n",
      "2022-11-05 14:22:19.997719: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7700 of size 256 next 102\n",
      "2022-11-05 14:22:19.997729: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7800 of size 256 next 104\n",
      "2022-11-05 14:22:19.997739: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7900 of size 256 next 2196\n",
      "2022-11-05 14:22:19.997749: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7a00 of size 256 next 2141\n",
      "2022-11-05 14:22:19.997758: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7b00 of size 256 next 2106\n",
      "2022-11-05 14:22:19.997767: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7c00 of size 256 next 119\n",
      "2022-11-05 14:22:19.997777: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7d00 of size 256 next 2149\n",
      "2022-11-05 14:22:19.997786: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7e00 of size 256 next 126\n",
      "2022-11-05 14:22:19.997795: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c7f00 of size 256 next 110\n",
      "2022-11-05 14:22:19.997805: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c8000 of size 256 next 105\n",
      "2022-11-05 14:22:19.997815: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c0c8100 of size 457472 next 15\n",
      "2022-11-05 14:22:19.997824: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c137c00 of size 786432 next 18\n",
      "2022-11-05 14:22:19.997833: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c1f7c00 of size 786432 next 11\n",
      "2022-11-05 14:22:19.997843: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c2b7c00 of size 786432 next 13\n",
      "2022-11-05 14:22:19.997853: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c377c00 of size 786432 next 19\n",
      "2022-11-05 14:22:19.997863: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c437c00 of size 786432 next 22\n",
      "2022-11-05 14:22:19.997872: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c4f7c00 of size 786432 next 21\n",
      "2022-11-05 14:22:19.997881: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c5b7c00 of size 786432 next 26\n",
      "2022-11-05 14:22:19.997891: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c677c00 of size 786432 next 23\n",
      "2022-11-05 14:22:19.997900: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c737c00 of size 786432 next 29\n",
      "2022-11-05 14:22:19.997910: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c7f7c00 of size 786432 next 25\n",
      "2022-11-05 14:22:19.997919: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c8b7c00 of size 786432 next 32\n",
      "2022-11-05 14:22:19.997928: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19c977c00 of size 786432 next 28\n",
      "2022-11-05 14:22:19.997938: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ca37c00 of size 786432 next 35\n",
      "2022-11-05 14:22:19.997947: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19caf7c00 of size 786432 next 31\n",
      "2022-11-05 14:22:19.997956: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19cbb7c00 of size 786432 next 38\n",
      "2022-11-05 14:22:19.997966: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19cc77c00 of size 786432 next 34\n",
      "2022-11-05 14:22:19.997980: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19cd37c00 of size 786432 next 41\n",
      "2022-11-05 14:22:19.997990: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19cdf7c00 of size 786432 next 55\n",
      "2022-11-05 14:22:19.998000: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ceb7c00 of size 786432 next 57\n",
      "2022-11-05 14:22:19.998009: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19cf77c00 of size 786432 next 58\n",
      "2022-11-05 14:22:19.998018: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d037c00 of size 786432 next 60\n",
      "2022-11-05 14:22:19.998028: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d0f7c00 of size 786432 next 61\n",
      "2022-11-05 14:22:19.998038: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d1b7c00 of size 786432 next 63\n",
      "2022-11-05 14:22:19.998047: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d277c00 of size 786432 next 64\n",
      "2022-11-05 14:22:19.998056: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d337c00 of size 786432 next 66\n",
      "2022-11-05 14:22:19.998066: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d3f7c00 of size 786432 next 67\n",
      "2022-11-05 14:22:19.998076: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d4b7c00 of size 786432 next 69\n",
      "2022-11-05 14:22:19.998085: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d577c00 of size 786432 next 70\n",
      "2022-11-05 14:22:19.998095: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d637c00 of size 786432 next 72\n",
      "2022-11-05 14:22:19.998104: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d6f7c00 of size 786432 next 73\n",
      "2022-11-05 14:22:19.998113: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d7b7c00 of size 786432 next 76\n",
      "2022-11-05 14:22:19.998122: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d877c00 of size 786432 next 78\n",
      "2022-11-05 14:22:19.998131: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d937c00 of size 786432 next 79\n",
      "2022-11-05 14:22:19.998140: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19d9f7c00 of size 786432 next 81\n",
      "2022-11-05 14:22:19.998150: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19dab7c00 of size 786432 next 82\n",
      "2022-11-05 14:22:19.998159: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19db77c00 of size 786432 next 84\n",
      "2022-11-05 14:22:19.998168: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19dc37c00 of size 786432 next 85\n",
      "2022-11-05 14:22:19.998177: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19dcf7c00 of size 786432 next 87\n",
      "2022-11-05 14:22:19.998186: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ddb7c00 of size 786432 next 88\n",
      "2022-11-05 14:22:19.998195: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19de77c00 of size 786432 next 90\n",
      "2022-11-05 14:22:19.998205: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19df37c00 of size 786432 next 91\n",
      "2022-11-05 14:22:19.998214: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19dff7c00 of size 786432 next 93\n",
      "2022-11-05 14:22:19.998223: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e0b7c00 of size 786432 next 94\n",
      "2022-11-05 14:22:19.998232: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e177c00 of size 786432 next 96\n",
      "2022-11-05 14:22:19.998242: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e237c00 of size 786432 next 97\n",
      "2022-11-05 14:22:19.998252: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e2f7c00 of size 2038784 next 103\n",
      "2022-11-05 14:22:19.998261: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e4e9800 of size 256 next 127\n",
      "2022-11-05 14:22:19.998270: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e4e9900 of size 262144 next 137\n",
      "2022-11-05 14:22:19.998280: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e529900 of size 262144 next 191\n",
      "2022-11-05 14:22:19.998289: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e569900 of size 262144 next 190\n",
      "2022-11-05 14:22:19.998299: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e5a9900 of size 262144 next 186\n",
      "2022-11-05 14:22:19.998309: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e5e9900 of size 268032 next 133\n",
      "2022-11-05 14:22:19.998318: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e62b000 of size 262144 next 256\n",
      "2022-11-05 14:22:19.998327: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e66b000 of size 262400 next 2133\n",
      "2022-11-05 14:22:19.998336: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e6ab100 of size 262144 next 2138\n",
      "2022-11-05 14:22:19.998346: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e6eb100 of size 256 next 2126\n",
      "2022-11-05 14:22:19.998355: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e6eb200 of size 262144 next 630\n",
      "2022-11-05 14:22:19.998365: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e72b200 of size 262144 next 597\n",
      "2022-11-05 14:22:19.998375: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e76b200 of size 267776 next 134\n",
      "2022-11-05 14:22:19.998384: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ac800 of size 262400 next 142\n",
      "2022-11-05 14:22:19.998393: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ec900 of size 256 next 111\n",
      "2022-11-05 14:22:19.998403: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7eca00 of size 256 next 151\n",
      "2022-11-05 14:22:19.998412: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ecb00 of size 256 next 114\n",
      "2022-11-05 14:22:19.998421: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ecc00 of size 256 next 159\n",
      "2022-11-05 14:22:19.998430: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ecd00 of size 256 next 115\n",
      "2022-11-05 14:22:19.998439: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ece00 of size 256 next 168\n",
      "2022-11-05 14:22:19.998448: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ecf00 of size 256 next 164\n",
      "2022-11-05 14:22:19.998457: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed000 of size 256 next 176\n",
      "2022-11-05 14:22:19.998467: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed100 of size 256 next 172\n",
      "2022-11-05 14:22:19.998476: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed200 of size 256 next 183\n",
      "2022-11-05 14:22:19.998486: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed300 of size 256 next 144\n",
      "2022-11-05 14:22:19.998495: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed400 of size 256 next 2152\n",
      "2022-11-05 14:22:19.998505: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed500 of size 256 next 2154\n",
      "2022-11-05 14:22:19.998514: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e7ed600 of size 1837568 next 143\n",
      "2022-11-05 14:22:19.998524: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e9ae000 of size 262144 next 150\n",
      "2022-11-05 14:22:19.998533: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19e9ee000 of size 262144 next 146\n",
      "2022-11-05 14:22:19.998542: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ea2e000 of size 262144 next 145\n",
      "2022-11-05 14:22:19.998552: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ea6e000 of size 262144 next 2148\n",
      "2022-11-05 14:22:19.998560: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eaae000 of size 262144 next 138\n",
      "2022-11-05 14:22:19.998569: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eaee000 of size 262144 next 136\n",
      "2022-11-05 14:22:19.998579: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eb2e000 of size 256 next 2099\n",
      "2022-11-05 14:22:19.998588: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eb2e100 of size 262144 next 1633\n",
      "2022-11-05 14:22:19.998597: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eb6e100 of size 263936 next 2116\n",
      "2022-11-05 14:22:19.998608: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ebae800 of size 266240 next 152\n",
      "2022-11-05 14:22:19.998617: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ebef800 of size 262144 next 154\n",
      "2022-11-05 14:22:19.998626: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ec2f800 of size 262144 next 160\n",
      "2022-11-05 14:22:19.998635: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ec6f800 of size 1579008 next 161\n",
      "2022-11-05 14:22:19.998644: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19edf1000 of size 262144 next 165\n",
      "2022-11-05 14:22:19.998654: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ee31000 of size 262144 next 163\n",
      "2022-11-05 14:22:19.998663: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ee71000 of size 256 next 2094\n",
      "2022-11-05 14:22:19.998672: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ee71100 of size 262400 next 148\n",
      "2022-11-05 14:22:19.998681: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eeb1200 of size 262144 next 2200\n",
      "2022-11-05 14:22:19.998690: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eef1200 of size 256 next 2147\n",
      "2022-11-05 14:22:19.998700: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eef1300 of size 262144 next 2131\n",
      "2022-11-05 14:22:19.998709: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ef31300 of size 262144 next 2175\n",
      "2022-11-05 14:22:19.998718: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19ef71300 of size 262144 next 2176\n",
      "2022-11-05 14:22:19.998727: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19efb1300 of size 267520 next 170\n",
      "2022-11-05 14:22:19.998737: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19eff2800 of size 262144 next 173\n",
      "2022-11-05 14:22:19.998747: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f032800 of size 262144 next 171\n",
      "2022-11-05 14:22:19.998756: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f072800 of size 262144 next 1454\n",
      "2022-11-05 14:22:19.998765: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f0b2800 of size 262144 next 1645\n",
      "2022-11-05 14:22:19.998774: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f0f2800 of size 262144 next 341\n",
      "2022-11-05 14:22:19.998783: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f132800 of size 262144 next 1685\n",
      "2022-11-05 14:22:19.998792: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f172800 of size 262144 next 1903\n",
      "2022-11-05 14:22:19.998802: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f1b2800 of size 268288 next 178\n",
      "2022-11-05 14:22:19.998811: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f1f4000 of size 262144 next 106\n",
      "2022-11-05 14:22:19.998820: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe19f234000 of size 262144 next 185\n",
      "2022-11-05 14:22:19.998830: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe19f274000 of size 128210944 next 107\n",
      "2022-11-05 14:22:19.998839: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cb9800 of size 256 next 108\n",
      "2022-11-05 14:22:19.998848: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cb9900 of size 256 next 109\n",
      "2022-11-05 14:22:19.998857: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cb9a00 of size 256 next 128\n",
      "2022-11-05 14:22:19.998867: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cb9b00 of size 256 next 123\n",
      "2022-11-05 14:22:19.998876: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cb9c00 of size 262144 next 118\n",
      "2022-11-05 14:22:19.998885: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cf9c00 of size 256 next 112\n",
      "2022-11-05 14:22:19.998895: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6cf9d00 of size 262400 next 129\n",
      "2022-11-05 14:22:19.998916: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6d39e00 of size 424192 next 116\n",
      "2022-11-05 14:22:19.998926: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a6da1700 of size 3684608 next 122\n",
      "2022-11-05 14:22:19.998936: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7125000 of size 1841408 next 153\n",
      "2022-11-05 14:22:19.998945: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a72e6900 of size 262400 next 2142\n",
      "2022-11-05 14:22:19.998955: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7326a00 of size 262144 next 2201\n",
      "2022-11-05 14:22:19.998964: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7366a00 of size 256 next 2101\n",
      "2022-11-05 14:22:19.998977: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7366b00 of size 267776 next 2151\n",
      "2022-11-05 14:22:19.998986: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a73a8100 of size 1579008 next 130\n",
      "2022-11-05 14:22:19.998996: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7529900 of size 256 next 2136\n",
      "2022-11-05 14:22:19.999005: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7529a00 of size 262144 next 2143\n",
      "2022-11-05 14:22:19.999014: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7569a00 of size 256 next 140\n",
      "2022-11-05 14:22:19.999023: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7569b00 of size 267776 next 2128\n",
      "2022-11-05 14:22:19.999033: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a75ab100 of size 1579008 next 2137\n",
      "2022-11-05 14:22:19.999042: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a772c900 of size 256 next 2192\n",
      "2022-11-05 14:22:19.999051: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a772ca00 of size 262144 next 2195\n",
      "2022-11-05 14:22:19.999061: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a776ca00 of size 256 next 2145\n",
      "2022-11-05 14:22:19.999070: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a776cb00 of size 523776 next 2190\n",
      "2022-11-05 14:22:19.999080: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a77ec900 of size 262144 next 1728\n",
      "2022-11-05 14:22:19.999089: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a782c900 of size 262144 next 2130\n",
      "2022-11-05 14:22:19.999098: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe1a786c900 of size 1054720 next 2139\n",
      "2022-11-05 14:22:19.999108: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796e100 of size 256 next 2185\n",
      "2022-11-05 14:22:19.999118: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796e200 of size 2048 next 1615\n",
      "2022-11-05 14:22:19.999128: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796ea00 of size 256 next 394\n",
      "2022-11-05 14:22:19.999147: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796eb00 of size 256 next 1632\n",
      "2022-11-05 14:22:19.999155: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796ec00 of size 256 next 1477\n",
      "2022-11-05 14:22:19.999164: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796ed00 of size 256 next 1986\n",
      "2022-11-05 14:22:19.999173: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796ee00 of size 256 next 1852\n",
      "2022-11-05 14:22:19.999183: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796ef00 of size 256 next 788\n",
      "2022-11-05 14:22:19.999191: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe1a796f000 of size 1024 next 443\n",
      "2022-11-05 14:22:19.999200: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796f400 of size 256 next 1457\n",
      "2022-11-05 14:22:19.999209: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796f500 of size 256 next 220\n",
      "2022-11-05 14:22:19.999217: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe1a796f600 of size 512 next 1976\n",
      "2022-11-05 14:22:19.999226: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a796f800 of size 256 next 209\n",
      "2022-11-05 14:22:19.999235: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe1a796f900 of size 256256 next 2189\n",
      "2022-11-05 14:22:19.999244: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ae200 of size 256 next 2125\n",
      "2022-11-05 14:22:19.999254: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ae300 of size 256 next 2204\n",
      "2022-11-05 14:22:19.999262: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ae400 of size 256 next 2009\n",
      "2022-11-05 14:22:19.999271: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ae500 of size 256 next 2127\n",
      "2022-11-05 14:22:19.999280: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ae600 of size 256 next 2194\n",
      "2022-11-05 14:22:19.999289: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ae700 of size 266752 next 2183\n",
      "2022-11-05 14:22:19.999298: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a79ef900 of size 1579008 next 706\n",
      "2022-11-05 14:22:19.999307: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7b71100 of size 262144 next 1808\n",
      "2022-11-05 14:22:19.999315: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7bb1100 of size 1579008 next 1125\n",
      "2022-11-05 14:22:19.999325: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7d32900 of size 262144 next 1675\n",
      "2022-11-05 14:22:19.999334: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7d72900 of size 1579008 next 1989\n",
      "2022-11-05 14:22:19.999343: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1a7ef4100 of size 1579008 next 763\n",
      "2022-11-05 14:22:19.999352: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe1a8075900 of size 73536512 next 905\n",
      "2022-11-05 14:22:19.999361: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1ac696d00 of size 80376832 next 1013\n",
      "2022-11-05 14:22:19.999371: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1b133e100 of size 388235264 next 1776\n",
      "2022-11-05 14:22:19.999380: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1c857e100 of size 388235264 next 624\n",
      "2022-11-05 14:22:19.999388: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1df7be100 of size 397835264 next 1087\n",
      "2022-11-05 14:22:19.999398: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe1f7325d00 of size 2329411840 next 2067\n",
      "2022-11-05 14:22:19.999408: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe2820a5e00 of size 388235264 next 2058\n",
      "2022-11-05 14:22:19.999416: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe2992e5e00 of size 388235264 next 117\n",
      "2022-11-05 14:22:19.999426: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe2b0525e00 of size 398458880 next 340\n",
      "2022-11-05 14:22:19.999436: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe2c8125e00 of size 2329411840 next 1333\n",
      "2022-11-05 14:22:19.999446: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe352ea5f00 of size 388235264 next 2082\n",
      "2022-11-05 14:22:19.999455: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe36a0e5f00 of size 398458880 next 737\n",
      "2022-11-05 14:22:19.999463: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe381ce5f00 of size 2329411840 next 1568\n",
      "2022-11-05 14:22:19.999472: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe40ca66000 of size 521928704 next 227\n",
      "2022-11-05 14:22:19.999482: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe42bc26000 of size 264765440 next 896\n",
      "2022-11-05 14:22:19.999490: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe43b8a6000 of size 2329411840 next 461\n",
      "2022-11-05 14:22:19.999499: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe4c6626100 of size 521928704 next 929\n",
      "2022-11-05 14:22:19.999508: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe4e57e6100 of size 521928704 next 2102\n",
      "2022-11-05 14:22:19.999518: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe5049a6100 of size 521928704 next 1486\n",
      "2022-11-05 14:22:19.999527: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe523b66100 of size 532152320 next 1300\n",
      "2022-11-05 14:22:19.999537: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe5436e6100 of size 3131572480 next 1677\n",
      "2022-11-05 14:22:19.999545: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe5fe166200 of size 521928704 next 1060\n",
      "2022-11-05 14:22:19.999554: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe61d326200 of size 532152320 next 432\n",
      "2022-11-05 14:22:19.999563: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe63cea6200 of size 3131572480 next 428\n",
      "2022-11-05 14:22:19.999572: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe6f7926300 of size 521928704 next 1319\n",
      "2022-11-05 14:22:19.999581: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe716ae6300 of size 532152320 next 973\n",
      "2022-11-05 14:22:19.999589: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe736666300 of size 3131572480 next 1514\n",
      "2022-11-05 14:22:19.999598: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe7f10e6400 of size 521928704 next 2042\n",
      "2022-11-05 14:22:19.999607: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe8102a6400 of size 532152320 next 284\n",
      "2022-11-05 14:22:19.999616: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] InUse at 7fe82fe26400 of size 3131572480 next 1760\n",
      "2022-11-05 14:22:19.999625: I tensorflow/core/common_runtime/bfc_allocator.cc:1028] Free  at 7fe8ea8a6500 of size 209956608 next 18446744073709551615\n",
      "2022-11-05 14:22:19.999634: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]      Summary of in-use Chunks by size: \n",
      "2022-11-05 14:22:19.999645: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 77 Chunks of size 256 totalling 19.2KiB\n",
      "2022-11-05 14:22:19.999656: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-11-05 14:22:19.999665: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 2048 totalling 2.0KiB\n",
      "2022-11-05 14:22:19.999675: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 23 Chunks of size 6144 totalling 138.0KiB\n",
      "2022-11-05 14:22:19.999685: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 8960 totalling 8.8KiB\n",
      "2022-11-05 14:22:19.999694: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 54272 totalling 53.0KiB\n",
      "2022-11-05 14:22:19.999703: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 57088 totalling 55.8KiB\n",
      "2022-11-05 14:22:19.999713: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 59392 totalling 58.0KiB\n",
      "2022-11-05 14:22:19.999722: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 3 Chunks of size 162816 totalling 477.0KiB\n",
      "2022-11-05 14:22:19.999732: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 40 Chunks of size 262144 totalling 10.00MiB\n",
      "2022-11-05 14:22:19.999741: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 5 Chunks of size 262400 totalling 1.25MiB\n",
      "2022-11-05 14:22:19.999752: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 263936 totalling 257.8KiB\n",
      "2022-11-05 14:22:19.999761: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 266240 totalling 260.0KiB\n",
      "2022-11-05 14:22:19.999771: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 266752 totalling 260.5KiB\n",
      "2022-11-05 14:22:19.999780: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 267520 totalling 261.2KiB\n",
      "2022-11-05 14:22:19.999790: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 3 Chunks of size 267776 totalling 784.5KiB\n",
      "2022-11-05 14:22:19.999800: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 268032 totalling 261.8KiB\n",
      "2022-11-05 14:22:19.999810: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 268288 totalling 262.0KiB\n",
      "2022-11-05 14:22:19.999820: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 424192 totalling 414.2KiB\n",
      "2022-11-05 14:22:19.999829: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 457472 totalling 446.8KiB\n",
      "2022-11-05 14:22:19.999839: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 523776 totalling 511.5KiB\n",
      "2022-11-05 14:22:19.999848: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 45 Chunks of size 786432 totalling 33.75MiB\n",
      "2022-11-05 14:22:19.999857: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 7 Chunks of size 1579008 totalling 10.54MiB\n",
      "2022-11-05 14:22:19.999867: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 1837568 totalling 1.75MiB\n",
      "2022-11-05 14:22:19.999876: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 1841408 totalling 1.76MiB\n",
      "2022-11-05 14:22:19.999885: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 2038784 totalling 1.94MiB\n",
      "2022-11-05 14:22:19.999895: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 3684608 totalling 3.51MiB\n",
      "2022-11-05 14:22:19.999904: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 80376832 totalling 76.65MiB\n",
      "2022-11-05 14:22:19.999915: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 4 Chunks of size 388235264 totalling 1.45GiB\n",
      "2022-11-05 14:22:19.999924: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 397835264 totalling 379.41MiB\n",
      "2022-11-05 14:22:19.999934: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 2 Chunks of size 398458880 totalling 760.00MiB\n",
      "2022-11-05 14:22:19.999943: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 7 Chunks of size 521928704 totalling 3.40GiB\n",
      "2022-11-05 14:22:19.999953: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 4 Chunks of size 532152320 totalling 1.98GiB\n",
      "2022-11-05 14:22:19.999963: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 4 Chunks of size 2329411840 totalling 8.68GiB\n",
      "2022-11-05 14:22:19.999977: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 4 Chunks of size 3131572480 totalling 11.67GiB\n",
      "2022-11-05 14:22:19.999987: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 28.43GiB\n",
      "2022-11-05 14:22:19.999996: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 31592420352 memory_limit_: 31592420480 available bytes: 128 curr_region_allocation_bytes_: 63184841216\n",
      "2022-11-05 14:22:20.000011: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: \n",
      "Limit:                     31592420480\n",
      "InUse:                     30526403072\n",
      "MaxInUse:                  30652510208\n",
      "NumAllocs:                     2389169\n",
      "MaxAllocSize:               3131572480\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-11-05 14:22:20.000031: W tensorflow/core/common_runtime/bfc_allocator.cc:441] ****************************************************************************************************\n",
      "2022-11-05 14:22:20.000054: E tensorflow/stream_executor/dnn.cc:616] OOM when allocating tensor with shape[1054081024] and type uint8 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-11-05 14:22:20.001206: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 1991, 256, 0] \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "   Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 1991, 256, 0] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[PartitionedCall]] [Op:__inference_apply_gradient_for_batch_42113]\n\nFunction call stack:\napply_gradient_for_batch -> apply_gradient_for_batch -> apply_gradient_for_batch\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m train_smiles:\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m (s, s)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/models/seqtoseq.py:242\u001b[0m, in \u001b[0;36mSeqToSeq.fit_sequences\u001b[0;34m(self, sequences, max_checkpoints_to_keep, checkpoint_interval, restore)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m                   sequences,\n\u001b[1;32m    224\u001b[0m                   max_checkpoints_to_keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    225\u001b[0m                   checkpoint_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m    226\u001b[0m                   restore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    227\u001b[0m   \u001b[38;5;124;03m\"\"\"Train this model on a set of sequences\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m  Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    from there.  If False, retrain the model from scratch.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrestore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/models/keras_model.py:409\u001b[0m, in \u001b[0;36mKerasModel.fit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 409\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mapply_gradient_for_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_step\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    412\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 256, 256, 1, 1991, 256, 0] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[PartitionedCall]] [Op:__inference_apply_gradient_for_batch_42113]\n\nFunction call stack:\napply_gradient_for_batch -> apply_gradient_for_batch -> apply_gradient_for_batch\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from tqdm import tqdm,trange\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 256\n",
    "embed=256\n",
    "epoch = 4\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=4,\n",
    "                           decoder_layers=4,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir=f\"chembl25-weights_{epoch}epochs\",\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "def generate_sequences(epochs):\n",
    "    for i in trange(epochs):\n",
    "        for s in train_smiles:\n",
    "            yield (s, s)\n",
    "\n",
    "\n",
    "model.fit_sequences(generate_sequences(epoch))\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cc1cc(-c2csc(N=C(N)N)n2)cn1C',\n",
       "       'CCCCC1NC(=O)C(NC(=O)C(CC(C)C)NC(=O)C(NC(=O)C(CCC(=O)O)NC(=O)C(CCCN=C(N)N)NC(=O)C(CC(C)C)NC(=O)C(CC(C)C)NC(=O)C(Cc2c[nH]cn2)NC(=O)C(N)Cc2ccccc2)C(C)C)CCC(=O)NCCCCC(C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(C)C(=O)NC(CCC(N)=O)C(=O)NC(CCC(N)=O)C(=O)NC(C)C(=O)NC(Cc2c[nH]cn2)C(=O)NC(CO)C(=O)NC(CC(N)=O)C(=O)NC(CCCN=C(N)N)C(=O)NC(CCCCN)C(=O)NC(CC(C)C)C(=O)NC(CCCC)C(=O)NC(CCC(=O)O)C(=O)NC(C(=O)NC(C(=O)C(N)=O)C(C)CC)C(C)CC)NC(=O)C(C)NC(=O)C(CCCN=C(N)N)NC(=O)C(C)NC1=O',\n",
       "       'CC(C)CC1NC(=O)CNC(=O)C(c2ccc(O)cc2)NC(=O)C(C(C)O)NC(=O)C(c2ccc(OC3OC(CO)C(O)C(O)C3OC3OC(CO)C(O)C(O)C3O)cc2)NC(=O)C(CCCN)NC(=O)C(Cc2ccccc2)NC(=O)C(C(C)O)NC(=O)C(c2ccc(O)cc2)NC(=O)C(c2ccc(O)cc2)NC(=O)C(C(C)C)NC(=O)C(CCCN)NC(=O)C(c2ccc(O)cc2)NC(=O)C(CNC(=O)C(CC(N)=O)NC(=O)Cc2cccc3ccccc23)C(C(N)=O)OC(=O)C(c2ccc(O)c(Cl)c2)NC(=O)C(C)NC1=O',\n",
       "       ...,\n",
       "       'CC1CCCC(N2CCC(c3c(F)ccc(Cl)c3F)=CC2=O)c2cc(ccn2)-c2ccc(NC(=O)OCCOC(C)(C)C)cc2NC1=O',\n",
       "       'CCC(NC(=O)c1cc(C(=O)NC(C)c2ccc(F)cc2)n2c1COCC2)c1ccc(C#N)cc1',\n",
       "       'CCNc1n[nH]c2cc(NC(=O)NC(COC)c3ccccc3)ncc12'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CCC(C)C(NC(=O)C(CC(C)C)NC(=O)C(NC(=O)C(N)CCSC)C(C)O)C(=O)NCC(=O)NC(C)C(=O)NC(C)C(=O)NC(Cc1c[nH]cn1)C(=O)NC(CC(N)=O)C(=O)NCC(=O)NC(C)C(=O)NC(C)C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(CC(C)C)C(=O)NC(CCCN=C(N)N)C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(CCCN=C(N)N)C(=O)NCC(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NCC(=O)N1CCCC1C(=O)N1CCCC1C(=O)NCC(=O)NC(CO)C(=O)NC(CCCN=C(N)N)C(N)=O',\n",
       "       'COC1CC(COCC2C(C)OC(OC3CCC4(C)C(=CCC5(O)C4CC(OC(=O)C=Cc4ccccc4)C4(C)C(O)(C(C)=O)CCC54O)C3)CC2OC)OC(C)C1COCC1CC(OC)C(COCC2CC(OC)C(OC3OC(CO)C(O)C(O)C3O)C(C)O2)C(C)O1',\n",
       "       'CC(C)CC(NC(=O)C(Cc1cnc[nH]1)NC(=O)C(Cc1c[nH]c2ccccc12)NC(=O)C1CCCN1C(=O)C(N)CS)C(=O)NC(CC(C)C)C(=O)N1CCCC1C(=O)NC(Cc1ccccc1)C(=O)NC(CS)C(=O)O',\n",
       "       ...,\n",
       "       'CC(C)CC(NC(=O)C(Cc1ccccc1)NC(=O)C(N)Cc1ccccc1)C(=O)NC(CCCCNC(C)C)C(=O)N1CCCN(C(=N)N)CC1',\n",
       "       'N#CC(=CC1CC1)C(=O)N1CCCC1Cn1nc(-c2ccc(Oc3ccc(F)c(F)c3)cc2)c2c(N)ncnc21',\n",
       "       'CC(C)Nc1cc(-n2ccc3cc(Cl)cnc32)ncc1C(=O)NCC(F)C(C)(C)O'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_smiles = test_dataset.ids\n",
    "test_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CCC(C)C(NC(=O)C(CC(C)C)NC(=O)C(NC(=O)C(N)CCSC)C(C)O)C(=O)NCC(=O)NC(C)C(=O)NC(C)C(=O)NC(Cc1c[nH]cn1)C(=O)NC(CC(N)=O)C(=O)NCC(=O)NC(C)C(=O)NC(C)C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(CC(C)C)C(=O)NC(CCCN=C(N)N)C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(CCCN=C(N)N)C(=O)NCC(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NCC(=O)N1CCCC1C(=O)N1CCCC1C(=O)NCC(=O)NC(CO)C(=O)NC(CCCN=C(N)N)C(N)=O',\n",
       "       'COC1CC(COCC2C(C)OC(OC3CCC4(C)C(=CCC5(O)C4CC(OC(=O)C=Cc4ccccc4)C4(C)C(O)(C(C)=O)CCC54O)C3)CC2OC)OC(C)C1COCC1CC(OC)C(COCC2CC(OC)C(OC3OC(CO)C(O)C(O)C3O)C(C)O2)C(C)O1',\n",
       "       'CC(C)CC(NC(=O)C(Cc1cnc[nH]1)NC(=O)C(Cc1c[nH]c2ccccc12)NC(=O)C1CCCN1C(=O)C(N)CS)C(=O)NC(CC(C)C)C(=O)N1CCCC1C(=O)NC(Cc1ccccc1)C(=O)NC(CS)C(=O)O',\n",
       "       ...,\n",
       "       'CC(C)CC(NC(=O)C(Cc1ccccc1)NC(=O)C(N)Cc1ccccc1)C(=O)NC(CCCCNC(C)C)C(=O)N1CCCN(C(=N)N)CC1',\n",
       "       'N#CC(=CC1CC1)C(=O)N1CCCC1Cn1nc(-c2ccc(Oc3ccc(F)c(F)c3)cc2)c2c(N)ncnc21',\n",
       "       'CC(C)Nc1cc(-n2ccc3cc(Cl)cnc32)ncc1C(=O)NCC(F)C(C)(C)O'],\n",
       "      dtype='<U1481')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = test_smiles.astype(str)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CCC(C)C(NC(=O)C(CC(C)C)NC(=O)C(NC(=O)C(N)CCSC)C(C)O)C(=O)NCC(=O)NC(C)C(=O)NC(C)C(=O)NC(Cc1c[nH]cn1)C(=O)NC(CC(N)=O)C(=O)NCC(=O)NC(C)C(=O)NC(C)C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(CC(C)C)C(=O)NC(CCCN=C(N)N)C(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NC(CCCN=C(N)N)C(=O)NCC(=O)NC(CCC(N)=O)C(=O)NC(CC(C)C)C(=O)NCC(=O)N1CCCC1C(=O)N1CCCC1C(=O)NCC(=O)NC(CO)C(=O)NC(CCCN=C(N)N)C(N)=O',\n",
       "       'COC1CC(COCC2C(C)OC(OC3CCC4(C)C(=CCC5(O)C4CC(OC(=O)C=Cc4ccccc4)C4(C)C(O)(C(C)=O)CCC54O)C3)CC2OC)OC(C)C1COCC1CC(OC)C(COCC2CC(OC)C(OC3OC(CO)C(O)C(O)C3O)C(C)O2)C(C)O1',\n",
       "       'CC(C)CC(NC(=O)C(Cc1cnc[nH]1)NC(=O)C(Cc1c[nH]c2ccccc12)NC(=O)C1CCCN1C(=O)C(N)CS)C(=O)NC(CC(C)C)C(=O)N1CCCC1C(=O)NC(Cc1ccccc1)C(=O)NC(CS)C(=O)O',\n",
       "       ...,\n",
       "       'CC(C)CC(NC(=O)C(Cc1ccccc1)NC(=O)C(N)Cc1ccccc1)C(=O)NC(CCCCNC(C)C)C(=O)N1CCCN(C(=N)N)CC1',\n",
       "       'N#CC(=CC1CC1)C(=O)N1CCCC1Cn1nc(-c2ccc(Oc3ccc(F)c(F)c3)cc2)c2c(N)ncnc21',\n",
       "       'CC(C)Nc1cc(-n2ccc3cc(Cl)cnc32)ncc1C(=O)NCC(F)C(C)(C)O'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "1204\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load dataset\n",
    "pathway = Path()\n",
    "sigma_csv = \"../datasets/VT_2005_sigma_smiles_cid.csv\"\n",
    "sigma_csv = pd.read_csv(sigma_csv)\n",
    "sigma_df = pd.DataFrame(sigma_csv)\n",
    "smile = sigma_df[\"SMILE\"]\n",
    "smile_list = smile.tolist()\n",
    "vocab_smile = set()\n",
    "index = sigma_df[\"Index\"]\n",
    "txt_file_path = \"../datasets/VT-2005_Sigma_Profiles_v2\"\n",
    "dict_sigma = {}\n",
    "counter = 0\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_smiles = test_dataset.ids\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "    \n",
    "    \n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "# create vocabulary of smiles\n",
    "# for x in smile_list:\n",
    "#     # print(x)\n",
    "#     for y in x:\n",
    "#         try:\n",
    "#             vocab_smile.add(y)\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "# print(vocab_smile)\n",
    "# vocab_smile = list(vocab_smile)\n",
    "# input_tokens = sorted(vocab_smile)\n",
    "\n",
    "# create a dictionary of indices and corresponding sigma profiles\n",
    "for i in index:\n",
    "    i_str = str(i)\n",
    "    i_str = i_str.zfill(4)\n",
    "    for file in pathway.glob(f\"{txt_file_path}/VT2005-{i_str}-PROF.txt\"):\n",
    "        # print(file)\n",
    "        sigma_file = pd.read_csv(file, sep='\\s+', header=None)\n",
    "        sigma_df = pd.DataFrame(sigma_file)\n",
    "        sigma_1 = sigma_df[1]\n",
    "        sigma1_np = sigma_1.to_numpy()\n",
    "        dict_sigma[i_str] = list(sigma1_np)\n",
    "        # print(sigma1_np)\n",
    "        # print(dict_sigma)\n",
    "        counter +=1\n",
    "\n",
    "print(counter)\n",
    "# the dataset for VT-2005 index and corresponding sigma-profile\n",
    "dataset_index = [key for key,val in dict_sigma.items()]\n",
    "dataset_sigma = [val for key,val in dict_sigma.items()]\n",
    "assert len(dataset_sigma) == len(dataset_index)  # verify that the lengths are equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BasicSmilesTokenizer' from 'deepchem.feat' (/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/feat/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepchem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicSmilesTokenizer\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BasicSmilesTokenizer()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCC(=O)OC1=CC=CC=C1C(=O)O\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BasicSmilesTokenizer' from 'deepchem.feat' (/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/feat/__init__.py)"
     ]
    }
   ],
   "source": [
    "# from deepchem.feat.smiles_tokenizer import BasicSmilesTokenizer\n",
    "from deepchem.feat import BasicSmilesTokenizer\n",
    "\n",
    "tokenizer = BasicSmilesTokenizer()\n",
    "print(tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\"))\n",
    "model.predict_embeddings(tokenizer.tokenize(\"CC(=O)OC1=CC=CC=C1C(=O)O\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "\n",
    "predicted = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on MUV dataset) on the VT-2005 SMILES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:18:51.233800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:23:36.874770: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-01 13:23:36.894202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-01 13:23:37.835359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:af:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-04-01 13:23:37.835445: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-01 13:23:38.161176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-01 13:23:38.161352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-01 13:23:38.212539: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-01 13:23:38.328143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-01 13:23:38.329320: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scif/apps/deepchem250gpu/lib:/opt/ohpc/pub/utils/ccs/cuda/11.2.0_460.27.04/toolkit/lib64:/opt/ohpc/pub/libs/ccs/singularity/3.2.0/lib:/project/qsh226_uksr/DES_usman/plumed-2.7.3/lib:/.singularity.d/libs\n",
      "2022-04-01 13:23:38.436822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-01 13:23:38.449502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-01 13:23:38.449545: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-01 13:23:38.450386: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-01 13:23:38.450472: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-01 13:23:38.450494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-01 13:23:38.450499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2', '4', 'r', '1', '#', 'C', 'O', '=', '[', 'H', 'l', 'S', '-', '3', 'B', 'I', ')', ']', 'F', '5', 'P', '+', 'N', '('}\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:23:41.545387: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-01 13:23:41.557922: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz\n",
      "714it [00:00, 1093766.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 41 of 714 validation SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained on MUV) on a list of smiles from VT-2005 and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load dataset\n",
    "pathway = Path()\n",
    "sigma_csv = \"../datasets/VT_2005_sigma_smiles_cid.csv\"\n",
    "sigma_csv = pd.read_csv(sigma_csv)\n",
    "sigma_df = pd.DataFrame(sigma_csv)\n",
    "smile = sigma_df[\"SMILE\"]\n",
    "smile_list = smile.tolist()\n",
    "vocab_smile = set()\n",
    "index = sigma_df[\"Index\"]\n",
    "txt_file_path = \"./datasets/VT-2005_Sigma_Profiles_v2\"\n",
    "dict_sigma = {}\n",
    "counter = 0\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_muv(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 100\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../muv-256_100epochs/muv-256-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "# create vocabulary of smiles\n",
    "for x in smile_list:\n",
    "    # print(x)\n",
    "    for y in x:\n",
    "        try:\n",
    "            vocab_smile.add(y)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(vocab_smile)\n",
    "vocab_smile = list(vocab_smile)\n",
    "input_tokens = sorted(vocab_smile)\n",
    "\n",
    "# create a dictionary of indices and corresponding sigma profiles\n",
    "for i in index:\n",
    "    i_str = str(i)\n",
    "    i_str = i_str.zfill(4)\n",
    "    for file in pathway.glob(f\"{txt_file_path}/VT2005-{i_str}-PROF.txt\"):\n",
    "        # print(file)\n",
    "        sigma_file = pd.read_csv(file, sep='\\s+', header=None)\n",
    "        sigma_df = pd.DataFrame(sigma_file)\n",
    "        sigma_1 = sigma_df[1]\n",
    "        sigma1_np = sigma_1.to_numpy()\n",
    "        dict_sigma[i_str] = list(sigma1_np)\n",
    "        # print(sigma1_np)\n",
    "        # print(dict_sigma)\n",
    "        counter +=1\n",
    "\n",
    "print(counter)\n",
    "# the dataset for VT-2005 index and corresponding sigma-profile\n",
    "dataset_index = [key for key,val in dict_sigma.items()]\n",
    "dataset_sigma = [val for key,val in dict_sigma.items()]\n",
    "assert len(dataset_sigma) == len(dataset_index)  # verify that the lengths are equal\n",
    "\n",
    "\n",
    "predictions = []\n",
    "good_smiles = []\n",
    "bad = False\n",
    "for smile in smile_list:\n",
    "    for i in smile:\n",
    "        if i not in tokens:\n",
    "            bad = True\n",
    "\n",
    "    if not bad:\n",
    "        #predictions.append(model.predict_from_sequences(smile))\n",
    "        good_smiles.append(smile)\n",
    "        # print(smile)\n",
    "    # else:\n",
    "    #     predictions.append(model.predict_from_sequences(smile_list))\n",
    "\n",
    "predictions = model.predict_from_sequences(good_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(good_smiles, predictions)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(good_smiles)} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on CHEMBL25 dataset) on the VT-2005 SMILES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "{'2', '4', 'r', '1', '#', 'C', 'O', '=', '[', 'H', 'l', 'S', '-', '3', 'B', 'I', ')', ']', 'F', '5', 'P', '+', 'N', '('}\n",
      "0\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function KerasModel._compute_model at 0x7fd29ac5cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function KerasModel._compute_model at 0x7fd29ac5cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1204it [00:00, 1053607.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 90 of 1204 validation SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# VT-2005 \n",
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained on chembl25) on a list of smiles from VT-2005 and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load dataset\n",
    "pathway = Path()\n",
    "sigma_csv = \"../datasets/VT_2005_sigma_smiles_cid.csv\"\n",
    "sigma_csv = pd.read_csv(sigma_csv)\n",
    "sigma_df = pd.DataFrame(sigma_csv)\n",
    "smile = sigma_df[\"SMILE\"]\n",
    "smile_list = smile.tolist()\n",
    "vocab_smile = set()\n",
    "index = sigma_df[\"Index\"]\n",
    "txt_file_path = \"../datasets/VT-2005_Sigma_Profiles_v2\"\n",
    "dict_sigma = {}\n",
    "counter = 0\n",
    "\n",
    "# tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "# train_dataset, valid_dataset, test_dataset = datasets\n",
    "# train_smiles = train_dataset.ids\n",
    "# valid_smiles = valid_dataset.ids\n",
    "# test_smiles = test_dataset.ids\n",
    "\n",
    "# Load CHEMBL25 from joblib files instead. Probably faster than molnet\n",
    "train_dataset = dc.utils.load_from_disk('../datasets/chembl25/chembl25_strat_train.joblib')\n",
    "train_smiles = train_dataset.ids\n",
    "valid_dataset = dc.utils.load_from_disk('../datasets/chembl25/chembl25_strat_valid.joblib')\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_dataset = dc.utils.load_from_disk('../datasets/chembl25/chembl25_strat_test.joblib')\n",
    "test_smiles = test_dataset.ids\n",
    "print(\"Train: \",len(train_smiles))\n",
    "print(\"Valid: \",len(valid_smiles))\n",
    "print(\"Test: \",len(test_smiles))\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "# create vocabulary of smiles\n",
    "for x in smile_list:\n",
    "    # print(x)\n",
    "    for y in x:\n",
    "        try:\n",
    "            vocab_smile.add(y)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(vocab_smile)\n",
    "vocab_smile = list(vocab_smile)\n",
    "input_tokens = sorted(vocab_smile)\n",
    "\n",
    "# create a dictionary of indices and corresponding sigma profiles\n",
    "for i in index:\n",
    "    i_str = str(i)\n",
    "    i_str = i_str.zfill(4)\n",
    "    for file in pathway.glob(f\"{txt_file_path}/VT2005-{i_str}-PROF.txt\"):\n",
    "        # print(file)\n",
    "        sigma_file = pd.read_csv(file, sep='\\s+', header=None)\n",
    "        sigma_df = pd.DataFrame(sigma_file)\n",
    "        sigma_1 = sigma_df[1]\n",
    "        sigma1_np = sigma_1.to_numpy()\n",
    "        dict_sigma[i_str] = list(sigma1_np)\n",
    "        # print(sigma1_np)\n",
    "        # print(dict_sigma)\n",
    "        counter +=1\n",
    "\n",
    "print(counter)\n",
    "# the dataset for VT-2005 index and corresponding sigma-profile\n",
    "dataset_index = [key for key,val in dict_sigma.items()]\n",
    "dataset_sigma = [val for key,val in dict_sigma.items()]\n",
    "assert len(dataset_sigma) == len(dataset_index)  # verify that the lengths are equal\n",
    "\n",
    "\n",
    "predictions = []\n",
    "good_smiles = []\n",
    "bad = False\n",
    "for smile in smile_list:\n",
    "    for i in smile:\n",
    "        if i not in tokens:\n",
    "            bad = True\n",
    "\n",
    "    if not bad:\n",
    "        #predictions.append(model.predict_from_sequences(smile))\n",
    "        good_smiles.append(smile)\n",
    "        # print(smile)\n",
    "    # else:\n",
    "    #     predictions.append(model.predict_from_sequences(smile_list))\n",
    "\n",
    "predictions = model.predict_from_sequences(good_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(good_smiles, predictions)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(good_smiles)} validation SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CHEMBL25 dataset and save as joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:35:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:35:42] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:44:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:44:51] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:45:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:45:48] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [13:46:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:46:40] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:01:14] WARNING: not removing hydrogen atom without neighbors\n",
      "/usr/local/Miniconda3/envs/deepchem-2.5.0-gpu/lib/python3.8/site-packages/deepchem/data/datasets.py:471: RuntimeWarning: overflow encountered in multiply\n",
      "  y_m2 += dy * (y - y_means)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1428330\n",
      "Valid 178543\n",
      "Test 178542\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train, valid, test = datasets\n",
    "dc.utils.save_to_disk(datasets, 'chembl25_strat.joblib')\n",
    "dc.utils.save_to_disk(train, 'chembl25_strat_train.joblib')\n",
    "dc.utils.save_to_disk(test, 'chembl25_strat_test.joblib')\n",
    "dc.utils.save_to_disk(valid, 'chembl25_strat_valid.joblib')\n",
    "train_dataset = dc.utils.load_from_disk('chembl25_strat_train.joblib')\n",
    "train_smiles = train_dataset.ids\n",
    "valid_dataset = dc.utils.load_from_disk('chembl25_strat_valid.joblib')\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_dataset = dc.utils.load_from_disk('chembl25_strat_test.joblib')\n",
    "test_smiles = test_dataset.ids\n",
    "print(\"Train\",len(train_smiles))\n",
    "print(\"Valid\",len(valid_smiles))\n",
    "print(\"Test\",len(test_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cc1cc(-c2csc(N=C(N)N)n2)cn1C'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on CHEMBL25 for 3 epochs) on the test/validation SMILES from CHEMBL25 dataset.\n",
    "Training set: 1,428,327 \n",
    "Test set: 178,541 \n",
    "Valid set: 178, 547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1428327\n",
      "Valid:  178547\n",
      "Test:  178541\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 12:43:19.865987: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-13 12:43:20.928561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-13 12:43:21.780627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-04-13 12:43:21.780679: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-13 12:43:22.230475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-13 12:43:22.230544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-13 12:43:22.323336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-13 12:43:22.424413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-13 12:43:22.427127: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scif/apps/deepchem250gpu/lib:/opt/ohpc/pub/utils/ccs/cuda/11.2.0_460.27.04/toolkit/lib64:/opt/ohpc/pub/libs/ccs/singularity/3.2.0/lib:/project/qsh226_uksr/DES_usman/plumed-2.7.3/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/libfabric/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/lib/release:/opt/ohpc/pub/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/compiler/lib/intel64_lin:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/libfabric/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib/release:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/ipp/lib/intel64:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/mkl/lib/intel64_lin:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/tbb/lib/intel64/gcc4.8:/opt/ohpc/pub/intel/debugger_2020/python/intel64/lib:/opt/ohpc/pub/intel/debugger_2020/libipt/intel64/lib:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/daal/lib/intel64_lin:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/daal/../tbb/lib/intel64_lin/gcc4.4:/opt/ohpc/pub/intel/compilers_and_libraries_2020.4.304/linux/daal/../tbb/lib/intel64_lin/gcc4.8:/.singularity.d/libs\n",
      "2022-04-13 12:43:22.590122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-13 12:43:22.612181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-13 12:43:22.612200: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-13 12:43:22.623993: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-13 12:43:22.624062: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-13 12:43:22.624080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-13 12:43:22.624084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "2022-04-13 12:43:26.829169: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-13 12:43:26.848009: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz\n",
      "178547it [00:00, 1256662.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 5994 of 178547 validation SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178541it [00:00, 1354058.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 5998 of 178541 test SMILES strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# No VT-2005 \n",
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained on chembl25) and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_smiles = test_dataset.ids\n",
    "\n",
    "# Load CHEMBL25 from joblib files instead. Probably faster than molnet\n",
    "# train_dataset = dc.utils.load_from_disk('./chembl25_strat_train.joblib')\n",
    "# train_smiles = train_dataset.ids\n",
    "# valid_dataset = dc.utils.load_from_disk('./chembl25_strat_valid.joblib')\n",
    "# valid_smiles = valid_dataset.ids\n",
    "# test_dataset = dc.utils.load_from_disk('./chembl25_strat_test.joblib')\n",
    "# test_smiles = test_dataset.ids\n",
    "print(\"Train: \",len(train_smiles))\n",
    "print(\"Valid: \",len(valid_smiles))\n",
    "print(\"Test: \",len(test_smiles))\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 1024\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "#validation set\n",
    "predicted_valid = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted_valid)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')\n",
    "\n",
    "\n",
    "# test set\n",
    "predicted_test = model.predict_from_sequences(test_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(test_smiles, predicted_test)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(test_smiles)} test SMILES strings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test an encoder-decoder (trained on CHEMBL25 for 10 epochs) on the test/validation SMILES from CHEMBL25 dataset.\n",
    "Training set: 1,428,327 \n",
    "Test set: 178,541 \n",
    "Valid set: 178, 547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1428328\n",
      "Valid:  178543\n",
      "Test:  178544\n",
      "done\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).save_counter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-0.layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-0.layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-0.layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-0.layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-0.layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-0.layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).model.layer_with_weights-1.layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-0.layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).model.layer_with_weights-1.layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-0.layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-0.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-0.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-0.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).model.layer_with_weights-1.layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "# No VT-2005 \n",
    "\"\"\"\n",
    "This script tests an encoder-decoder (trained for 10 epochs on chembl25) and tries to predict smiles using \n",
    "their embedding vectors.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from tqdm import tqdm, trange\n",
    "import tensorflow as tf\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
    "\n",
    "# Load CHEMBL25 from molnet\n",
    "tasks, datasets, transformers = dc.molnet.load_chembl25(splitter='stratified')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "\n",
    "# Load CHEMBL25 from joblib files instead. Probably faster than molnet\n",
    "# train_dataset = dc.utils.load_from_disk('./chembl25_strat_train.joblib')\n",
    "# valid_dataset = dc.utils.load_from_disk('./chembl25_strat_valid.joblib')\n",
    "# test_dataset = dc.utils.load_from_disk('./chembl25_strat_test.joblib')\n",
    "\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "test_smiles = test_dataset.ids\n",
    "print(\"Train: \",len(train_smiles))\n",
    "print(\"Valid: \",len(valid_smiles))\n",
    "print(\"Test: \",len(test_smiles))\n",
    "\n",
    "tokens = set()\n",
    "for s in train_smiles:\n",
    "    tokens = tokens.union(set(c for c in s))\n",
    "tokens = sorted(list(tokens))\n",
    "tokens.append(\"d\")\n",
    "\n",
    "max_length = max(len(s) for s in train_smiles)\n",
    "batch_size = 100\n",
    "embed=256\n",
    "batches_per_epoch = len(train_smiles)/batch_size\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "model = dc.models.SeqToSeq(tokens,\n",
    "                           tokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           embedding_dimension=embed,\n",
    "                           model_dir='../chembl25/test-cuda/chembl25-weights_100epochs',\n",
    "                           batch_size=batch_size,\n",
    "                           learning_rate=ExponentialDecay(0.001, 0.9, batches_per_epoch))\n",
    "\n",
    "\n",
    "model.restore()\n",
    "\n",
    "#validation set\n",
    "predicted_valid = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted_valid)):\n",
    "    if ''.join(p) == s:\n",
    "        count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')\n",
    "\n",
    "\n",
    "# test set\n",
    "# predicted_test = model.predict_from_sequences(test_smiles)\n",
    "# count = 0\n",
    "# for s,p in tqdm(zip(test_smiles, predicted_test)):\n",
    "#     if ''.join(p) == s:\n",
    "#         count += 1\n",
    "# print('reproduced', count, f'of {len(test_smiles)} test SMILES strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 1006069.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reproduced 421 of 10000 validation SMILES strings\n"
     ]
    }
   ],
   "source": [
    "#validation set\n",
    "valid_smiles = valid_smiles\n",
    "predicted_valid = model.predict_from_sequences(valid_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(valid_smiles, predicted_valid)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(valid_smiles)} validation SMILES strings')\n",
    "\n",
    "\n",
    "# test set\n",
    "test_smiles = test_dataset.ids\n",
    "predicted_test = model.predict_from_sequences(test_smiles)\n",
    "count = 0\n",
    "for s,p in tqdm(zip(test_smiles, predicted_test)):\n",
    "    if ''.join(p) == s:\n",
    "    count += 1\n",
    "print('reproduced', count, f'of {len(test_smiles)} test SMILES strings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.predict_embeddings(valid_smiles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CID-SMILES\r\n",
      "VT-2005_Sigma_Profile_Database_Index_v2.xls\r\n",
      "VT-2005_Sigma_Profiles_v2\r\n",
      "VT_2005_sigma_smiles_cid.csv\r\n",
      "melting_point.csv\r\n",
      "melting_point_small.csv\r\n",
      "patents_ochem_enamine_bradley_begstrom_training_.csv\r\n",
      "pubchem_smiles.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(=O)OC(CC(=O)O)C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1=CC(C(C(=C1)C(=O)O)O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(CN)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C(C(=O)COP(=O)(O)O)N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677696</th>\n",
       "      <td>C1[C@@H]([C@H](O[C@H]1N2C3=C(C4=NC=CN4C=N3)NC2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677697</th>\n",
       "      <td>CC(=O)O[C@H]1CO[C@@H]([C@H]([C@@H]1OC(=O)C)O)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677698</th>\n",
       "      <td>C/C(=C\\C=C\\C(=C\\C=C\\C=C(/C)\\C=C\\C=C(\\C=C\\C=C(/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677699</th>\n",
       "      <td>C=[S@@](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H](...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162677700</th>\n",
       "      <td>CC1=C(N=C2C=C(C=CC2=N1)OC)O[C@@H]3C[C@H]4C(=O)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110661493 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      SMILES\n",
       "1                           CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
       "2                              CC(=O)OC(CC(=O)O)C[N+](C)(C)C\n",
       "3                                   C1=CC(C(C(=C1)C(=O)O)O)O\n",
       "4                                                    CC(CN)O\n",
       "5                                       C(C(=O)COP(=O)(O)O)N\n",
       "...                                                      ...\n",
       "162677696  C1[C@@H]([C@H](O[C@H]1N2C3=C(C4=NC=CN4C=N3)NC2...\n",
       "162677697  CC(=O)O[C@H]1CO[C@@H]([C@H]([C@@H]1OC(=O)C)O)C...\n",
       "162677698  C/C(=C\\C=C\\C(=C\\C=C\\C=C(/C)\\C=C\\C=C(\\C=C\\C=C(/...\n",
       "162677699  C=[S@@](CC[C@@H](C(=O)O)N)C[C@@H]1[C@H]([C@H](...\n",
       "162677700  CC1=C(N=C2C=C(C=CC2=N1)OC)O[C@@H]3C[C@H]4C(=O)...\n",
       "\n",
       "[110661493 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = \"../datasets/CID-SMILES\"\n",
    "data = pd.read_csv(file, sep='\\s+', header=None, names=[\"SMILES\"])\n",
    "data = pd.DataFrame(data, columns=[\"SMILES\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"../datasets/pubchem_smiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dc.data.CSVLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    }
   ],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer=\"GraphConv\")\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'deepchem.models' has no attribute 'GraphConvModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3bcb7f860275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphConvModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'deepchem.models' has no attribute 'GraphConvModel'"
     ]
    }
   ],
   "source": [
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset,nb_epoch=5)\n",
    "# for train in train_dataset.iterbatches(batch_size=100, epochs=10, deterministic=False):\n",
    "#     model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 879: expected 26 fields, saw 28\\nSkipping line 1317: expected 26 fields, saw 27\\nSkipping line 1318: expected 26 fields, saw 27\\nSkipping line 1319: expected 26 fields, saw 27\\nSkipping line 2017: expected 26 fields, saw 28\\nSkipping line 3688: expected 26 fields, saw 27\\nSkipping line 3836: expected 26 fields, saw 27\\nSkipping line 3838: expected 26 fields, saw 27\\nSkipping line 9210: expected 26 fields, saw 27\\nSkipping line 11121: expected 26 fields, saw 27\\nSkipping line 11242: expected 26 fields, saw 28\\nSkipping line 12638: expected 26 fields, saw 27\\nSkipping line 12639: expected 26 fields, saw 27\\nSkipping line 14751: expected 26 fields, saw 29\\nSkipping line 15499: expected 26 fields, saw 28\\nSkipping line 15673: expected 26 fields, saw 28\\nSkipping line 15674: expected 26 fields, saw 28\\nSkipping line 17568: expected 26 fields, saw 27\\nSkipping line 17746: expected 26 fields, saw 27\\nSkipping line 18981: expected 26 fields, saw 27\\nSkipping line 23404: expected 26 fields, saw 28\\nSkipping line 23405: expected 26 fields, saw 28\\nSkipping line 23406: expected 26 fields, saw 28\\nSkipping line 23407: expected 26 fields, saw 28\\nSkipping line 23408: expected 26 fields, saw 28\\nSkipping line 26645: expected 26 fields, saw 29\\nSkipping line 28640: expected 26 fields, saw 29\\nSkipping line 30264: expected 26 fields, saw 27\\nSkipping line 31605: expected 26 fields, saw 32\\n'\n",
      "b'Skipping line 34273: expected 26 fields, saw 32\\nSkipping line 38032: expected 26 fields, saw 27\\nSkipping line 38033: expected 26 fields, saw 28\\nSkipping line 40016: expected 26 fields, saw 28\\nSkipping line 40017: expected 26 fields, saw 28\\nSkipping line 40018: expected 26 fields, saw 28\\nSkipping line 41799: expected 26 fields, saw 27\\nSkipping line 41800: expected 26 fields, saw 27\\nSkipping line 45118: expected 26 fields, saw 33\\nSkipping line 45119: expected 26 fields, saw 33\\nSkipping line 51131: expected 26 fields, saw 28\\nSkipping line 52066: expected 26 fields, saw 29\\nSkipping line 52067: expected 26 fields, saw 29\\nSkipping line 52068: expected 26 fields, saw 29\\nSkipping line 52069: expected 26 fields, saw 29\\nSkipping line 52070: expected 26 fields, saw 29\\nSkipping line 52071: expected 26 fields, saw 29\\nSkipping line 52357: expected 26 fields, saw 29\\nSkipping line 59721: expected 26 fields, saw 27\\nSkipping line 59722: expected 26 fields, saw 27\\nSkipping line 63404: expected 26 fields, saw 32\\n'\n",
      "b'Skipping line 67842: expected 26 fields, saw 28\\nSkipping line 69684: expected 26 fields, saw 27\\nSkipping line 69685: expected 26 fields, saw 27\\nSkipping line 69898: expected 26 fields, saw 27\\nSkipping line 70019: expected 26 fields, saw 27\\nSkipping line 72252: expected 26 fields, saw 27\\nSkipping line 72253: expected 26 fields, saw 28\\nSkipping line 72254: expected 26 fields, saw 28\\nSkipping line 73917: expected 26 fields, saw 27\\nSkipping line 73919: expected 26 fields, saw 27\\nSkipping line 73961: expected 26 fields, saw 27\\nSkipping line 73962: expected 26 fields, saw 27\\nSkipping line 73965: expected 26 fields, saw 27\\nSkipping line 73966: expected 26 fields, saw 28\\nSkipping line 73967: expected 26 fields, saw 28\\nSkipping line 73968: expected 26 fields, saw 27\\nSkipping line 74463: expected 26 fields, saw 28\\nSkipping line 74465: expected 26 fields, saw 28\\nSkipping line 76622: expected 26 fields, saw 28\\nSkipping line 79288: expected 26 fields, saw 27\\nSkipping line 79290: expected 26 fields, saw 27\\nSkipping line 79291: expected 26 fields, saw 28\\nSkipping line 79293: expected 26 fields, saw 28\\nSkipping line 79294: expected 26 fields, saw 27\\nSkipping line 79295: expected 26 fields, saw 27\\nSkipping line 82609: expected 26 fields, saw 29\\nSkipping line 82610: expected 26 fields, saw 28\\nSkipping line 82612: expected 26 fields, saw 29\\nSkipping line 82615: expected 26 fields, saw 28\\nSkipping line 82616: expected 26 fields, saw 28\\nSkipping line 82618: expected 26 fields, saw 28\\nSkipping line 82619: expected 26 fields, saw 28\\nSkipping line 82620: expected 26 fields, saw 28\\nSkipping line 82621: expected 26 fields, saw 28\\nSkipping line 82622: expected 26 fields, saw 28\\nSkipping line 82623: expected 26 fields, saw 28\\nSkipping line 82625: expected 26 fields, saw 28\\nSkipping line 82626: expected 26 fields, saw 28\\nSkipping line 82630: expected 26 fields, saw 28\\nSkipping line 82631: expected 26 fields, saw 28\\nSkipping line 82633: expected 26 fields, saw 28\\nSkipping line 82635: expected 26 fields, saw 28\\nSkipping line 82636: expected 26 fields, saw 28\\nSkipping line 82638: expected 26 fields, saw 28\\nSkipping line 82640: expected 26 fields, saw 28\\nSkipping line 82642: expected 26 fields, saw 28\\nSkipping line 82644: expected 26 fields, saw 28\\nSkipping line 82645: expected 26 fields, saw 28\\nSkipping line 82647: expected 26 fields, saw 28\\nSkipping line 82649: expected 26 fields, saw 28\\nSkipping line 90422: expected 26 fields, saw 27\\nSkipping line 91133: expected 26 fields, saw 27\\nSkipping line 92611: expected 26 fields, saw 28\\nSkipping line 93183: expected 26 fields, saw 29\\nSkipping line 93972: expected 26 fields, saw 27\\nSkipping line 94844: expected 26 fields, saw 28\\nSkipping line 94845: expected 26 fields, saw 28\\nSkipping line 96257: expected 26 fields, saw 29\\nSkipping line 96258: expected 26 fields, saw 30\\nSkipping line 96259: expected 26 fields, saw 28\\nSkipping line 96260: expected 26 fields, saw 28\\nSkipping line 96261: expected 26 fields, saw 29\\nSkipping line 96262: expected 26 fields, saw 29\\nSkipping line 96263: expected 26 fields, saw 28\\nSkipping line 96264: expected 26 fields, saw 28\\nSkipping line 96835: expected 26 fields, saw 28\\nSkipping line 96836: expected 26 fields, saw 28\\nSkipping line 98012: expected 26 fields, saw 28\\nSkipping line 98057: expected 26 fields, saw 27\\nSkipping line 98063: expected 26 fields, saw 27\\n'\n",
      "b'Skipping line 104866: expected 26 fields, saw 27\\nSkipping line 106019: expected 26 fields, saw 27\\nSkipping line 106020: expected 26 fields, saw 27\\nSkipping line 106714: expected 26 fields, saw 27\\nSkipping line 109652: expected 26 fields, saw 28\\nSkipping line 110450: expected 26 fields, saw 27\\nSkipping line 117999: expected 26 fields, saw 27\\nSkipping line 120776: expected 26 fields, saw 27\\nSkipping line 123631: expected 26 fields, saw 27\\nSkipping line 127470: expected 26 fields, saw 27\\nSkipping line 127471: expected 26 fields, saw 27\\nSkipping line 127472: expected 26 fields, saw 27\\nSkipping line 127473: expected 26 fields, saw 27\\nSkipping line 127474: expected 26 fields, saw 27\\nSkipping line 127475: expected 26 fields, saw 27\\nSkipping line 127476: expected 26 fields, saw 27\\nSkipping line 127477: expected 26 fields, saw 27\\nSkipping line 127478: expected 26 fields, saw 27\\nSkipping line 127479: expected 26 fields, saw 27\\nSkipping line 127480: expected 26 fields, saw 27\\nSkipping line 127481: expected 26 fields, saw 27\\nSkipping line 127483: expected 26 fields, saw 28\\nSkipping line 128334: expected 26 fields, saw 32\\n'\n",
      "b'Skipping line 135857: expected 26 fields, saw 28\\nSkipping line 135861: expected 26 fields, saw 27\\nSkipping line 135883: expected 26 fields, saw 27\\nSkipping line 136381: expected 26 fields, saw 28\\n'\n",
      "b'Skipping line 237268: expected 26 fields, saw 30\\n'\n",
      "b'Skipping line 274456: expected 26 fields, saw 33\\nSkipping line 274512: expected 26 fields, saw 28\\nSkipping line 274591: expected 26 fields, saw 34\\nSkipping line 274602: expected 26 fields, saw 30\\nSkipping line 274624: expected 26 fields, saw 30\\nSkipping line 274789: expected 26 fields, saw 28\\nSkipping line 274837: expected 26 fields, saw 33\\n'\n",
      "/home/AD/ulab222/anaconda3/envs/deepchem-tf/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (7,14,15,16,17,18,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# clean up original csv and save \n",
    "import pandas as pd\n",
    "\n",
    "file ='patents_ochem_enamine_bradley_begstrom_training_.csv'\n",
    "data = pd.read_csv(file,sep=',', header=0,error_bad_lines=False, names=[\"SMILES\",\"CASRN\",\"EXTERNALID\",\"N\",\"NAME1\",\"NAME2\",\"ARTICLEID\",\"PUBMEDID\",\"PAGE\",\"TABLE\",\"Melting Point\",\"UNIT1 {Melting Point}\",\"Melting Point {measured, converted}\",\"UNIT2 {Melting Point}\",\"Dataset\",\"comment (property)\",\"comment (chemical)\",\"measurement method\",\"Pressure\",\"UNIT {Pressure}\",\"Resp set numeric condition\",\"UNIT {Resp set numeric condition}\",\"Som set numeric condition\",\"UNIT {Som set numeric condition}\",\"Quality code\",\"UNIT {Quality code}\"])\n",
    "data = pd.DataFrame(data)\n",
    "# data\n",
    "melting_point = data[[\"SMILES\", 'Melting Point {measured, converted}']]\n",
    "melting_point = melting_point.iloc[0:50000, :]\n",
    "melting_point.rename(columns={\"Melting Point {measured, converted}\": \"Melting Point\"}, inplace=True)\n",
    "melting_point.to_csv(\"melting point small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file ='patents_ochem_enamine_bradley_begstrom_training_.csv'\n",
    "data = pd.read_csv(file, header=0, error_bad_lines=False)\n",
    "data = pd.DataFrame(data)\n",
    "# data[['Melting Point {measured, converted}', \"UNIT {Melting Point}\"]]\n",
    "len(data[\"Melting Point {measured, converted}\"].notna())\n",
    "# mp = data['Melting Point'].to_numpy()\n",
    "# mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = [\"SMILES\",\"CASRN\",\"EXTERNALID\",\"N\",\"NAME1\",\"NAME2\",\"ARTICLEID\",\"PUBMEDID\",\"PAGE\",\"TABLE\",\"Melting Point\",\"UNIT1 {Melting Point}\",\"Melting Point {measured, converted}\",\"UNIT2 {Melting Point}\",\"Dataset\",\"comment (property)\",\"comment (chemical)\",\"measurement method\",\"Pressure\",\"UNIT {Pressure}\",\"Resp set numeric condition\",\"UNIT {Resp set numeric condition}\",\"Som set numeric condition\",\"UNIT {Som set numeric condition}\",\"Quality code\",\"UNIT {Quality code}\"]\n",
    "len(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file ='melting point small.csv'\n",
    "loader = dc.data.CSVLoader([\"Melting Point\"], feature_field=\"SMILES\", featurizer=dc.feat.ConvMolFeaturizer())\n",
    "\n",
    "dataset = loader.create_dataset(csv_file)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DiskDataset X.shape: (50000,), y.shape: (50000, 1), w.shape: (50000, 1), task_names: ['Melting Point']>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = dc.splits.RandomSplitter()\n",
    "train_set, valid_set, test_set = splitter.train_valid_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 09:59:16.441666: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-01-26 09:59:16.443527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-26 09:59:16.467209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.468186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.468228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-26 09:59:16.471295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-26 09:59:16.471366: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-01-26 09:59:16.474329: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-26 09:59:16.474778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-26 09:59:16.478081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-26 09:59:16.479929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-26 09:59:16.486538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-26 09:59:16.490658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-01-26 09:59:16.491267: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-26 09:59:16.700763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.701739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:06:00.0 name: GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.71GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2022-01-26 09:59:16.701799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-26 09:59:16.701856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-01-26 09:59:16.701890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-01-26 09:59:16.701922: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-01-26 09:59:16.701955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-01-26 09:59:16.701987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-01-26 09:59:16.702019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-01-26 09:59:16.702051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-26 09:59:16.705703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2022-01-26 09:59:16.705759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-01-26 09:59:18.100462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-26 09:59:18.100500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2022-01-26 09:59:18.100527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \n",
      "2022-01-26 09:59:18.100533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \n",
      "2022-01-26 09:59:18.104218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6677 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:05:00.0, compute capability: 7.5)\n",
      "2022-01-26 09:59:18.106197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7259 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:06:00.0, compute capability: 7.5)\n",
      "2022-01-26 09:59:18.106390: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "model_mp = dc.models.GraphConvModel(n_tasks=1, batch_size=50, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mp.fit(train_set, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_featurizer = dc.feat.RDKitDescriptors()\n",
    "features = rdkit_featurizer(['CCC'])[0]\n",
    "for feature, descriptor in zip(features[:10], rdkit_featurizer.descriptors):\n",
    "    print(descriptor, feature)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2674cf01e429b22e44fdf835f0657fdc4e794ce879752b76e7b10dc55fa0a096"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
