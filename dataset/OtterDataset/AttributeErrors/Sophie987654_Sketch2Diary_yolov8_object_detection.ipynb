{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1704730231821,
     "user": {
      "displayName": "jina kim",
      "userId": "01101908893114905171"
     },
     "user_tz": -540
    },
    "id": "Y8cDtxLIBHgQ",
    "outputId": "50a52ae0-6765-463f-ade0-813cde2fc260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb  2 13:36:19 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   30C    P8    22W / 450W |    169MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1664939      G   /usr/lib/xorg/Xorg                 91MiB |\n",
      "|    0   N/A  N/A   1665084      G   /usr/bin/gnome-shell               61MiB |\n",
      "|    0   N/A  N/A   1665623      G   ...8/usr/lib/firefox/firefox       14MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# GPU 정보 확인\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1704730234909,
     "user": {
      "displayName": "jina kim",
      "userId": "01101908893114905171"
     },
     "user_tz": -540
    },
    "id": "uOgOud1ER4WF",
    "outputId": "5585a09d-4ab7-4d11-c37e-713b5810ea23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/datacamp/team9/t9user2/pic\n"
     ]
    }
   ],
   "source": [
    "%cd /project/datacamp/team9/t9user2/pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.145 🚀 Python-3.7.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24209MiB)\n",
      "Setup complete ✅ (32 CPUs, 62.6 GB RAM, 1816.0/1832.2 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics # YOLOv8 설치\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임포트 라이브러리\n",
    "import os\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1704730239461,
     "user": {
      "displayName": "jina kim",
      "userId": "01101908893114905171"
     },
     "user_tz": -540
    },
    "id": "CjpPg4mGKc1v",
    "outputId": "b6c420f8-e882-42cf-ec3f-5af65f896709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/datacamp/team9/t9user2/pic\n"
     ]
    }
   ],
   "source": [
    "# 현재 작업 디렉토리 출력\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253461,
     "status": "ok",
     "timestamp": 1704730562519,
     "user": {
      "displayName": "jina kim",
      "userId": "01101908893114905171"
     },
     "user_tz": -540
    },
    "id": "OA7jb0LJj3O8",
    "outputId": "416dcdd8-efd7-4046-87aa-0ab59d10db30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.9 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.145 🚀 Python-3.7.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24209MiB)\n",
      "WARNING ⚠️ Upgrade to torch>=2.0.0 for deterministic training.\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=/project/datacamp/team9/t9user2/pic/datasets/data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8721820  ultralytics.nn.modules.head.Detect           [4, [320, 640, 640]]          \n",
      "Model summary: 365 layers, 68156460 parameters, 68156444 gradients\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /project/datacamp/team9/t9user2/pic/datasets/Training/labels/hou\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /project/datacamp/team9/t9user2/pic/datasets/Training/labels/house.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /project/datacamp/team9/t9user2/pic/datasets/Validation/labels/hou\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /project/datacamp/team9/t9user2/pic/datasets/Validation/labels/house.cache\n",
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      15.3G     0.9185      2.146      1.414         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400    0.00446    0.00667     0.0022   0.000547\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      14.3G      1.073      1.558      1.514         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.525      0.227      0.151     0.0529\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      14.3G     0.9571      1.278      1.404         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.394      0.623      0.475      0.278\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      14.3G     0.8192      1.127      1.292         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.584      0.614      0.651      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      14.3G     0.7621      1.019      1.239         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.773      0.867      0.897      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      14.3G     0.6919     0.9163      1.188         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.764      0.866      0.904      0.733\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      14.3G     0.6269     0.8501       1.14         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.852      0.842      0.907      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      14.3G     0.5616     0.7516      1.093         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400       0.84      0.856      0.917      0.741\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      14.3G     0.5114     0.6716      1.064         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.936      0.822      0.982      0.857\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      14.3G     0.4595     0.6195      1.028         16        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.902      0.949       0.98      0.867\n",
      "\n",
      "10 epochs completed in 0.176 hours.\n",
      "Optimizer stripped from runs/detect/train4/weights/last.pt, 136.7MB\n",
      "Optimizer stripped from runs/detect/train4/weights/best.pt, 136.7MB\n",
      "\n",
      "Validating runs/detect/train4/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.145 🚀 Python-3.7.16 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4090, 24209MiB)\n",
      "Model summary (fused): 268 layers, 68127420 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        400        400      0.902      0.949       0.98      0.867\n",
      "                  tree        400        100      0.838          1      0.981      0.847\n",
      "                   man        400        200      0.989      0.918      0.992      0.949\n",
      "                 house        400        100      0.879       0.93      0.966      0.806\n",
      "Speed: 0.2ms preprocess, 6.2ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(model='yolov8x.pt')\n",
    "\n",
    "model.train(data='/project/datacamp/team9/t9user2/pic/datasets/data.yaml', epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/project/datacamp/team9/t9user2/pic'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24209MiB)\n",
      "Model summary (fused): 268 layers, 68127420 parameters, 0 gradients, 257.4 GFLOPs\n",
      "\n",
      "image 1/1 /project/datacamp/team9/t9user2/pic/63681951.jpg: 640x608 1 tree, 46.7ms\n",
      "Speed: 2.1ms preprocess, 46.7ms inference, 172.8ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Results saved to \u001b[1mruns/detect/predict10\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model='runs/detect/train4/weights/best.pt' source='63681951.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/project/datacamp/team9/t9user2/pic'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ultralytics.models.yolo.model.YOLO object at 0x7efaa5f12850>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /project/datacamp/team9/t9user2/pic/test_pic.png: 416x640 1 woman, 21.1ms\n",
      "Speed: 0.8ms preprocess, 21.1ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1682195/3488049141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 탐지 결과 시각화 (결과 이미지를 직접 보여줌)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 또는 탐지 결과를 이미지 파일로 저장하려면\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# 학습된 모델 파일 경로\n",
    "model_path = 'runs/detect/train4/weights/best.pt'\n",
    "\n",
    "# 모델 로드\n",
    "model = YOLO(model_path)\n",
    "print(model)\n",
    "# 탐지를 수행할 이미지 파일 경로\n",
    "image_path = 'test_pic.png'\n",
    "\n",
    "# 이미지에 대한 탐지 수행\n",
    "results = model(image_path)\n",
    "\n",
    "# 탐지 결과 시각화 (결과 이미지를 직접 보여줌)\n",
    "results.show()\n",
    "\n",
    "# 또는 탐지 결과를 이미지 파일로 저장하려면\n",
    "results.save(save_dir='t9user2/pic/test_pic.png')\n",
    "\n",
    "# 추가적으로, 탐지된 객체의 정보를 확인하려면\n",
    "detected_objects = results.pandas().xyxy[0]  # 탐지된 객체의 정보가 DataFrame 형태로 저장됨\n",
    "print(detected_objects)\n",
    "\n",
    "# 이미지를 OpenCV를 사용하여 불러오고 시각화하기 (선택적)\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # OpenCV는 BGR을 사용하므로 RGB로 변환\n",
    "plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ultralytics.models.yolo.model.YOLO object at 0x7efaa5f12850>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1682195/4209481091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 탐지된 객체의 정보를 DataFrame으로부터 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdetected_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 이미지 읽기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'pandas'"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# 탐지된 객체의 정보를 DataFrame으로부터 추출\n",
    "detected_objects = results.pandas().xyxy[0]\n",
    "\n",
    "# 이미지 읽기\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # OpenCV는 BGR 형식을 사용하므로 RGB로 변환\n",
    "\n",
    "# 그림을 그릴 준비\n",
    "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "ax.imshow(img)\n",
    "\n",
    "# 각 탐지된 객체에 대해 바운딩 박스와 레이블 추가\n",
    "for index, row in detected_objects.iterrows():\n",
    "    box = patches.Rectangle((row['xmin'], row['ymin']), row['xmax']-row['xmin'], row['ymax']-row['ymin'], linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(box)\n",
    "    plt.text(row['xmin'], row['ymin'], f\"{row['name']} {row['confidence']:.2f}\", bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "keys: ['boxes']\n",
      "masks: None\n",
      "names: {0: 'tree', 1: 'man', 2: 'woman', 3: 'house'}\n",
      "orig_img: array([[[213, 218, 217],\n",
      "        [213, 218, 217],\n",
      "        [213, 218, 217],\n",
      "        ...,\n",
      "        [141, 151, 154],\n",
      "        [146, 151, 148],\n",
      "        [164, 148, 124]],\n",
      "\n",
      "       [[213, 218, 217],\n",
      "        [213, 218, 217],\n",
      "        [213, 218, 217],\n",
      "        ...,\n",
      "        [141, 151, 154],\n",
      "        [142, 152, 155],\n",
      "        [149, 151, 146]],\n",
      "\n",
      "       [[213, 218, 217],\n",
      "        [213, 218, 217],\n",
      "        [213, 218, 217],\n",
      "        ...,\n",
      "        [141, 151, 154],\n",
      "        [142, 152, 155],\n",
      "        [144, 152, 153]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[200, 202, 197],\n",
      "        [200, 202, 197],\n",
      "        [200, 202, 197],\n",
      "        ...,\n",
      "        [229, 231, 226],\n",
      "        [229, 231, 226],\n",
      "        [229, 231, 226]],\n",
      "\n",
      "       [[200, 202, 197],\n",
      "        [200, 202, 197],\n",
      "        [200, 202, 197],\n",
      "        ...,\n",
      "        [229, 231, 226],\n",
      "        [229, 231, 226],\n",
      "        [229, 231, 226]],\n",
      "\n",
      "       [[200, 202, 197],\n",
      "        [200, 202, 197],\n",
      "        [200, 202, 197],\n",
      "        ...,\n",
      "        [229, 231, 226],\n",
      "        [229, 231, 226],\n",
      "        [229, 231, 226]]], dtype=uint8)\n",
      "orig_shape: (770, 1208)\n",
      "path: '/project/datacamp/team9/t9user2/pic/test_pic.png'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 0.7765293121337891, 'inference': 21.812915802001953, 'postprocess': 0.46372413635253906}]\n"
     ]
    }
   ],
   "source": [
    "# results 객체의 타입과 내용 확인\n",
    "print(type(results))\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1682195/1889267440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetected_boxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 바운딩 박스 정보 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "# results 리스트에서 첫 번째 Results 객체 추출\n",
    "result = results[0]\n",
    "\n",
    "# 탐지된 객체의 정보 접근하기\n",
    "detected_boxes = result.boxes  # 탐지된 객체의 바운딩 박스 정보\n",
    "names = result.names  # 클래스 이름\n",
    "\n",
    "# 이미지 읽기\n",
    "img = result.orig_img\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # OpenCV는 BGR 형식을 사용하므로 RGB로 변환\n",
    "\n",
    "# 그림을 그릴 준비\n",
    "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "ax.imshow(img)\n",
    "\n",
    "# 각 탐지된 객체에 대해 바운딩 박스와 레이블 추가\n",
    "for box in detected_boxes:\n",
    "    # 바운딩 박스 정보 추출\n",
    "    x1, y1, x2, y2 = box.xyxy\n",
    "    label = names[box.class_idx]\n",
    "    confidence = box.confidence\n",
    "\n",
    "    # 바운딩 박스 그리기\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    # 레이블과 신뢰도 표시\n",
    "    plt.text(x1, y1, f\"{label} {confidence:.2f}\", bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 모델 파일 경로 지정\n",
    "model_path = 'runs/detect/train4/weights/best.pt'\n",
    "\n",
    "# 모델 로드\n",
    "model = YOLO(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /project/datacamp/team9/t9user2/pic/test_pic.png: 416x640 1 woman, 21.4ms\n",
      "Speed: 0.7ms preprocess, 21.4ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    }
   ],
   "source": [
    "# 탐지를 수행할 새로운 이미지 파일 경로\n",
    "image_path = 'test_pic.png'\n",
    "\n",
    "# 이미지에 대한 탐지 수행\n",
    "results = model(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1682195/4261146470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 탐지 결과 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 탐지 결과 저장 (선택적)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'path/to/save/results/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# 탐지 결과 시각화\n",
    "results.show()\n",
    "\n",
    "# 탐지 결과 저장 (선택적)\n",
    "results.save(save_dir='path/to/save/results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ratios(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Calculate the relative position and size of the bounding box in relation to the image size.\n",
    "\n",
    "    Parameters:\n",
    "    bbox (tuple): A tuple of (x_min, y_min, x_max, y_max) coordinates of the bounding box.\n",
    "    img_width (int): Width of the image.\n",
    "    img_height (int): Height of the image.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the relative x_center, y_center, width, and height of the bounding box.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    \n",
    "    # Calculate the center coordinates\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    \n",
    "    # Calculate width and height of the bounding box\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    \n",
    "    # Calculate ratios\n",
    "    x_center_ratio = x_center / img_width\n",
    "    y_center_ratio = y_center / img_height\n",
    "    width_ratio = width / img_width\n",
    "    height_ratio = height / img_height\n",
    "    \n",
    "    return x_center_ratio, y_center_ratio, width_ratio, height_ratio\n",
    "\n",
    "# Example usage:\n",
    "bbox = (100, 150, 400, 300)  # Replace with actual bounding box coordinates\n",
    "img_width = 1280  # Replace with actual image width\n",
    "img_height = 720  # Replace with actual image height\n",
    "ratios = calculate_ratios(bbox, img_width, img_height)\n",
    "print(ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /project/datacamp/team9/t9user2/pic/63681951.jpg: 640x608 1 tree, 26.8ms\n",
      "Speed: 1.0ms preprocess, 26.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 608)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1682195/1784214395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 탐지된 객체의 바운딩 박스 좌표를 가져옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 결과에서 바운딩 박스 좌표를 numpy 배열로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 모든 탐지된 객체에 대한 비율 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'xyxy'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# 이미지 로드 및 모델 초기화\n",
    "image_path = '63681951.jpg'  # 이미지 경로 설정\n",
    "model_path = 'runs/detect/train4/weights/best.pt'  # 모델 경로 설정\n",
    "\n",
    "# 모델 로드\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# 이미지에 대한 탐지 수행\n",
    "results = model(image_path)\n",
    "\n",
    "# 이미지의 원래 크기 가져오기\n",
    "img = cv2.imread(image_path)  # 이미지를 읽어옴\n",
    "img_height, img_width, _ = img.shape  # 이미지의 높이와 너비 추출\n",
    "\n",
    "# 탐지된 객체의 바운딩 박스 좌표를 가져옴\n",
    "bboxes = results.xyxy[0].numpy()  # 결과에서 바운딩 박스 좌표를 numpy 배열로 변환\n",
    "\n",
    "# 모든 탐지된 객체에 대한 비율 계산\n",
    "for bbox in bboxes:\n",
    "    ratios = calculate_ratios((bbox[0], bbox[1], bbox[2], bbox[3]), img_width, img_height)\n",
    "    print(f\"Object Class: {int(bbox[5])} - Ratios: {ratios}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
