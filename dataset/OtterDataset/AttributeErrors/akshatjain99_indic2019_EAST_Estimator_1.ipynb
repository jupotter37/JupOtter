{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EAST - Estimator 1",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnlg6gimJhUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import sys\n",
        "import csv\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import itertools\n",
        "from multiprocessing import Pool\n",
        "import threading\n",
        "import numpy as np\n",
        "import scipy.optimize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as Patches\n",
        "from shapely.geometry import Polygon\n",
        "import keras\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, concatenate, BatchNormalization, Lambda, Input, multiply, add, ZeroPadding2D, Activation, Layer, MaxPooling2D, Dropout\n",
        "from keras import regularizers\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnHY6I6TJnk4",
        "colab_type": "code",
        "outputId": "c87b8992-fe25-42ae-9d02-660f64e382fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSzGrdTczXgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_pattern = '/gdrive/My Drive/indic2019/ICDAR Dataset/images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9-PfD9FVzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RESIZE_FACTOR = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClubWHvIPCOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_images(data_path):\n",
        "    files = []\n",
        "    idx = 0\n",
        "    for ext in ['jpg', 'png', 'jpeg', 'JPG']:\n",
        "        files.extend(glob.glob(\n",
        "            os.path.join(data_path, '*.{}'.format(ext))))\n",
        "        idx += 1\n",
        "    return files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n_sPaZZPGQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_annotation(p):\n",
        "    '''\n",
        "    load annotation from the text file\n",
        "    :param p:\n",
        "    :return:\n",
        "    '''\n",
        "    text_polys = []\n",
        "    text_tags = []\n",
        "    with open(p, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        for line in reader:\n",
        "            label = line[-1]\n",
        "            # strip BOM. \\ufeff for python3,  \\xef\\xbb\\bf for python2\n",
        "            line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line]\n",
        "\n",
        "            x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n",
        "            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n",
        "            if label == '*' or label == '###':\n",
        "                text_tags.append(True)\n",
        "            else:\n",
        "                text_tags.append(False)\n",
        "        return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J20dcTl1H17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def polygon_area(poly):\n",
        "    '''\n",
        "    compute area of a polygon\n",
        "    :param poly:\n",
        "    :return:\n",
        "    '''\n",
        "    edge = [\n",
        "        (poly[1][0] - poly[0][0]) * (poly[1][1] + poly[0][1]),\n",
        "        (poly[2][0] - poly[1][0]) * (poly[2][1] + poly[1][1]),\n",
        "        (poly[3][0] - poly[2][0]) * (poly[3][1] + poly[2][1]),\n",
        "        (poly[0][0] - poly[3][0]) * (poly[0][1] + poly[3][1])\n",
        "    ]\n",
        "    return np.sum(edge)/2."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_T8j2pePZ8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_and_validate_polys(FLAGS, polys, tags, size):\n",
        "    '''\n",
        "    check so that the text poly is in the same direction,\n",
        "    and also filter some invalid polygons\n",
        "    :param polys:\n",
        "    :param tags:\n",
        "    :return:\n",
        "    '''\n",
        "    (h, w) = size\n",
        "    if polys.shape[0] == 0:\n",
        "        return polys\n",
        "    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w-1)\n",
        "    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h-1)\n",
        "\n",
        "    validated_polys = []\n",
        "    validated_tags = []\n",
        "    for poly, tag in zip(polys, tags):\n",
        "        p_area = polygon_area(poly)\n",
        "        if abs(p_area) < 1:\n",
        "            # print poly\n",
        "            if not FLAGS.suppress_warnings_and_error_messages:\n",
        "                print('invalid poly')\n",
        "            continue\n",
        "        if p_area > 0:\n",
        "            if not FLAGS.suppress_warnings_and_error_messages:\n",
        "                print('poly in wrong direction')\n",
        "            poly = poly[(0, 3, 2, 1), :]\n",
        "        validated_polys.append(poly)\n",
        "        validated_tags.append(tag)\n",
        "    return np.array(validated_polys), np.array(validated_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abbApzPjPdKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_area(FLAGS, im, polys, tags, crop_background=False, max_tries=50):\n",
        "    '''\n",
        "    make random crop from the input image\n",
        "    :param im:\n",
        "    :param polys:\n",
        "    :param tags:\n",
        "    :param crop_background:\n",
        "    :param max_tries:\n",
        "    :return:\n",
        "    '''\n",
        "    h, w, _ = im.shape\n",
        "    pad_h = h//10\n",
        "    pad_w = w//10\n",
        "    h_array = np.zeros((h + pad_h*2), dtype=np.int32)\n",
        "    w_array = np.zeros((w + pad_w*2), dtype=np.int32)\n",
        "    for poly in polys:\n",
        "        poly = np.round(poly, decimals=0).astype(np.int32)\n",
        "        minx = np.min(poly[:, 0])\n",
        "        maxx = np.max(poly[:, 0])\n",
        "        w_array[minx+pad_w:maxx+pad_w] = 1\n",
        "        miny = np.min(poly[:, 1])\n",
        "        maxy = np.max(poly[:, 1])\n",
        "        h_array[miny+pad_h:maxy+pad_h] = 1\n",
        "    # ensure the cropped area not across a text\n",
        "    h_axis = np.where(h_array == 0)[0]\n",
        "    w_axis = np.where(w_array == 0)[0]\n",
        "    if len(h_axis) == 0 or len(w_axis) == 0:\n",
        "        return im, polys, tags\n",
        "    for i in range(max_tries):\n",
        "        xx = np.random.choice(w_axis, size=2)\n",
        "        xmin = np.min(xx) - pad_w\n",
        "        xmax = np.max(xx) - pad_w\n",
        "        xmin = np.clip(xmin, 0, w-1)\n",
        "        xmax = np.clip(xmax, 0, w-1)\n",
        "        yy = np.random.choice(h_axis, size=2)\n",
        "        ymin = np.min(yy) - pad_h\n",
        "        ymax = np.max(yy) - pad_h\n",
        "        ymin = np.clip(ymin, 0, h-1)\n",
        "        ymax = np.clip(ymax, 0, h-1)\n",
        "        if xmax - xmin < FLAGS.min_crop_side_ratio*w or ymax - ymin < FLAGS.min_crop_side_ratio*h:\n",
        "            # area too small\n",
        "            continue\n",
        "        if polys.shape[0] != 0:\n",
        "            poly_axis_in_area = (polys[:, :, 0] >= xmin) & (polys[:, :, 0] <= xmax) \\\n",
        "                                & (polys[:, :, 1] >= ymin) & (polys[:, :, 1] <= ymax)\n",
        "            selected_polys = np.where(np.sum(poly_axis_in_area, axis=1) == 4)[0]\n",
        "        else:\n",
        "            selected_polys = []\n",
        "        if len(selected_polys) == 0:\n",
        "            # no text in this area\n",
        "            if crop_background:\n",
        "                return im[ymin:ymax+1, xmin:xmax+1, :], polys[selected_polys], tags[selected_polys]\n",
        "            else:\n",
        "                continue\n",
        "        im = im[ymin:ymax+1, xmin:xmax+1, :]\n",
        "        polys = polys[selected_polys]\n",
        "        tags = tags[selected_polys]\n",
        "        polys[:, :, 0] -= xmin\n",
        "        polys[:, :, 1] -= ymin\n",
        "        return im, polys, tags\n",
        "\n",
        "    return im, polys, tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGw4ByoCPhdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shrink_poly(poly, r):\n",
        "    '''\n",
        "    fit a poly inside the origin poly, maybe bugs here...\n",
        "    used for generating the score map\n",
        "    :param poly: the text poly\n",
        "    :param r: r in the paper\n",
        "    :return: the shrinked poly\n",
        "    '''\n",
        "    # shrink ratio\n",
        "    R = 0.3\n",
        "    # find the longer pair\n",
        "    if np.linalg.norm(poly[0] - poly[1]) + np.linalg.norm(poly[2] - poly[3]) > \\\n",
        "                    np.linalg.norm(poly[0] - poly[3]) + np.linalg.norm(poly[1] - poly[2]):\n",
        "        # first move (p0, p1), (p2, p3), then (p0, p3), (p1, p2)\n",
        "        ## p0, p1\n",
        "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
        "        poly[0][0] += R * r[0] * np.cos(theta)\n",
        "        poly[0][1] += R * r[0] * np.sin(theta)\n",
        "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
        "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
        "        ## p2, p3\n",
        "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
        "        poly[3][0] += R * r[3] * np.cos(theta)\n",
        "        poly[3][1] += R * r[3] * np.sin(theta)\n",
        "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
        "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
        "        ## p0, p3\n",
        "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
        "        poly[0][0] += R * r[0] * np.sin(theta)\n",
        "        poly[0][1] += R * r[0] * np.cos(theta)\n",
        "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
        "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
        "        ## p1, p2\n",
        "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
        "        poly[1][0] += R * r[1] * np.sin(theta)\n",
        "        poly[1][1] += R * r[1] * np.cos(theta)\n",
        "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
        "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
        "    else:\n",
        "        ## p0, p3\n",
        "        # print poly\n",
        "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
        "        poly[0][0] += R * r[0] * np.sin(theta)\n",
        "        poly[0][1] += R * r[0] * np.cos(theta)\n",
        "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
        "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
        "        ## p1, p2\n",
        "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
        "        poly[1][0] += R * r[1] * np.sin(theta)\n",
        "        poly[1][1] += R * r[1] * np.cos(theta)\n",
        "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
        "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
        "        ## p0, p1\n",
        "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
        "        poly[0][0] += R * r[0] * np.cos(theta)\n",
        "        poly[0][1] += R * r[0] * np.sin(theta)\n",
        "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
        "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
        "        ## p2, p3\n",
        "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
        "        poly[3][0] += R * r[3] * np.cos(theta)\n",
        "        poly[3][1] += R * r[3] * np.sin(theta)\n",
        "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
        "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
        "    return poly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yylEZ1g8Plja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_dist_to_line(p1, p2, p3):\n",
        "    # compute the distance from p3 to p1-p2\n",
        "    return np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euMMR4tAPnSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_line(p1, p2):\n",
        "    # fit a line ax+by+c = 0\n",
        "    if p1[0] == p1[1]:\n",
        "        return [1., 0., -p1[0]]\n",
        "    else:\n",
        "        [k, b] = np.polyfit(p1, p2, deg=1)\n",
        "        return [k, -1., b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKsSWnTFPpAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def line_cross_point(FLAGS, line1, line2):\n",
        "    # line1 0= ax+by+c, compute the cross point of line1 and line2\n",
        "    if line1[0] != 0 and line1[0] == line2[0]:\n",
        "        if not FLAGS.suppress_warnings_and_error_messages:\n",
        "            print('Cross point does not exist')\n",
        "        return None\n",
        "    if line1[0] == 0 and line2[0] == 0:\n",
        "        if not FLAGS.suppress_warnings_and_error_messages:\n",
        "            print('Cross point does not exist')\n",
        "        return None\n",
        "    if line1[1] == 0:\n",
        "        x = -line1[2]\n",
        "        y = line2[0] * x + line2[2]\n",
        "    elif line2[1] == 0:\n",
        "        x = -line2[2]\n",
        "        y = line1[0] * x + line1[2]\n",
        "    else:\n",
        "        k1, _, b1 = line1\n",
        "        k2, _, b2 = line2\n",
        "        x = -(b1-b2)/(k1-k2)\n",
        "        y = k1*x + b1\n",
        "    return np.array([x, y], dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD5ioz9nPrDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def line_verticle(line, point):\n",
        "    # get the verticle line from line across point\n",
        "    if line[1] == 0:\n",
        "        verticle = [0, -1, point[1]]\n",
        "    else:\n",
        "        if line[0] == 0:\n",
        "            verticle = [1, 0, -point[0]]\n",
        "        else:\n",
        "            verticle = [-1./line[0], -1, point[1] - (-1/line[0] * point[0])]\n",
        "    return verticle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U53Y9x0lPs4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rectangle_from_parallelogram(FLAGS, poly):\n",
        "    '''\n",
        "    fit a rectangle from a parallelogram\n",
        "    :param poly:\n",
        "    :return:\n",
        "    '''\n",
        "    p0, p1, p2, p3 = poly\n",
        "    angle_p0 = np.arccos(np.dot(p1-p0, p3-p0)/(np.linalg.norm(p0-p1) * np.linalg.norm(p3-p0)))\n",
        "    if angle_p0 < 0.5 * np.pi:\n",
        "        if np.linalg.norm(p0 - p1) > np.linalg.norm(p0-p3):\n",
        "            # p0 and p2\n",
        "            ## p0\n",
        "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
        "            p2p3_verticle = line_verticle(p2p3, p0)\n",
        "\n",
        "            new_p3 = line_cross_point(FLAGS, p2p3, p2p3_verticle)\n",
        "            ## p2\n",
        "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
        "            p0p1_verticle = line_verticle(p0p1, p2)\n",
        "\n",
        "            new_p1 = line_cross_point(FLAGS, p0p1, p0p1_verticle)\n",
        "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
        "        else:\n",
        "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
        "            p1p2_verticle = line_verticle(p1p2, p0)\n",
        "\n",
        "            new_p1 = line_cross_point(FLAGS, p1p2, p1p2_verticle)\n",
        "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
        "            p0p3_verticle = line_verticle(p0p3, p2)\n",
        "\n",
        "            new_p3 = line_cross_point(FLAGS, p0p3, p0p3_verticle)\n",
        "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
        "    else:\n",
        "        if np.linalg.norm(p0-p1) > np.linalg.norm(p0-p3):\n",
        "            # p1 and p3\n",
        "            ## p1\n",
        "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
        "            p2p3_verticle = line_verticle(p2p3, p1)\n",
        "\n",
        "            new_p2 = line_cross_point(FLAGS, p2p3, p2p3_verticle)\n",
        "            ## p3\n",
        "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
        "            p0p1_verticle = line_verticle(p0p1, p3)\n",
        "\n",
        "            new_p0 = line_cross_point(FLAGS, p0p1, p0p1_verticle)\n",
        "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
        "        else:\n",
        "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
        "            p0p3_verticle = line_verticle(p0p3, p1)\n",
        "\n",
        "            new_p0 = line_cross_point(FLAGS, p0p3, p0p3_verticle)\n",
        "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
        "            p1p2_verticle = line_verticle(p1p2, p3)\n",
        "\n",
        "            new_p2 = line_cross_point(FLAGS, p1p2, p1p2_verticle)\n",
        "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO8RN9sAPza5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_rectangle(FLAGS, poly):\n",
        "    # sort the four coordinates of the polygon, points in poly should be sorted clockwise\n",
        "    # First find the lowest point\n",
        "    p_lowest = np.argmax(poly[:, 1])\n",
        "    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n",
        "        # if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n",
        "        p0_index = np.argmin(np.sum(poly, axis=1))\n",
        "        p1_index = (p0_index + 1) % 4\n",
        "        p2_index = (p0_index + 2) % 4\n",
        "        p3_index = (p0_index + 3) % 4\n",
        "        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n",
        "    else:\n",
        "        # find the point that sits right to the lowest point\n",
        "        p_lowest_right = (p_lowest - 1) % 4\n",
        "        p_lowest_left = (p_lowest + 1) % 4\n",
        "        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\n",
        "        # assert angle > 0\n",
        "        if angle <= 0:\n",
        "            if not FLAGS.suppress_warnings_and_error_messages:\n",
        "                print(angle, poly[p_lowest], poly[p_lowest_right])\n",
        "        if angle/np.pi * 180 > 45:\n",
        "            # 这个点为p2 - this point is p2\n",
        "            p2_index = p_lowest\n",
        "            p1_index = (p2_index - 1) % 4\n",
        "            p0_index = (p2_index - 2) % 4\n",
        "            p3_index = (p2_index + 1) % 4\n",
        "            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\n",
        "        else:\n",
        "            # 这个点为p3 - this point is p3\n",
        "            p3_index = p_lowest\n",
        "            p0_index = (p3_index + 1) % 4\n",
        "            p1_index = (p3_index + 2) % 4\n",
        "            p2_index = (p3_index + 3) % 4\n",
        "            return poly[[p0_index, p1_index, p2_index, p3_index]], angle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHqnXBvqP2ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def restore_rectangle_rbox(origin, geometry):\n",
        "    d = geometry[:, :4]\n",
        "    angle = geometry[:, 4]\n",
        "    # for angle > 0\n",
        "    origin_0 = origin[angle >= 0]\n",
        "    d_0 = d[angle >= 0]\n",
        "    angle_0 = angle[angle >= 0]\n",
        "    if origin_0.shape[0] > 0:\n",
        "        p = np.array([np.zeros(d_0.shape[0]), -d_0[:, 0] - d_0[:, 2],\n",
        "                      d_0[:, 1] + d_0[:, 3], -d_0[:, 0] - d_0[:, 2],\n",
        "                      d_0[:, 1] + d_0[:, 3], np.zeros(d_0.shape[0]),\n",
        "                      np.zeros(d_0.shape[0]), np.zeros(d_0.shape[0]),\n",
        "                      d_0[:, 3], -d_0[:, 2]])\n",
        "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
        "\n",
        "        rotate_matrix_x = np.array([np.cos(angle_0), np.sin(angle_0)]).transpose((1, 0))\n",
        "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
        "\n",
        "        rotate_matrix_y = np.array([-np.sin(angle_0), np.cos(angle_0)]).transpose((1, 0))\n",
        "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
        "\n",
        "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "\n",
        "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
        "\n",
        "        p3_in_origin = origin_0 - p_rotate[:, 4, :]\n",
        "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
        "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
        "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
        "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
        "\n",
        "        new_p_0 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
        "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
        "    else:\n",
        "        new_p_0 = np.zeros((0, 4, 2))\n",
        "    # for angle < 0\n",
        "    origin_1 = origin[angle < 0]\n",
        "    d_1 = d[angle < 0]\n",
        "    angle_1 = angle[angle < 0]\n",
        "    if origin_1.shape[0] > 0:\n",
        "        p = np.array([-d_1[:, 1] - d_1[:, 3], -d_1[:, 0] - d_1[:, 2],\n",
        "                      np.zeros(d_1.shape[0]), -d_1[:, 0] - d_1[:, 2],\n",
        "                      np.zeros(d_1.shape[0]), np.zeros(d_1.shape[0]),\n",
        "                      -d_1[:, 1] - d_1[:, 3], np.zeros(d_1.shape[0]),\n",
        "                      -d_1[:, 1], -d_1[:, 2]])\n",
        "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
        "\n",
        "        rotate_matrix_x = np.array([np.cos(-angle_1), -np.sin(-angle_1)]).transpose((1, 0))\n",
        "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
        "\n",
        "        rotate_matrix_y = np.array([np.sin(-angle_1), np.cos(-angle_1)]).transpose((1, 0))\n",
        "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
        "\n",
        "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "\n",
        "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
        "\n",
        "        p3_in_origin = origin_1 - p_rotate[:, 4, :]\n",
        "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
        "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
        "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
        "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
        "\n",
        "        new_p_1 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
        "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
        "    else:\n",
        "        new_p_1 = np.zeros((0, 4, 2))\n",
        "    return np.concatenate([new_p_0, new_p_1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROAAONULP5yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def restore_rectangle(origin, geometry):\n",
        "    return restore_rectangle_rbox(origin, geometry)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82JnJo2iP7-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_rbox(FLAGS, im_size, polys, tags):\n",
        "    h, w = im_size\n",
        "    shrinked_poly_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    orig_poly_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    score_map = np.zeros((h, w), dtype=np.uint8)\n",
        "    geo_map = np.zeros((h, w, 5), dtype=np.float32)\n",
        "    # mask used during traning, to ignore some hard areas\n",
        "    overly_small_text_region_training_mask = np.ones((h, w), dtype=np.uint8)\n",
        "    for poly_idx, poly_data in enumerate(zip(polys, tags)):\n",
        "        poly = poly_data[0]\n",
        "        tag = poly_data[1]\n",
        "\n",
        "        r = [None, None, None, None]\n",
        "        for i in range(4):\n",
        "            r[i] = min(np.linalg.norm(poly[i] - poly[(i + 1) % 4]),\n",
        "                       np.linalg.norm(poly[i] - poly[(i - 1) % 4]))\n",
        "        # score map\n",
        "        shrinked_poly = shrink_poly(poly.copy(), r).astype(np.int32)[np.newaxis, :, :]\n",
        "        cv2.fillPoly(score_map, shrinked_poly, 1)\n",
        "        cv2.fillPoly(shrinked_poly_mask, shrinked_poly, poly_idx + 1)\n",
        "        cv2.fillPoly(orig_poly_mask, poly.astype(np.int32)[np.newaxis, :, :], 1)\n",
        "        # if the poly is too small, then ignore it during training\n",
        "        poly_h = min(np.linalg.norm(poly[0] - poly[3]), np.linalg.norm(poly[1] - poly[2]))\n",
        "        poly_w = min(np.linalg.norm(poly[0] - poly[1]), np.linalg.norm(poly[2] - poly[3]))\n",
        "        if min(poly_h, poly_w) < FLAGS.min_text_size:\n",
        "            cv2.fillPoly(overly_small_text_region_training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
        "        if tag:\n",
        "            cv2.fillPoly(overly_small_text_region_training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
        "\n",
        "        xy_in_poly = np.argwhere(shrinked_poly_mask == (poly_idx + 1))\n",
        "        # if geometry == 'RBOX':\n",
        "        # generate a parallelogram for any combination of two vertices\n",
        "        fitted_parallelograms = []\n",
        "        for i in range(4):\n",
        "            p0 = poly[i]\n",
        "            p1 = poly[(i + 1) % 4]\n",
        "            p2 = poly[(i + 2) % 4]\n",
        "            p3 = poly[(i + 3) % 4]\n",
        "            edge = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
        "            backward_edge = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
        "            forward_edge = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
        "            if point_dist_to_line(p0, p1, p2) > point_dist_to_line(p0, p1, p3):\n",
        "                # parallel lines through p2\n",
        "                if edge[1] == 0:\n",
        "                    edge_opposite = [1, 0, -p2[0]]\n",
        "                else:\n",
        "                    edge_opposite = [edge[0], -1, p2[1] - edge[0] * p2[0]]\n",
        "            else:\n",
        "                # after p3\n",
        "                if edge[1] == 0:\n",
        "                    edge_opposite = [1, 0, -p3[0]]\n",
        "                else:\n",
        "                    edge_opposite = [edge[0], -1, p3[1] - edge[0] * p3[0]]\n",
        "            # move forward edge\n",
        "            new_p0 = p0\n",
        "            new_p1 = p1\n",
        "            new_p2 = p2\n",
        "            new_p3 = p3\n",
        "            new_p2 = line_cross_point(FLAGS, forward_edge, edge_opposite)\n",
        "            if point_dist_to_line(p1, new_p2, p0) > point_dist_to_line(p1, new_p2, p3):\n",
        "                # across p0\n",
        "                if forward_edge[1] == 0:\n",
        "                    forward_opposite = [1, 0, -p0[0]]\n",
        "                else:\n",
        "                    forward_opposite = [forward_edge[0], -1, p0[1] - forward_edge[0] * p0[0]]\n",
        "            else:\n",
        "                # across p3\n",
        "                if forward_edge[1] == 0:\n",
        "                    forward_opposite = [1, 0, -p3[0]]\n",
        "                else:\n",
        "                    forward_opposite = [forward_edge[0], -1, p3[1] - forward_edge[0] * p3[0]]\n",
        "            new_p0 = line_cross_point(FLAGS, forward_opposite, edge)\n",
        "            new_p3 = line_cross_point(FLAGS, forward_opposite, edge_opposite)\n",
        "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
        "            # or move backward edge\n",
        "            new_p0 = p0\n",
        "            new_p1 = p1\n",
        "            new_p2 = p2\n",
        "            new_p3 = p3\n",
        "            new_p3 = line_cross_point(FLAGS, backward_edge, edge_opposite)\n",
        "            if point_dist_to_line(p0, p3, p1) > point_dist_to_line(p0, p3, p2):\n",
        "                # across p1\n",
        "                if backward_edge[1] == 0:\n",
        "                    backward_opposite = [1, 0, -p1[0]]\n",
        "                else:\n",
        "                    backward_opposite = [backward_edge[0], -1, p1[1] - backward_edge[0] * p1[0]]\n",
        "            else:\n",
        "                # across p2\n",
        "                if backward_edge[1] == 0:\n",
        "                    backward_opposite = [1, 0, -p2[0]]\n",
        "                else:\n",
        "                    backward_opposite = [backward_edge[0], -1, p2[1] - backward_edge[0] * p2[0]]\n",
        "            new_p1 = line_cross_point(FLAGS, backward_opposite, edge)\n",
        "            new_p2 = line_cross_point(FLAGS, backward_opposite, edge_opposite)\n",
        "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
        "        areas = [Polygon(t).area for t in fitted_parallelograms]\n",
        "        parallelogram = np.array(fitted_parallelograms[np.argmin(areas)][:-1], dtype=np.float32)\n",
        "        # sort thie polygon\n",
        "        parallelogram_coord_sum = np.sum(parallelogram, axis=1)\n",
        "        min_coord_idx = np.argmin(parallelogram_coord_sum)\n",
        "        parallelogram = parallelogram[\n",
        "            [min_coord_idx, (min_coord_idx + 1) % 4, (min_coord_idx + 2) % 4, (min_coord_idx + 3) % 4]]\n",
        "\n",
        "        rectange = rectangle_from_parallelogram(FLAGS, parallelogram)\n",
        "        rectange, rotate_angle = sort_rectangle(FLAGS, rectange)\n",
        "\n",
        "        p0_rect, p1_rect, p2_rect, p3_rect = rectange\n",
        "        for y, x in xy_in_poly:\n",
        "            point = np.array([x, y], dtype=np.float32)\n",
        "            # top\n",
        "            geo_map[y, x, 0] = point_dist_to_line(p0_rect, p1_rect, point)\n",
        "            # right\n",
        "            geo_map[y, x, 1] = point_dist_to_line(p1_rect, p2_rect, point)\n",
        "            # down\n",
        "            geo_map[y, x, 2] = point_dist_to_line(p2_rect, p3_rect, point)\n",
        "            # left\n",
        "            geo_map[y, x, 3] = point_dist_to_line(p3_rect, p0_rect, point)\n",
        "            # angle\n",
        "            geo_map[y, x, 4] = rotate_angle\n",
        "    \n",
        "    shrinked_poly_mask = (shrinked_poly_mask > 0).astype('uint8')\n",
        "    text_region_boundary_training_mask = 1 - (orig_poly_mask - shrinked_poly_mask)\n",
        "\n",
        "    return score_map, geo_map, overly_small_text_region_training_mask, text_region_boundary_training_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe_oPznxQEki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def all(iterable):\n",
        "    for element in iterable:\n",
        "        if not element:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzsjD4BkQR3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_file(image_file):\n",
        "    txt_file = image_file.replace(os.path.basename(image_file).split('.')[1], 'txt')\n",
        "    txt_file_name = txt_file.split('/')[-1]\n",
        "    txt_file = txt_file.replace(txt_file_name, 'gt_' + txt_file_name)\n",
        "    return txt_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9KdIREUQTcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_image(img, input_size, is_train):\n",
        "    new_h, new_w, _ = img.shape\n",
        "    max_h_w_i = np.max([new_h, new_w, input_size])\n",
        "    img_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8)\n",
        "    if is_train:\n",
        "        shift_h = np.random.randint(max_h_w_i - new_h + 1)\n",
        "        shift_w = np.random.randint(max_h_w_i - new_w + 1)\n",
        "    else:\n",
        "        shift_h = (max_h_w_i - new_h) // 2\n",
        "        shift_w = (max_h_w_i - new_w) // 2\n",
        "    img_padded[shift_h:new_h+shift_h, shift_w:new_w+shift_w, :] = img.copy()\n",
        "    img = img_padded\n",
        "    return img, shift_h, shift_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH6JQDqwQWAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_image(img, text_polys, input_size, shift_h, shift_w):\n",
        "    new_h, new_w, _ = img.shape\n",
        "    img = cv2.resize(img, dsize=(input_size, input_size))\n",
        "    # pad and resize text polygons\n",
        "    resize_ratio_3_x = input_size/float(new_w)\n",
        "    resize_ratio_3_y = input_size/float(new_h)\n",
        "    text_polys[:, :, 0] += shift_w\n",
        "    text_polys[:, :, 1] += shift_h\n",
        "    text_polys[:, :, 0] *= resize_ratio_3_x\n",
        "    text_polys[:, :, 1] *= resize_ratio_3_y\n",
        "    return img, text_polys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCT0sq6BQYB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data_process(image_file, FLAGS, is_train):\n",
        "  img_raw = tf.read_file(image_file)\n",
        "  img = tf.image.decode_image(img_raw)\n",
        "  #img = cv2.imread(image_file)  #read the image file\n",
        "  h = img.shape[0] #get the shapes\n",
        "  w = img.shape[1]\n",
        "  txt_file = get_text_file(image_file) #get the annotation file. assuming that the annotation file is named gt_img_15.txt for an image file called img_15.jpg which is the standard ICDAR Format\n",
        "\n",
        "  text_polys, text_tags = load_annotation(txt_file) #load the annotation file\n",
        "  text_polys, text_tags = check_and_validate_polys(FLAGS, text_polys, text_tags, (h, w)) \n",
        "  img, shift_h, shift_w = pad_image(img, FLAGS.input_size, is_train=is_train) \n",
        "  img, text_polys = resize_image(img, text_polys, FLAGS.input_size, shift_h, shift_w) #resizing the image\n",
        "  new_h, new_w, channels = img.shape\n",
        "  score_map, geo_map, overly_small_text_region_training_mask, text_region_boundary_training_mask = generate_rbox(FLAGS, (new_h, new_w), text_polys, text_tags) #generating features that will be used in the model to train\n",
        "  img = (img / 127.5) - 1. #normalizing\n",
        "  features = {'image' : img[:, :, ::-1].astype(np.float32)}\n",
        "  labels = {'score_map' : score_map[::4, ::4, np.newaxis].astype(np.float32),\n",
        "           'geo_map' : geo_map[::4, ::4, :].astype(np.float32),\n",
        "           'overly_small_text_region_training_mask' : overly_small_text_region_training_mask[::4, ::4, np.newaxis].astype(np.float32),\n",
        "           'text_region_boundary_training_mask': text_region_boundary_training_mask[::4, ::4, np.newaxis].astype(np.float32)}\n",
        "\n",
        "  return features, labels\n",
        "\n",
        "\n",
        "# return img[:, :, ::-1].astype(np.float32), image_file, score_map[::4, ::4, np.newaxis].astype(np.float32), geo_map[::4, ::4, :].astype(np.float32), overly_small_text_region_training_mask[::4, ::4, np.newaxis].astype(np.float32), text_region_boundary_training_mask[::4, ::4, np.newaxis].astype(np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkYKYw3PC4eB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_fn(image_file):\n",
        "  features, labels = load_data_process(image_file, FLAGS, True)\n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05bbt5bJQhyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FLAGS():\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj0jFo2-__N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_columns():\n",
        "  feature_columns = {'images': tf.feature_column.numeric_column('images', (512, 512, 3))}\n",
        "  return feature_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwVxRAT0qT9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS.input_size=512 # input size for training of the network\n",
        "FLAGS.batch_size=16 # batch size for training\n",
        "FLAGS.nb_workers=4 # number of processes to spin up when using process based threading, as defined in https://keras.io/models/model/#fit_generator\n",
        "FLAGS.init_learning_rate=0.0001 # initial learning rate\n",
        "FLAGS.lr_decay_rate=0.94 # decay rate for the learning rate\n",
        "FLAGS.lr_decay_steps=130 # number of steps after which the learning rate is decayed by decay rate\n",
        "FLAGS.max_epochs=800 # maximum number of epochs\n",
        "FLAGS.gpu_list='0' # list of gpus to use\n",
        "FLAGS.checkpoint_path='tmp/east_resnet_50_rbox' # path to a directory to save model checkpoints during training\n",
        "FLAGS.save_checkpoint_epochs=10 # period at which checkpoints are saved (defaults to every 10 epochs)\n",
        "FLAGS.training_data_path='/gdrive/My Drive/indic2019/ICDAR Dataset/images/' # path to training data\n",
        "FLAGS.validation_data_path='/gdrive/My Drive/indic2019/ICDAR Dataset/images/' # path to validation data\n",
        "FLAGS.max_image_large_side=1280 # maximum size of the large side of a training image before cropping a patch for training\n",
        "FLAGS.max_text_size=800 # maximum size of a text instance in an image; image resized if this limit is exceeded\n",
        "FLAGS.min_text_size=10 # minimum size of a text instance; if smaller, then it is ignored during training\n",
        "FLAGS.min_crop_side_ratio=0.1 # the minimum ratio of min(H, W), the smaller side of the image, when taking a random crop from thee input image\n",
        "FLAGS.geometry='RBOX' # geometry type to be used; only RBOX is implemented now, but the original paper also uses QUAD\n",
        "FLAGS.suppress_warnings_and_error_messages=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdWvD_hcFyMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn(files_pattern,batch_size, mode):\n",
        "  print(batch_size)\n",
        "  all_image_paths = get_images(files_pattern)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(all_image_paths)\n",
        "    \n",
        "  #three variables for three mode\n",
        "  is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "  is_eval = (mode == tf.estimator.ModeKeys.EVAL)\n",
        "  is_predict = (mode== tf.estimator.ModeKeys.PREDICT)\n",
        "  \n",
        "  buffer_size = batch_size * 2 + 1\n",
        "  dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "\n",
        "  # Transformation\n",
        "  dataset = dataset.map(extract_fn)\n",
        "  \n",
        "  if is_training or is_predict:\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(2 * batch_size)\n",
        "    \n",
        "  if is_eval:\n",
        "    buffer_size = batch_size * 10\n",
        "    dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(10 * batch_size)\n",
        "\n",
        "  features, labels = dataset.make_one_shot_iterator().get_next()\n",
        "  #features = {'images': image}\n",
        "  \n",
        "  if is_training or is_eval:\n",
        "    return features, label\n",
        "  \n",
        "  if is_predict:\n",
        "    return features "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h7fsJW4Cocn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_bilinear(x):\n",
        "    return tf.image.resize_bilinear(x, size=[K.shape(x)[1]*RESIZE_FACTOR, K.shape(x)[2]*RESIZE_FACTOR])\n",
        "\n",
        "def resize_output_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    assert len(shape) == 4\n",
        "    shape[1] *= RESIZE_FACTOR\n",
        "    shape[2] *= RESIZE_FACTOR\n",
        "    return tuple(shape)\n",
        "\n",
        "def base_model(input_image, overly_small_text_region_training_mask, text_region_boundary_training_mask, target_score_map, input_size=512):\n",
        "  resnet = ResNet50(input_tensor=input_image, weights='imagenet', include_top=False, pooling=None)\n",
        "  x = resnet.get_layer('activation_49').output\n",
        "  \n",
        "  x = Lambda(resize_bilinear, name='resize_1')(x)\n",
        "  x = concatenate([x, resnet.get_layer('activation_40').output], axis=3)\n",
        "  x = Conv2D(128, (1, 1), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  \n",
        "  x = Lambda(resize_bilinear, name='resize_2')(x)\n",
        "  x = concatenate([x, resnet.get_layer('activation_22').output], axis=3)\n",
        "  x = Conv2D(64, (1, 1), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  x = Lambda(resize_bilinear, name='resize_3')(x)\n",
        "  x = concatenate([x, ZeroPadding2D(((1, 0),(1, 0)))(resnet.get_layer('activation_10').output)], axis=3)\n",
        "  x = Conv2D(32, (1, 1), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  x = Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(1e-5))(x)\n",
        "  x = BatchNormalization(momentum=0.997, epsilon=1e-5, scale=True)(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  pred_score_map = Conv2D(1, (1, 1), activation=tf.nn.sigmoid, name='pred_score_map')(x)\n",
        "  rbox_geo_map = Conv2D(4, (1, 1), activation=tf.nn.sigmoid, name='rbox_geo_map')(x) \n",
        "  rbox_geo_map = Lambda(lambda x: x * input_size)(rbox_geo_map)\n",
        "  angle_map = Conv2D(1, (1, 1), activation=tf.nn.sigmoid, name='rbox_angle_map')(x)\n",
        "  angle_map = Lambda(lambda x: (x - 0.5) * np.pi / 2)(angle_map)\n",
        "  pred_geo_map = concatenate([rbox_geo_map, angle_map], axis=3, name='pred_geo_map')\n",
        "  \n",
        "  return pred_score_map, pred_geo_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEwilSZ2DJSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_loss(overly_small_text_region_training_mask, text_region_boundary_training_mask, loss_weight, small_text_weight):\n",
        "    def loss(y_true, y_pred):\n",
        "        eps = 1e-5\n",
        "        _training_mask = tf.minimum(overly_small_text_region_training_mask + small_text_weight, 1) * text_region_boundary_training_mask\n",
        "        intersection = tf.reduce_sum(y_true * y_pred * _training_mask)\n",
        "        union = tf.reduce_sum(y_true * _training_mask) + tf.reduce_sum(y_pred * _training_mask) + eps\n",
        "        loss = 1. - (2. * intersection / union)\n",
        "        return loss * loss_weight\n",
        "    return loss\n",
        "\n",
        "def rbox_loss(overly_small_text_region_training_mask, text_region_boundary_training_mask, small_text_weight, target_score_map):\n",
        "    def loss(y_true, y_pred):\n",
        "        # d1 -> top, d2->right, d3->bottom, d4->left\n",
        "        d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true, num_or_size_splits=5, axis=3)\n",
        "        d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred, num_or_size_splits=5, axis=3)\n",
        "        area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt)\n",
        "        area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred)\n",
        "        w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred)\n",
        "        h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred)\n",
        "        area_intersect = w_union * h_union\n",
        "        area_union = area_gt + area_pred - area_intersect\n",
        "        L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0))\n",
        "        L_theta = 1 - tf.cos(theta_pred - theta_gt)\n",
        "        L_g = L_AABB + 20 * L_theta\n",
        "        _training_mask = tf.minimum(overly_small_text_region_training_mask + small_text_weight, 1) * text_region_boundary_training_mask\n",
        "        return tf.reduce_mean(L_g * target_score_map * _training_mask)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArP0Wl--GBTw",
        "colab_type": "code",
        "outputId": "fb4e48fa-67b2-47be-ace5-d58d1f83f0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dice_loss(1,1,1,1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.dice_loss.<locals>.loss>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIKx5q3PGIw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "                                                                                                \n",
        "  feature_columns = list(get_feature_columns().values())\n",
        "  images = tf.feature_column.input_layer(features=features, feature_columns=feature_columns)\n",
        "\n",
        "  print(labels)\n",
        "  \n",
        "  pred_score_map, pred_geo_map = base_model(images, labels['overly_small_text_region_training_mask'], labels['text_region_boundary_training_mask'], labels['score_map'])\n",
        "                                                                                                            \n",
        "  # Create the input layers from the features \n",
        "  if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\n",
        "    global_step = tf.train.get_or_create_global_step()#create default graph\n",
        "    \n",
        "    score_map_loss = tf.math.subtract(pred_score_map, labels['score_map'])\n",
        "    geo_map_loss = tf.math.subtract(pred_geo_map, labels['geo_map'])\n",
        "    \n",
        "    loss = tf.math.add(score_map_loss,geo_map_loss)\n",
        "    tf.summary.scalar('Loss', loss)\n",
        "\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgOPobTWUMxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edd24d05-28fa-4ec0-8c5a-30d307d0067a"
      },
      "source": [
        "run_config = tf.estimator.RunConfig(model_dir='./Graph', save_summary_steps=10, save_checkpoints_secs = 300, keep_checkpoint_max = 5)\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(input_fn= lambda:input_fn(files_pattern, batch_size=10, mode=tf.estimator.ModeKeys.TRAIN))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0712 06:27:15.307456 140176228743040 model_fn.py:630] Estimator's model_fn (<function model_fn at 0x7f7d0dc4dd90>) includes params argument, but params are not passed to Estimator.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omB7eaFx0EJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73895165-bdd6-4596-c9fb-7d27fe812fbf"
      },
      "source": [
        "estimator.train(lambda:input_fn(files_pattern, batch_size=10, mode=tf.estimator.ModeKeys.TRAIN), steps=50)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-5febbd286e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1183\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m   1184\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m-> 1185\u001b[0;31m               input_fn, ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m   1186\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m       estimator_spec = self._call_model_fn(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1022\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1111\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-5febbd286e3d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-fa5a49024079>\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m(files_pattern, batch_size, mode)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_predict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 1772\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   1773\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m       return DatasetV1Adapter(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3189\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3190\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3191\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3192\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2553\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1355\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1543\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1545\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   1546\u001b[0m         self._function_attributes)\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                           converted_func)\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2547\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   2548\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2549\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-de6b55a3b697>\u001b[0m in \u001b[0;36mextract_fn\u001b[0;34m(image_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-f673cad0aa71>\u001b[0m in \u001b[0;36mload_data_process\u001b[0;34m(image_file, FLAGS, is_train)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#get the shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mtxt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get the annotation file. assuming that the annotation file is named gt_img_15.txt for an image file called img_15.jpg which is the standard ICDAR Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtext_polys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#load the annotation file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-fda9b56d008a>\u001b[0m in \u001b[0;36mget_text_file\u001b[0;34m(image_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtxt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtxt_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtxt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gt_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtxt_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'replace'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2smmt7l_0W43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}