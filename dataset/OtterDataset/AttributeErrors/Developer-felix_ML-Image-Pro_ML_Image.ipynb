{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "has8rSjjEmOq"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Starter's Guide to building a CNN with tensorflow, openCV and google drive for image storage\n",
        "\n",
        "This guide is meant to be a simple tutorial which lets you take images classes, train a model with tensorflow and then predict a new image with your \"intelligent\" model.\n",
        "\n",
        "You can use this python script to get sample training images from a website: https://github.com/ibbad/image-scrapper\n",
        "\n",
        "To have this notebook work you will need to:\n",
        "- Have a google drive folder; copy this notebook to it\n",
        "- Put the notebook and training images in a separate folder e.g. drive/Simple CNN Tutorial/\n",
        "- Images: https://drive.google.com/drive/folders/1RcWFv-PDtGK4n1JgkAgL77fB-N0I4oMP?usp=sharing\n",
        "\n",
        "To run this notebook select each cell and click shift+enter on your keyboard, wait until the cell is finished running before going to the next\n",
        "\n",
        "Below outline the standard import and installation settings for openCV and tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "lZD1vHOjBa__"
      },
      "cell_type": "code",
      "source": [
        "# #This section is base copy from google's sample import of tensorflow, keras, ploting and support libraries\n",
        "\n",
        "# #install libraries\n",
        "# !pip install -q keras\n",
        "# !pip install -q tqdm\n",
        "\n",
        "# #openCV\n",
        "# #openCV is used to process and filter images\n",
        "# !apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
        "\n",
        "# #Tensorflow is used to train the neural network which will perform your image recognition\n",
        "# # To determine which version you're using:\n",
        "# !pip show tensorflow\n",
        "\n",
        "# # For the current version: \n",
        "# !pip install --upgrade tensorflow\n",
        "\n",
        "# # For a specific version:\n",
        "# !pip install tensorflow==1.2\n",
        "\n",
        "# # For the latest nightly build:\n",
        "# !pip install tf-nightly\n",
        "\n",
        "# # http://pytorch.org/\n",
        "# from os import path\n",
        "# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "# accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "  \n",
        "# # https://github.com/dmlc/xgboost\n",
        "# # This specific version is a work-around for a build issue in newer versions.\n",
        "# !pip install -q xgboost==0.4a30\n",
        "\n",
        "# # https://pypi.python.org/pypi/pydot\n",
        "# !apt-get -qq install -y graphviz && pip install -q pydot\n",
        "\n",
        "# # https://pypi.python.org/pypi/libarchive\n",
        "# !apt-get -qq install -y libarchive-dev && pip install -q -U libarchive\n",
        "\n",
        "#turicreate lets you setup and run image classifiers without having to create the different layers and have an indepth understanding\n",
        "# !pip install turicreate\n",
        "\n",
        "# !pip install urlopen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SEdOktt7Egdr"
      },
      "cell_type": "markdown",
      "source": [
        "The CNN training and loading data functions are from:\n",
        "- https://github.com/Fdevmsy/Image_Classification_with_5_methods/blob/master/src_code/method2/CNN_Image_Classification.ipynb\n",
        "- http://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/ (good resource for understanding each step below in more detail)"
      ]
    },
    {
      "metadata": {
        "id": "aA-SDDfUc0pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586d24c2-2d3b-447e-d0a1-2544bb707a75"
      },
      "cell_type": "code",
      "source": [
        "# In this step we import the libraries needed to import files from google drive\n",
        "\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155514 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tuQDqezEmLtS"
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "chrx4sSqOXWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d87fab-e762-43dd-a05c-3445b7336b76"
      },
      "cell_type": "code",
      "source": [
        "# Here we connect to your google drive folder.\n",
        "# You can download sample puppy and rabbit images using this link: https://drive.google.com/drive/folders/1RcWFv-PDtGK4n1JgkAgL77fB-N0I4oMP?usp=sharing\n",
        "\n",
        "# WARNING YOU MAY NEED TO RUN THIS CELL MULTIPLE TIMES TO GET A CONNECTION + KEY ESTABLISHED to your google drive.\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "#vcode = getpass.getpass()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=None&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Cannot retrieve auth tokens.\n",
            "Failure(\"Unexpected error response: {\\n  \\\"error\\\": \\\"invalid_client\\\",\\n  \\\"error_description\\\": \\\"The OAuth client was not found.\\\"\\n}\")\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FOKbAuXGOcIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5454f5f9-391d-422f-e1c4-a264506f2359"
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "#print 'Files in Drive:'\n",
        "#!ls drive/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: www-browser: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links2: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: elinks: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: links: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: lynx: not found\n",
            "/usr/bin/xdg-open: 851: /usr/bin/xdg-open: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Error: Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=%7Bcreds.client_id%7D&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf drive/\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/images',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAWoASnZdDiL",
        "outputId": "46d6764c-cfc6-4f74-eacd-09ed87b27a52"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'drive/': Transport endpoint is not connected\n",
            "Mounted at /content/drive/images\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "T2GoIQXPdSkG"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/images\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/images')"
      ],
      "metadata": {
        "id": "5lqGRkFoctv9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LEhJW09LdrE5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ed51264-7d56-48fe-c8eb-debef04d63f1"
      },
      "cell_type": "code",
      "source": [
        "# img_folder = 'images'\n",
        "\n",
        "#1. Get sample file\n",
        "#2. Read image and display\n",
        "from IPython.display import Image\n",
        "Image(\"01-washington.jpg\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "01-washington.jpg"
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rozor7BJfnD4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_gs2rIOblJa"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import math\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "# https://opencv.org/\n",
        "#!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
        "\n",
        "import cv2\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W5oq2LZBQrng"
      },
      "cell_type": "code",
      "source": [
        "#Functions for loading training and testing data in a vector format instead of an image\n",
        "\n",
        "def load_train(train_path, image_size, classes):\n",
        "    images = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "    cls = []\n",
        "\n",
        "    print('Reading training images')\n",
        "    for fld in classes:   # assuming data directory has a separate folder for each class, and that each folder is named after the class\n",
        "        index = classes.index(fld)\n",
        "        print('Loading {} files (Index: {})'.format(fld, index))\n",
        "        path = os.path.join(train_path, fld, '*g')\n",
        "        files = glob.glob(path)\n",
        "        for fl in files:\n",
        "            image = cv2.imread(fl)\n",
        "            image = cv2.resize(image, (image_size, image_size), interpolation = cv2.INTER_LINEAR)\n",
        "            images.append(image)\n",
        "            label = np.zeros(len(classes))\n",
        "            label[index] = 1.0\n",
        "            labels.append(label)\n",
        "            flbase = os.path.basename(fl)\n",
        "            ids.append(flbase)\n",
        "            cls.append(fld)\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    ids = np.array(ids)\n",
        "    cls = np.array(cls)\n",
        "\n",
        "    return images, labels, ids, cls\n",
        "\n",
        "\n",
        "def load_test(test_path, image_size):\n",
        "    path = os.path.join(test_path, '*g')\n",
        "    files = sorted(glob.glob(path))\n",
        "\n",
        "    X_test = []\n",
        "    X_test_id = []\n",
        "    print(\"Reading test images\")\n",
        "    for fl in files:\n",
        "        flbase = os.path.basename(fl)\n",
        "        img = cv2.imread(fl)\n",
        "        img = cv2.resize(img, (image_size, image_size), interpolation = cv2.INTER_LINEAR)\n",
        "        X_test.append(img)\n",
        "        X_test_id.append(flbase)\n",
        "\n",
        "### because we're not creating a DataSet object for the test images, normalization happens here\n",
        "    X_test = np.array(X_test, dtype=np.uint8)\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_test = X_test / 255\n",
        "\n",
        "    return X_test, X_test_id\n",
        "\n",
        "\n",
        "\n",
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self, images, labels, ids, cls):\n",
        "        \"\"\"Construct a DataSet. one_hot arg is used only if fake_data is true.\"\"\"\n",
        "\n",
        "        self._num_examples = images.shape[0]\n",
        "\n",
        "\n",
        "        # Convert shape from [num examples, rows, columns, depth]\n",
        "        # to [num examples, rows*columns] (assuming depth == 1)\n",
        "        # Convert from [0, 255] -> [0.0, 1.0].\n",
        "\n",
        "        images = images.astype(np.float32)\n",
        "        images = np.multiply(images, 1.0 / 255.0)\n",
        "\n",
        "        self._images = images\n",
        "        self._labels = labels\n",
        "        self._ids = ids\n",
        "        self._cls = cls\n",
        "        self._epochs_completed = 0\n",
        "        self._index_in_epoch = 0\n",
        "\n",
        "    @property\n",
        "    def images(self):\n",
        "        return self._images\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    @property\n",
        "    def ids(self):\n",
        "        return self._ids\n",
        "\n",
        "    @property\n",
        "    def cls(self):\n",
        "        return self._cls\n",
        "\n",
        "    @property\n",
        "    def num_examples(self):\n",
        "        return self._num_examples\n",
        "\n",
        "    @property\n",
        "    def epochs_completed(self):\n",
        "        return self._epochs_completed\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "        start = self._index_in_epoch\n",
        "        self._index_in_epoch += batch_size\n",
        "\n",
        "        if self._index_in_epoch > self._num_examples:\n",
        "            # Finished epoch\n",
        "            self._epochs_completed += 1\n",
        "\n",
        "            # # Shuffle the data (maybe)\n",
        "            # perm = np.arange(self._num_examples)\n",
        "            # np.random.shuffle(perm)\n",
        "            # self._images = self._images[perm]\n",
        "            # self._labels = self._labels[perm]\n",
        "            # Start next epoch\n",
        "\n",
        "            start = 0\n",
        "            self._index_in_epoch = batch_size\n",
        "            #print (self._num_examples)\n",
        "            assert batch_size <= self._num_examples\n",
        "            \n",
        "        end = self._index_in_epoch\n",
        "\n",
        "        return self._images[start:end], self._labels[start:end], self._ids[start:end], self._cls[start:end]\n",
        "\n",
        "\n",
        "def read_train_sets(train_path, image_size, classes, validation_size=0):\n",
        "    class DataSets(object):\n",
        "        pass\n",
        "    data_sets = DataSets()\n",
        "\n",
        "    images, labels, ids, cls = load_train(train_path, image_size, classes)\n",
        "    images, labels, ids, cls = shuffle(images, labels, ids, cls)  # shuffle the data\n",
        "\n",
        "    if isinstance(validation_size, float):\n",
        "        validation_size = int(validation_size * images.shape[0])\n",
        "\n",
        "        validation_images = images[:validation_size]\n",
        "        validation_labels = labels[:validation_size]\n",
        "        validation_ids = ids[:validation_size]\n",
        "        validation_cls = cls[:validation_size]\n",
        "\n",
        "        train_images = images[validation_size:]\n",
        "        train_labels = labels[validation_size:]\n",
        "        train_ids = ids[validation_size:]\n",
        "        train_cls = cls[validation_size:]\n",
        "\n",
        "        data_sets.train = DataSet(train_images, train_labels, train_ids, train_cls)\n",
        "        data_sets.valid = DataSet(validation_images, validation_labels, validation_ids, validation_cls)\n",
        "\n",
        "    return data_sets\n",
        "\n",
        "\n",
        "def read_test_set(test_path, image_size):\n",
        "    images, ids  = load_test(test_path, image_size)\n",
        "    return images, ids"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfWRY3wRQwmB"
      },
      "cell_type": "code",
      "source": [
        "#Steps to setup a convolutional neural network, 2 convolutional layers and 2 fully connected layers\n",
        "\n",
        "# Convolutional Layer 1.\n",
        "filter_size1 = 5 \n",
        "num_filters1 = 64\n",
        "\n",
        "# Convolutional Layer 2.\n",
        "filter_size2 = 3\n",
        "num_filters2 = 64\n",
        "\n",
        "# Fully-connected layer 1.\n",
        "fc1_size = 128             # Number of neurons in fully-connected layer.\n",
        "\n",
        "# Fully-connected layer 2.\n",
        "fc2_size = 128             # Number of neurons in fully-connected layer.\n",
        "\n",
        "# Number of color channels for the images: 1 channel for gray-scale.\n",
        "num_channels = 3\n",
        "\n",
        "# image dimensions (only squares for now)\n",
        "img_size = 64\n",
        "\n",
        "# Size of image when flattened to a single dimension\n",
        "img_size_flat = img_size * img_size * num_channels\n",
        "\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "\n",
        "# class info\n",
        "\n",
        "# HERE IS WHERE YOU LABEL YOUR IMAGE CLASSES - change to the folder names you have in your google drive can be anything\n",
        "\n",
        "classes = ['craters','ridges']\n",
        "\n",
        "num_classes = len(classes)\n",
        "\n",
        "# batch size\n",
        "batch_size = 10\n",
        "\n",
        "# validation split simple 80% of data used for training and 20% for testing standard\n",
        "validation_size = .2\n",
        "\n",
        "# how long to wait after validation loss stops improving before terminating training\n",
        "early_stopping = None  # use None if you don't want to implement early stoping\n",
        "\n",
        "# HERE IS THE FOLDER WHICH STORES YOUR IMAGE CLASS DATA, e.g. \"images\"/bunny/bunny1.jpg etc.\n",
        "train_path = 'images'\n",
        "test_path = 'images'\n",
        "checkpoint_dir = \"ckpoint\""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "id": "my4Gsd6AQz1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba57e89-e432-4add-8a4e-5f166fda886f"
      },
      "cell_type": "code",
      "source": [
        "# load training dataset\n",
        "data = read_train_sets(train_path, img_size, classes, validation_size=validation_size)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading training images\n",
            "Loading craters files (Index: 0)\n",
            "Loading ridges files (Index: 1)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xJO_5blV-zje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2434639e-ea92-47f0-890c-e552db763d9f"
      },
      "cell_type": "code",
      "source": [
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
        "#print(\"- Test-set:\\t\\t{}\".format(len(test_images)))\n",
        "print(\"- Validation:\\t{}\".format(len(data.valid.labels)))\n",
        "#print(images)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of:\n",
            "- Training-set:\t\t0\n",
            "- Validation:\t0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "NUS5s6tb_DA6"
      },
      "cell_type": "code",
      "source": [
        "##Sample function which allows for images to be plotted from sample data set for inspection\n",
        "\n",
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    \n",
        "    if len(images) == 0:\n",
        "        print(\"no images to show\")\n",
        "        return \n",
        "    else:\n",
        "        random_indices = random.sample(range(len(images)), min(len(images), 9))\n",
        "        \n",
        "        \n",
        "    images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        ax.imshow(images[i].reshape(img_size, img_size, num_channels))\n",
        "\n",
        "        # Show true and predicted classes.\n",
        "        if cls_pred is None:\n",
        "            xlabel = \"True: {0}\".format(cls_true[i])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "        # Show the classes as the label on the x-axis.\n",
        "        ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "id": "slZsEg54ETaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1c6e90-be1b-4c50-e145-89e6f22492b6"
      },
      "cell_type": "code",
      "source": [
        "# Get some random images and their labels from the train set.\n",
        "\n",
        "images, cls_true  = data.train.images, data.train.cls\n",
        "\n",
        "# Plot the images and labels using our helper-function above.\n",
        "plot_images(images=images, cls_true=cls_true)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no images to show\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ULKHViq9EU0m"
      },
      "cell_type": "code",
      "source": [
        "#weight and biases functions from tensorflow functions\n",
        "\n",
        "def new_weights(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
        "def new_biases(length):\n",
        "    return tf.Variable(tf.constant(0.05, shape=[length]))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IiOcxACxEZUk"
      },
      "cell_type": "code",
      "source": [
        "#Function for convolutional layer\n",
        "\n",
        "def new_conv_layer(input,              # The previous layer.\n",
        "                   num_input_channels, # Num. channels in prev. layer.\n",
        "                   filter_size,        # Width and height of each filter.\n",
        "                   num_filters,        # Number of filters.\n",
        "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
        "\n",
        "    # Shape of the filter-weights for the convolution.\n",
        "    # This format is determined by the TensorFlow API.\n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "    # Create new weights aka. filters with the given shape.\n",
        "    weights = new_weights(shape=shape)\n",
        "\n",
        "    # Create new biases, one for each filter.\n",
        "    biases = new_biases(length=num_filters)\n",
        "\n",
        "    # Create the TensorFlow operation for convolution.\n",
        "    # Note the strides are set to 1 in all dimensions.\n",
        "    # The first and last stride must always be 1,\n",
        "    # because the first is for the image-number and\n",
        "    # the last is for the input-channel.\n",
        "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
        "    # is moved 2 pixels across the x- and y-axis of the image.\n",
        "    # The padding is set to 'SAME' which means the input image\n",
        "    # is padded with zeroes so the size of the output is the same.\n",
        "    layer = tf.nn.conv2d(input=input,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "\n",
        "    # Add the biases to the results of the convolution.\n",
        "    # A bias-value is added to each filter-channel.\n",
        "    layer += biases\n",
        "\n",
        "    # Use pooling to down-sample the image resolution?\n",
        "    if use_pooling:\n",
        "        # This is 2x2 max-pooling, which means that we\n",
        "        # consider 2x2 windows and select the largest value\n",
        "        # in each window. Then we move 2 pixels to the next window.\n",
        "        layer = tf.nn.max_pool(value=layer,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME')\n",
        "\n",
        "    # Rectified Linear Unit (ReLU).\n",
        "    # It calculates max(x, 0) for each input pixel x.\n",
        "    # This adds some non-linearity to the formula and allows us\n",
        "    # to learn more complicated functions.\n",
        "    layer = tf.nn.relu(layer)\n",
        "\n",
        "    # Note that ReLU is normally executed before the pooling,\n",
        "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
        "    # save 75% of the relu-operations by max-pooling first.\n",
        "\n",
        "    # We return both the resulting layer and the filter-weights\n",
        "    # because we will plot the weights later.\n",
        "    return layer, weights"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3hhRXoVhEbPG"
      },
      "cell_type": "code",
      "source": [
        "#Function that defines the flatten layer in your network\n",
        "\n",
        "def flatten_layer(layer):\n",
        "    # Get the shape of the input layer.\n",
        "    layer_shape = layer.get_shape()\n",
        "\n",
        "    # The shape of the input layer is assumed to be:\n",
        "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
        "\n",
        "    # The number of features is: img_height * img_width * num_channels\n",
        "    # We can use a function from TensorFlow to calculate this.\n",
        "    num_features = layer_shape[1:4].num_elements()\n",
        "    \n",
        "    # Reshape the layer to [num_images, num_features].\n",
        "    # Note that we just set the size of the second dimension\n",
        "    # to num_features and the size of the first dimension to -1\n",
        "    # which means the size in that dimension is calculated\n",
        "    # so the total size of the tensor is unchanged from the reshaping.\n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    # The shape of the flattened layer is now:\n",
        "    # [num_images, img_height * img_width * num_channels]\n",
        "\n",
        "    # Return both the flattened layer and the number of features.\n",
        "    return layer_flat, num_features"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I6pa-PXiEc_t"
      },
      "cell_type": "code",
      "source": [
        "#Function which defines a fully connected layer in your network\n",
        "\n",
        "def new_fc_layer(input,          # The previous layer.\n",
        "                 num_inputs,     # Num. inputs from prev. layer.\n",
        "                 num_outputs,    # Num. outputs.\n",
        "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
        "\n",
        "    # Create new weights and biases.\n",
        "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
        "    biases = new_biases(length=num_outputs)\n",
        "\n",
        "    # Calculate the layer as the matrix multiplication of\n",
        "    # the input and weights, and then add the bias-values.\n",
        "    layer = tf.matmul(input, weights) + biases\n",
        "\n",
        "    # Use ReLU?\n",
        "    if use_relu:\n",
        "        layer = tf.nn.relu(layer)\n",
        "\n",
        "    return layer"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MmWnkyIaEzrE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "7dad1638-77c2-4e4f-c552-4bd7ffd330de"
      },
      "cell_type": "code",
      "source": [
        "#Define x and y values for network\n",
        "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
        "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
        "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
        "y_true_cls = tf.argmax(y_true, dimension=1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-40-a6a77a78ff38>\", line 2, in <module>\n",
            "    x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
            "AttributeError: module 'tensorflow' has no attribute 'placeholder'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HFa9AHSgE1L8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "outputId": "cc68ff0d-b3c9-4a37-af09-c77a499004bd"
      },
      "cell_type": "code",
      "source": [
        "#Convolutional Layer 1\n",
        "#num_channels = 3 (red, greed, blue) (RGB), filter size = 5, pooling\n",
        "\n",
        "layer_conv1, weights_conv1 = \\\n",
        "    new_conv_layer(input=x_image,\n",
        "                   num_input_channels=num_channels,\n",
        "                   filter_size=filter_size1,\n",
        "                   num_filters=num_filters1,\n",
        "                   use_pooling=True)\n",
        "layer_conv1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-41-2f14e90e1698>\", line 4, in <module>\n",
            "    layer_conv1, weights_conv1 =     new_conv_layer(input=x_image,\n",
            "NameError: name 'x_image' is not defined\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uHmqCKYJE24s"
      },
      "cell_type": "code",
      "source": [
        "#Convolutional Layer 2\n",
        "layer_conv2, weights_conv2 = \\\n",
        "    new_conv_layer(input=layer_conv1,\n",
        "                   num_input_channels=num_filters1,\n",
        "                   filter_size=filter_size2,\n",
        "                   num_filters=num_filters2,\n",
        "                   use_pooling=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpcJUDKXFDmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74c5f9e3-d402-4975-ed88-65815acf52ee"
      },
      "cell_type": "code",
      "source": [
        "#Flat Layer\n",
        "layer_flat, num_features = flatten_layer(layer_conv2)\n",
        "print(layer_flat, num_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor 'Reshape_1:0' shape=(?, 16384) dtype=float32>, 16384)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pn0uW9OxFFLj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d8f7b86-b230-4836-92b0-7b83b16865c1"
      },
      "cell_type": "code",
      "source": [
        "#Fully Connected Layer 1\n",
        "layer_fc1 = new_fc_layer(input=layer_flat,\n",
        "                         num_inputs=num_features,\n",
        "                         num_outputs=fc1_size,\n",
        "                         use_relu=True)\n",
        "layer_fc1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "bjVWaBrdFGtT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3076bd3-cb1b-4795-8803-f59e3af1649e"
      },
      "cell_type": "code",
      "source": [
        "#Fully Connected Layer 2\n",
        "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
        "                         num_inputs=fc1_size,\n",
        "                         num_outputs=num_classes,\n",
        "                         use_relu=False)\n",
        "layer_fc2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'add_3:0' shape=(?, 2) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "K_0lODWEFIGa"
      },
      "cell_type": "code",
      "source": [
        "#Prediction Outputs using Softmax decision\n",
        "y_pred = tf.nn.softmax(layer_fc2)\n",
        "y_pred_cls = tf.argmax(y_pred, dimension=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UqTl2fljFJlT"
      },
      "cell_type": "code",
      "source": [
        "#Cost, Entropy and Optimizer SEttins\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2,\n",
        "                                                        labels=y_true)\n",
        "cost = tf.reduce_mean(cross_entropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5UnYbYZVFLaa"
      },
      "cell_type": "code",
      "source": [
        "#Prediction and Accuracy Setting to tensorflow functions\n",
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eYmSHMdrFSob"
      },
      "cell_type": "code",
      "source": [
        "#Initialize Tensorflow session\n",
        "session = tf.Session()\n",
        "session.run(tf.global_variables_initializer())\n",
        "\n",
        "train_batch_size = batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiAarImuFUBa"
      },
      "cell_type": "code",
      "source": [
        "#Helper function which prints the training progress of accuracy and loss\n",
        "\n",
        "def print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss):\n",
        "    # Calculate the accuracy on the training-set.\n",
        "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
        "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
        "    msg = \"Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%}, Validation Loss: {3:.3f}\"\n",
        "    print(msg.format(epoch + 1, acc, val_acc, val_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-WuX4_9xFWBj"
      },
      "cell_type": "code",
      "source": [
        "# Counter for total number of iterations performed so far.\n",
        "total_iterations = 0\n",
        "\n",
        "# Function which runs the training process \n",
        "def optimize(num_iterations):\n",
        "    # Ensure we update the global variable rather than a local copy.\n",
        "    global total_iterations\n",
        "\n",
        "    # Start-time used for printing time-usage below.\n",
        "    start_time = time.time()\n",
        "    \n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 0\n",
        "\n",
        "    for i in range(total_iterations,\n",
        "                   total_iterations + num_iterations):\n",
        "\n",
        "        # Get a batch of training examples.\n",
        "        # x_batch now holds a batch of images and\n",
        "        # y_true_batch are the true labels for those images.\n",
        "        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size)\n",
        "        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size)\n",
        "\n",
        "        # Convert shape from [num examples, rows, columns, depth]\n",
        "        # to [num examples, flattened image shape]\n",
        "\n",
        "        x_batch = x_batch.reshape(train_batch_size, img_size_flat)\n",
        "        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat)\n",
        "\n",
        "        # Put the batch into a dict with the proper names\n",
        "        # for placeholder variables in the TensorFlow graph.\n",
        "        feed_dict_train = {x: x_batch,\n",
        "                           y_true: y_true_batch}\n",
        "        \n",
        "        feed_dict_validate = {x: x_valid_batch,\n",
        "                              y_true: y_valid_batch}\n",
        "\n",
        "        # Run the optimizer using this batch of training data.\n",
        "        # TensorFlow assigns the variables in feed_dict_train\n",
        "        # to the placeholder variables and then runs the optimizer.\n",
        "        session.run(optimizer, feed_dict=feed_dict_train)\n",
        "        \n",
        "\n",
        "        # Print status at end of each epoch (defined as full pass through training dataset).\n",
        "        if i % int(data.train.num_examples/batch_size) == 0: \n",
        "            val_loss = session.run(cost, feed_dict=feed_dict_validate)\n",
        "            epoch = int(i / int(data.train.num_examples/batch_size))\n",
        "            \n",
        "            print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss)\n",
        "            \n",
        "            if early_stopping:    \n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience = 0\n",
        "                else:\n",
        "                    patience += 1\n",
        "\n",
        "                if patience == early_stopping:\n",
        "                    break\n",
        "\n",
        "    # Update the total number of iterations performed.\n",
        "    total_iterations += num_iterations\n",
        "\n",
        "    # Ending time.\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Difference between start and end-times.\n",
        "    time_dif = end_time - start_time\n",
        "\n",
        "    # Print the time-usage.\n",
        "    print(\"Time elapsed: \" + str(timedelta(seconds=int(round(time_dif)))))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bp4mgIWHFYWJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "6273fd14-6e58-4b61-ea43-7b853d3bbb2f"
      },
      "cell_type": "code",
      "source": [
        "#THIS STEP RUNS YOUR MODEL TO TEST TRAINING AND VALIDATION ACCURACY\n",
        "\n",
        "#Improving accuracy here is quite difficult as you need to have an indepth understanding of how and why different convolutional, fully connection and pooling layers are used\n",
        "#Depending on the image types you use it will converge faster than 100 iterations e.g. 25\n",
        "\n",
        "optimize(num_iterations=100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 --- Training Accuracy:  50.0%, Validation Accuracy:  30.0%, Validation Loss: 0.746\n",
            "Epoch 2 --- Training Accuracy:  50.0%, Validation Accuracy:  30.0%, Validation Loss: 0.776\n",
            "Epoch 3 --- Training Accuracy:  60.0%, Validation Accuracy:  50.0%, Validation Loss: 0.749\n",
            "Epoch 4 --- Training Accuracy:  60.0%, Validation Accuracy:  50.0%, Validation Loss: 0.776\n",
            "Epoch 5 --- Training Accuracy:  70.0%, Validation Accuracy:  50.0%, Validation Loss: 0.742\n",
            "Epoch 6 --- Training Accuracy:  70.0%, Validation Accuracy:  50.0%, Validation Loss: 0.732\n",
            "Epoch 7 --- Training Accuracy:  70.0%, Validation Accuracy:  50.0%, Validation Loss: 0.737\n",
            "Epoch 8 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.708\n",
            "Epoch 9 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.730\n",
            "Epoch 10 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.711\n",
            "Epoch 11 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.711\n",
            "Epoch 12 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.703\n",
            "Epoch 13 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.702\n",
            "Epoch 14 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.698\n",
            "Epoch 15 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.695\n",
            "Epoch 16 --- Training Accuracy:  80.0%, Validation Accuracy:  50.0%, Validation Loss: 0.702\n",
            "Epoch 17 --- Training Accuracy:  90.0%, Validation Accuracy:  50.0%, Validation Loss: 0.694\n",
            "Epoch 18 --- Training Accuracy:  90.0%, Validation Accuracy:  50.0%, Validation Loss: 0.705\n",
            "Epoch 19 --- Training Accuracy:  90.0%, Validation Accuracy:  50.0%, Validation Loss: 0.703\n",
            "Epoch 20 --- Training Accuracy:  90.0%, Validation Accuracy:  50.0%, Validation Loss: 0.711\n",
            "Time elapsed: 0:00:02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8CrYarETFaeq"
      },
      "cell_type": "code",
      "source": [
        "#Test Data and Validation loss and accuracy output\n",
        "x_test = data.valid.images.reshape(13, img_size_flat)\n",
        "\n",
        "feed_dict_test = {x: x_test, y_true: data.valid.labels}\n",
        "\n",
        "val_loss = session.run(cost, feed_dict=feed_dict_test)\n",
        "\n",
        "val_acc = session.run(accuracy, feed_dict=feed_dict_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "deyF6KvPrrIO"
      },
      "cell_type": "markdown",
      "source": [
        "An accuracy better than 60%? (AMAZING!), the training data you use will drastically effect this, if you only have lets say photos of your dog and one type of cat the accuracy should be quite high but if you were to test against another type of dog with cat like features it would probably be classified incorrectly."
      ]
    },
    {
      "metadata": {
        "id": "4L0s0oD4Fdmy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be34f09f-b882-49f3-bc84-24445100605a"
      },
      "cell_type": "code",
      "source": [
        "#Print Test Accuracy\n",
        "msg_test = \"Test Accuracy: {0:>6.1%}\"\n",
        "print(msg_test.format(val_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  46.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6LmeoBfsBxv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0c67ebf-8e26-4d62-97ef-bff24653e372"
      },
      "cell_type": "code",
      "source": [
        "# This step can be used to save your model so it can be used later on.\n",
        "# Store the model\n",
        "# Save the model for future predicting\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "save_dir = './model/'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "checkpoint_path = os.path.join(save_dir, 'model')\n",
        "save_path=saver.save(session, checkpoint_path)\n",
        "print ('Model saved in file: %s'% save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved in file: ./model/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zD2b0a-Y1TDv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7073a396-e4f6-43ac-f66a-77ae5f3d2df4"
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.import_meta_graph('./model/model.meta')\n",
        "\n",
        "saver.restore(session, tf.train.latest_checkpoint('./model/'))\n",
        "\n",
        "filename = 'images/bunny/enhanced-buzz-504-1296591038-21.jpg'\n",
        "\n",
        "image_size = 64\n",
        "num_channels = 3\n",
        "images = []\n",
        "\n",
        "# Resizing the image to our desired size and\n",
        "# preprocessing will be done exactly as done during training\n",
        "# image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
        "# images.append(image)\n",
        "\n",
        "image = cv2.imread(filename)\n",
        "image = cv2.resize(image, (image_size, image_size), interpolation = cv2.INTER_LINEAR)\n",
        "images.append(image)\n",
        "\n",
        "images = np.array(images, dtype=np.uint8)\n",
        "images = images.astype('float32')\n",
        "images = np.multiply(images, 1.0/255.0) \n",
        "\n",
        "#The input to the network is of shape [None image_size image_size num_channels]. Hence we reshape.\n",
        "# x_batch = images.reshape(1, image_size,image_size,num_channels)\n",
        "x_batch = images.reshape(1, img_size_flat)\n",
        " \n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "print(tf.global_variables())\n",
        " \n",
        "y_pred = graph.get_tensor_by_name(\"y_true:0\")\n",
        " \n",
        "## Let's feed the images to the input placeholders\n",
        "x= graph.get_tensor_by_name(\"x:0\") \n",
        "y_true = graph.get_tensor_by_name(\"y_true:0\") \n",
        "y_test_images = np.zeros((1, 2)) \n",
        " \n",
        "feed_dict_testing = {x: x_batch, y_true: y_test_images}\n",
        "result=session.run(y_pred, feed_dict=feed_dict_testing)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31merror\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-cd9090a7dddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: /io/opencv/modules/imgproc/src/resize.cpp:4044: error: (-215) ssize.width > 0 && ssize.height > 0 in function resize\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Dof6x_eAEV9S"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}