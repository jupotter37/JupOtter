{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scholarly import scholarly\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from threading import Thread\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import queue\n",
    "import undetected_chromedriver as uc\n",
    "import os, csv, re\n",
    "from selenium import webdriver\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SCHOLAR = '''https://scholar.google.com/scholar?start=0&q=(reinforcement+learning+OR+RL)+AND+(protocol+routing+OR+routing+strategy+OR+routing+algorithm)+AND+(Wireless+Sensor+Network+OR+WSN+OR+Sensor+Network)&hl=id&as_sdt=0,5&as_ylo=2010'''\n",
    "\n",
    "URL_IEEE = '''\n",
    "https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=(reinforcement%20learning%20OR%20RL)%20AND%20(protocol%20routing%20OR%20routing%20strategy%20OR%20routing%20algorithm%20OR%20routing)%20AND%20(Wireless%20Sensor%20Network%20OR%20WSN%20OR%20Sensor%20Network)&highlight=true&returnFacets=ALL&returnType=SEARCH&matchPubs=true&ranges=2010_2024_Year\n",
    "'''\n",
    "\n",
    "URL_ACM = '''\n",
    "https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&field1=AllField&text1=%28%22reinforcement+learning%22+OR+%22RL%22%29+AND+%28%22protocol+routing%22+OR+%22routing+strategy%22+OR+%22routing+algorithm%22+OR+%22routing%22%29+AND+%28%22Wireless+Sensor+Network%22+OR+%22WSN%22+OR+%22Sensor+Network%22%29&searchArea%5B0%5D=SeriesKey&operator%5B0%5D=And&text%5B0%5D=&searchArea%5B1%5D=SeriesKey&operator%5B1%5D=And&text%5B1%5D=&EpubDate=&AfterMonth=1&AfterYear=2010&BeforeMonth=&BeforeYear=&pageSize=20&expand=all\n",
    "'''\n",
    "\n",
    "URL_ELSEVIER = '''\n",
    "https://www.sciencedirect.com/search?qs=%28%22reinforcement%20learning%22%20OR%20%22RL%22%29%20AND%20%28%22protocol%20routing%22%20OR%20%22routing%20strategy%22%20OR%20%22routing%20algorithm%22%20OR%20%22routing%22%29%20AND%20%28%22Wireless%20Sensor%20Network%22%20OR%20%22WSN%22%20OR%20%22Sensor%20Network%22%29&date=2010-2024&show=100&offset=0\n",
    "'''\n",
    "\n",
    "URL_WILEY = '''\n",
    "https://onlinelibrary.wiley.com/action/doSearch?AfterYear=2010&AllField=%28%22reinforcement+learning%22+OR+%22RL%22%29+AND+%28%22protocol+routing%22+OR+%22routing+strategy%22+OR+%22routing+algorithm%22+OR+%22routing%22%29+AND+%28%22Wireless+Sensor+Network%22+OR+%22WSN%22+OR+%22Sensor+Network%22%29&BeforeYear=2024&content=articlesChapters&target=default&pageSize=100&startPage=0\n",
    "'''\n",
    "\n",
    "SEARCH_QUERY = '''\n",
    "(reinforcement learning OR RL) AND (protocol routing OR routing strategy OR routing algorithm OR routing) AND (Wireless Sensor Network OR WSN OR Sensor Network)\n",
    "'''\n",
    "SEARCH_QUERY2 = '''\n",
    "(\"reinforcement learning\" OR \"RL\") AND (\"protocol routing\" OR \"routing strategy\" OR \"routing algorithm\" OR \"routing\") AND (\"Wireless Sensor Network\" OR \"WSN\" OR \"Sensor Network\")\n",
    "'''\n",
    "DIGITAL_LIBRARIES_ACCEPTED = ['ieee', 'Springer', 'ACM', 'Elsevier', 'Wiley', 'Sage']\n",
    "\n",
    "\n",
    "software_names = [SoftwareName.CHROME.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value,\n",
    "                     OperatingSystem.LINUX.value]\n",
    "user_agent_rotator = UserAgent(\n",
    "    software_names=software_names,\n",
    "    operating_systems=operating_systems,\n",
    "    limit=100\n",
    ")\n",
    "user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "proxy_server_url = \"195.252.236.91\"\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "# chrome_options.add_argument(f'--proxy-server={proxy_server_url}')\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--ignore-ssl-errors')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "# chrome_options.add_argument('--incognito')\n",
    "# chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"normal\"\n",
    "\n",
    "uc_options = uc.ChromeOptions()\n",
    "uc_options.add_argument(\"--no-sandbox\")\n",
    "uc_options.add_argument(f'--proxy-server={proxy_server_url}')\n",
    "uc_options.add_argument('--ignore-certificate-errors')\n",
    "uc_options.add_argument('--ignore-ssl-errors')\n",
    "uc_options.add_argument('--disable-gpu')\n",
    "uc_options.add_argument('--incognito')\n",
    "uc_options.add_argument(\"--headless=new\")\n",
    "uc_options.add_argument(\"--start-maximized\")\n",
    "\n",
    "\n",
    "information_keys = ['title', 'link', 'number_of_citation', 'article_type', 'publisher', 'publication_date', 'abstract', 'keyword']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IEEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work fine, however consume lot computation if using multiple agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from threading import Thread\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "import queue\n",
    "import os, csv, re\n",
    "from selenium import webdriver\n",
    "from csv import DictWriter\n",
    "\n",
    "global FILE_NAME\n",
    "# user input arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--start', type=int, required=True)\n",
    "parser.add_argument('--end', type=int, required=True)\n",
    "parser.add_argument('--number_of_agent', type=int, required=True)\n",
    "\n",
    "# Webdriver Properties\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--ignore-ssl-errors')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--incognito')\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"normal\"\n",
    "\n",
    "# Journal Scrapping Argument\n",
    "information_keys = ['title', 'link', 'number_of_citation', 'article_type', 'publisher', 'publication_date', 'abstract', 'keyword']\n",
    "DIGITAL_LIBRARIES_ACCEPTED = ['ieee', 'Springer', 'ACM', 'Elsevier', 'Wiley', 'Sage']\n",
    "URL_IEEE = '''https://ieeexplore.ieee.org/search/searchresult.jsp?queryText=(reinforcement%20learning%20OR%20RL)%20AND%20(protocol%20routing%20OR%20routing%20strategy%20OR%20routing%20algorithm%20OR%20routing)%20AND%20(Wireless%20Sensor%20Network%20OR%20WSN%20OR%20Sensor%20Network)&highlight=true&returnFacets=ALL&returnType=SEARCH&matchPubs=true&ranges=2010_2024_Year'''\n",
    "df = {}\n",
    "\n",
    "\n",
    "\n",
    "def split_range(_range, parts): #split a range to chunks\n",
    "    chunk_size = int(len(_range)/parts)\n",
    "    chunks = [_range[x:x+chunk_size] for x in range(0, len(_range), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Collecting title, link, number of citation, article type, and publication date\n",
    "def getLinkEachPage(links):\n",
    "    for link in links:\n",
    "        print(f\"Scraping at page: {link[-1]}\")\n",
    "        count = 1\n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.get(link)\n",
    "            article_bodies = WebDriverWait(driver, 30).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"List-results-items\"))\n",
    "            )\n",
    "            html = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            df_temp = {}\n",
    "            for param in information_keys:\n",
    "                df_temp[param] = []\n",
    "\n",
    "            # Check all the paper in div body\n",
    "            for article in soup.find_all('div', class_='List-results-items'):\n",
    "                title               = article.find('h3', class_='text-md-md-lh').get_text()\n",
    "                link_article        = 'https://ieeexplore.ieee.org' + article.find('h3', class_='text-md-md-lh').find('a')['href']\n",
    "\n",
    "                # Get publisher information such as year, article type and publisher\n",
    "                publisher_information   = article.find('div', class_='publisher-info-container').find_all('span')\n",
    "                publisher_information   = [i.get_text() for i in publisher_information if '|' not in i.get_text()]\n",
    "                publication_date        = publisher_information[0][-4:]\n",
    "                article_type            = 'other'\n",
    "\n",
    "                for i in publisher_information:\n",
    "                    if ((i == 'Conference Paper') or (i == 'Journal Article')):\n",
    "                        article_type = i\n",
    "                \n",
    "                citations = [citation.get_text() for citation in article.find('div', class_='description text-base-md-lh').find_all('span')]\n",
    "\n",
    "                for citation in citations:\n",
    "                    if 'Papers' in citation:\n",
    "                        number_of_citation = int(re.search(r'\\d+', citation).group())\n",
    "                    else:\n",
    "                        number_of_citation = 0\n",
    "                \n",
    "                df_temp['title'].append(title)\n",
    "                df_temp['link'].append(link_article)\n",
    "                df_temp['number_of_citation'].append(number_of_citation)\n",
    "                df_temp['article_type'].append(article_type)\n",
    "                df_temp['publication_date'].append(publication_date)\n",
    "                df_temp['publisher'].append('IEEE')\n",
    "\n",
    "            \n",
    "            for link_article in df_temp['link']:\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "                driver.get(link_article)\n",
    "                title = WebDriverWait(driver, 30).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//*[@id=\"xplMainContentLandmark\"]/div/xpl-document-details/div/div[1]/section[2]/div/xpl-document-header/section/div[2]/div/div/div[1]/div/div[1]/h1'))\n",
    "                )\n",
    "                button = driver.find_element(By.ID, 'keywords')\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", button)\n",
    "                button.click()\n",
    "                button = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"keywords\"]/xpl-document-keyword-list/section/div/ul/li[1]/strong')))\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                abstract = soup.find_all('div', class_='u-mb-1')\n",
    "                abstract = abstract[1].get_text()[len('Abstract:')+1:]\n",
    "                keyword = soup.find_all('li', class_='doc-keywords-list-item')[1]\n",
    "                keyword = keyword.get_text()[len(\"Index Terms\"):]\n",
    "\n",
    "                df_temp['abstract'].append(abstract)\n",
    "                df_temp['keyword'].append(keyword)\n",
    "                print(f\"\\tAt page {link[-1]} Paper found: {count} - {title.text}\")\n",
    "                \n",
    "\n",
    "                driver.close()\n",
    "                count += 1\n",
    "\n",
    "            df_temp = pd.DataFrame(df_temp)\n",
    "            df = pd.read_csv(FILE_NAME)\n",
    "            df = pd.concat([df, df_temp], ignore_index=True)\n",
    "            df.to_csv(FILE_NAME, index=False)\n",
    "            print(df.tail(5))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at page: {link[-1]} at paper: {count} \\n {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    start = time.time()\n",
    "    if ((args.end-args.start) == 1):\n",
    "        print(f\"\\nParsing IEEE Articles single page at: {args.start}\") \n",
    "        FILE_NAME = str(np.random.randint(0, 100000)) + f'IEEE_page_single_page_{args.start}_only.csv'\n",
    "    else:\n",
    "        FILE_NAME = str(np.random.randint(0, 100000)) + f'IEEE_page_{args.start}_{args.end}.csv'\n",
    "        print(f\"\\nParsing IEEE Articles from page {args.start}-{args.end}\") \n",
    "\n",
    "    for param in information_keys:\n",
    "        df[param] = []\n",
    "\n",
    "    pd.DataFrame(df).to_csv(FILE_NAME, index=False)\n",
    "\n",
    "    # Store link for the whole pages\n",
    "    URL_IEEE_LIST = [f'{URL_IEEE}&pageNumber={i}' for i in range(args.start, args.end)]\n",
    "    count = 0\n",
    "    chunks = split_range(URL_IEEE_LIST, args.number_of_agent)\n",
    "    \n",
    "    # Creating a thread agent for opening each page.\n",
    "    thread_workers = []\n",
    "    for chunk in chunks:\n",
    "        t = Thread(target=getLinkEachPage, args=(chunk, ))\n",
    "        thread_workers.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    for t in thread_workers:\n",
    "        t.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement from the IEEE scraping is adding audio notification, increase efficiency by using one loop for searching abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from threading import Thread\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import queue\n",
    "import os, csv, re\n",
    "from selenium import webdriver\n",
    "from csv import DictWriter\n",
    "import winsound\n",
    "import time\n",
    "frequency = 1000  # You can adjust this value\n",
    "duration = 500  # 5000 milliseconds = 5 seconds\n",
    "\n",
    "# Beep for 5 seconds\n",
    "\n",
    "\n",
    "global FILE_NAME\n",
    "# user input arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--start', type=int, required=True)\n",
    "parser.add_argument('--end', type=int, required=True)\n",
    "parser.add_argument('--number_of_agent', type=int, required=True)\n",
    "\n",
    "# Webdriver Properties\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--ignore-ssl-errors')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--incognito')\n",
    "chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"normal\"\n",
    "\n",
    "# Journal Scrapping Argument\n",
    "information_keys = ['title', 'link', 'number_of_citation', 'article_type', 'publisher', 'publication_date', 'abstract', 'keyword']\n",
    "DIGITAL_LIBRARIES_ACCEPTED = ['ieee', 'Springer', 'ACM', 'Elsevier', 'Wiley', 'Sage']\n",
    "URL_ACM = '''https://dl.acm.org/action/doSearch?fillQuickSearch=false&target=advanced&field1=AllField&text1=%28%22reinforcement+learning%22+OR+%22RL%22%29+AND+%28%22protocol+routing%22+OR+%22routing+strategy%22+OR+%22routing+algorithm%22+OR+%22routing%22%29+AND+%28%22Wireless+Sensor+Network%22+OR+%22WSN%22+OR+%22Sensor+Network%22%29&searchArea%5B0%5D=SeriesKey&operator%5B0%5D=And&text%5B0%5D=&searchArea%5B1%5D=SeriesKey&operator%5B1%5D=And&text%5B1%5D=&EpubDate=&AfterMonth=1&AfterYear=2010&BeforeMonth=&BeforeYear=&pageSize=50&expand=all'''\n",
    "df = {}\n",
    "\n",
    "\n",
    "\n",
    "def split_range(_range, parts): #split a range to chunks\n",
    "    chunk_size = int(len(_range)/parts)\n",
    "    chunks = [_range[x:x+chunk_size] for x in range(0, len(_range), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "# Collecting title, link, number of citation, article type, and publication date\n",
    "def getLinkEachPage(links):\n",
    "    for link in links:\n",
    "        \n",
    "        count = 1\n",
    "    \n",
    "        driver = webdriver.Chrome(\n",
    "    executable_path= ChromeDriverManager().install(), options=chrome_options)\n",
    "        driver.get(link)\n",
    "        allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "        )\n",
    "\n",
    "        # Click the 'Allow all cookies' button\n",
    "        allow_all_cookies_button.click()\n",
    "        html = driver.page_source\n",
    "\n",
    "        driver.quit()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Check all the paper in div body\n",
    "        count_article = 0\n",
    "        for article in soup.find_all('li', class_='search__item issue-item-container'):\n",
    "            df_temp = {}\n",
    "            for param in information_keys:\n",
    "                df_temp[param] = []\n",
    "            title               = article.find('h5', class_='issue-item__title').get_text()\n",
    "            link_article        = 'https://dl.acm.org' + article.find('h5', class_='issue-item__title').find('a')['href']\n",
    "\n",
    "            # Get publisher information such as year, article type and publisher\n",
    "            publication_date   = int(article.find('div', class_='bookPubDate simple-tooltip__block--b').get_text()[-4:])\n",
    "            article_type       = article.find('div', class_='issue-heading').get_text()\n",
    "            number_of_citation = int(article.find('span', class_='citation').get_text())\n",
    "\n",
    "            df_temp['title'].append(title)\n",
    "            df_temp['link'].append(link_article)\n",
    "            df_temp['number_of_citation'].append(number_of_citation)\n",
    "            df_temp['article_type'].append(article_type)\n",
    "            df_temp['publication_date'].append(publication_date)\n",
    "            df_temp['publisher'].append('ACM')\n",
    "\n",
    "    \n",
    "            link_article = df_temp['link'][len(df_temp['link'])-1]\n",
    "            try:\n",
    "                t = np.random.random()*3\n",
    "                time.sleep(t)\n",
    "                start_time = time.time()\n",
    "                driver = webdriver.Chrome(\n",
    "    executable_path= ChromeDriverManager().install(), options=chrome_options)\n",
    "                driver.get(link_article)\n",
    "                allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "                )\n",
    "\n",
    "                # Click the 'Allow all cookies' button\n",
    "                allow_all_cookies_button.click()\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                abstract = soup.find(\"div\", id=\"abstracts\").get_text()[len(\"Abstract\"):]\n",
    "                driver.close()\n",
    "\n",
    "                df_temp['abstract'].append(abstract)\n",
    "                df_temp['keyword'].append('none')\n",
    "                \n",
    "                count += 1\n",
    "                print(f\"{count*(2/100)*100:.0f}%{\"-\"*50}{time.time()-start_time} s\")\n",
    "            \n",
    "            # If error or wont load, try to get\n",
    "            except:\n",
    "                driver.close()\n",
    "                try:\n",
    "                    retrieve_failed = True\n",
    "                    for i in range(5):\n",
    "                        print(f\"Trying to retrieve iteration: {i}\")\n",
    "                        if retrieve_failed:\n",
    "                            t = np.random.random()*3\n",
    "                            time.sleep(t)\n",
    "                            start_time = time.time()\n",
    "                            driver = webdriver.Chrome(\n",
    "    executable_path= ChromeDriverManager().install(), options=chrome_options)\n",
    "                            driver.get(link_article)\n",
    "                            allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "                            EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "                            )\n",
    "\n",
    "                            # Click the 'Allow all cookies' button\n",
    "                            allow_all_cookies_button.click()\n",
    "                            html = driver.page_source\n",
    "                            soup = BeautifulSoup(html, 'html.parser')\n",
    "                            abstract = soup.find(\"div\", id=\"abstracts\").get_text()[len(\"Abstract\"):]\n",
    "                            driver.close()\n",
    "\n",
    "                            df_temp['abstract'].append(abstract)\n",
    "                            df_temp['keyword'].append('none')\n",
    "                            \n",
    "                            count += 1\n",
    "\n",
    "                            print(f\"\\n{\"-\"*10} Successfuly retrieved at iteration: {i} {\"-\"*10}\")\n",
    "                            print(f\"{count*(2/100)*100:.0f}%{\"-\"*50}{time.time()-start_time} ms\")\n",
    "                            \n",
    "\n",
    "\n",
    "                            retrieve_failed = False\n",
    "                except:\n",
    "                    print(f\"Failed to retrive at iteration-{i}\")\n",
    "\n",
    "            print(f\"Scraping at page: {link[-1]} | Article number: {count_article}\")\n",
    "           \n",
    "            \n",
    "            count_article += 1\n",
    "\n",
    "            try:\n",
    "                print(\"\\t\" + \"-\"*100)\n",
    "                df_temp = pd.DataFrame(df_temp)\n",
    "                df = pd.read_csv(FILE_NAME)\n",
    "                df = pd.concat([df, df_temp], ignore_index=True)\n",
    "                df.to_csv(FILE_NAME, index=False)\n",
    "                print(df.tail(5))\n",
    "                df_temp = ''\n",
    "                df = ''\n",
    "            except:\n",
    "                print(f\"Cant input article numner: {count_article} | title: {title}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    start = time.time()\n",
    "    if ((args.end-args.start) == 1):\n",
    "        print(f\"\\nParsing ACM Articles single page at: {args.start}\") \n",
    "        FILE_NAME = f'ACM_page_single_page_{args.start}_only_{np.random.randint(10000, 99999)}.csv'\n",
    "    else:\n",
    "        FILE_NAME = f'ACM_page_{args.start}_{args.end}_{np.random.randint(10000, 99999)}.csv'\n",
    "        print(f\"\\nParsing ACM Articles from page {args.start}-{args.end}\") \n",
    "\n",
    "    for param in information_keys:\n",
    "        df[param] = []\n",
    "\n",
    "    pd.DataFrame(df).to_csv(FILE_NAME, index=False)\n",
    "\n",
    "    # Store link for the whole pages\n",
    "    URL_ACM_LIST = [f'{URL_ACM}&startPage={i-1}' for i in range(args.start, args.end)]\n",
    "    count = 0\n",
    "    chunks = split_range(URL_ACM_LIST, args.number_of_agent)\n",
    "    \n",
    "    # Creating a thread agent for opening each page.\n",
    "    thread_workers = []\n",
    "    for chunk in chunks:\n",
    "        t = Thread(target=getLinkEachPage, args=(chunk, ))\n",
    "        thread_workers.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    for t in thread_workers:\n",
    "        t.join()\n",
    "    \n",
    "    for i in range(4):\n",
    "        winsound.Beep(frequency, duration)\n",
    "        time.sleep(0.05)\n",
    "        winsound.Beep(frequency//2, duration)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sciencedirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\n",
    "    executable_path= ChromeDriverManager().install(), options=chrome_options)\n",
    "driver.get(URL_ELSEVIER)\n",
    "# allow_all_cookies_button = WebDriverWait(driver, 10).until(\n",
    "#     EC.visibility_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from threading import Thread\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import queue\n",
    "import os, csv, re\n",
    "from selenium import webdriver\n",
    "from csv import DictWriter\n",
    "import winsound\n",
    "import time\n",
    "frequency = 1000  # You can adjust this value\n",
    "duration = 500  # 5000 milliseconds = 5 seconds\n",
    "\n",
    "\n",
    "\n",
    "global FILE_NAME\n",
    "# user input arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--start', type=int, required=True)\n",
    "parser.add_argument('--end', type=int, required=True)\n",
    "parser.add_argument('--number_of_agent', type=int, required=True)\n",
    "\n",
    "# Webdriver Properties\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument('--ignore-ssl-errors')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--incognito')\n",
    "# chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "caps = DesiredCapabilities().CHROME\n",
    "caps[\"pageLoadStrategy\"] = \"normal\"\n",
    "\n",
    "# Journal Scrapping Argument\n",
    "information_keys = ['title', 'link', 'number_of_citation', 'article_type', 'publisher', 'publication_date', 'abstract', 'keyword']\n",
    "DIGITAL_LIBRARIES_ACCEPTED = ['ieee', 'Springer', 'ACM', 'Elsevier', 'Wiley', 'Sage']\n",
    "\n",
    "URL_WILEY = f'https://onlinelibrary.wiley.com/action/doSearch?AfterYear=2010&AllField=%28%22reinforcement+learning%22+OR+%22RL%22%29+AND+%28%22protocol+routing%22+OR+%22routing+strategy%22+OR+%22routing+algorithm%22+OR+%22routing%22%29+AND+%28%22Wireless+Sensor+Network%22+OR+%22WSN%22+OR+%22Sensor+Network%22%29&BeforeYear=2024&content=articlesChapters&target=default&pageSize=100'\n",
    " \n",
    "df = {}\n",
    "\n",
    "def split_range(_range, parts): #split a range to chunks\n",
    "    chunk_size = int(len(_range)/parts)\n",
    "    chunks = [_range[x:x+chunk_size] for x in range(0, len(_range), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def getAllArticles(links):\n",
    "    soups = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            count = 1\n",
    "            driver = webdriver.Chrome(\n",
    "                executable_path= ChromeDriverManager().install(), \n",
    "                options=chrome_options\n",
    "            )\n",
    "            driver.get(link)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located(\n",
    "                    (\n",
    "                        By.ID, \"search-result\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            html = driver.page_source\n",
    "            driver.close()\n",
    "            soups.append(BeautifulSoup(html, 'html.parser'))\n",
    "        except:\n",
    "            driver.close()\n",
    "            retrieve_failed = True\n",
    "            i = 1\n",
    "            while ((retrieve_failed) and i<4):\n",
    "                try:\n",
    "                    print(f\"\\nTrying to retrieve all articles at iteration: {i}\\n\")\n",
    "                    driver = webdriver.Chrome(\n",
    "                        executable_path= ChromeDriverManager().install(), \n",
    "                        options=chrome_options\n",
    "                    )\n",
    "                    driver.get(link)\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.visibility_of_element_located(\n",
    "                            (\n",
    "                                By.ID, \"search-result\"\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                    html = driver.page_source\n",
    "                    driver.close()\n",
    "                    retrieve_failed = False\n",
    "                    print(f\"Successfully retrieve all articles at iteration: {i}\")\n",
    "                    soups.append(BeautifulSoup(html, 'html.parser'))\n",
    "                except:\n",
    "                    driver.close()\n",
    "                    print(f\"Fail to retrieve all articles at iteration: {i}\")\n",
    "                \n",
    "                i+=1\n",
    "    return soups\n",
    "\n",
    "def getInfoArticle(soups):\n",
    "    if len(soups) == 0:\n",
    "        print(f\"Failed to retrieve anything\")\n",
    "        return None\n",
    "    else:\n",
    "        for soup in soups:\n",
    "            count_article = 0\n",
    "            for article in soup.find_all('li', class_='clearfix separator search__item bulkDownloadWrapper'):\n",
    "                t = np.random.random()*3\n",
    "                time.sleep(t)\n",
    "                start_time = time.time()\n",
    "\n",
    "                title = article.find('h2').find('div').find('input')['title'][len('Select article for bulk download or export: '):]\n",
    "                link_article = 'https://onlinelibrary.wiley.com' + article.find('h2').find('a')['href']\n",
    "                publication_date = article.find('div', class_='meta__info hlFld-Title').find('div', class_='meta__details').find('p').get_text()[-4:]\n",
    "                article_type = article.find('div', class_='meta__header').find('span').get_text()\n",
    "\n",
    "                try:\n",
    "                    count = 1\n",
    "                    driver = webdriver.Chrome(\n",
    "                        executable_path= ChromeDriverManager().install(), \n",
    "                        options=chrome_options\n",
    "                    )\n",
    "                    driver.get(link_article)\n",
    "                    accept_all_button = WebDriverWait(driver, 10).until(\n",
    "                        EC.visibility_of_element_located(\n",
    "                            (\n",
    "                                By.XPATH, \"//button[contains(@class, 'osano-cm-accept-all')]\"\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                    accept_all_button.click()\n",
    "\n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    driver.close()\n",
    "                except:\n",
    "                    driver.close()\n",
    "                    retrieve_failed = True\n",
    "                    i = 1\n",
    "                    while ((retrieve_failed) and i<4):\n",
    "                        try:\n",
    "                            print(f\"\\nTrying to retrieve iteration: {i}\\n\")\n",
    "                            driver = webdriver.Chrome(\n",
    "                                executable_path= ChromeDriverManager().install(), \n",
    "                                options=chrome_options\n",
    "                            )\n",
    "                            driver.get(link_article)\n",
    "                            accept_all_button = WebDriverWait(driver, 10).until(\n",
    "                                EC.visibility_of_element_located(\n",
    "                                    (\n",
    "                                        By.XPATH, \"//button[contains(@class, 'osano-cm-accept-all')]\"\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                            accept_all_button.click()\n",
    "\n",
    "                            html = driver.page_source\n",
    "                            soup = BeautifulSoup(html, 'html.parser')\n",
    "                            driver.close()\n",
    "                            retrieve_failed = False\n",
    "                            print(f\"Successfully retrieve at itration: {i}\")\n",
    "                            \n",
    "                        except:\n",
    "                            driver.close()\n",
    "                            print(f\"Fail to retrieve at iteration: {i}\")\n",
    "                        \n",
    "                        i+=1\n",
    "\n",
    "\n",
    "                try:\n",
    "                    abstract = soup.find('div', class_='article-section__content en main').find('p').get_text()\n",
    "\n",
    "                    print(soup.find('div', class_='article-section__content en main'))\n",
    "\n",
    "                    number_of_citation = 'none' if soup.find('div', class_='epub-section cited-by-count') == None else soup.find('div', class_='epub-section cited-by-count').find('a').get_text()\n",
    "                    df_temp = {}\n",
    "                    for param in information_keys:\n",
    "                        df_temp[param] = []\n",
    "\n",
    "                    df_temp['title'].append(title)\n",
    "                    df_temp['link'].append(link_article)\n",
    "                    df_temp['number_of_citation'].append(number_of_citation)\n",
    "                    df_temp['article_type'].append(article_type)\n",
    "                    df_temp['publication_date'].append(publication_date)\n",
    "                    df_temp['publisher'].append('Wiley')\n",
    "                    df_temp['abstract'].append(abstract)\n",
    "                    df_temp['keyword'].append('none')\n",
    "                    print(\"\\t\" + \"-\"*100)\n",
    "                    df_temp = pd.DataFrame(df_temp)\n",
    "                    df = pd.read_csv(FILE_NAME)\n",
    "                    count = len(df.index)\n",
    "                    df = pd.concat([df, df_temp], ignore_index=True)\n",
    "                    df.to_csv(FILE_NAME, index=False)\n",
    "                    print(df.tail(5))\n",
    "                    df_temp = ''\n",
    "                    df = ''\n",
    "                    print(f\"{count*(2/100)*100:.0f}%{\"-\"*50}{time.time()-start_time}\")\n",
    "                except:\n",
    "                    print(f\"Cant input article numner: {count_article} | title: {title}\")\n",
    "                \n",
    "\n",
    "\n",
    "def main(links):\n",
    "    soup = getAllArticles(links)\n",
    "    getInfoArticle(soup)\n",
    "    for i in range(4):\n",
    "        winsound.Beep(frequency, duration)\n",
    "        time.sleep(0.05)\n",
    "        winsound.Beep(frequency//2, duration)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    start = time.time()\n",
    "    if ((args.end-args.start) == 1):\n",
    "        print(f\"\\nParsing Wiley Articles single page at: {args.start}\") \n",
    "        FILE_NAME = f'Wiley_page_single_page_{args.start}_only_{np.random.randint(10000, 99999)}.csv'\n",
    "    else:\n",
    "        FILE_NAME = f'Wiley_page_{args.start}_{args.end}_{np.random.randint(10000, 99999)}.csv'\n",
    "        print(f\"\\nParsing Wiley Articles from page {args.start}-{args.end}\")\n",
    "    \n",
    "    for param in information_keys:\n",
    "        df[param] = []\n",
    "\n",
    "    pd.DataFrame(df).to_csv(FILE_NAME, index=False)\n",
    "    URL_WILEY_LIST = [f'{URL_WILEY}&startPage={i-1}' for i in range(args.start, args.end)]\n",
    "\n",
    "    count = 0\n",
    "    chunks = split_range(URL_WILEY_LIST, args.number_of_agent)\n",
    "    \n",
    "    # Creating a thread agent for opening each page.\n",
    "    thread_workers = []\n",
    "    for chunk in chunks:\n",
    "        t = Thread(target=main, args=(chunk, ))\n",
    "        thread_workers.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    for t in thread_workers:\n",
    "        t.join()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Springer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SPRINGER = '''https://link.springer.com/search/page/1?query=%28%22reinforcement+learning%22+OR+%22RL%22%29+AND+%28%22protocol+routing%22+OR+%22routing+strategy%22+OR+%22routing+algorithm%22+OR+%22routing%22%29+AND+%28%22Wireless+Sensor+Network%22+OR+%22WSN%22+OR+%22Sensor+Network%22%29&facet-content-type=%22Article%22'''\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path= ChromeDriverManager().install(), \n",
    "    options=chrome_options\n",
    ")\n",
    "driver.get(URL_SPRINGER)\n",
    "allow_cookies_button = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located(\n",
    "        (\n",
    "            By.CSS_SELECTOR, \"button[data-cc-action='accept']\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "allow_cookies_button.click()\n",
    "html = driver.page_source\n",
    "driver.close()\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Optimizing IoT-enabled WSN routing strategies using whale optimization-driven multi-criterion correlation approach employs the reinforcement learning agent\n",
      "Link: https://link.springer.com/article/10.1007/s11082-023-06269-4\n",
      "Publication_date: 2024\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: An intelligent fault detection approach based on reinforcement learning system in wireless sensor network\n",
      "Link: https://link.springer.com/article/10.1007/s11227-021-04001-1\n",
      "Publication_date: 2022\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: A novel algorithm for wireless sensor network routing protocols based on reinforcement learning\n",
      "Link: https://link.springer.com/article/10.1007/s13198-021-01414-2\n",
      "Publication_date: 2022\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Exploring machine learning solutions for overcoming challenges in IoT-based wireless sensor network routing: a comprehensive review\n",
      "Link: https://link.springer.com/article/10.1007/s11276-024-03697-2\n",
      "Publication_date: 2024\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Multi-objective intelligent clustering routing schema for internet of things enabled wireless sensor networks using deep reinforcement learning\n",
      "Link: https://link.springer.com/article/10.1007/s10586-023-04218-0\n",
      "Publication_date: 2024\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Energy-balanced routing in wireless sensor networks with reinforcement learning using greedy action chains\n",
      "Link: https://link.springer.com/article/10.1007/s00500-023-08734-4\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Energy Efficient and Delay Aware Optimization Reverse Routing Strategy for Forecasting Link Quality in Wireless Sensor Networks\n",
      "Link: https://link.springer.com/article/10.1007/s11277-022-09982-7\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Weighted-based path rediscovery routing algorithm for improving the routing decision in wireless sensor network\n",
      "Link: https://link.springer.com/article/10.1007/s12652-020-02709-1\n",
      "Publication_date: 2021\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Routing Algorithm for Energy Efficiency Optimizing of Wireless Sensor Networks based on Genetic Algorithms\n",
      "Link: https://link.springer.com/article/10.1007/s11277-023-10849-8\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Particle-Water Wave Optimization for Secure Routing in Wireless Sensor Network Using Cluster Head Selection\n",
      "Link: https://link.springer.com/article/10.1007/s11277-021-08335-0\n",
      "Publication_date: 2021\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Recent trends and future directions of congestion management strategies for routing in IoT-based wireless sensor network: a thematic review\n",
      "Link: https://link.springer.com/article/10.1007/s11276-023-03598-w\n",
      "Publication_date: 2024\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Betweenness centrality based connectivity aware routing algorithm for prolonging network lifetime in wireless sensor networks\n",
      "Link: https://link.springer.com/article/10.1007/s11276-015-1054-5\n",
      "Publication_date: 2016\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: LEO laser microwave hybrid inter-satellite routing strategy based on modified Q-routing algorithm\n",
      "Link: https://link.springer.com/article/10.1186/s13638-022-02119-1\n",
      "Publication_date: 2022\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Machine Learning Based Low Redundancy Prediction Model for IoT-Enabled Wireless Sensor Network\n",
      "Link: https://link.springer.com/article/10.1007/s42979-023-01898-8\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Optimizing optical network longevity via Q-learning-based routing protocol for energy efficiency and throughput enhancement\n",
      "Link: https://link.springer.com/article/10.1007/s11082-023-05658-z\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: An ant colony optimization based routing algorithm for extending network lifetime in wireless sensor networks\n",
      "Link: https://link.springer.com/article/10.1007/s11276-015-1061-6\n",
      "Publication_date: 2016\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Multiple agent based reinforcement learning for energy efficient routing in WSN\n",
      "Link: https://link.springer.com/article/10.1007/s11276-022-03198-0\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Missile thermal emission flow field under the synergistic effect of deep reinforcement learning and wireless sensor network\n",
      "Link: https://link.springer.com/article/10.1007/s11276-023-03415-4\n",
      "Publication_date: 2023\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Routing in mobile wireless sensor network: a survey\n",
      "Link: https://link.springer.com/article/10.1007/s11235-013-9766-2\n",
      "Publication_date: 2014\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n",
      "Title: Lifetime maximization of wireless sensor network using fuzzy based unequal clustering and ACO based routing hybrid protocol\n",
      "Link: https://link.springer.com/article/10.1007/s10489-017-1077-y\n",
      "Publication_date: 2018\n",
      "article_type: Article\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for article in soup.find('ol', id='results-list').find_all('li'):\n",
    "    title = article.find('h2').get_text()[1:-1]\n",
    "    article_type = article.find('p', class_='content-type').get_text(strip=True)\n",
    "    link_article = 'https://link.springer.com' + article.find('h2').find('a')['href']\n",
    "    publication_date   = int(article.find('span', class_='year').get_text()[-5:-1])\n",
    "    \n",
    "    print(f'Title: {title}')\n",
    "    print(f'Link: {link_article}')\n",
    "    print(f'Publication_date: {publication_date}')\n",
    "    print(f'article_type: {article_type}')\n",
    "    # print(f'\\t\\tPublisher: {publisher[-1]}')\n",
    "    # print(f'\\t\\tSnippet: {snippet}')\n",
    "    # print(f'\\t\\tCitations: {citations}')\n",
    "    print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SPRINGER = '''https://link.springer.com/article/10.1007/s10586-023-04218-0'''\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    executable_path= ChromeDriverManager().install(), \n",
    "    options=chrome_options\n",
    ")\n",
    "driver.get(URL_SPRINGER)\n",
    "allow_cookies_button = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located(\n",
    "        (\n",
    "            By.CSS_SELECTOR, \"button[data-cc-action='accept']\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "allow_cookies_button.click()\n",
    "html = driver.page_source\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[264], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     number_of_citation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 13\u001b[0m abstract \u001b[38;5;241m=\u001b[39m \u001b[43msoup2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc-article-section__content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m()\n\u001b[0;32m     14\u001b[0m keyword \u001b[38;5;241m=\u001b[39m soup2\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc-article-subject-list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m keyword \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m keyword]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(html, 'html.parser')\n",
    "number_of_citation = soup2.find_all('p', class_=\"app-article-metrics-bar__count\")\n",
    "if number_of_citation != None:\n",
    "    if (len(number_of_citation) > 1):\n",
    "        number_of_citation = soup2.find_all('p', class_=\"app-article-metrics-bar__count\")[1].get_text(strip=True)\n",
    "        if 'Citations' in number_of_citation:\n",
    "            number_of_citation = int(re.search(r'\\d+', number_of_citation).group())\n",
    "    else:\n",
    "        number_of_citation = 'none'\n",
    "else:\n",
    "    number_of_citation = 'none'\n",
    "    \n",
    "abstract = soup2.find('div', class_=\"c-article-section__content\").get_text()\n",
    "keyword = soup2.find('ul', class_=\"c-article-subject-list\").find_all('li')\n",
    "keyword = [f\"{i.get_text()}\" for i in keyword]\n",
    "keyword = ','.join(keyword)\n",
    "\n",
    "print(number_of_citation)\n",
    "print(abstract)\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(number_of_citation) >= 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google scholar tidak dapat bekerja karena masalah captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua=UserAgent()\n",
    "headers = {'User-Agent': ua.random,\n",
    "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "      'Accept-Encoding': 'none',\n",
    "      'Accept-Language': 'en-US,en;q=0.8',\n",
    "      'Connection': 'keep-alive'}\n",
    "\n",
    "response = requests.get(URL_SCHOLAR, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "next_links = soup.find_all('div', id='gs_nml')\n",
    "print(soup.prettify())\n",
    "next_links = next_links[0].find_all('a')\n",
    "\n",
    "df_IEEE_links = {}\n",
    "\n",
    "# Untuk semua page\n",
    "for link in next_links:\n",
    "    response = requests.get(URL_SCHOLAR, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    next_links = soup.find_all('div', id='gs_nml')\n",
    "    next_links = next_links[0].find_all('a')\n",
    "    articles = soup.find_all('div', class_='gs_r gs_or gs_scl')\n",
    "\n",
    "    for article in articles:\n",
    "        title_tag = article.find('h3', class_='gs_rt')\n",
    "        if title_tag:\n",
    "            title = title_tag.get_text()\n",
    "            article_links = title_tag.find('a')['href'] if title_tag.find('a') else 'No link'\n",
    "\n",
    "        author_info_tag = article.find('div', class_='gs_a')\n",
    "        if author_info_tag:\n",
    "            author_info = author_info_tag.get_text()\n",
    "\n",
    "        snippet_tag = article.find('div', class_='gs_rs')\n",
    "        if snippet_tag:\n",
    "            snippet = snippet_tag.get_text()\n",
    "\n",
    "        citation_tag = article.find('div', class_='gs_fl')\n",
    "        if citation_tag:\n",
    "            citation_info = citation_tag.find_all('a')\n",
    "            if citation_info and len(citation_info) > 2:\n",
    "                citations = citation_info[2].get_text(strip=True)\n",
    "            else:\n",
    "                citations = 'No citations'\n",
    "\n",
    "        publisher = author_info.replace('\\xa0-', ',').split(',')\n",
    "        print(f'\\t\\tTitle: {title}')\n",
    "        print(f'\\t\\tLink: {article_links}')\n",
    "        print(f'\\t\\tAuthor Info: {author_info}')\n",
    "        print(f'\\t\\tPublisher: {publisher[-1]}')\n",
    "        print(f'\\t\\tSnippet: {snippet}')\n",
    "        print(f'\\t\\tCitations: {citations}')\n",
    "        print('\\t\\t-' * 80)\n",
    "    \n",
    "    \n",
    "    URL_SCHOLAR = 'https://scholar.google.com' + link['href']\n",
    "    print(f'Page {link.get_text()} : {'\\t\\t-' * 80}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_LIST = []\n",
    "count = 0\n",
    "for folder in os.listdir():\n",
    "    if os.path.isdir(folder):\n",
    "        for data in os.listdir(folder):\n",
    "            PATH_LIST.append(\"./\" + folder + '/' + data)\n",
    "\n",
    "for data in PATH_LIST:\n",
    "    if count > 1:\n",
    "        df_temp = pd.read_csv(data)\n",
    "        df = pd.concat([df, df_temp])\n",
    "    else:\n",
    "        df = pd.read_csv(data)\n",
    "    count += 1\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_csv('ALL_ARTICLES.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
