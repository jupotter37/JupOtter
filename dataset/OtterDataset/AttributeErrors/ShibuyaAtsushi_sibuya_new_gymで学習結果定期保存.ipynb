{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定期保存追加学習１/２\n",
    "\n",
    "# ログを作って、\n",
    "\n",
    "import gymnasium as gym\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Separate evaluation env\n",
    "# eval_env = gym.make('Humanoid-v4')\n",
    "# Use deterministic actions for evaluation\n",
    "# eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',log_path='./logs/', eval_freq=500,deterministic=True, render=True)\n",
    "\n",
    "# ログフォルダの生成 (1)\n",
    "log_dir = './logs/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 環境の生成\n",
    "env = gym.make('Ant-v4')\n",
    "\n",
    "\n",
    "\n",
    "title_name = 'gravitest'\n",
    "init_step = 0\n",
    "first_step = 0\n",
    "HowManyStepPerSave = 1000\n",
    "HowManyLoop = 20\n",
    "Now_Loop = 0\n",
    "\n",
    "# bot = LINENotifyBot(access_token='aHwsSGlYHfLYC7Wdl4iFXxQ13mN8TaA6VLiUuithsKk')\n",
    "\n",
    "#＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃＃\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "確認：init_stepは0です\n",
      "0回目からスタートし、1000 回の 学習を 20 セット 学習させます.１セットごとにモデルを保存します\n",
      "確認：init_stepは0です\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "checkpoint:if文のなかを通過しました.環境を新しく作成します\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -130     |\n",
      "| time/              |          |\n",
      "|    fps             | 275      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# モデルの推論\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 1ステップ実行\u001b[39;00m\n\u001b[0;32m     57\u001b[0m state, rewards, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "#定期保存追加学習2/2\n",
    "Now_Loop = 0\n",
    "first_step = 0\n",
    "\n",
    "print('確認：init_stepは' + str(init_step) + 'です')\n",
    "# モデルの生成\n",
    "#model = PPO.load(title_name+str(init_step),env) # こう書くことで、保存してある学習データと環境をを持って来ることができて、その続きを学習させられる\n",
    "# model = PPO.set_parameters('sample')\n",
    "# model = PPO('MlpPolicy', env, verbose=1) #ここは新規作成の欄\n",
    "\n",
    "\n",
    "print(str(init_step) + '回目からスタートし、' + str(HowManyStepPerSave) + ' 回の 学習を ' + str(HowManyLoop) + ' セット 学習させます.１セットごとにモデルを保存します')\n",
    "\n",
    "for num in range(HowManyLoop):\n",
    "    \n",
    "\n",
    "    # Separate evaluation env\n",
    "    # eval_env = gym.make('Humanoid-v4')\n",
    "    # Use deterministic actions for evaluation\n",
    "    # eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',log_path='./logs/', eval_freq=500,deterministic=True, render=True)\n",
    "\n",
    "    # ログフォルダの生成 (1)\n",
    "    log_dir = './logs/'\n",
    "    # おためしlog_dir = \"./title_name + 'init_step' + str(init_step + HowManyStepPerSave*Now_Loop) + 'steps'/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # 環境の生成\n",
    "    env = gym.make('Ant-v4', render_mode='human')\n",
    "\n",
    "    print('確認：init_stepは' + str(init_step) + 'です')\n",
    "\n",
    "    if init_step == 0 and first_step == 0:\n",
    "        model = PPO('MlpPolicy', env, verbose=1) #環境を新規に作成\n",
    "        first_step = 1\n",
    "        print('checkpoint:if文のなかを通過しました.環境を新しく作成します')\n",
    "        \n",
    "    else:\n",
    "        model = PPO.load(title_name + 'init_step' + str(init_step + HowManyStepPerSave*num) + 'steps',env) # こう書くことで、保存してある学習データと環境をを持って来ることができて、その続きを学習させられる\n",
    "        print('checkpoint:else文のなかを通過しました.追加学習します')\n",
    "\n",
    "    \n",
    "    Now_Loop = Now_Loop+1\n",
    "\n",
    "    # モデルの学習\n",
    "    model.learn(total_timesteps = HowManyStepPerSave) #10万回あたり６分かかります\n",
    "\n",
    "    # モデルのテスト\n",
    "    state = env.reset()\n",
    "    for i in range(400):\n",
    "        # 環境の描画\n",
    "        env.render()\n",
    "\n",
    "        # モデルの推論\n",
    "        action, _ = model.predict(state)\n",
    "\n",
    "        # 1ステップ実行\n",
    "        state, rewards, done, info = env.step(action)\n",
    "\n",
    "        # エピソード完了\n",
    "        if done:\n",
    "            break\n",
    "    # モデルの保存 (1)\n",
    "    model.save(title_name + 'init_step' + str(init_step + HowManyStepPerSave*Now_Loop) + 'steps')\n",
    "    print('ファイル名を' + title_name + 'init_step' + str(init_step + HowManyStepPerSave*Now_Loop) + 'steps' + 'で保存しました')\n",
    "    print(str(HowManyLoop)+ 'セット中' + str(Now_Loop) +'セット終わりました。現在は' + str(init_step + HowManyStepPerSave*Now_Loop) + 'ステップ終わったところです。')\n",
    "    # 環境のクローズ\n",
    "    env.close()\n",
    "    \n",
    "\n",
    "print(str(HowManyLoop)+ 'セット中' + str(Now_Loop) +'セット終わりました')\n",
    "\n",
    "init_step = init_step + HowManyLoop * HowManyStepPerSave\n",
    "print('init_step変数を更新しました。すぐに続きの' + str(init_step) + 'ステップ目から学習できます')\n",
    "Now_Loop=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.1     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 1348     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'gym' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 環境の作成 (推論用)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# test_env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# モデルの保存\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 環境のクローズ\u001b[39;00m\n\u001b[0;32m     31\u001b[0m train_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:844\u001b[0m, in \u001b[0;36mBaseAlgorithm.save\u001b[1;34m(self, path, exclude, include)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# Build dict of state_dicts\u001b[39;00m\n\u001b[0;32m    842\u001b[0m params_to_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m--> 844\u001b[0m \u001b[43msave_to_zip_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_to_save\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytorch_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytorch_variables\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:333\u001b[0m, in \u001b[0;36msave_to_zip_file\u001b[1;34m(save_path, data, params, pytorch_variables, verbose)\u001b[0m\n\u001b[0;32m    331\u001b[0m     archive\u001b[38;5;241m.\u001b[39mwritestr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_stable_baselines3_version\u001b[39m\u001b[38;5;124m\"\u001b[39m, sb3\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# Save system info about the current python env\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     archive\u001b[38;5;241m.\u001b[39mwritestr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_info.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mget_system_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(save_path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m    336\u001b[0m     file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:543\u001b[0m, in \u001b[0;36mget_system_info\u001b[1;34m(print_info)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopenai_gym\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m     env_info\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI Gym\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mopenai_gym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m})\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# 環境の作成 (トレーニング用)\n",
    "train_env = gym.make(\"Humanoid-v4\")\n",
    "model = PPO(\"MlpPolicy\", train_env, verbose=1)\n",
    "model.learn(total_timesteps=1000)\n",
    "\n",
    "# 環境の作成 (推論用)\n",
    "# test_env = gym.make(\"Humanoid-v4\", render_mode=\"human\")\n",
    "\n",
    "# # モデルのテスト\n",
    "# obs, _ = test_env.reset()\n",
    "# for i in range(4000):\n",
    "#     # 環境の描画\n",
    "#     test_env.render()\n",
    "\n",
    "#     # モデルの推論\n",
    "#     action, _ = model.predict(obs)\n",
    "\n",
    "#     # 1ステップ実行\n",
    "#     obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "#     # エピソード完了\n",
    "#     if terminated or truncated:\n",
    "#         obs, _ = test_env.reset()  # エピソードが終了したら、環境をリセット\n",
    "\n",
    "# モデルの保存\n",
    "model.save('test')\n",
    "# 環境のクローズ\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;66;03m#progress_bar = True, callback=callback)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#     # モデルのテスト\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#     obs, _ = env.reset()  # ここを修正\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# #モデルの保存 (1)\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m相対位置でpinだけ頼りに真ん中に行けるか実験LSTMPPO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mモデルを保存しました\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;66;03m# グラフを作成\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:844\u001b[0m, in \u001b[0;36mBaseAlgorithm.save\u001b[1;34m(self, path, exclude, include)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# Build dict of state_dicts\u001b[39;00m\n\u001b[0;32m    842\u001b[0m params_to_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parameters()\n\u001b[1;32m--> 844\u001b[0m \u001b[43msave_to_zip_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_to_save\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytorch_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytorch_variables\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:333\u001b[0m, in \u001b[0;36msave_to_zip_file\u001b[1;34m(save_path, data, params, pytorch_variables, verbose)\u001b[0m\n\u001b[0;32m    331\u001b[0m     archive\u001b[38;5;241m.\u001b[39mwritestr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_stable_baselines3_version\u001b[39m\u001b[38;5;124m\"\u001b[39m, sb3\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# Save system info about the current python env\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m     archive\u001b[38;5;241m.\u001b[39mwritestr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_info.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mget_system_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(save_path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m    336\u001b[0m     file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\atusi\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:543\u001b[0m, in \u001b[0;36mget_system_info\u001b[1;34m(print_info)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopenai_gym\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m     env_info\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI Gym\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mopenai_gym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m})\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC, PPO ,DQN\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import datetime\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "env = gym.make(\"Ant-v4\" )\n",
    "\n",
    "# video_path = \"./\"  # 保存先のpath\n",
    "# env = RecordVideo(env, video_path, video_length=500)\n",
    "# env.model.opt.timestep = 0.01  # タイムステップを設定 RecordVideoする場合はそれ以降に書かないとerrorが出る\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "# model = PPO.load('神視点位置と角度、半径0.1円上ランダム出現',env)\n",
    "# render_freq = 1000  # 1000ステップごとにレンダリング\n",
    "# callback = CustomCallback(render_freq)\n",
    "for i in range(1):\n",
    "    model.learn(total_timesteps=1000)#progress_bar = True, callback=callback)\n",
    "\n",
    "#     # モデルのテスト\n",
    "#     obs, _ = env.reset()  # ここを修正\n",
    "    \n",
    "#     for unk in trange(10000):\n",
    "#         # 環境の描画\n",
    "#         env.render()\n",
    "\n",
    "#         # モデルの推論\n",
    "#         # print(\"本物のobs!\",obs)\n",
    "#         action, _ = model.predict(obs)\n",
    "#         # print(\"本物のaction\", action)\n",
    "\n",
    "#         # 1ステップ実行\n",
    "#         obs, reward, terminated, truncated, info = env.step(action) #以前の4要素のタプルから5要素のタプルに変更され，`observation, reward, terminated, truncated, info`という形式になった。\n",
    "#         # print(obs)\n",
    "\n",
    "\n",
    "#         # エピソード完了（終了または切り捨て）のチェック terminated(終了した)は目的を達成してエピソードを終了したことを表す，truncated(切り捨てられた)は，達成できずにエピソードが終了したことを表す\n",
    "#         if terminated or truncated:\n",
    "#             # print(obs)\n",
    "#             obs, _ = env.reset() # エピソードが終了したら、環境をリセット\n",
    "\n",
    "\n",
    "            \n",
    "    # #モデルの保存 (1)\n",
    "    model.save('相対位置でpinだけ頼りに真ん中に行けるか実験LSTMPPO')\n",
    "    print(\"モデルを保存しました\")\n",
    "        # グラフを作成\n",
    "    plt.plot(env.epi_reward_graph)\n",
    "    plt.show()\n",
    "    # model.save('hidarimawari_housyuu_godsight_onlyxy_notight')\n",
    "\n",
    "# print(video_path)\n",
    "plt.title(\"各ステップの報酬値\", fontname=\"MS Gothic\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"ステップ数\", fontname=\"MS Gothic\")\n",
    "plt.ylabel(\"報酬値\", fontname=\"MS Gothic\")\n",
    "\n",
    "#前進関係のプロット\n",
    "t = list(range(len(env.reward_graph)))  # 0から始まるインデックスのリストを作成 エピソードの数分報酬（収益）がリストに保存されるので、その数を数えてリストにしただけ　データ数なら自動で出るから必要なかった\n",
    "plt.plot(t, env.reward_graph, linestyle='solid', label=\"報酬\")\n",
    "plt.legend()\n",
    "# 環境のクローズ\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
