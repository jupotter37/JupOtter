{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class logistic_regression():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lr_rate=0.001, \n",
    "                 alpha=0.001, \n",
    "                 n_iter_no_change=5, \n",
    "                 max_iter=1000, \n",
    "                 tol=1e-9):\n",
    "        \n",
    "        self.lr_rate = lr_rate # eta (constant that multiplies the weight update term in update function)\n",
    "        self.alpha = alpha #lambda (constant that multiplies the regularization term in optimization function)\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self._early_stoing = False\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-(x@self.w+self.b)))\n",
    "        \n",
    "    def log_loss(self, x, y):\n",
    "        log_loss, infi = 0, 1e-90\n",
    "        for x1, y1 in zip(x, y):\n",
    "            prob_pos_class = self.sigmoid(x1)\n",
    "            # adding 1e-90 to avoid log(0) math error\n",
    "            log_loss += y1*np.log(prob_pos_class+infi) + (1-y1)*np.log(1-prob_pos_class+infi)\n",
    "        return -log_loss/x.shape[0]\n",
    "        \n",
    "    def initializations(self, \n",
    "                        x_tr, \n",
    "                        y_tr, \n",
    "                        x_cv, \n",
    "                        y_cv):\n",
    "        self.wait = 0\n",
    "        self.n = x_tr.shape[0]\n",
    "        self.x_tr, self.y_tr = x_tr, y_tr\n",
    "        self.x_cv, self.y_cv = x_cv, y_cv\n",
    "        self.w, self.b = self.np.zeros_like(x_tr[0]), 0\n",
    "        self.tr_log_loss_lst = [self.log_loss(self.x_tr, self.y_tr)]\n",
    "        self.cv_log_loss_lst = [self.log_loss(self.x_cv, self.y_cv)]\n",
    "        \n",
    "    def evaluate_the_model(self, epoch):\n",
    "        self.tr_log_loss_lst.append(self.log_loss(self.x_tr, self.y_tr))\n",
    "        self.cv_log_loss_lst.append(self.log_loss(self.x_cv, self.y_cv))\n",
    "        print(f\"---> Epoch {epoch}\")\n",
    "        print(f\"Tr ave loss {self.tr_log_loss_lst[-1]} & CV ave loss {self.cv_log_loss_lst[-1]}.\")\n",
    "        print(f\"Total training time: {np.round(time.time()-self.now, 3)} seconds.\")\n",
    "        \n",
    "    def early_stopping(self, epoch):\n",
    "        if (self.tr_log_loss_lst[-2] - self.tr_log_loss_lst[-1]) <= self.tol:\n",
    "            if self.wait == self.n_iter_no_change:\n",
    "                temp = np.round(time.time()-self.now, 3)\n",
    "                print(f\"Convergence after {epoch} epochs time took: {temp} seconds.\")\n",
    "                self._early_stoing = True\n",
    "            self.wait += 1\n",
    "        else:\n",
    "            self.wait = 0\n",
    "        \n",
    "    def fit(self, \n",
    "            x_tr, \n",
    "            y_tr, \n",
    "            x_cv, \n",
    "            y_cv):\n",
    "        \n",
    "        self.initializations(x_tr, \n",
    "                             y_tr, \n",
    "                             x_cv, \n",
    "                             y_cv)\n",
    "        self.now = time.time()\n",
    "        for epoch in range(1, self.max_iter+1):\n",
    "            if self._early_stoing == False:\n",
    "                for x, y in zip(self.x_tr, self.y_tr):\n",
    "                    common = self.alpha*(y-self.sigmoid(x))\n",
    "                    self.w = (1-self.lr_rate*self.alpha/self.n)*self.w + common*x\n",
    "                    self.b += common\n",
    "                self.evaluate_the_model(epoch)\n",
    "                self.early_stopping(epoch)\n",
    "            else: break\n",
    "    \n",
    "    def predict_class(self, x):\n",
    "        return np.array([1 if (self.sigmoid(x1)>0.5) else 0 for x1 in x])\n",
    "    \n",
    "    def plot_log_loss_per_epoch(self, ):\n",
    "        plt.figure(figsize=(18, 8))\n",
    "        plt.suptitle('Ploting the log_loss VS Epochs with our Custom SGD Results', fontsize=30)\n",
    "        \n",
    "        plt.plot(range(1, len(self.tr_log_loss_lst)+1), \n",
    "                 self.tr_log_loss_lst, \n",
    "                 marker='o', \n",
    "                 label='Change in the tr_log_loss per each epoch')\n",
    "        plt.plot(range(1, len(self.cv_log_loss_lst)+1), \n",
    "                 self.cv_log_loss_lst, \n",
    "                 marker='o', \n",
    "                 label='Change in the te_log_loss per each epoch')\n",
    "        \n",
    "        plt.xticks(range(1, len(self.tr_log_loss_lst)+1), fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n",
    "        plt.xlabel('Number of Epochs', fontsize=15)\n",
    "        plt.ylabel('log_loss Value', fontsize=15)\n",
    "        plt.legend(fontsize=15)\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=50000, \n",
    "                           n_features=15, \n",
    "                           n_informative=10, \n",
    "                           n_redundant=5,\n",
    "                           n_classes=2, \n",
    "                           weights=[0.7], \n",
    "                           class_sep=0.7, \n",
    "                           random_state=15)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'logistic_regression' object has no attribute 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m logistic_regression()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, X_test, y_test)\n",
      "\u001b[1;32m/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         x_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         y_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         x_cv, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         y_cv):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitializations(x_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                          y_tr, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                          x_cv, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                          y_cv)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnow \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;32m/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_tr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_tr \u001b[39m=\u001b[39m x_tr, y_tr\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_cv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_cv \u001b[39m=\u001b[39m x_cv, y_cv\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnp\u001b[39m.\u001b[39mzeros_like(x_tr[\u001b[39m0\u001b[39m]), \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtr_log_loss_lst \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_tr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_tr)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rohansridhar/Desktop/deep_learning/SGD.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv_log_loss_lst \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_cv, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_cv)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'logistic_regression' object has no attribute 'np'"
     ]
    }
   ],
   "source": [
    "model = logistic_regression()\n",
    "model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 10:47:16.953591: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-03-30 10:47:16.953619: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-03-30 10:47:16.953627: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-03-30 10:47:16.953703: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-30 10:47:16.953750: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name) self.hidden1 = keras.layers.Dense(units, activation=activation) self.hidden2 = keras.layers.Dense(units, activation=activation) self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2]) \n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "#gradient clipping \n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
