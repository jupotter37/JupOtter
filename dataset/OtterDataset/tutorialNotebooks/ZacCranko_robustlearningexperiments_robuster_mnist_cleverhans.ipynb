{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robuster_mnist_cleverhans.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ZacCranko/robustlearningexperiments/blob/master/robuster_mnist_cleverhans.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gX4sSuqzzJl6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copyright 2018 Google LLC and Zac Cranko\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import sys, os\n",
        "!pip install -qq -e git+http://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "sys.path.append('/content/src/cleverhans')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This tutorial shows how to generate adversarial examples using FGSM\n",
        "and train a model using adversarial training with TensorFlow.\n",
        "It is very similar to mnist_tutorial_keras_tf.py, which does the same\n",
        "thing but with a dependence on keras.\n",
        "The original paper can be found at:\n",
        "https://arxiv.org/abs/1412.6572\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import flags\n",
        "import logging\n",
        "\n",
        "from cleverhans.loss import LossCrossEntropy\n",
        "from cleverhans.utils_mnist import data_mnist\n",
        "from cleverhans.utils_tf import train, model_eval\n",
        "from cleverhans.attacks import FastGradientMethod, MomentumIterativeMethod, CarliniWagnerL2\n",
        "from cleverhans.utils import AccuracyReport, set_log_level\n",
        "from cleverhans_tutorials.tutorial_models import ModelBasicCNN\n",
        "\n",
        "FLAGS = flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ticDzou0k82l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from cleverhans.model   import Model\n",
        "from cleverhans.loss    import Loss\n",
        "from cleverhans.compat  import softmax_cross_entropy_with_logits\n",
        "\n",
        "# Copyright 2018 Google LLC.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "def power_iterate_conv(layer, num_iter):\n",
        "  \"\"\"Perform power iteration for a convolutional layer.\"\"\"\n",
        "  assert isinstance(layer, tf.keras.layers.Conv2D)\n",
        "  weights = layer.kernel\n",
        "  strides = (1,) + layer.strides + (1,)\n",
        "  padding = layer.padding.upper()\n",
        "  \n",
        "  with tf.variable_scope(None, default_name='power_iteration'):\n",
        "    u_var = tf.get_variable(\n",
        "       'u_conv', [1] + map(int, layer.output_shape[1:]),\n",
        "       initializer=tf.random_normal_initializer(),\n",
        "       trainable=False)\n",
        "    u = u_var\n",
        "    \n",
        "    for _ in xrange(num_iter):\n",
        "      v = tf.nn.conv2d_transpose(\n",
        "         u, weights, [1] + map(int, layer.input_shape[1:]), strides, padding)\n",
        "      v /= tf.sqrt(tf.maximum(2 * tf.nn.l2_loss(v), 1e-12))\n",
        "      u = tf.nn.conv2d(v, weights, strides, padding)\n",
        "      u /= tf.sqrt(tf.maximum(2 * tf.nn.l2_loss(u), 1e-12))\n",
        "      \n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, tf.assign(u_var, u))\n",
        "\n",
        "    u = tf.stop_gradient(u)\n",
        "    v = tf.stop_gradient(v)\n",
        "    return tf.reduce_sum(u * tf.nn.conv2d(v, weights, strides, padding))\n",
        "  \n",
        "def power_iterate(layer, num_iter):\n",
        "  \"\"\"Perform power iteration for a fully connected layer.\"\"\"\n",
        "  assert isinstance(layer, tf.keras.layers.Dense)\n",
        "  weights = layer.kernel\n",
        "  output_shape, input_shape = weights.get_shape().as_list()\n",
        "\n",
        "  with tf.variable_scope(None, default_name='power_iteration'):\n",
        "    u_var = tf.get_variable(\n",
        "       'u',  map(int, [output_shape]) + [1],\n",
        "       initializer=tf.random_normal_initializer(),\n",
        "       trainable=False)\n",
        "    u = u_var\n",
        "\n",
        "    for _ in xrange(num_iter):\n",
        "      v = tf.matmul(weights, u, transpose_a=True)\n",
        "      v /= tf.sqrt(tf.maximum(2 * tf.nn.l2_loss(v), 1e-12))\n",
        "      u = tf.matmul(weights, v)\n",
        "      u /= tf.sqrt(tf.maximum(2 * tf.nn.l2_loss(u), 1e-12))\n",
        "\n",
        "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, tf.assign(u_var, u))\n",
        "\n",
        "    u = tf.stop_gradient(u)\n",
        "    v = tf.stop_gradient(v)\n",
        "    return tf.reduce_sum(u * tf.matmul(weights, v))\n",
        "\n",
        "class ModelRobustCNN(Model):\n",
        "    def __init__(self, scope, num_classes, **kwargs):\n",
        "        del kwargs\n",
        "        Model.__init__(self, scope, num_classes, locals())\n",
        "        self.num_pow     = 5\n",
        "        self.num_pow_iter = 1\n",
        "        self.lip = []\n",
        "\n",
        "        with tf.variable_scope(scope):\n",
        "          self.model = tf.keras.Sequential()\n",
        "\n",
        "          conv1 = tf.keras.layers.Conv2D(32, 5, 1, padding='SAME',\n",
        "                                         input_shape=(28, 28, 1))\n",
        "          self.model.add(conv1)\n",
        "          self.lip.append(power_iterate_conv(conv1, self.num_pow_iter))\n",
        "\n",
        "          self.model.add(tf.keras.layers.Activation('relu'))\n",
        "          self.model.add(tf.keras.layers.MaxPooling2D(2, 2, padding='SAME'))\n",
        "\n",
        "          conv2 = tf.keras.layers.Conv2D(64, 5, 1, padding='SAME')\n",
        "          self.model.add(conv2)\n",
        "          self.lip.append(power_iterate_conv(conv2, self.num_pow_iter))\n",
        "\n",
        "          self.model.add(tf.keras.layers.Activation('relu'))\n",
        "          self.model.add(tf.keras.layers.MaxPooling2D(2, 2, padding='SAME'))\n",
        "          self.model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "          fc1 = tf.keras.layers.Dense(1024)\n",
        "          self.model.add(fc1)\n",
        "          self.lip.append(power_iterate(fc1, self.num_pow_iter))\n",
        "\n",
        "          self.model.add(tf.keras.layers.Activation('relu'))\n",
        "\n",
        "          fc2 = tf.keras.layers.Dense(10)\n",
        "          self.model.add(fc2)\n",
        "          self.lip.append(power_iterate(fc2, self.num_pow_iter))\n",
        "        \n",
        "    def lipschitz(self):\n",
        "        \"\"\"Return the Lipschitz product.\"\"\"\n",
        "        return tf.reduce_prod(self.lip)\n",
        "      \n",
        "    def young_surrogate(self, weights=None):\n",
        "        \"\"\"Build the Young convex surrogate for the Lipschitz product.\"\"\"\n",
        "        if weights == None:\n",
        "            weights = np.full(len(self.lip), 1/len(self.lip))\n",
        "        else:\n",
        "            assert all(w > 0 for w in weights)\n",
        "            assert len(self.lips) == len(weights)\n",
        "\n",
        "            if sum(weights) != 1.0:\n",
        "                weights = np.array(weights)/sum(weights)\n",
        "\n",
        "        return tf.reduce_sum([w * l ** (1/w) for l,w in zip(self.lip, weights)])\n",
        "\n",
        "    def fprop(self, x, **kwargs):\n",
        "        del kwargs\n",
        "        logits = self.model(tf.reshape(x, [-1, 28, 28, 1]))\n",
        "\n",
        "        return {self.O_LOGITS: logits,\n",
        "                self.O_PROBS:  tf.nn.softmax(logits=logits)}\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hvb4rNa-YRh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Some of theses variables get used by cleverhans code I Frankensteined,\n",
        "# so this remains here\n",
        "train_start=0; train_end=60000; test_start=0; test_end=10000;\n",
        "num_epochs=50; batch_size=128; learning_rate=0.001;                    \n",
        "\n",
        "\n",
        "# Get MNIST test data\n",
        "x_train, y_train, x_test, y_test = data_mnist(train_start=train_start,\n",
        "                                              train_end=train_end,\n",
        "                                              test_start=test_start,\n",
        "                                              test_end=test_end)\n",
        "# Use Image Parameters\n",
        "img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "# Define input TF placeholder\n",
        "x = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols,\n",
        "                                      nchannels))\n",
        "y = tf.placeholder(tf.float32, shape=(None, num_classes))\n",
        "\n",
        "img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
        "num_classes = y_train.shape[1]\n",
        "model = ModelRobustCNN('robust_model', num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2Aq5yNLepjE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "13830de4-c54f-4aaa-b694-45ca1118e74f"
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "from cleverhans.utils import batch_indices\n",
        "\n",
        "eps = 0.3\n",
        "\n",
        "cross_ent = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=model.get_logits(x)))\n",
        "accuracy   = tf.reduce_mean(tf.cast(\n",
        "     tf.equal(tf.argmax(model.get_logits(x), 1), tf.argmax(y, 1)), tf.float32))\n",
        "\n",
        "lip_cst = model.lipschitz()\n",
        "lip_reg = model.young_surrogate()\n",
        "rho     = tf.placeholder(tf.float32)\n",
        "\n",
        "# ojective function to minimise\n",
        "obj     = cross_ent + rho * lip_reg\n",
        "\n",
        "# bound on the adversarial entropy\n",
        "cross_ent_bound = cross_ent + rho * lip_cst\n",
        "\n",
        "# optimisation\n",
        "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "  train_step = tf.train.AdamOptimizer(1e-4).minimize(obj)\n",
        "\n",
        "fgsm_params = {\n",
        "    'eps': eps,\n",
        "    'clip_min': 0.,\n",
        "    'clip_max': 1.,\n",
        "    'batch_size': 100,\n",
        "    'ord': 2 # spectral regularisation is only certified against l2 adversary\n",
        "}\n",
        "rng = np.random.RandomState([2017, 8, 30])\n",
        "\n",
        "sess =  tf.Session()\n",
        "\n",
        "fgsm  = MomentumIterativeMethod(model, sess=sess)\n",
        "\n",
        "adv_x = fgsm.generate(x, y=y, **fgsm_params)\n",
        "adv_logits = model.get_logits(adv_x)\n",
        "adv_cross_ent = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=adv_logits))\n",
        "adv_accuracy = tf.reduce_mean(tf.cast(\n",
        "     tf.equal(tf.argmax(adv_logits, 1), tf.argmax(y, 1)), tf.float32))\n",
        "\n",
        "metrics = (\n",
        "    ('objective', obj),\n",
        "    ('adv cross entropy', adv_cross_ent),\n",
        "    ('cross entropy bound', cross_ent_bound),\n",
        "    ('accuracy', accuracy),\n",
        "    ('adv accuracy', adv_accuracy)\n",
        ")\n",
        "\n",
        "def evaluate(feed_dict):\n",
        "  computed_metrics = sess.run(dict(metrics), feed_dict=feed_dict)\n",
        "  for metric in zip(*metrics)[0]:\n",
        "    print('{:>21} {:.04f}'.format(metric + ':', computed_metrics[metric]))\n",
        "  print('')\n",
        "  \n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "feed_dict      = {rho : eps, x : None,   y : None}\n",
        "test_feed_dict = {rho : eps, x : x_test, y : y_test}\n",
        "\n",
        "with sess.as_default():\n",
        "  for epoch in xrange(1, num_epochs + 1):\n",
        "      # Compute number of batches\n",
        "      num_batches = int(math.ceil(float(len(x_train)) / batch_size))\n",
        "      assert num_batches * batch_size >= len(x_train)\n",
        "\n",
        "      # Indices to shuffle training set\n",
        "      index_shuf = list(range(len(x_train)))\n",
        "      rng.shuffle(index_shuf)\n",
        "\n",
        "      prev = time.time()\n",
        "      for batch in range(num_batches):\n",
        "          # Compute batch start and end indices\n",
        "          start, end = batch_indices(batch, len(x_train), batch_size)\n",
        "\n",
        "          # Perform one training step\n",
        "          feed_dict[x] = x_train[index_shuf[start:end]]\n",
        "          feed_dict[y] = y_train[index_shuf[start:end]]\n",
        "\n",
        "          sess.run(train_step, feed_dict=feed_dict)\n",
        "\n",
        "      assert end >= len(x_train)  # Check that all examples were used\n",
        "      if epoch % 5 == 0:\n",
        "        evaluate(test_feed_dict)\n",
        "        \n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-02648495cd7d>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n",
            "           objective: 1.7099\n",
            "   adv cross entropy: 0.8043\n",
            " cross entropy bound: 1.6465\n",
            "            accuracy: 0.9012\n",
            "        adv accuracy: 0.8646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}