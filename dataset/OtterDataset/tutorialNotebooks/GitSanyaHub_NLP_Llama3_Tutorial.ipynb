{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitSanyaHub/NLP/blob/main/LLaMA/Llama3_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's code Meta's Llama 3 Step-by-step in PyTorch\n",
        "\n",
        "The purpose of this guide is to illustrate the specific architecture choices implemented in Llama 3, which you will find are very similar to prior versions. Check out the YouTube video where i walk through this colab notebook and explain everything step-by-step\n",
        "\n",
        "[![ERROR DISPLAYING IMAGE, CLICK HERE FOR VIDEO](https://img.youtube.com/vi/lZj8F6EspVU/0.jpg)](https://www.youtube.com/watch?v=lZj8F6EspVU)\n",
        "\n",
        "This notebook guide is designed for beginners; if you already feel confident coding a transformer in pytorch on your own then i recommend instead skimming through the model.py file in the [github repo](https://github.com/evintunador/minLlama3). By beginner, i mean someone who understands matrix/tensor multiplication, general deep learning concepts like what a loss function is, and is capable of looking up pytorch documentation on any given function that they don't recognize, but maybe isn't well versed on transformers specifically. For an even better beginner's guide that uses an outdated architecture, check out [Andrej Karpathy's video on how to build GPT2](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5014s) and then come back here to learn about the more up-to-date methods that Llama 3 utilizes.\n",
        "\n",
        "Also, check out the original open-source release of Llama 3 [here](https://github.com/meta-llama/llama3).\n",
        "\n",
        "If you enjoy this guide, then check out my analogous ones for [Google's Gemma](https://www.youtube.com/watch?v=WW7ZxaC3OtA) and [XAI's Grok](https://www.youtube.com/watch?v=K9Rdc848EBs).\n",
        "\n",
        "**Note:** It's very easy to convince yourself that you understand something after watching a youtube video about it, but chances are you don't actually understand unless you can code it from scratch on your own. I highly recommend you mess around with this notebook and try to build your own minLlama from scratch\n",
        "\n",
        "# What this guide does NOT include\n",
        "The focus here is on architecture rather than optimization techniques, distributed training/inference, quantization, etc. As such, there are many parts of [the original Llama repo](https://github.com/meta-llama/llama3) that will not be included:\n",
        "- the 15 trillion tokens of high quality data that Llama 3 was trained on (we'll be using TinyShakespeare instead)\n",
        "- the original tokenizer--we'll be using a very simple one based on our datset and not going over how it works. Check out [Andrej Karpathy's great video on tokenizers](https://youtu.be/zduSFxRajkE?si=Q2uq_nilHhOegbRi) for a better explanation\n",
        "- the specifics of their training setup that I could not ascertain from their open-sourced inference code (for example: parameter initialization distributions, location of dropout, whether they used regular or chunked attention during training, etc.)\n",
        "- this guide is focused on training rather than inference; we'll do a quick greedy sampling at the end but for real sampling with temperature and whatnot as well as for kv caching during the run check out the code in section 2\n",
        "- other stuff i'm prolly forgetting\n",
        "\n",
        "# Table of Contents\n",
        "1. [Spelled out walkthrough of every single tensor operation](#one)\n",
        "  \n",
        "  1a. [Setup stuff](#a)\n",
        "  \n",
        "  1b. [Initializing the first residual state](#b)\n",
        "  \n",
        "  1c. [Precomputing our RoPE Frequencies](#c)\n",
        "  \n",
        "  1d. [Precomputing the Causal Mask](#d)\n",
        "  \n",
        "  1e. [Our First Normalization](#e)\n",
        "  \n",
        "  1f. [Initializing Multi-Query Attention](#f)\n",
        "\n",
        "  1g. [Rotary Position Embeddings](#g)\n",
        "\n",
        "  1h. [Calculating Self-Attention](#h)\n",
        "\n",
        "  1i. [Our first residual connection](#i)\n",
        "\n",
        "  1j. [The SwiGLU Feedforward Network](#j)\n",
        "\n",
        "  1k. [Output](#k)\n",
        "\n",
        "  1l. [The Loss Functions](#l)\n",
        "\n",
        "2. [Actually functional model code](#two)\n",
        "\n",
        "  2a. [parameters](#twoa)\n",
        "\n",
        "  2b. [RMSNorm](#twob)\n",
        "\n",
        "  2c. [RoPE](#twoc)\n",
        "\n",
        "  2d. [Attention](#twod)\n",
        "\n",
        "  2e. [Ffwd](#twoe)\n",
        "\n",
        "  2f. [Residual Layers](#twof)\n",
        "\n",
        "  2g. [The model itself](#twog)\n",
        "\n",
        "3. [Train and test your own minLlama3 (or load mine)](#three)\n",
        "\n",
        "  3a. [Setup](#threea)\n",
        "\n",
        "  3b. [Training your own](#threeb)\n",
        "\n",
        "  3c. [Alternatively, you can load the 2m parameter model I already trained](#threec)\n",
        "\n",
        "  3d. [Testing (performing inference)](#threed)"
      ],
      "metadata": {
        "id": "4WUJqR8FeXSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Spelled out walkthrough of every single tensor operation\n",
        "<a id='one'></a>\n",
        "In this section we'll walk through every important operation that Llama 3's architecture carries out using laughably small tensors. We've chosen tensors so small so that if you want to, you can literally pull out a calculator to 100% ensure you undersand what's happening. we'll begin with basic imports and whatnot"
      ],
      "metadata": {
        "id": "x-nuFaBitYeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Setup stuff\n",
        "<a id='a'></a>"
      ],
      "metadata": {
        "id": "xXFTnCXm_Dfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "JOHHIHcjeWzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary length. Llama's real vocab size is 128256. Here let's just use an absurdly small number\n",
        "v = 10\n",
        "\n",
        "# Llama's maximum sequence length is 8192, but for inference they cache 3/4 of it and only use an effective length of 2048. more on that later\n",
        "seq_len = 5\n",
        "\n",
        "# we'll use a batch size of 1 for simplicity when visualizing our tensors\n",
        "b = 1\n",
        "\n",
        "# now let's make ourselves a list of token indices. Each represents somewhere between a letter and a word\n",
        "tokens = torch.randint(v, (b, seq_len))\n",
        "tokens.shape, tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEje5kPPeW1Y",
        "outputId": "c853acb2-0b82-479a-8c10-8c15c1a31cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5]), tensor([[8, 4, 7, 2, 9]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Initializing the first residual state\n",
        "<a id='b'></a>"
      ],
      "metadata": {
        "id": "bN6ycHjMeJbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our embedding dimension. Llama 3 8b's is 4096\n",
        "d = 16\n",
        "\n",
        "# initializing our token embedding matrix\n",
        "embedding = nn.Embedding(v, d)\n",
        "embedding.weight.shape, embedding.weight\n",
        "# each row in this embedding is a high dimensional repersentation of its corresponding token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItItwY2eW3m",
        "outputId": "b1fc18c5-985a-4ccc-8ca8-b1272fe0040d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 16]),\n",
              " Parameter containing:\n",
              " tensor([[ 9.3121e-01, -9.6560e-02, -5.7902e-02, -9.5194e-01,  1.3470e+00,\n",
              "          -1.0196e+00,  2.1526e-01,  4.3648e-01,  7.3091e-01,  1.0472e+00,\n",
              "          -8.3610e-01, -1.1353e+00,  1.0956e+00,  1.3382e+00,  8.4271e-02,\n",
              "           2.0557e+00],\n",
              "         [-2.2929e+00, -1.2869e+00, -9.3078e-01,  9.4351e-02, -1.0933e+00,\n",
              "          -1.4797e+00, -1.5054e-01, -4.6406e-01,  8.2742e-01,  8.3517e-01,\n",
              "          -4.6111e-01,  2.7773e-01, -4.3511e-01,  2.6676e+00,  1.8905e+00,\n",
              "          -8.3954e-01],\n",
              "         [ 5.7819e-02, -6.4342e-01,  5.1670e-02,  1.0794e+00,  5.1993e-01,\n",
              "          -8.9062e-01, -1.3466e-01, -1.2324e-01, -3.0727e-01,  8.0144e-02,\n",
              "           1.0582e+00,  7.0094e-01,  6.6939e-01,  1.5953e+00,  7.9977e-01,\n",
              "           1.3172e+00],\n",
              "         [ 1.8516e+00,  3.7621e-01, -1.1883e-01,  6.9927e-01,  8.1808e-01,\n",
              "          -1.5548e+00, -8.7562e-02,  1.4753e-01, -5.7552e-01, -4.3554e-01,\n",
              "          -2.8826e-01, -2.9157e-02, -1.1238e+00,  2.4546e-01, -2.0982e+00,\n",
              "           7.2495e-01],\n",
              "         [-3.1679e-01,  7.2865e-01,  1.9062e+00, -1.3156e-01,  1.3459e+00,\n",
              "           2.0975e-01,  1.0406e+00,  4.3450e-02, -1.1801e+00,  1.4507e-01,\n",
              "           1.1746e+00, -4.5699e-02,  1.1738e+00, -2.2508e-01, -1.0406e+00,\n",
              "           6.2829e-01],\n",
              "         [ 2.1030e+00,  1.5348e-01,  1.6185e+00, -4.5868e-01, -1.7362e+00,\n",
              "          -9.0004e-01, -7.9351e-01,  8.8160e-01,  3.1517e-01,  9.2230e-01,\n",
              "          -3.5740e+00,  1.3147e-01, -2.1971e-01,  2.5131e+00,  3.3696e-01,\n",
              "          -5.3898e-01],\n",
              "         [ 8.7074e-02,  1.0243e-01,  1.0042e-01, -1.8577e+00,  7.6364e-01,\n",
              "           2.9465e-01,  9.9392e-01, -3.7437e-01, -5.1543e-02,  8.1453e-01,\n",
              "           9.7591e-01, -1.5637e+00,  1.8369e+00,  1.2687e+00, -4.1115e-01,\n",
              "           4.6950e-02],\n",
              "         [ 1.1500e+00, -2.6253e-01,  3.0063e-01,  3.0295e-01,  6.7835e-01,\n",
              "           1.1275e+00,  1.9257e-02,  1.2530e+00, -3.7039e-02,  2.6630e-01,\n",
              "          -3.4087e-01, -1.2126e-01, -6.9901e-01,  6.3690e-01,  2.2050e-01,\n",
              "          -7.8657e-01],\n",
              "         [ 1.2460e+00,  7.3661e-01, -1.0649e+00, -3.2030e-03, -4.8006e-01,\n",
              "           2.6695e-01, -2.2108e+00, -1.0922e+00, -1.4346e+00,  5.1067e-01,\n",
              "          -8.5944e-01,  1.8760e-01, -4.2899e-01, -7.0839e-01,  5.5540e-01,\n",
              "           1.3216e+00],\n",
              "         [ 3.7443e-01, -1.0260e+00,  2.8509e-01,  1.4560e+00, -1.7206e-01,\n",
              "           3.4226e-01, -1.4262e-01,  4.8106e-01,  9.5998e-01, -2.6945e+00,\n",
              "          -1.4607e-01,  4.6943e-01,  1.5548e+00,  4.6408e-01,  6.8833e-01,\n",
              "          -9.3618e-02]], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grabbing the embeddings that correspond to our sequence of token indices\n",
        "x = embedding(tokens)\n",
        "x.shape, x\n",
        "# at this points many models would multiply the embeddings by the square root of the embedding dimension, but Llama 3 foregoes that strategy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqKNqorqSshT",
        "outputId": "39efcf33-8882-45d8-991d-1e84bc70605c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 1.2460,  0.7366, -1.0649, -0.0032, -0.4801,  0.2669, -2.2108,\n",
              "           -1.0922, -1.4346,  0.5107, -0.8594,  0.1876, -0.4290, -0.7084,\n",
              "            0.5554,  1.3216],\n",
              "          [-0.3168,  0.7286,  1.9062, -0.1316,  1.3459,  0.2098,  1.0406,\n",
              "            0.0434, -1.1801,  0.1451,  1.1746, -0.0457,  1.1738, -0.2251,\n",
              "           -1.0406,  0.6283],\n",
              "          [ 1.1500, -0.2625,  0.3006,  0.3029,  0.6783,  1.1275,  0.0193,\n",
              "            1.2530, -0.0370,  0.2663, -0.3409, -0.1213, -0.6990,  0.6369,\n",
              "            0.2205, -0.7866],\n",
              "          [ 0.0578, -0.6434,  0.0517,  1.0794,  0.5199, -0.8906, -0.1347,\n",
              "           -0.1232, -0.3073,  0.0801,  1.0582,  0.7009,  0.6694,  1.5953,\n",
              "            0.7998,  1.3172],\n",
              "          [ 0.3744, -1.0260,  0.2851,  1.4560, -0.1721,  0.3423, -0.1426,\n",
              "            0.4811,  0.9600, -2.6945, -0.1461,  0.4694,  1.5548,  0.4641,\n",
              "            0.6883, -0.0936]]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c. Precomputing our RoPE Frequencies\n",
        "<a id='c'></a>\n",
        "\n",
        "Rotary Positional Encoding (RoPE) is a method [originally proposed in 2019](https://arxiv.org/abs/2104.09864) that quickly became the defacto standard for enabling transformers to understand positional information (by default the attention mechanism is blind to the ordering of tokens). The method utilizes trigonometry to \"rotate\" the entries in two matrices before they are multiplied together. A small amount of rotation indicates that two tokens are close together, while a large amount of rotation corresponds to being far apart. I'm going to skim over this topic so for a better conceptual explanation, I recommend checking out [this video](https://www.youtube.com/watch?v=GQPOtyITy54).\n",
        "\n",
        "\"Precompute\" means we're going to calculate the frequencies ahead of time so that they can be sent through and reused throughout the model as opposed to creating them from scratch every time we need them."
      ],
      "metadata": {
        "id": "53BqJ9NzeAUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = 10000 # 10,000 is the most common value but Llama 3 uses 50,000. In theory smaller models should use a smaller value\n",
        "num_heads = 4 # Llama 3 8b has 32 total attention heads\n",
        "head_dim = d // num_heads # Llama 3 ties its head dimension to the embedding dimension. This value comes out to 128 in Llama 3, which is purposeful to\n",
        "\n",
        "# go watch the video to get a better explanation of what's happening here\n",
        "freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
        "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
        "\n",
        "t = torch.arange(seq_len * 2, device=freqs.device, dtype=torch.float32)\n",
        "print(f't: {t.shape}\\n{t}\\n')\n",
        "\n",
        "freqs = torch.outer(t, freqs)\n",
        "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
        "\n",
        "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)[:seq_len]  # complex64\n",
        "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y0jy0fjeAhs",
        "outputId": "8591e0f1-65b2-4a97-8b53-17118f4b993b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freqs: torch.Size([2])\n",
            "tensor([1.0000, 0.0100])\n",
            "\n",
            "t: torch.Size([10])\n",
            "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
            "\n",
            "freqs: torch.Size([10, 2])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [1.0000, 0.0100],\n",
            "        [2.0000, 0.0200],\n",
            "        [3.0000, 0.0300],\n",
            "        [4.0000, 0.0400],\n",
            "        [5.0000, 0.0500],\n",
            "        [6.0000, 0.0600],\n",
            "        [7.0000, 0.0700],\n",
            "        [8.0000, 0.0800],\n",
            "        [9.0000, 0.0900]])\n",
            "\n",
            "freqs_cis: torch.Size([5, 2])\n",
            "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j],\n",
            "        [ 0.5403+0.8415j,  0.9999+0.0100j],\n",
            "        [-0.4161+0.9093j,  0.9998+0.0200j],\n",
            "        [-0.9900+0.1411j,  0.9996+0.0300j],\n",
            "        [-0.6536-0.7568j,  0.9992+0.0400j]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d. Precomputing the Causal Mask\n",
        "<a id='d'></a>\n",
        "\n",
        "Similar to RoPE embeddings, the causal mask is another part of the attention mechanism that we can create ahead of time to then be reused in every layer.\n",
        "\n",
        "The basic idea of a causal mask is that by default, attention mechanisms allow every single token to pay attention to every single other token. This is okay or even preferable for some model types, but Llama is auto-regressive, meaning it would be bad if a given token to be predicted was able to see itself and future tokens during training but not during inference. The negative infinity's in the upper-triangle prevent the model from attending to the corresponding token; how this works will be more clear later when we do the attention softmax"
      ],
      "metadata": {
        "id": "wccyOLWQjyQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.full(\n",
        "    (seq_len, seq_len),\n",
        "    float(\"-inf\")\n",
        ")\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eL1OwBVjxiC",
        "outputId": "d7f704c6-2994-452c-c0ae-19efa16d0077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1e. Our First Normalization\n",
        "<a id='e'></a>\n",
        "\n",
        "Root Mean Square Normalization has also been the norm for quite awhile. Like its predecessor LayerNorm, RMSNorm restricts the variability of the entries in each embedding vector such that the vector lies on a hypersphere with radius $\\sqrt{d}$. However unlike LayerNorm which centers that hypersphere with a mean of zero, RMSNorm does not mess with the mean, which is an important source of data for networks that utilize residual connections."
      ],
      "metadata": {
        "id": "iL87gVeGnLUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first let's setup the residual connection that we'll use later\n",
        "h = x\n",
        "print(f'h: {h.shape}\\n{h}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz7KOCA3oYBY",
        "outputId": "b6255dc5-5271-4cab-9c85-4d6a4645bf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h: torch.Size([1, 5, 16])\n",
            "tensor([[[ 1.2460,  0.7366, -1.0649, -0.0032, -0.4801,  0.2669, -2.2108,\n",
            "          -1.0922, -1.4346,  0.5107, -0.8594,  0.1876, -0.4290, -0.7084,\n",
            "           0.5554,  1.3216],\n",
            "         [-0.3168,  0.7286,  1.9062, -0.1316,  1.3459,  0.2098,  1.0406,\n",
            "           0.0434, -1.1801,  0.1451,  1.1746, -0.0457,  1.1738, -0.2251,\n",
            "          -1.0406,  0.6283],\n",
            "         [ 1.1500, -0.2625,  0.3006,  0.3029,  0.6783,  1.1275,  0.0193,\n",
            "           1.2530, -0.0370,  0.2663, -0.3409, -0.1213, -0.6990,  0.6369,\n",
            "           0.2205, -0.7866],\n",
            "         [ 0.0578, -0.6434,  0.0517,  1.0794,  0.5199, -0.8906, -0.1347,\n",
            "          -0.1232, -0.3073,  0.0801,  1.0582,  0.7009,  0.6694,  1.5953,\n",
            "           0.7998,  1.3172],\n",
            "         [ 0.3744, -1.0260,  0.2851,  1.4560, -0.1721,  0.3423, -0.1426,\n",
            "           0.4811,  0.9600, -2.6945, -0.1461,  0.4694,  1.5548,  0.4641,\n",
            "           0.6883, -0.0936]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we'll perform our first normalization\n",
        "# first we square each entry in x and then take the mean of those values across each embedding vector\n",
        "mean_squared = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "mean_squared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Oz0Gw2Z7qd",
        "outputId": "9b7c0d2f-10cb-4fe2-ef25-5aa0b5510dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.9653],\n",
              "         [0.8077],\n",
              "         [0.4150],\n",
              "         [0.6101],\n",
              "         [0.9582]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then we multiply x by the reciprocal of the square roots of mean_squared\n",
        "# 1e-6 is a very small number added for stability just in case an entry happens to be equal to 0 (since you can't divide by 0)\n",
        "x_normed = x * torch.rsqrt(mean_squared + 1e-6)\n",
        "print(f'x_normed: {x_normed.shape}\\n{x_normed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE0hSKbSa06u",
        "outputId": "30845887-118f-4de7-e474-2b4ef1c615b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_normed: torch.Size([1, 5, 16])\n",
            "tensor([[[ 1.2682,  0.7497, -1.0838, -0.0033, -0.4886,  0.2717, -2.2502,\n",
            "          -1.1116, -1.4601,  0.5198, -0.8747,  0.1909, -0.4366, -0.7210,\n",
            "           0.5653,  1.3452],\n",
            "         [-0.3525,  0.8107,  2.1209, -0.1464,  1.4975,  0.2334,  1.1578,\n",
            "           0.0483, -1.3131,  0.1614,  1.3070, -0.0508,  1.3060, -0.2504,\n",
            "          -1.1579,  0.6991],\n",
            "         [ 1.7851, -0.4075,  0.4667,  0.4703,  1.0530,  1.7502,  0.0299,\n",
            "           1.9450, -0.0575,  0.4134, -0.5291, -0.1882, -1.0851,  0.9886,\n",
            "           0.3423, -1.2210],\n",
            "         [ 0.0740, -0.8237,  0.0662,  1.3819,  0.6656, -1.1402, -0.1724,\n",
            "          -0.1578, -0.3934,  0.1026,  1.3548,  0.8974,  0.8570,  2.0424,\n",
            "           1.0239,  1.6863],\n",
            "         [ 0.3825, -1.0481,  0.2912,  1.4874, -0.1758,  0.3496, -0.1457,\n",
            "           0.4914,  0.9807, -2.7526, -0.1492,  0.4796,  1.5883,  0.4741,\n",
            "           0.7032, -0.0956]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally, we multiply by a learnable scale parameter\n",
        "# This scale is initialized to 1's but if we were to train then those values would change\n",
        "rms_scale = torch.ones(d)\n",
        "print(f'rms_scale: {rms_scale.shape}\\n{rms_scale}\\n')\n",
        "\n",
        "x_normed *= rms_scale\n",
        "print(f'x_normed: {x_normed.shape}\\n{x_normed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Z4LQQnbuNG",
        "outputId": "9b82c0b5-3a0c-45df-dcb6-63e78bdca548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rms_scale: torch.Size([16])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "\n",
            "x_normed: torch.Size([1, 5, 16])\n",
            "tensor([[[ 1.2682,  0.7497, -1.0838, -0.0033, -0.4886,  0.2717, -2.2502,\n",
            "          -1.1116, -1.4601,  0.5198, -0.8747,  0.1909, -0.4366, -0.7210,\n",
            "           0.5653,  1.3452],\n",
            "         [-0.3525,  0.8107,  2.1209, -0.1464,  1.4975,  0.2334,  1.1578,\n",
            "           0.0483, -1.3131,  0.1614,  1.3070, -0.0508,  1.3060, -0.2504,\n",
            "          -1.1579,  0.6991],\n",
            "         [ 1.7851, -0.4075,  0.4667,  0.4703,  1.0530,  1.7502,  0.0299,\n",
            "           1.9450, -0.0575,  0.4134, -0.5291, -0.1882, -1.0851,  0.9886,\n",
            "           0.3423, -1.2210],\n",
            "         [ 0.0740, -0.8237,  0.0662,  1.3819,  0.6656, -1.1402, -0.1724,\n",
            "          -0.1578, -0.3934,  0.1026,  1.3548,  0.8974,  0.8570,  2.0424,\n",
            "           1.0239,  1.6863],\n",
            "         [ 0.3825, -1.0481,  0.2912,  1.4874, -0.1758,  0.3496, -0.1457,\n",
            "           0.4914,  0.9807, -2.7526, -0.1492,  0.4796,  1.5883,  0.4741,\n",
            "           0.7032, -0.0956]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's turn that RMSNorm into a function that we'll be able to reuse repeatedly later\n",
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ],
      "metadata": {
        "id": "L7biEvzBSsnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1f. Initializing Multi-Query Attention\n",
        "<a id='f'></a>\n",
        "[multi-query attention](https://arxiv.org/abs/1911.02150) is the de facto standard for saving on parameter counts in order to get a bigger model. The idea is that the model can make multiple queries to the residual state and have those many queries be answered by shared keys & values."
      ],
      "metadata": {
        "id": "b5gp8C6hb2tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first up, remember we're currently working with two separate objects\n",
        "# x is for the residual connection and x_normed will go into our Attention calculation\n",
        "h, x_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpQZkXhLc9RJ",
        "outputId": "553e2d9a-c38e-4e63-9947-de43335615cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 1.2460,  0.7366, -1.0649, -0.0032, -0.4801,  0.2669, -2.2108,\n",
              "           -1.0922, -1.4346,  0.5107, -0.8594,  0.1876, -0.4290, -0.7084,\n",
              "            0.5554,  1.3216],\n",
              "          [-0.3168,  0.7286,  1.9062, -0.1316,  1.3459,  0.2098,  1.0406,\n",
              "            0.0434, -1.1801,  0.1451,  1.1746, -0.0457,  1.1738, -0.2251,\n",
              "           -1.0406,  0.6283],\n",
              "          [ 1.1500, -0.2625,  0.3006,  0.3029,  0.6783,  1.1275,  0.0193,\n",
              "            1.2530, -0.0370,  0.2663, -0.3409, -0.1213, -0.6990,  0.6369,\n",
              "            0.2205, -0.7866],\n",
              "          [ 0.0578, -0.6434,  0.0517,  1.0794,  0.5199, -0.8906, -0.1347,\n",
              "           -0.1232, -0.3073,  0.0801,  1.0582,  0.7009,  0.6694,  1.5953,\n",
              "            0.7998,  1.3172],\n",
              "          [ 0.3744, -1.0260,  0.2851,  1.4560, -0.1721,  0.3423, -0.1426,\n",
              "            0.4811,  0.9600, -2.6945, -0.1461,  0.4694,  1.5548,  0.4641,\n",
              "            0.6883, -0.0936]]], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([[[ 1.2682,  0.7497, -1.0838, -0.0033, -0.4886,  0.2717, -2.2502,\n",
              "           -1.1116, -1.4601,  0.5198, -0.8747,  0.1909, -0.4366, -0.7210,\n",
              "            0.5653,  1.3452],\n",
              "          [-0.3525,  0.8107,  2.1209, -0.1464,  1.4975,  0.2334,  1.1578,\n",
              "            0.0483, -1.3131,  0.1614,  1.3070, -0.0508,  1.3060, -0.2504,\n",
              "           -1.1579,  0.6991],\n",
              "          [ 1.7851, -0.4075,  0.4667,  0.4703,  1.0530,  1.7502,  0.0299,\n",
              "            1.9450, -0.0575,  0.4134, -0.5291, -0.1882, -1.0851,  0.9886,\n",
              "            0.3423, -1.2210],\n",
              "          [ 0.0740, -0.8237,  0.0662,  1.3819,  0.6656, -1.1402, -0.1724,\n",
              "           -0.1578, -0.3934,  0.1026,  1.3548,  0.8974,  0.8570,  2.0424,\n",
              "            1.0239,  1.6863],\n",
              "          [ 0.3825, -1.0481,  0.2912,  1.4874, -0.1758,  0.3496, -0.1457,\n",
              "            0.4914,  0.9807, -2.7526, -0.1492,  0.4796,  1.5883,  0.4741,\n",
              "            0.7032, -0.0956]]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define the hyperparameters of MQA\n",
        "num_kv_heads = 2 # Llama uses 8 key and value heads per layer\n",
        "assert num_heads % num_kv_heads == 0 # each q needs to match up to a kv\n",
        "print(f\"as a reminder: num_heads = {num_heads}, head_dim = {head_dim}\")"
      ],
      "metadata": {
        "id": "eDl4ypypdK4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238ea1ac-9096-4818-8b68-d8a241c899be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "as a reminder: num_heads = 4, head_dim = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we'll initialize our self-attention weight matrices\n",
        "wq = nn.Linear(d, num_heads * head_dim, bias=False)\n",
        "wk = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
        "wv = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
        "print(\"Attention weights: \", wq.weight.shape, wk.weight.shape, wv.weight.shape)\n",
        "\n",
        "# and project x_normed out to get our queries, keys and values\n",
        "xq = wq(x_normed)\n",
        "xk = wk(x_normed)\n",
        "xv = wv(x_normed)\n",
        "print(\"Attention projections: \", xq.shape, xk.shape, xv.shape)\n",
        "\n",
        "# then reshape them to separate out by head\n",
        "xq = xq.view(b, seq_len, num_heads, head_dim)\n",
        "xk = xk.view(b, seq_len, num_kv_heads, head_dim)\n",
        "xv = xv.view(b, seq_len, num_kv_heads, head_dim)\n",
        "print(\"Reshaped: \", xq.shape, xk.shape, xv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH0rZAHnffMn",
        "outputId": "52275747-f29d-4d29-8da1-a12ecb51c6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  torch.Size([16, 16]) torch.Size([8, 16]) torch.Size([8, 16])\n",
            "Attention projections:  torch.Size([1, 5, 16]) torch.Size([1, 5, 8]) torch.Size([1, 5, 8])\n",
            "Reshaped:  torch.Size([1, 5, 4, 4]) torch.Size([1, 5, 2, 4]) torch.Size([1, 5, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1g. Rotary Position Embeddings\n",
        "<a id='g'></a>\n",
        "\n",
        "Earlier we pre-computed the frequencies for rotation. Now we'll actually apply our rotary embeddings."
      ],
      "metadata": {
        "id": "85zBiEBikLB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we reshape and then view our queries and keys as complex values, the type of number that works well with rotation\n",
        "xq = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "xk = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
        "print(f'xk: {xk.shape}\\n{xk}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sFhg0hOUQyi",
        "outputId": "b99cd78b-6f9f-4ac5-9cc7-1a684834280e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq: torch.Size([1, 5, 4, 2])\n",
            "tensor([[[[ 0.2170-0.2767j, -0.1258-0.7064j],\n",
            "          [ 0.0471+0.0557j, -0.1916-0.3776j],\n",
            "          [-0.2885+0.0269j, -0.1583-0.1648j],\n",
            "          [ 0.1905-0.0708j,  0.4597+0.4966j]],\n",
            "\n",
            "         [[ 0.8161-0.8226j,  1.1365-0.0757j],\n",
            "          [ 0.3695-0.7906j,  1.3074-0.2812j],\n",
            "          [ 0.6162-0.1722j, -0.4068+1.3610j],\n",
            "          [-0.9063+0.0619j, -0.3543-0.0561j]],\n",
            "\n",
            "         [[ 0.1515-0.3930j, -0.2620-0.2216j],\n",
            "          [ 0.2307+0.0930j, -0.1416-1.3326j],\n",
            "          [ 0.0783+0.3804j,  0.2594+0.7840j],\n",
            "          [ 0.5426-0.0978j,  0.9323-0.1451j]],\n",
            "\n",
            "         [[ 0.9782-0.7228j, -0.8617-0.5507j],\n",
            "          [ 0.4022+0.1999j, -0.1467+0.4029j],\n",
            "          [ 0.0165+0.3566j, -0.0704+0.1177j],\n",
            "          [-1.3812-0.7442j, -0.1762+0.4091j]],\n",
            "\n",
            "         [[-0.1869+0.1524j,  0.5146-0.1804j],\n",
            "          [ 0.4554+0.4472j, -0.9757-0.4605j],\n",
            "          [-0.2682+0.3110j, -0.1756+0.3017j],\n",
            "          [ 1.0174-0.2027j, -0.0683+0.3217j]]]],\n",
            "       grad_fn=<ViewAsComplexBackward0>)\n",
            "\n",
            "xk: torch.Size([1, 5, 2, 2])\n",
            "tensor([[[[-0.6518-0.4543j, -0.8076+0.0228j],\n",
            "          [-0.3781-0.1843j, -0.5233+0.3647j]],\n",
            "\n",
            "         [[ 0.8574-0.8504j, -0.3372-0.5174j],\n",
            "          [ 0.5004-0.0524j,  1.2191+0.1451j]],\n",
            "\n",
            "         [[-0.6592-0.6126j,  0.6929+0.0612j],\n",
            "          [-0.0851+0.2471j, -1.3159+0.0357j]],\n",
            "\n",
            "         [[ 0.1234+0.0649j,  0.3264+0.4136j],\n",
            "          [ 0.3447-0.8855j,  0.1949-0.1915j]],\n",
            "\n",
            "         [[-0.4424+0.4733j,  0.8785+0.1532j],\n",
            "          [-0.6717-0.4233j, -0.3188-0.4305j]]]],\n",
            "       grad_fn=<ViewAsComplexBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ndim = xq.ndim\n",
        "assert 0 <= 1 < ndim\n",
        "assert freqs_cis.shape == (xq.shape[1], xq.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != xq.shape[1], xq.shape[-1] {(xq.shape[1], xq.shape[-1])}'\n",
        "\n",
        "# reshape our queries\n",
        "shape = [d if i == 1 or i == xq.ndim - 1 else 1 for i, d in enumerate(xq.shape)]\n",
        "print(f'shape: {shape}\\n')\n",
        "\n",
        "freqs_cis = freqs_cis.view(*shape)\n",
        "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62knt5SXUQ3T",
        "outputId": "a99e87b2-9414-410e-f216-0de362773eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: [1, 5, 1, 2]\n",
            "\n",
            "freqs_cis: torch.Size([1, 5, 1, 2])\n",
            "tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j]],\n",
            "\n",
            "         [[ 0.5403+0.8415j,  0.9999+0.0100j]],\n",
            "\n",
            "         [[-0.4161+0.9093j,  0.9998+0.0200j]],\n",
            "\n",
            "         [[-0.9900+0.1411j,  0.9996+0.0300j]],\n",
            "\n",
            "         [[-0.6536-0.7568j,  0.9992+0.0400j]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now multiply the data by the frequencies, turn them back into real numbers, revert the shape and make sure they're of the right type\n",
        "xq = torch.view_as_real(xq * freqs_cis).flatten(3).type_as(xv)\n",
        "xk = torch.view_as_real(xk * freqs_cis).flatten(3).type_as(xv)\n",
        "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
        "print(f'xk: {xk.shape}\\n{xk}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RyblxHYURBT",
        "outputId": "543b4504-2e4b-40f4-edc6-0ad163a0624f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq: torch.Size([1, 5, 4, 4])\n",
            "tensor([[[[ 2.1695e-01, -2.7675e-01, -1.2576e-01, -7.0642e-01],\n",
            "          [ 4.7134e-02,  5.5681e-02, -1.9157e-01, -3.7763e-01],\n",
            "          [-2.8846e-01,  2.6856e-02, -1.5825e-01, -1.6482e-01],\n",
            "          [ 1.9047e-01, -7.0766e-02,  4.5974e-01,  4.9656e-01]],\n",
            "\n",
            "         [[ 1.1331e+00,  2.4227e-01,  1.1372e+00, -6.4345e-02],\n",
            "          [ 8.6493e-01, -1.1629e-01,  1.3101e+00, -2.6810e-01],\n",
            "          [ 4.7785e-01,  4.2547e-01, -4.2034e-01,  1.3569e+00],\n",
            "          [-5.4173e-01, -7.2918e-01, -3.5373e-01, -5.9591e-02]],\n",
            "\n",
            "         [[ 2.9432e-01,  3.0135e-01, -2.5751e-01, -2.2682e-01],\n",
            "          [-1.8063e-01,  1.7110e-01, -1.1491e-01, -1.3351e+00],\n",
            "          [-3.7844e-01, -8.7070e-02,  2.4368e-01,  7.8901e-01],\n",
            "          [-1.3687e-01,  5.3407e-01,  9.3506e-01, -1.2645e-01]],\n",
            "\n",
            "         [[-8.6646e-01,  8.5359e-01, -8.4484e-01, -5.7628e-01],\n",
            "          [-4.2638e-01, -1.4113e-01, -1.5868e-01,  3.9828e-01],\n",
            "          [-6.6629e-02, -3.5071e-01, -7.3944e-02,  1.1552e-01],\n",
            "          [ 1.4723e+00,  5.4181e-01, -1.8834e-01,  4.0365e-01]],\n",
            "\n",
            "         [[ 2.3748e-01,  4.1875e-02,  5.2135e-01, -1.5966e-01],\n",
            "          [ 4.0716e-02, -6.3695e-01, -9.5652e-01, -4.9914e-01],\n",
            "          [ 4.1063e-01, -2.9024e-04, -1.8756e-01,  2.9443e-01],\n",
            "          [-8.1840e-01, -6.3743e-01, -8.1110e-02,  3.1873e-01]]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "xk: torch.Size([1, 5, 2, 4])\n",
            "tensor([[[[-0.6518, -0.4543, -0.8076,  0.0228],\n",
            "          [-0.3781, -0.1843, -0.5233,  0.3647]],\n",
            "\n",
            "         [[ 1.1788,  0.2620, -0.3320, -0.5207],\n",
            "          [ 0.3145,  0.3928,  1.2176,  0.1573]],\n",
            "\n",
            "         [[ 0.8314, -0.3445,  0.6915,  0.0750],\n",
            "          [-0.1893, -0.1802, -1.3164,  0.0094]],\n",
            "\n",
            "         [[-0.1313, -0.0469,  0.3138,  0.4232],\n",
            "          [-0.2163,  0.9253,  0.2005, -0.1856]],\n",
            "\n",
            "         [[ 0.6474,  0.0255,  0.8717,  0.1882],\n",
            "          [ 0.1187,  0.7850, -0.3014, -0.4429]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1h. Calculating Self-Attention\n",
        "<a id='h'></a>\n",
        "now we get to perform the actual attention calculation"
      ],
      "metadata": {
        "id": "rvpeoAFRt51t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the number of K & V heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
        "if num_kv_heads != num_heads:\n",
        "  num_queries_per_kv = num_heads // num_kv_heads\n",
        "  xk = torch.repeat_interleave(xk, num_queries_per_kv, dim=2)\n",
        "  xv = torch.repeat_interleave(xv, num_queries_per_kv, dim=2)\n",
        "\n",
        "xq.shape, xk.shape, xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVeOQpzrs1Ej",
        "outputId": "7db53974-7713-48c6-9ee3-71b3f03323b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4, 4]), torch.Size([1, 5, 4, 4]), torch.Size([1, 5, 4, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
        "xq = xq.transpose(1, 2)\n",
        "xk = xk.transpose(1, 2)\n",
        "xv = xv.transpose(1, 2)\n",
        "\n",
        "xq.shape, xk.shape, xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOLwaqDs1MS",
        "outputId": "6938522d-db4c-46d5-cb10-61f1562a05c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 4]), torch.Size([1, 4, 5, 4]), torch.Size([1, 4, 5, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates attention logits by performing a batch matrix multiplication between queries and keys\n",
        "scores = torch.matmul(xq, xk.transpose(2, 3))\n",
        "\n",
        "# then we scale the logits by the reciprocal of the square root of the head dimension\n",
        "scores = scores / math.sqrt(head_dim)\n",
        "\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CCcntgts1fG",
        "outputId": "58e73042-3e03-433b-909e-a647656da299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 5]),\n",
              " tensor([[[[ 0.0349,  0.2964,  0.0679, -0.1770, -0.0546],\n",
              "           [-0.8843,  0.5276,  0.8201,  0.0848,  0.8594],\n",
              "           [-0.0630,  0.3148, -0.0271, -0.1148, -0.0345],\n",
              "           [ 0.4231, -0.1086, -0.8209, -0.2176, -0.6920],\n",
              "           [-0.2993,  0.1005,  0.2658,  0.0315,  0.2896]],\n",
              " \n",
              "          [[ 0.0450,  0.1652, -0.0704, -0.1144, -0.1031],\n",
              "           [-0.7876,  0.3469,  0.8225,  0.0948,  0.8243],\n",
              "           [ 0.0512,  0.2826, -0.1944, -0.2927, -0.2320],\n",
              "           [ 0.2396, -0.3472, -0.1929,  0.0907, -0.1715],\n",
              "           [ 0.5120,  0.2293, -0.2228, -0.2435, -0.4588]],\n",
              " \n",
              "          [[ 0.0634, -0.1494,  0.1283,  0.0430,  0.0538],\n",
              "           [ 0.2279,  0.0095,  0.1995, -0.0229, -0.0418],\n",
              "           [ 0.1597,  0.1338, -0.1130, -0.0481, -0.2681],\n",
              "           [ 0.0853, -0.1153,  0.0871, -0.1732, -0.1560],\n",
              "           [ 0.0252, -0.0265,  0.0860, -0.0907, -0.0127]],\n",
              " \n",
              "          [[-0.0592,  0.3350, -0.3119, -0.0533, -0.1957],\n",
              "           [ 0.2513, -0.4484,  0.3495, -0.3087, -0.2519],\n",
              "           [-0.2911,  0.6427, -0.6512,  0.3674,  0.0886],\n",
              "           [-0.2054,  0.2550, -0.0623,  0.0351,  0.2391],\n",
              "           [ 0.2928, -0.2782,  0.1898, -0.2441, -0.3571]]]],\n",
              "        grad_fn=<DivBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we get to use the mask that we precomputed earlier\n",
        "scores = scores + mask\n",
        "\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPmJK1zbwjuS",
        "outputId": "1bd2812a-9d54-41d7-ebaa-2f0a0caffcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 5]),\n",
              " tensor([[[[ 0.0349,    -inf,    -inf,    -inf,    -inf],\n",
              "           [-0.8843,  0.5276,    -inf,    -inf,    -inf],\n",
              "           [-0.0630,  0.3148, -0.0271,    -inf,    -inf],\n",
              "           [ 0.4231, -0.1086, -0.8209, -0.2176,    -inf],\n",
              "           [-0.2993,  0.1005,  0.2658,  0.0315,  0.2896]],\n",
              " \n",
              "          [[ 0.0450,    -inf,    -inf,    -inf,    -inf],\n",
              "           [-0.7876,  0.3469,    -inf,    -inf,    -inf],\n",
              "           [ 0.0512,  0.2826, -0.1944,    -inf,    -inf],\n",
              "           [ 0.2396, -0.3472, -0.1929,  0.0907,    -inf],\n",
              "           [ 0.5120,  0.2293, -0.2228, -0.2435, -0.4588]],\n",
              " \n",
              "          [[ 0.0634,    -inf,    -inf,    -inf,    -inf],\n",
              "           [ 0.2279,  0.0095,    -inf,    -inf,    -inf],\n",
              "           [ 0.1597,  0.1338, -0.1130,    -inf,    -inf],\n",
              "           [ 0.0853, -0.1153,  0.0871, -0.1732,    -inf],\n",
              "           [ 0.0252, -0.0265,  0.0860, -0.0907, -0.0127]],\n",
              " \n",
              "          [[-0.0592,    -inf,    -inf,    -inf,    -inf],\n",
              "           [ 0.2513, -0.4484,    -inf,    -inf,    -inf],\n",
              "           [-0.2911,  0.6427, -0.6512,    -inf,    -inf],\n",
              "           [-0.2054,  0.2550, -0.0623,  0.0351,    -inf],\n",
              "           [ 0.2928, -0.2782,  0.1898, -0.2441, -0.3571]]]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we perform the softmax operation to get our actual probabilities\n",
        "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "scores\n",
        "# notice that thanks to the causal mask, 0 probability is placed on future tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knsd9x-iwjxd",
        "outputId": "0e45ead3-5ecc-4ce9-87d3-f9189d62129e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1959, 0.8041, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2861, 0.4174, 0.2965, 0.0000, 0.0000],\n",
              "          [0.4162, 0.2446, 0.1200, 0.2193, 0.0000],\n",
              "          [0.1343, 0.2003, 0.2363, 0.1870, 0.2420]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2433, 0.7567, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3287, 0.4143, 0.2571, 0.0000, 0.0000],\n",
              "          [0.3261, 0.1813, 0.2116, 0.2810, 0.0000],\n",
              "          [0.3245, 0.2446, 0.1556, 0.1524, 0.1229]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5544, 0.4456, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3655, 0.3562, 0.2783, 0.0000, 0.0000],\n",
              "          [0.2784, 0.2278, 0.2789, 0.2150, 0.0000],\n",
              "          [0.2055, 0.1952, 0.2184, 0.1830, 0.1979]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.6681, 0.3319, 0.0000, 0.0000, 0.0000],\n",
              "          [0.2358, 0.5998, 0.1645, 0.0000, 0.0000],\n",
              "          [0.1996, 0.3163, 0.2303, 0.2538, 0.0000],\n",
              "          [0.2798, 0.1581, 0.2524, 0.1636, 0.1461]]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then matmul by our values projection\n",
        "output = torch.matmul(scores, xv)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtEbx0D1wj0K",
        "outputId": "730ee2a5-b9b2-4992-a692-28ee6b9d2dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 4]),\n",
              " tensor([[[[ 0.8224,  0.4530,  0.5962,  0.6171],\n",
              "           [-0.0093,  0.1717,  0.1173,  0.5434],\n",
              "           [ 0.1605,  0.0397,  0.2777,  0.3978],\n",
              "           [ 0.3163,  0.1755,  0.4137,  0.3426],\n",
              "           [ 0.0670,  0.1109,  0.1758,  0.1951]],\n",
              " \n",
              "          [[ 0.8224,  0.4530,  0.5962,  0.6171],\n",
              "           [ 0.0397,  0.1883,  0.1455,  0.5478],\n",
              "           [ 0.1944,  0.0763,  0.2888,  0.4222],\n",
              "           [ 0.2656,  0.0915,  0.4273,  0.2422],\n",
              "           [ 0.2215,  0.1752,  0.2870,  0.3211]],\n",
              " \n",
              "          [[ 0.7174,  0.0402,  0.2111, -0.7315],\n",
              "           [ 0.7096, -0.2251,  0.1030, -0.1930],\n",
              "           [ 0.4068, -0.1231, -0.0190, -0.2104],\n",
              "           [ 0.1971, -0.0743,  0.1910, -0.2105],\n",
              "           [ 0.1888, -0.0599,  0.0187, -0.0097]],\n",
              " \n",
              "          [[ 0.7174,  0.0402,  0.2111, -0.7315],\n",
              "           [ 0.7116, -0.1574,  0.1306, -0.3304],\n",
              "           [ 0.5270, -0.2881, -0.0193,  0.0469],\n",
              "           [ 0.2104, -0.1405,  0.2270, -0.0914],\n",
              "           [ 0.1992, -0.0298,  0.0423, -0.1315]]]],\n",
              "        grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and reshape to put the sequence length back into place and the outputs of our heads lined up\n",
        "output = output.transpose(1, 2).contiguous().view(b, seq_len, -1)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bpSgyBA41vV",
        "outputId": "2ec34648-3a3c-40ba-8220-835bb5e43b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 0.8224,  0.4530,  0.5962,  0.6171,  0.8224,  0.4530,  0.5962,\n",
              "            0.6171,  0.7174,  0.0402,  0.2111, -0.7315,  0.7174,  0.0402,\n",
              "            0.2111, -0.7315],\n",
              "          [-0.0093,  0.1717,  0.1173,  0.5434,  0.0397,  0.1883,  0.1455,\n",
              "            0.5478,  0.7096, -0.2251,  0.1030, -0.1930,  0.7116, -0.1574,\n",
              "            0.1306, -0.3304],\n",
              "          [ 0.1605,  0.0397,  0.2777,  0.3978,  0.1944,  0.0763,  0.2888,\n",
              "            0.4222,  0.4068, -0.1231, -0.0190, -0.2104,  0.5270, -0.2881,\n",
              "           -0.0193,  0.0469],\n",
              "          [ 0.3163,  0.1755,  0.4137,  0.3426,  0.2656,  0.0915,  0.4273,\n",
              "            0.2422,  0.1971, -0.0743,  0.1910, -0.2105,  0.2104, -0.1405,\n",
              "            0.2270, -0.0914],\n",
              "          [ 0.0670,  0.1109,  0.1758,  0.1951,  0.2215,  0.1752,  0.2870,\n",
              "            0.3211,  0.1888, -0.0599,  0.0187, -0.0097,  0.1992, -0.0298,\n",
              "            0.0423, -0.1315]]], grad_fn=<ViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we can initialize and apply our output projection that mixes the information from the heads together\n",
        "wo = nn.Linear(num_heads * head_dim, d, bias=False)\n",
        "Xout = wo(output)\n",
        "Xout.shape, Xout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paC0J9h549KI",
        "outputId": "19eb2066-cbaa-41cd-8261-7e0114ffe255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 7.1982e-01, -4.1950e-01,  4.7529e-02,  1.0536e-01, -4.7111e-01,\n",
              "           -1.5629e-01, -2.7153e-02,  2.5266e-01, -1.9366e-01, -2.6828e-01,\n",
              "           -1.8550e-02,  3.5433e-01,  4.0302e-01, -2.3292e-01, -2.2380e-01,\n",
              "           -2.0106e-01],\n",
              "          [ 3.3439e-01, -2.0078e-01,  8.8455e-02, -5.8876e-02, -1.3279e-01,\n",
              "            1.6281e-01,  3.3500e-02,  7.2977e-02, -1.7260e-01, -1.2296e-01,\n",
              "            1.3433e-01,  1.4968e-01,  2.3543e-01, -2.5995e-01, -1.0525e-05,\n",
              "           -1.9271e-01],\n",
              "          [ 3.4752e-01, -2.4674e-01,  7.5127e-02, -1.0779e-01, -1.7768e-01,\n",
              "            1.9428e-01,  3.9898e-02,  2.2347e-02,  7.5406e-02, -7.1007e-02,\n",
              "            1.5316e-01,  9.0577e-02,  1.2806e-01, -1.8189e-01, -8.5211e-02,\n",
              "           -1.5611e-01],\n",
              "          [ 3.0263e-01, -2.0786e-01, -1.8208e-02, -7.5674e-02, -1.7330e-01,\n",
              "           -3.8195e-02,  5.9163e-02, -1.6124e-02,  3.3142e-02, -1.3451e-01,\n",
              "            2.5009e-04,  1.5753e-01,  2.0458e-01, -5.7169e-02, -2.1645e-02,\n",
              "            1.1124e-02],\n",
              "          [ 2.4274e-01, -1.6512e-01, -2.4988e-02, -5.7066e-02, -4.8759e-02,\n",
              "            7.8460e-03,  2.9181e-02,  5.6676e-02, -4.8108e-03, -1.0721e-01,\n",
              "           -2.6498e-02,  9.3713e-02,  1.6361e-01, -1.0842e-01, -5.9305e-03,\n",
              "           -2.5160e-02]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1i. Our first residual connection\n",
        "<a id='i'></a>\n",
        "Here we'll normalize the output of our attention mechanism and then add it to our residual state"
      ],
      "metadata": {
        "id": "IFdEv7wg6Vm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h += Xout\n",
        "h.shape, h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdVEPPxi6hcc",
        "outputId": "79d0b650-daa9-4ba2-8b20-5c8b0a6f203d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 3.5506, -0.6064, -0.9127,  0.3341, -1.9883, -0.2334, -2.2978,\n",
              "           -0.2833, -2.0546, -0.3482, -0.9188,  1.3220,  0.8613, -1.4541,\n",
              "           -0.1611,  0.6779],\n",
              "          [ 1.6564, -0.4561,  2.4281, -0.4790,  0.5623,  1.1705,  1.2383,\n",
              "            0.4741, -2.1986, -0.5805,  1.9673,  0.8375,  2.5630, -1.7590,\n",
              "           -1.0407, -0.5088],\n",
              "          [ 3.3650, -1.8352,  0.7795, -0.3841, -0.4541,  2.3658,  0.2736,\n",
              "            1.3954,  0.4436, -0.1863,  0.6353,  0.4561,  0.1173, -0.5224,\n",
              "           -0.3226, -1.7816],\n",
              "          [ 2.4056, -2.2560, -0.0896,  0.4923, -0.8245, -1.1869,  0.3243,\n",
              "           -0.2483, -0.0502, -0.9634,  1.0602,  1.9231,  2.2565,  1.1518,\n",
              "            0.6319,  1.4035],\n",
              "          [ 2.8266, -2.6940,  0.0327,  0.8795, -0.6646,  0.4215,  0.1522,\n",
              "            1.0536,  0.9114, -3.7775, -0.4138,  1.4161,  3.2076, -0.6312,\n",
              "            0.6284, -0.3478]]], grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# then we'll normalize the current state of our residual for use in our MoE later\n",
        "pre_ffwd_norm = RMSNorm(d)\n",
        "h_normed = pre_ffwd_norm(h)\n",
        "# so now we're working with x, which we'll use later for our next residual conenction, and x_normed which is used by our MoE MLP"
      ],
      "metadata": {
        "id": "Anfh-ziI6hhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1j. The SwiGLU Feedforward Network\n",
        "<a id='j'></a>\n",
        "\n",
        "Llama models have surprisingly not opted for a mixture of experts strategy which i was assuming they'd go for by now. Their feedforward networks use the SwiGLU activation which basically uses the activation function as a gate that dynamically determines what information gets through"
      ],
      "metadata": {
        "id": "d-bamJOGuZ52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we need to define our actual hidden dimension, which Llama's code does in an unnecessarily complicated manner\n",
        "hidden_dim = 4 * d # usually i would designate a hyperparameter for this 4, but in llama's code it was just there\n",
        "print(hidden_dim)\n",
        "hidden_dim = int(2 * hidden_dim / 3)\n",
        "print(hidden_dim)\n",
        "multiple_of = 256 # their description of this was \"make SwiGLU hidden layer size multiple of large power of 2\"\n",
        "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "print(hidden_dim)\n",
        "# so basically this overly convoluted setup is designed to ensure that hidden_dim is a multiple of 256, likely for hardware efficiency reasons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbRg3C6ovcXr",
        "outputId": "b25e6576-3a72-4da5-d355-5abc9228f79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "42\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "up = nn.Linear(d, hidden_dim, bias=False)\n",
        "gate = nn.Linear(d, hidden_dim, bias=False)\n",
        "down = nn.Linear(hidden_dim, d, bias=False)"
      ],
      "metadata": {
        "id": "6vp7-QkPufSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_proj = up(h_normed)\n",
        "print(up_proj.shape, up_proj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtuRJosB0MNf",
        "outputId": "60472df0-9f02-4a80-d5c1-ae69369f7022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 256]) tensor([[[-0.3775,  0.2147,  0.2070,  ...,  1.1758,  0.5350,  0.1489],\n",
            "         [-0.4961,  0.0802,  0.6036,  ...,  0.5064, -0.1309, -1.0673],\n",
            "         [ 0.0369,  0.0817, -0.2353,  ...,  0.1451,  0.1230, -0.7031],\n",
            "         [ 0.4918,  0.0617,  0.7253,  ...,  0.4688, -0.1773,  0.2506],\n",
            "         [ 0.1094,  0.7400,  0.3883,  ...,  0.3386, -0.1453, -0.4714]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gate_proj = F.silu(gate(h_normed))\n",
        "print(gate_proj.shape, gate_proj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hPlRqhb0MWN",
        "outputId": "638dff86-3cd3-45e3-e8bd-7d47339e4299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 256]) tensor([[[-0.2654,  0.4762, -0.1746,  ...,  0.1283,  0.5745,  0.1288],\n",
            "         [-0.2330, -0.0939, -0.2176,  ...,  0.2495,  0.1948,  0.2031],\n",
            "         [ 0.1397,  0.5607, -0.2677,  ...,  0.3105,  0.0060, -0.1339],\n",
            "         [-0.2461,  0.3907,  0.3002,  ...,  0.5885,  0.6722,  0.6079],\n",
            "         [-0.0112,  0.6475,  0.1435,  ...,  0.8659,  0.3234,  0.0028]]],\n",
            "       grad_fn=<SiluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ffwd_output = down(up_proj * gate_proj)\n",
        "print(ffwd_output.shape, ffwd_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE2bH6-hz2NV",
        "outputId": "a3673380-6840-4343-cc59-9db3782e47e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16]) tensor([[[ 0.0056,  0.0167,  0.0514,  0.0153, -0.0333, -0.1343, -0.1392,\n",
            "           0.0715,  0.0349,  0.0315,  0.0690, -0.0434,  0.0209,  0.0681,\n",
            "           0.1831,  0.0926],\n",
            "         [ 0.0958,  0.0138,  0.1765,  0.0728, -0.0919, -0.1499,  0.0478,\n",
            "          -0.1806, -0.1907,  0.0043,  0.0631, -0.1799, -0.0557,  0.0860,\n",
            "          -0.0789, -0.0315],\n",
            "         [-0.0702,  0.0152, -0.0829,  0.0884,  0.0553,  0.0950,  0.1276,\n",
            "          -0.1985, -0.0489,  0.1400, -0.0942,  0.1672,  0.0407,  0.0818,\n",
            "          -0.1313,  0.0097],\n",
            "         [-0.0949,  0.0313,  0.1437,  0.0006, -0.0451, -0.0712, -0.0642,\n",
            "          -0.0783,  0.0467,  0.1035,  0.0802,  0.0331, -0.0398,  0.0468,\n",
            "           0.0242,  0.1243],\n",
            "         [-0.0432,  0.0420,  0.1084, -0.0500, -0.0397,  0.0344,  0.0966,\n",
            "           0.0896,  0.0391,  0.0631, -0.0043,  0.0644,  0.0658,  0.0548,\n",
            "           0.0436,  0.1454]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and then do our final residual connection of this layer\n",
        "out = h + ffwd_output\n",
        "print(out.shape, out)"
      ],
      "metadata": {
        "id": "Aal1G2gpmp3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81966650-ce63-49c0-9742-968dce4f8081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16]) tensor([[[ 3.5562e+00, -5.8969e-01, -8.6136e-01,  3.4942e-01, -2.0216e+00,\n",
            "          -3.6778e-01, -2.4369e+00, -2.1177e-01, -2.0197e+00, -3.1668e-01,\n",
            "          -8.4981e-01,  1.2786e+00,  8.8223e-01, -1.3860e+00,  2.2040e-02,\n",
            "           7.7058e-01],\n",
            "         [ 1.7521e+00, -4.4236e-01,  2.6046e+00, -4.0619e-01,  4.7042e-01,\n",
            "           1.0206e+00,  1.2861e+00,  2.9346e-01, -2.3893e+00, -5.7619e-01,\n",
            "           2.0303e+00,  6.5757e-01,  2.5073e+00, -1.6729e+00, -1.1196e+00,\n",
            "          -5.4033e-01],\n",
            "         [ 3.2949e+00, -1.8201e+00,  6.9662e-01, -2.9575e-01, -3.9885e-01,\n",
            "           2.4609e+00,  4.0119e-01,  1.1970e+00,  3.9473e-01, -4.6282e-02,\n",
            "           5.4112e-01,  6.2323e-01,  1.5797e-01, -4.4065e-01, -4.5393e-01,\n",
            "          -1.7719e+00],\n",
            "         [ 2.3108e+00, -2.2247e+00,  5.4144e-02,  4.9290e-01, -8.6964e-01,\n",
            "          -1.2582e+00,  2.6018e-01, -3.2664e-01, -3.4225e-03, -8.5995e-01,\n",
            "           1.1404e+00,  1.9562e+00,  2.2167e+00,  1.1986e+00,  6.5606e-01,\n",
            "           1.5278e+00],\n",
            "         [ 2.7834e+00, -2.6521e+00,  1.4105e-01,  8.2948e-01, -7.0429e-01,\n",
            "           4.5597e-01,  2.4877e-01,  1.1432e+00,  9.5052e-01, -3.7144e+00,\n",
            "          -4.1808e-01,  1.4806e+00,  3.2735e+00, -5.7644e-01,  6.7201e-01,\n",
            "          -2.0237e-01]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1k. Output\n",
        "<a id='k'></a>\n",
        "So usually we'd run it back on steps 1e through 1j for however many layers our model has (Llama 3 8b uses 32) using different weight matrices but you get the point. Since our current `out` is of the same shape that it would be if we were to do more layers, let's go ahead and just see what Llama's output mechanism looks like. It's nothing interesting though, just a linear layer. Notably they chose to use a separate linear layer rather than re-using the embedding layer as is relatively common"
      ],
      "metadata": {
        "id": "9uQuAb91PaQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we norm the residual state\n",
        "final_norm = RMSNorm(d)\n",
        "out_normed = final_norm(out)"
      ],
      "metadata": {
        "id": "Apc0fqBT3bkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then multiply by the linear layer to get our final output logits\n",
        "final_output = nn.Linear(d, v, bias=False)\n",
        "logits = final_output(out_normed).float()\n",
        "logits.shape, logits"
      ],
      "metadata": {
        "id": "4Do_Yyppmp6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8060615d-0f31-4758-9add-d432037c8f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 10]),\n",
              " tensor([[[ 7.6650e-01,  4.1170e-01,  4.2533e-01,  1.1216e-01,  7.7646e-01,\n",
              "            1.4128e-01, -8.2421e-01, -4.4003e-02,  1.5142e-01, -4.3939e-01],\n",
              "          [ 6.2985e-01,  5.7959e-01, -1.1361e-01, -5.2311e-01,  3.3086e-01,\n",
              "           -9.6442e-01, -1.1810e+00, -7.6030e-01,  4.5599e-01, -7.6835e-01],\n",
              "          [ 1.0984e+00,  1.2014e+00,  1.7533e-01,  8.2349e-02,  2.1564e-01,\n",
              "           -6.5409e-01, -2.9928e-01, -5.8757e-01, -3.7019e-04, -8.5932e-01],\n",
              "          [ 1.0524e-01,  4.8672e-02,  7.2490e-01,  4.1315e-02, -1.3067e-01,\n",
              "           -6.7627e-01, -8.2063e-01, -8.1367e-01, -6.3231e-01, -7.6414e-01],\n",
              "          [ 6.9820e-01,  1.2350e+00,  4.9250e-01,  5.3537e-01,  1.6038e-01,\n",
              "           -6.2946e-01, -7.7267e-01, -9.7907e-01,  2.7908e-01, -6.1061e-01]]],\n",
              "        grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax the logits to get the probability for each token's prediction across every token in the sequence\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "probs"
      ],
      "metadata": {
        "id": "IWQu_ua-lbpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d35723ee-2d44-488c-94fe-dbb07cfb6ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1676, 0.1176, 0.1192, 0.0871, 0.1693, 0.0897, 0.0342, 0.0745,\n",
              "          0.0906, 0.0502],\n",
              "         [0.1928, 0.1833, 0.0917, 0.0609, 0.1430, 0.0391, 0.0315, 0.0480,\n",
              "          0.1620, 0.0476],\n",
              "         [0.2293, 0.2542, 0.0911, 0.0830, 0.0948, 0.0397, 0.0567, 0.0425,\n",
              "          0.0764, 0.0324],\n",
              "         [0.1302, 0.1230, 0.2419, 0.1221, 0.1028, 0.0596, 0.0516, 0.0519,\n",
              "          0.0623, 0.0546],\n",
              "         [0.1523, 0.2604, 0.1239, 0.1294, 0.0889, 0.0404, 0.0350, 0.0285,\n",
              "          0.1001, 0.0411]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedily decode the probabilities to get our final predicted indices\n",
        "greedy_indices = torch.argmax(probs, dim=-1)\n",
        "greedy_indices\n",
        "# if we were performing inference rather than training, that final token in the list would be the one to show the user"
      ],
      "metadata": {
        "id": "Z-5_nzJXlbsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aba5325-3eec-4697-a112-8dd639ab6f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4, 0, 1, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1l. The loss functions\n",
        "<a id='l'></a>\n",
        "\n",
        "Of course we use [cross-entropy loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) which should need no introduction if this isn't your first machine-learning rodeo, so we'll be skimming past it. Basically the idea is that the single correct value is rewarded and all other values are suppressed"
      ],
      "metadata": {
        "id": "9E8ibfONO3uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create some random fake target indices to train on\n",
        "target_token_indices = torch.randint(0, v, greedy_indices.shape)\n",
        "print(target_token_indices)\n",
        "\n",
        "# initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# reshape logits to be compatible and calculate loss\n",
        "loss = loss_fn(logits.view(1,v,seq_len), target_token_indices)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJxo95p3P1KK",
        "outputId": "26ec16cc-cb1b-4d9b-fcfe-1d163e88733d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.7078, grad_fn=<NllLoss2DBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and that's it! those are all the essentail calcuations that Llama performs, most of which aren't any different from other open-source LLMs like Grok, Mixtral or Gemini (Llama is most similar to Gemini since Mixtral and Grok utilize [mixture of experts](https://huggingface.co/blog/moe) for their feedforward networks). Now let's code everything up the correct way into classes so that we can actually build a functioning model\n",
        "\n",
        "# 2. Actually functional model code\n",
        "<a id='two'></a>\n",
        "The bulk of the lesson is over, but the following code demosntrates how you'd actually take the concepts and turn them into functioning nn.Module classes. Alternatively to reading through them here, you can check out the .py files in [the repo](https://github.com/evintunador/minLlama). I'm not going to bother explaining this section in the same detail, except for a few places where things are different/new enough to add comments"
      ],
      "metadata": {
        "id": "5skJdzNNTXzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import time\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# we'll be using a crazy small & simple tokenizer that I made based on the TinyShakespeare dataset\n",
        "# Llama 3 8b's vocabulary size is 128256 including special tokens like <|endoftext|>\n",
        "\n",
        "# download the tokenizer code\n",
        "!wget https://raw.githubusercontent.com/evintunador/minLlama3/main/tiny_shakespeare_tokenizer.py\n",
        "# and the tokenizer model\n",
        "!wget https://raw.githubusercontent.com/evintunador/minLlama3/main/tokenizers/tiny_shakespeare_tokenizer_512.model\n",
        "!mkdir -p tokenizers\n",
        "!mv tiny_shakespeare_tokenizer_512.model tokenizers/\n",
        "from tiny_shakespeare_tokenizer import *\n",
        "tokenizer = get_tokenizer(size = 512)"
      ],
      "metadata": {
        "id": "suAJ3161DTpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ee717c-9d72-4b06-c84c-c0e9dabbabbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 18:17:09--  https://raw.githubusercontent.com/evintunador/minLlama3/main/tiny_shakespeare_tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2180 (2.1K) [text/plain]\n",
            "Saving to: tiny_shakespeare_tokenizer.py\n",
            "\n",
            "\r          tiny_shak   0%[                    ]       0  --.-KB/s               \rtiny_shakespeare_to 100%[===================>]   2.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-20 18:17:10 (45.5 MB/s) - tiny_shakespeare_tokenizer.py saved [2180/2180]\n",
            "\n",
            "--2024-04-20 18:17:10--  https://raw.githubusercontent.com/evintunador/minLlama3/main/tokenizers/tiny_shakespeare_tokenizer_512.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4298 (4.2K) [application/octet-stream]\n",
            "Saving to: tiny_shakespeare_tokenizer_512.model\n",
            "\n",
            "tiny_shakespeare_to 100%[===================>]   4.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-20 18:17:10 (61.3 MB/s) - tiny_shakespeare_tokenizer_512.model saved [4298/4298]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a. the parameters\n",
        "<a id='twoa'></a>\n",
        "\n",
        "here are the parameters that I've setup for my little minLLama3 test model"
      ],
      "metadata": {
        "id": "CauGV8VtxJ5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass # the hyperparameters of our minLlama3\n",
        "class ModelArgs:\n",
        "    dim: int = 128 # Llama 3 8b uses 4096\n",
        "    n_layers: int = 8 # Llama 3 8b uses 32\n",
        "    n_heads: int = 4 # Llama 3 8b uses 32\n",
        "    n_kv_heads: Optional[int] = 1 # Llama 3 8b's uses 8\n",
        "    vocab_size: int = tokenizer.vocab_len # Llama 3 uses a more complicated tokenizer of length 128256\n",
        "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2. Llama 3 8b's uses 1024\n",
        "    ffn_dim_multiplier: Optional[float] = None # Llama 3 8b's uses 1.3, which changes the ending hidden_dim slightly\n",
        "    norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000 # Llama 3 8b uses 500000\n",
        "    max_batch_size: int = 32 # who knows what batch size they trained with\n",
        "    max_seq_len: int = 512 # Llama 3 8b trained with 8192 but their maximum kv cache chunk size during inference is 2048\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    dropout_rate: float = 0.1 # who knows what dropout rate they trained with"
      ],
      "metadata": {
        "id": "bCsmICWv5rMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2b. RMSNorm\n",
        "<a id='twob'></a>"
      ],
      "metadata": {
        "id": "cvg6WPUzBpLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ],
      "metadata": {
        "id": "VJJUAg5oBpVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2c. RoPE\n",
        "<a id='twoc'></a>\n"
      ],
      "metadata": {
        "id": "8ZRhKbO3BcNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis.to(params.device)\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != (x.shape[1], x.shape[-1]) {(x.shape[1], x.shape[-1])}'\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)"
      ],
      "metadata": {
        "id": "9bJh8MHTUp7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2d. Attention\n",
        "<a id='twod'></a>"
      ],
      "metadata": {
        "id": "pksnkzKHCDyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
        "    bs, seqlen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bs, seqlen, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bs, seqlen, n_kv_heads * n_rep, head_dim)\n",
        "    ) # this code looks different from hwo we did it in section 1 bit it's effectively the same\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        self.n_rep = args.n_heads // self.n_kv_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        self.cache_k = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
        "            requires_grad = False\n",
        "        ).to(args.device)\n",
        "        self.cache_v = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
        "            requires_grad = False\n",
        "        ).to(args.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        freqs_cis: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        start_pos: int = None,\n",
        "    ):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
        "\n",
        "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "\n",
        "        if start_pos is not None: # if we're performing inference, use kv caching (not shown in section 1)\n",
        "            # make sure our cache is on the right device\n",
        "            self.cache_k = self.cache_k.to(xq)\n",
        "            self.cache_v = self.cache_v.to(xq)\n",
        "\n",
        "            # set the values in our cache according to the current input\n",
        "            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
        "            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
        "\n",
        "            # grab our key and value matrixes which have a longer sequence length than our queries\n",
        "            keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
        "            values = self.cache_v[:bsz, : start_pos + seqlen]\n",
        "        else:\n",
        "            # if we're training, do full sequence length (like in section 1)\n",
        "            keys, values = xk, xv\n",
        "\n",
        "        # repeat k/v heads if n_kv_heads < n_heads\n",
        "        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_heads, head_dim)\n",
        "        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_heads, head_dim)\n",
        "\n",
        "        xq = xq.transpose(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
        "        keys = keys.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
        "        values = values.transpose(1, 2)  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
        "\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores + mask  # (bs, n_heads, seqlen, cache_len + seqlen)\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "        output = torch.matmul(scores, values)  # (bs, n_heads, seqlen, head_dim)\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        return self.wo(output)"
      ],
      "metadata": {
        "id": "meweM31MCD97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2e. Ffwd\n",
        "<a id='twoe'></a>"
      ],
      "metadata": {
        "id": "HW3uAiv-xQWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,\n",
        "        ffn_dim_multiplier: Optional[float],\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # custom dim factor multiplier that ensures we're using a multiple of 256, likely for hardware efficiency reasons\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ],
      "metadata": {
        "id": "CEf5HsU4UqBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2f. Residual Layers\n",
        "<a id='twof'></a>"
      ],
      "metadata": {
        "id": "xF9wBRbNxUTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(\n",
        "            dim=args.dim,\n",
        "            hidden_dim=4 * args.dim,\n",
        "            multiple_of=args.multiple_of,\n",
        "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
        "        )\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.dropout_rate = args.dropout_rate\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        freqs_cis: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        start_pos: int = None,\n",
        "        training = False,\n",
        "    ):\n",
        "        # our two residual connections, plus dropout which will only happen if we're training\n",
        "        h = x + F.dropout(self.attention(self.attention_norm(x), freqs_cis, mask, start_pos), p=self.dropout_rate, training=training)\n",
        "        out = h + F.dropout(self.feed_forward(self.ffn_norm(h)), p=self.dropout_rate, training=training)\n",
        "        return out"
      ],
      "metadata": {
        "id": "nvfkGSrnUqFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2g. The model itself\n",
        "<a id='twog'></a>"
      ],
      "metadata": {
        "id": "xedETQt7xark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Llama3(nn.Module):\n",
        "    def __init__(self, params: ModelArgs, tokenizer):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "        self.max_seq_len = params.max_seq_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "\n",
        "        # final norm and linear layer\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        self.output = nn.Linear(\n",
        "            params.dim,\n",
        "            params.vocab_size,\n",
        "            bias=False)\n",
        "\n",
        "        # precompute RoPE frequencies\n",
        "        self.freqs_cis = precompute_freqs_cis(\n",
        "            params.dim // params.n_heads,\n",
        "            params.max_seq_len * 2,\n",
        "            params.rope_theta,)\n",
        "\n",
        "        # precompute the causal attention mask\n",
        "        mask = torch.full((params.max_seq_len, params.max_seq_len),\n",
        "                          float(\"-inf\"),\n",
        "                          device=params.device)\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, # specifically for training. this is what you saw in section 1\n",
        "                tokens: torch.Tensor,\n",
        "                targets: torch.Tensor):\n",
        "        bsz, seqlen = tokens.shape\n",
        "        assert tokens.shape == targets.shape\n",
        "        assert seqlen == self.max_seq_len\n",
        "\n",
        "        # initialize the first residual state\n",
        "        h = self.tok_embeddings(tokens)\n",
        "\n",
        "        # grab precomputes freqs_cis\n",
        "        freqs_cis = self.freqs_cis.to(h.device)\n",
        "        freqs_cis = self.freqs_cis[:seqlen]\n",
        "\n",
        "        # run the residual state through each layer\n",
        "        for layer in self.layers:\n",
        "            h = layer(\n",
        "                h,\n",
        "                freqs_cis,\n",
        "                self.mask,\n",
        "                start_pos = None,\n",
        "                training = True\n",
        "            )\n",
        "\n",
        "        # norm the final output then get the logits\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h).float()\n",
        "\n",
        "        loss = self.criterion(\n",
        "            logits.view(bsz * seqlen, self.vocab_size),\n",
        "            targets.reshape(bsz * seqlen))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward_inference(self,\n",
        "                          tokens: torch.Tensor,\n",
        "                          start_pos: int,\n",
        "                          max_context_window: int,\n",
        "                         ):\n",
        "        _bsz, seqlen = tokens.shape\n",
        "        h = self.tok_embeddings(tokens)\n",
        "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
        "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
        "\n",
        "        mask = self.mask[:seqlen, :seqlen]\n",
        "        # When performing key-value caching, we compute the attention scores\n",
        "        # only for the new sequence. Thus, the matrix of scores is of size\n",
        "        # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n",
        "        # j > cache_len + i, since row i corresponds to token cache_len + i.\n",
        "        mask = torch.hstack(\n",
        "            [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
        "        ).type_as(h)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(\n",
        "                h,\n",
        "                freqs_cis,\n",
        "                mask,\n",
        "                start_pos = start_pos\n",
        "            )\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h).float()\n",
        "        return logits\n",
        "\n",
        "    @torch.inference_mode() # no need to keep track of gradients during inference\n",
        "    def Sampler(\n",
        "        self,\n",
        "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
        "        temperature: float, # controls how boring vs random the outputs should be\n",
        "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
        "        top_k: int, # the maximum number of output options we're willing to consider\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        The Sampler function is responsible for generating token predictions\n",
        "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling\n",
        "        \"\"\"\n",
        "        # Select the last element for each sequence.\n",
        "        logits = logits[:,-1,:] # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        logits.div_(temperature) # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
        "\n",
        "        # Calculate probabilities with softmax.\n",
        "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
        "\n",
        "        # sort the probabilities to for use in top-p & top-k. both are (batch_size, vocab_size)\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "\n",
        "        ### calculating top-p\n",
        "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "        # mask where 0's are top-p selections & 1's are to be excluded\n",
        "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
        "        # the original probabilities with excluded tokens changed to 0.0\n",
        "        probs_sort = torch.where(top_ps_mask, 0, probs_sort)\n",
        "\n",
        "        ### calculating top_k\n",
        "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
        "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device)\n",
        "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
        "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
        "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
        "        top_ks_mask = top_ks_mask >= top_k\n",
        "\n",
        "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
        "        # this trims probs_sort to also fit within our top_k requirement\n",
        "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
        "\n",
        "        # Re-normalization so that total probabilities add up to 1\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "\n",
        "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
        "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
        "\n",
        "        # samples from the distribution\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return next_token_id # returns the predicted token\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_gen_len: int = None,\n",
        "        memory_saver_div: int = 1, # defaults to full max_seq_len**2 memory use. must be power of 2\n",
        "        temperature: float = 0.6, # default value in meta's code\n",
        "        top_p: float = 0.9, # default value in meta's code\n",
        "        top_k: int = tokenizer.vocab_len, # meta's code doesn't bother with topk\n",
        "    ) -> str:\n",
        "        \"\"\" Wrapper around sampler() that deals with manipulation of the sequence \"\"\"\n",
        "\n",
        "        # ensuring memory_saver_div, the setting that affects our kv caching, will work\n",
        "        assert ((memory_saver_div & (memory_saver_div-1)) == 0) & (memory_saver_div > 0), f'memory_saver_div {memory_saver_div} must be power of 2'\n",
        "        max_context_window = self.max_seq_len // memory_saver_div\n",
        "        if max_context_window < self.max_seq_len:\n",
        "            print(f'maximum attention matrix size will be {max_context_window}x{self.max_seq_len} rather than {self.max_seq_len}x{self.max_seq_len}\\n')\n",
        "\n",
        "        # encoding the prompt into token indices\n",
        "        tokens = self.tokenizer.encode(prompt)\n",
        "\n",
        "        if max_gen_len is None:\n",
        "            max_gen_len = self.max_seq_len - len(tokens)\n",
        "        elif max_gen_len + len(tokens) > self.max_seq_len:\n",
        "            print(f'capping max_gen_len at max_seq_len={self.max_seq_len} including input\\n')\n",
        "            max_gen_len = self.max_seq_len - len(tokens)\n",
        "\n",
        "        # turning it into the right tensor shape\n",
        "        tokens = torch.tensor(tokens, device=self.params.device)\n",
        "        tokens = tokens.unsqueeze(0) if len(tokens.shape)==1 else tokens # jic we need to add a batch dimension\n",
        "\n",
        "        # the offset used for kv caching\n",
        "        start_pos = max(tokens.shape[1] - max_context_window, 0)\n",
        "\n",
        "        for i in range(max_gen_len):\n",
        "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
        "            logits = self.forward_inference(\n",
        "                tokens[:,-max_context_window:],\n",
        "                start_pos = start_pos,\n",
        "                max_context_window = max_context_window\n",
        "            )\n",
        "\n",
        "            # sample th enext token to be used from the logit distribution\n",
        "            next_token = self.Sampler(\n",
        "                logits = logits,\n",
        "                temperature = temperature,\n",
        "                top_p = top_p,\n",
        "                top_k = top_k\n",
        "            )\n",
        "\n",
        "            # add our new token to the sequence\n",
        "            tokens = torch.cat((tokens, next_token), dim=1)\n",
        "\n",
        "            # iterate the offset used in kv caching\n",
        "            if tokens.shape[1] >= max_context_window:\n",
        "                start_pos += 1\n",
        "\n",
        "        # decode our list of tokens to an actual string\n",
        "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "4gRVdIZVUqHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train and test your own minLlama (or load mine)\n",
        "<a id='three'></a>"
      ],
      "metadata": {
        "id": "NMw-8l3kUA5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3a. Setup\n",
        "<a id='threea'></a>\n",
        "a bunch of data, functions and objects you'll need that are not already included with the above architecture"
      ],
      "metadata": {
        "id": "LMKyL6zcqMSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the TinyShakespeare dataset\n",
        "!wget -O input.txt https://raw.githubusercontent.com/evintunador/minLlama3/main/input.txt\n",
        "\n",
        "# load the dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
        "print(text[:200])\n",
        "\n",
        "# here are all the unique characters that occur in this text and how many there are\n",
        "chars = sorted(list(set(text)))\n",
        "v = len(chars)\n",
        "print(chars)\n",
        "print(v)\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "5oTbAKuix5nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79897f0c-c055-4d20-bc33-ef6d86ec9e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 18:17:22--  https://raw.githubusercontent.com/evintunador/minLlama3/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: input.txt\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-04-20 18:17:22 (36.6 MB/s) - input.txt saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3b. Training your own\n",
        "<a id='threeb'></a>\n",
        "\n",
        "you can feel free to train your own if you'd like, but i don't see a huge reason to do so in a colab notebook"
      ],
      "metadata": {
        "id": "oHQNS8gdvZQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a new model\n",
        "params = ModelArgs()\n",
        "print(params)\n",
        "model = Llama3(params, tokenizer).to(params.device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pdPqc-L_VqU",
        "outputId": "3cfd56db-7681-4aba-bc30-1f1e4153f2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelArgs(dim=128, n_layers=8, n_heads=4, n_kv_heads=1, vocab_size=512, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=10000, max_batch_size=32, max_seq_len=512, device='cuda', dropout_rate=0.1)\n",
            "2033.792 K parameters\n",
            "Llama3(\n",
            "  (tok_embeddings): Embedding(512, 128)\n",
            "  (layers): ModuleList(\n",
            "    (0-7): 8 x TransformerBlock(\n",
            "      (attention): Attention(\n",
            "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
            "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
            "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
            "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
            "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
            "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
            "      )\n",
            "      (attention_norm): RMSNorm()\n",
            "      (ffn_norm): RMSNorm()\n",
            "    )\n",
            "  )\n",
            "  (norm): RMSNorm()\n",
            "  (output): Linear(in_features=128, out_features=512, bias=False)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading for training which generates a small batch of data of inputs x and targets y\n",
        "def get_batch(split, batch_size):\n",
        "    # whether we grab from our training or validation dataset\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - params.max_seq_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+params.max_seq_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+params.max_seq_len+1] for i in ix])\n",
        "    x, y = x.to(params.device), y.to(params.device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "F6M2WCXs_Vld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, batch_size, eval_iters = 5): # to estimate loss during the training loop\n",
        "    out = {}\n",
        "    model.eval() # sets model to eval mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size)\n",
        "            logits, loss = model(X, targets=Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # just resets to training mode\n",
        "    return out"
      ],
      "metadata": {
        "id": "oaiM9_Od_Vnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "lr_init = 1e-2\n",
        "weight_decay = 0.02\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_init, weight_decay=weight_decay)\n",
        "\n",
        "# how long we want to train for\n",
        "max_iters = 1000\n",
        "\n",
        "# how often we want to check & see how our loss is doing\n",
        "eval_interval = 50\n",
        "\n",
        "# Warmup setup\n",
        "warmup_iters = 10  # Number of warmup iterations\n",
        "warmup_factor = 1e-3  # Warmup factor (initial learning rate is multiplied by this factor)\n",
        "\n",
        "lr_final = 1e-5  # Minimum learning rate\n",
        "\n",
        "def lr_lambda(current_iter):\n",
        "    if current_iter < warmup_iters:\n",
        "        # Warmup phase\n",
        "        return warmup_factor + (1 - warmup_factor) * current_iter / warmup_iters\n",
        "    else:\n",
        "        # Cosine decay phase with minimum learning rate\n",
        "        decay_iters = max_iters - warmup_iters\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * (current_iter - warmup_iters) / decay_iters))\n",
        "        return max(cosine_decay, lr_final / lr_init)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
      ],
      "metadata": {
        "id": "9KhAPH5g_VsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
        "#torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train', params.max_batch_size)\n",
        "\n",
        "    # train\n",
        "    logits, loss = model(xb, targets=yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        current_time = time.time()\n",
        "        elapsed_time = current_time - start_time\n",
        "        losses = estimate_loss(model, params.max_batch_size)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Disable anomaly detection after the training loop\n",
        "#torch.autograd.set_detect_anomaly(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OECt3NLpBGKc",
        "outputId": "0837d702-ba8e-4525-bede-32de750ba992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0000: lr 0.001009, train loss 6.4597, val loss 6.4662, time elapsed: 1.52 seconds\n",
            "step 0010: lr 0.009997, train loss 4.2119, val loss 4.2875, time elapsed: 4.13 seconds\n",
            "step 0020: lr 0.009636, train loss 3.8806, val loss 3.9501, time elapsed: 6.90 seconds\n",
            "step 0030: lr 0.008716, train loss 3.6459, val loss 3.7502, time elapsed: 9.69 seconds\n",
            "step 0040: lr 0.007347, train loss 3.4446, val loss 3.6237, time elapsed: 12.48 seconds\n",
            "step 0050: lr 0.005696, train loss 3.3880, val loss 3.5284, time elapsed: 15.27 seconds\n",
            "step 0060: lr 0.003960, train loss 3.3236, val loss 3.4926, time elapsed: 18.06 seconds\n",
            "step 0070: lr 0.002350, train loss 3.2639, val loss 3.4463, time elapsed: 20.86 seconds\n",
            "step 0080: lr 0.001060, train loss 3.2208, val loss 3.4008, time elapsed: 23.66 seconds\n",
            "step 0090: lr 0.000245, train loss 3.1910, val loss 3.4081, time elapsed: 26.46 seconds\n",
            "step 0099: lr 0.000010, train loss 3.2181, val loss 3.4169, time elapsed: 29.06 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3c. Alternatively, you can load the 2m parameter model I already trained\n",
        "<a id='threec'></a>"
      ],
      "metadata": {
        "id": "m3z_2iYbvqUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading it\n",
        "!wget https://github.com/evintunador/minLlama3/raw/main/models/Llama3_2024-04-19%7C15-18-16.pth\n",
        "!wget https://github.com/evintunador/minLlama3/raw/main/models/Llama3_2024-04-19%7C15-18-16.json\n",
        "\n",
        "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
        "name = 'Llama3_2024-04-19|15-18-16'\n",
        "\n",
        "# Deserialize the JSON file back to a dictionary\n",
        "with open(f'{name}.json', 'r') as f:\n",
        "    params_dict = json.load(f)\n",
        "\n",
        "# Convert the dictionary back to a dataclass object\n",
        "params = ModelArgs(**params_dict)\n",
        "params.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize a blank model\n",
        "model = Llama3(params, tokenizer).to(params.device)\n",
        "\n",
        "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
        "path = f'{name}.pth'\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model.load_state_dict(torch.load(path))\n",
        "# REMEMBER TO CHANGE VALUES IN params TO MATCH THE MODEL YOU'VE LOADED\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
        "\n",
        "# If you only plan to do inference, switch to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "B2exYhJGvxDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7a7eb0-1cb3-4058-be86-ba05011c8a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-20 18:19:55--  https://github.com/evintunador/minLlama3/raw/main/models/Llama3_2024-04-19%7C15-18-16.pth\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/evintunador/minLlama3/main/models/Llama3_2024-04-19%7C15-18-16.pth [following]\n",
            "--2024-04-20 18:19:55--  https://raw.githubusercontent.com/evintunador/minLlama3/main/models/Llama3_2024-04-19%7C15-18-16.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9213026 (8.8M) [application/octet-stream]\n",
            "Saving to: Llama3_2024-04-19|15-18-16.pth\n",
            "\n",
            "Llama3_2024-04-19|1 100%[===================>]   8.79M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-20 18:19:56 (140 MB/s) - Llama3_2024-04-19|15-18-16.pth saved [9213026/9213026]\n",
            "\n",
            "--2024-04-20 18:19:56--  https://github.com/evintunador/minLlama3/raw/main/models/Llama3_2024-04-19%7C15-18-16.json\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/evintunador/minLlama3/main/models/Llama3_2024-04-19%7C15-18-16.json [following]\n",
            "--2024-04-20 18:19:56--  https://raw.githubusercontent.com/evintunador/minLlama3/main/models/Llama3_2024-04-19%7C15-18-16.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245 [text/plain]\n",
            "Saving to: Llama3_2024-04-19|15-18-16.json\n",
            "\n",
            "Llama3_2024-04-19|1 100%[===================>]     245  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-20 18:19:56 (7.29 MB/s) - Llama3_2024-04-19|15-18-16.json saved [245/245]\n",
            "\n",
            "2033.792 K parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Llama3(\n",
              "  (tok_embeddings): Embedding(512, 128)\n",
              "  (layers): ModuleList(\n",
              "    (0-7): 8 x TransformerBlock(\n",
              "      (attention): Attention(\n",
              "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
              "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
              "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
              "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
              "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
              "      )\n",
              "      (attention_norm): RMSNorm()\n",
              "      (ffn_norm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm): RMSNorm()\n",
              "  (output): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3d. Testing (performing inference)\n",
        "<a id='threed'></a>"
      ],
      "metadata": {
        "id": "vFa4Pfi2vx3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line"
      ],
      "metadata": {
        "id": "rK5bkaFmv1dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doing everything with default values\n",
        "print(model.generate(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOlwC4iNQO87",
        "outputId": "cec5b3f9-a7a2-4baf-9c51-249d50da7cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n",
            "\n",
            "Nurse:\n",
            "Go what thou dost say you to this hour in death.\n",
            "\n",
            "ROMEO:\n",
            "Then never now of your life of Romeo's chamber\n",
            "What can be so burden to her son, thou art\n",
            "To pluck it in the heavens of death,\n",
            "And stand our brothers of the people's trial.\n",
            "\n",
            "ROMEO:\n",
            "Then makes me hear the measure of the world:\n",
            "Not it is hope to be so burthen for them an hour,\n",
            "Which she shall stay to the crown of the war.\n",
            "\n",
            "ROMEO:\n",
            "The weeds of war, and many sorrow of his head.\n",
            "\n",
            "BENVOLIO:\n",
            "Why, what say you shall be a kind of foot?\n",
            "\n",
            "ROMEO:\n",
            "The words of this cold part thou art:\n",
            "Stay I say I mean in my life,\n",
            "And I am near to the law of his death.\n",
            "\n",
            "ROMEO:\n",
            "That is the point that kill'd to the crown,\n",
            "And all the crown of the other plain,\n",
            "Which do shame were for what I am rather.\n",
            "\n",
            "ROMEO:\n",
            "Stay, where she \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### now let's use memory_saver_div to take advantage of KV caching for linear scaling of memory usage with sequence length increase in exchange for potential quality degradation. memory_saver_div must be a power of 2, and it is used to calculate the maximum length of the query's sequence length dimension in the attention matrix"
      ],
      "metadata": {
        "id": "b1a1W5kOGe_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(\n",
        "    input_str,\n",
        "    max_gen_len = params.max_seq_len - len(input_str), # our model doesn't have a built-in <endoftext> token so we have to specify when to stop generating\n",
        "    memory_saver_div = 8, # the largest value we'll allow our query sequence length to get. makes memory consumption linear with respect to sequence length\n",
        "    temperature = 0.6, # this is the default value that Llama3's official code has set\n",
        "    top_p = 0.9, # this is the default value that Llama3's official code has set\n",
        "    top_k = 32, # meta's code doesn't actually implement top_k selection but i've added it anyways as an alternative\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "Hikwp10DQQEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3bae1f-351a-4a23-a90e-9fbbce0c62ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximum attention matrix size will be 64x512 rather than 512x512\n",
            "\n",
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n",
            "\n",
            "JULIET:\n",
            "Ay, my lord, I have a gentle man,\n",
            "The happy man that would have been the banished.\n",
            "\n",
            "JULIET:\n",
            "Ay, I will have the duke of thy father's head,\n",
            "And I will not plead to leave the purpose.\n",
            "\n",
            "JULIET:\n",
            "Ay, but thou wilt be so much a man\n",
            "To steal into the law of a word of him.\n",
            "\n",
            "JULIET:\n",
            "What stand of your cousin, and holds to her?\n",
            "\n",
            "JULIET:\n",
            "What is the day?\n",
            "\n",
            "Nurse:\n",
            "Nay, then, my lord; here's a happy man!\n",
            "\n",
            "JULIET:\n",
            "I have not heard to be since for my sin,\n",
            "Nor to my father to my part of thee,\n",
            "To make a man and my servants in my heart,\n",
            "And that I had been so much dead her lady:\n",
            "Therefore, thou art doubt a part of mine honour,\n",
            "To see this wretched blind to crown my soul,\n",
            "Shall I did remain my servant love to me.\n",
            "\n",
            "JULIET:\n",
            "Nay, thou art words, as I say, I would\n",
            "W\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VOmoqliSGiun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}