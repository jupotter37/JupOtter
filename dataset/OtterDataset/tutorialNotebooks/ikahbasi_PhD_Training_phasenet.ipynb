{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9d6c819",
      "metadata": {
        "id": "f9d6c819",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seisbench/seisbench/blob/additional_example_workflows/examples/03a_training_phasenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241827c2",
      "metadata": {
        "id": "241827c2",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "![image](https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c95f074",
      "metadata": {
        "id": "5c95f074",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*This code is necessary on colab to install SeisBench. If SeisBench is already installed on your machine, you can skip this.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a13e49f1",
      "metadata": {
        "id": "a13e49f1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# !pip install seisbench"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960e4590",
      "metadata": {
        "id": "960e4590",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "*This cell is required to circumvent an issue with colab and obspy. For details, check this issue in the obspy documentation: https://github.com/obspy/obspy/issues/2547*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a77bdb48",
      "metadata": {
        "id": "a77bdb48",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     import obspy\n",
        "#     obspy.read()\n",
        "# except TypeError:\n",
        "#     # Needs to restart the runtime once, because obspy only works properly after restart.\n",
        "#     print('Stopping RUNTIME. If you run this code for the first time, this is expected. Colaboratory will restart automatically. Please run again.')\n",
        "#     exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f032984",
      "metadata": {
        "id": "7f032984",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Training PhaseNet\n",
        "\n",
        "This tutorial shows how to train a model with SeisBench, using PhaseNet as an example. This brings together the three main components of SeisBench: data, models and generate.\n",
        "\n",
        "The tutorial is intended to highlight the basic principles of training models in SeisBench. However, this will not necessarily be best practice for more elaborate experiments. As a reference how to set up larger studies and which augmentations can be used for which models, we refer to the implementation of our pick benchmark at [https://github.com/seisbench/pick-benchmark](https://github.com/seisbench/pick-benchmark).\n",
        "\n",
        "*Note: As this tutorial brings together different parts of seisbench, it is recommended to go through the basic tutorials first before beginning this tutorial. In addition, this tutorial assumes some familiarity with pytorch*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "689c1ea9",
      "metadata": {
        "id": "689c1ea9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import seisbench.data as sbd\n",
        "import seisbench.generate as sbg\n",
        "import seisbench.models as sbm\n",
        "from seisbench.util import worker_seeding\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2963655",
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import load_config\n",
        "cfg = load_config('Kaki-cfg.yml')\n",
        "cfg.training.learning_rate = eval(cfg.training.learning_rate)\n",
        "\n",
        "print(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b9649cd",
      "metadata": {
        "id": "6b9649cd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Model and data\n",
        "\n",
        "We create a randomly initialized PhaseNet model using `seisbench.models`. If available, you can move your model onto the GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b596d33f",
      "metadata": {
        "id": "b596d33f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "model = sbm.PhaseNet(phases=\"NPS\", norm=\"peak\")\n",
        "# model.cuda();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd83fdd",
      "metadata": {
        "id": "5dd83fdd",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "As training data we use the ETHZ dataset. Note that we set the sampling rate to 100 Hz to ensure that all examples are consistent in terms of sampling rate. We split the data into training, development and test sets according to the splits provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbbf8d17",
      "metadata": {
        "id": "dbbf8d17",
        "outputId": "fa32a588-a9be-4c83-e097-98cd8518c1f9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "data = sbd.WaveformDataset(cfg.path.dataset, sampling_rate=100)\n",
        "train, dev, test = data.train_dev_test()\n",
        "print(train, dev, test, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d876fa35",
      "metadata": {
        "id": "d876fa35",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Generation pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c037b8",
      "metadata": {
        "id": "27c037b8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The ETHZ dataset contains detailed labels for the phases. However, for this example we only want to differentiate between P and S picks. Therefore, we define a dictionary mapping the detailed picks to their phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "930b8ed6",
      "metadata": {
        "id": "930b8ed6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "phase_dict = {\n",
        "    \"trace_p_arrival_sample\": \"P\",\n",
        "    \"trace_pP_arrival_sample\": \"P\",\n",
        "    \"trace_P_arrival_sample\": \"P\",\n",
        "    \"trace_P1_arrival_sample\": \"P\",\n",
        "    \"trace_Pg_arrival_sample\": \"P\",\n",
        "    \"trace_Pn_arrival_sample\": \"P\",\n",
        "    \"trace_PmP_arrival_sample\": \"P\",\n",
        "    \"trace_pwP_arrival_sample\": \"P\",\n",
        "    \"trace_pwPm_arrival_sample\": \"P\",\n",
        "    \"trace_s_arrival_sample\": \"S\",\n",
        "    \"trace_S_arrival_sample\": \"S\",\n",
        "    \"trace_S1_arrival_sample\": \"S\",\n",
        "    \"trace_Sg_arrival_sample\": \"S\",\n",
        "    \"trace_SmS_arrival_sample\": \"S\",\n",
        "    \"trace_Sn_arrival_sample\": \"S\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc7cfc9",
      "metadata": {
        "id": "8bc7cfc9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now we define two generators with identical augmentations, one for training, one for validation. The augmentations are:\n",
        "1. Selection of a (long) window around a pick. This way, we ensure that out data always contains a pick.\n",
        "1. Selection of a random window with 3001 samples, the input length of PhaseNet.\n",
        "1. A normalization, consisting of demeaning and amplitude normalization.\n",
        "1. A change of datatype to float32, as this is expected by the pytorch model.\n",
        "1. A probabilistic label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "cce52377",
      "metadata": {
        "id": "cce52377",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train_generator = sbg.GenericGenerator(train)\n",
        "dev_generator = sbg.GenericGenerator(dev)\n",
        "\n",
        "augmentations = [\n",
        "    sbg.WindowAroundSample(list(phase_dict.keys()), samples_before=3000, windowlen=6000, selection=\"random\", strategy=\"variable\"),\n",
        "    sbg.RandomWindow(windowlen=3001, strategy=\"pad\"),\n",
        "    sbg.Normalize(demean_axis=-1, amp_norm_axis=-1, amp_norm_type=\"peak\"),\n",
        "    sbg.ChangeDtype(np.float32),\n",
        "    sbg.ProbabilisticLabeller(label_columns=phase_dict, sigma=30, dim=0)\n",
        "]\n",
        "\n",
        "train_generator.add_augmentations(augmentations)\n",
        "dev_generator.add_augmentations(augmentations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454e75a3",
      "metadata": {
        "id": "454e75a3",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's visualize a few training examples. Everytime you run the cell below, you'll see a different training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7ef0570",
      "metadata": {
        "id": "d7ef0570",
        "outputId": "83997bbd-7cbf-4b2c-a687-dcb909472004",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "sample = train_generator[np.random.randint(len(train_generator))]\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "axs = fig.subplots(2, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1]})\n",
        "axs[0].plot(sample[\"X\"].T)\n",
        "axs[1].plot(sample[\"y\"].T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005455e0",
      "metadata": {
        "id": "005455e0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "SeisBench generators are pytorch datasets. Therefore, we can pass them to pytorch data loaders. These will automatically take care of parallel loading and batching. Here we create one loader for training and one for validation. We choose a batch size of 256 samples. This batch size should fit on most hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "2bf2e42c",
      "metadata": {
        "id": "2bf2e42c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_generator,\n",
        "    batch_size=cfg.training.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.training.num_workers,\n",
        "    worker_init_fn=worker_seeding\n",
        ")\n",
        "dev_loader = DataLoader(\n",
        "    dev_generator,\n",
        "    batch_size=cfg.training.batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.training.num_workers,\n",
        "    worker_init_fn=worker_seeding\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9b009a",
      "metadata": {
        "id": "0e9b009a",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9024bf3c",
      "metadata": {
        "id": "9024bf3c",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now we got all components for training the model. What we still need to do is define the optimizer and the loss, and write the training and validation loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "9715e24b",
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.training.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5ef5581f",
      "metadata": {
        "id": "5ef5581f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def loss_fn(y_pred, y_true, eps=1e-5):\n",
        "    # vector cross entropy loss\n",
        "    h = y_true * torch.log(y_pred + eps)\n",
        "    h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
        "    h = h.mean()  # Mean over batch axis\n",
        "    return -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "b4035919",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop(dataloader):\n",
        "    lst_loss = []\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch_id, batch in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(batch[\"X\"].to(model.device))\n",
        "        loss = loss_fn(pred, batch[\"y\"].to(model.device))\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #\n",
        "        if batch_id % 5 == 0:\n",
        "            loss, current = loss.item(), batch_id * batch[\"X\"].shape[0]\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            lst_loss.append((batch_id//5, loss))\n",
        "    return lst_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4b9e5e96",
      "metadata": {
        "id": "4b9e5e96",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader):\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    model.eval()  # close the model for evaluation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            pred = model(batch[\"X\"].to(model.device))\n",
        "            test_loss += loss_fn(pred, batch[\"y\"].to(model.device)).item()\n",
        "\n",
        "    model.train()  # re-open model for training stage\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "975a1cd8",
      "metadata": {
        "id": "975a1cd8",
        "outputId": "4a236c8a-cb9a-4452-c4c2-753c0af0faaa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "train_loss_lst = []\n",
        "test_loss_lst = []\n",
        "for t in range(cfg.training.epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss = train_loop(train_loader)\n",
        "    test_loss = test_loop(dev_loader)\n",
        "    #\n",
        "    train_loss_lst.append(train_loss)\n",
        "    test_loss_lst.append(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "108d3928",
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_train = []\n",
        "#\n",
        "loss_test = test_loss_lst\n",
        "idx_test = []\n",
        "for epoch in train_loss_lst:\n",
        "    for batch in epoch:\n",
        "        loss_train.append(batch[1])\n",
        "    idx_test.append(batch[0])\n",
        "idx_test = [(idx+1)*val for idx, val in enumerate(idx_test)]\n",
        "idx_train = list(range(0, len(loss_train)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e8297e",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(idx_train, loss_train)\n",
        "plt.plot(idx_test, loss_test, '-o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608de96d",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(idx_train, loss_train)\n",
        "plt.plot(idx_test, loss_test, '-o')\n",
        "plt.yscale(\"log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff9c02a",
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = idx_test[0]\n",
        "\n",
        "plt.plot(idx_train[idx:], loss_train[idx:])\n",
        "plt.plot(idx_test, loss_test, '-o')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfdea65",
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = idx_test[0]\n",
        "\n",
        "plt.plot(idx_train[idx:], loss_train[idx:])\n",
        "plt.plot(idx_test, loss_test, '-o')\n",
        "plt.yscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15bf99c",
      "metadata": {},
      "source": [
        "## Saving Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "24ff561a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(os.path.abspath(cfg.path.dl_model), exist_ok=True)\n",
        "\n",
        "model.save(\n",
        "    path=cfg.path.dl_model,\n",
        "    weights_docstring=cfg.training.weights_docstring,\n",
        "    version_str=cfg.training.version_str\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd0ef68",
      "metadata": {
        "id": "bdd0ef68",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Remarks\n",
        "\n",
        "As discussed in the data basics tutorial, loading a SeisBench dataset only means loading the metadata into memory. The waveforms are only loaded once they are requested to save memory. By default, waveforms are **not** cached in memory. For training, this means that the data needs to be read from the file in every epoch again. Depending on your hardware, this will take a lot of time. To solve this issue, you can set the `cache` option, when creating the dataset. Then, all you have to do is call `preload_waveforms` and the data will be loaded into memory and automatically cached. For most practical applications, this option is recommended."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
