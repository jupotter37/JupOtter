{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3e6182-5a21-49dc-ad4d-04c5f1c10b53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src ='https://github.com/datasolut/databricks-tutorials/blob/main/images/datasolut_logo_quer.png?raw=true' alt=\"html image\" text-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3aeeb52-5979-4817-8641-e94bd40b9107",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# PySpark Tutorial für Einsteiger\n",
    "\n",
    "- [Gute Einleitungsartikel für Pyspark](https://www.guru99.com/de/pyspark-tutorial.html)\n",
    "- [Tutorial Notebook von Databricks für Apache Spark](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/2201444230243598/3601578643761083/latest.html)\n",
    "\n",
    "## Was ist Big Data?\n",
    "- **Speicherung und Verarbeitung von große Datenmengen**: Big Data bezeichnet extrem große Datensätze, die mit traditionellen Datenverarbeitungsmethoden schwer zu handhaben sind. Diese Datenmengen werden oft in Terabytes oder Petabytes gemessen.\n",
    "- **Herausforderungen**: Die Speicherung, Verwaltung und Verarbeitung riesiger Datenmengen erfordert spezialisierte Technologien und Infrastrukturen.\n",
    "\n",
    "## Was ist Apache Spark?\n",
    "\n",
    "- Apache Spark ist ein schnelles und universelles Cluster-Computing-System für die verteilte Datenverarbeitung großer Datenmengen.\n",
    "- Es bietet vielseitige API-Unterstützung für Java, Scala, Python und R und enthält integrierte Komponenten für SQL-Abfragen, maschinelles Lernen, Stream-Verarbeitung und Graphverarbeitung.\n",
    "\n",
    "Für mehr Informationen schau dir folgendes Youtube-Video von uns an:\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=q84z_ZQHJhs\"><img src ='https://github.com/datasolut/databricks-tutorials/blob/main/images/thumbnail_youtube_apache_spark.jpg?raw=true' alt=\"Youtube Thumbnail Apache Spark datasolut\" text-align=\"center\" width=\"500px\"> </a>\n",
    "\n",
    "## Was ist PySpark?\n",
    "- PySpark ist die Python-API für Apache Spark, die es ermöglicht, die leistungsstarken Funktionen von Spark zur verteilten Datenverarbeitung direkt in Python zu nutzen. \n",
    "- Syntax von PySpark ähnelt dabei stark der von SQL, was die Arbeit mit DataFrames und die Durchführung von Abfragen und Datenmanipulationen besonders intuitiv und benutzerfreundlich macht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3aebf21-ea16-42bc-a6a0-75c0433af1d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Cluster\n",
    "Zunächst muss ein Cluster erstellt werden, auf dem die Datenmanipulationen ausgeführt werden können.\n",
    "\n",
    "<img src ='https://github.com/datasolut/databricks-tutorials/blob/main/images/cluster_overview.png?raw=true' alt=\"Archtektur von Apache Spark\" text-align=\"center\" width=\"500px\">\n",
    "\n",
    "## SparkSession\n",
    "Die Klasse **`SparkSession`** ist der einzige Zugangspunkt zu allen Funktionen in Spark, die die DataFrame-API verwenden.\n",
    "\n",
    "In Databricks-Notebooks wird die SparkSession automatisch erstellt und in der Variable **`spark`** gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3e63e8-c9c4-4c63-9934-338daf872398",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3859458957466229#setting/sparkui/0626-110159-a5cn51dk/driver-7387858858185958091\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f94403440d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca34c054-812d-468a-a7df-e45f01890ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrames\n",
    "\n",
    "Spark Dataframes sind eine verteilte Sammlung von Datenpunkten, welche die Daten in benannten Spalten organisiert \n",
    "\n",
    "- DataFrames wurde erstmals in Spark Version 1.3 eingeführt, um die Einschränkungen des Spark RDD zu überwinden. \n",
    "- Sie ermöglichen es Entwicklern, den Code während der Laufzeit zu debuggen, was mit den RDDs nicht möglich war.\n",
    "- Dataframes können Daten in Formaten wie CSV, JSON, AVRO, HDFS und HIVE-Tabellen lesen und schreiben.\n",
    "- Sie sind bereits für die Verarbeitung großer Datensätze für die meisten Vorverarbeitungsaufgaben optimiert, sodass keine komplexen Funktionen eigenständig geschrieben werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312e4e79-4619-46ed-b97e-d5171b953e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Datum: string (nullable = true)\n",
      " |-- Kunde: string (nullable = true)\n",
      " |-- Produkt: string (nullable = true)\n",
      " |-- Menge: long (nullable = true)\n",
      " |-- Preis: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines DataFrames aus einer Liste\n",
    "data = [\n",
    "    (\"2023-01-01\", \"KundeA\", \"Produkt1\", 2, 10.0),\n",
    "    (\"2023-01-01\", \"KundeB\", \"Produkt2\", 1, 15.0),\n",
    "    (\"2023-01-02\", \"KundeA\", \"Produkt1\", 1, 10.0),\n",
    "    (\"2023-01-02\", \"KundeC\", \"Produkt3\", 3, 20.0)\n",
    "]\n",
    "\n",
    "columns = [\"Datum\", \"Kunde\", \"Produkt\", \"Menge\", \"Preis\"]\n",
    "\n",
    "# Erstellen des DataFrames\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Anzeigen des DataFrames\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6ed257-3710-417b-93bd-8e50e4e22d7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Daten anschauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76962cb3-4465-4888-b23d-ecde07de9036",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-----+-----+\n",
      "|     Datum| Kunde| Produkt|Menge|Preis|\n",
      "+----------+------+--------+-----+-----+\n",
      "|2023-01-01|KundeA|Produkt1|    2| 10.0|\n",
      "|2023-01-01|KundeB|Produkt2|    1| 15.0|\n",
      "|2023-01-02|KundeA|Produkt1|    1| 10.0|\n",
      "|2023-01-02|KundeC|Produkt3|    3| 20.0|\n",
      "+----------+------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e5dbff1-47f7-497c-b8a6-18c3e3572c4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Datum='2023-01-01', Kunde='KundeA', Produkt='Produkt1', Menge=2, Preis=10.0),\n",
       " Row(Datum='2023-01-01', Kunde='KundeB', Produkt='Produkt2', Menge=1, Preis=15.0),\n",
       " Row(Datum='2023-01-02', Kunde='KundeA', Produkt='Produkt1', Menge=1, Preis=10.0),\n",
       " Row(Datum='2023-01-02', Kunde='KundeC', Produkt='Produkt3', Menge=3, Preis=20.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da7298a-0bef-4bed-9ab5-6824f0648470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(Datum='2023-01-01', Kunde='KundeA', Produkt='Produkt1', Menge=2, Preis=10.0)],\n",
       " [Row(Datum='2023-01-02', Kunde='KundeC', Produkt='Produkt3', Menge=3, Preis=20.0)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1), df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b0fe412-9452-48ab-9fea-ec0b5d5146e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Auswählen und Zugreifen auf Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f07b95-1ce4-4e89-8b7d-63d8ad8531ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66889c2c-a425-46fb-a9b4-1c7ec444b55e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Kunde|\n",
      "+------+\n",
      "|KundeA|\n",
      "|KundeB|\n",
      "|KundeA|\n",
      "|KundeC|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "| Kunde|\n",
      "+------+\n",
      "|KundeA|\n",
      "|KundeB|\n",
      "|KundeA|\n",
      "|KundeC|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "| Kunde|\n",
      "+------+\n",
      "|KundeA|\n",
      "|KundeB|\n",
      "|KundeA|\n",
      "|KundeC|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verschiedene Schreibweisen\n",
    "df.select(df.Kunde).show()\n",
    "\n",
    "df.select(col(\"Kunde\")).show()\n",
    "\n",
    "df.select(\"Kunde\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0aeaaa8-b6eb-459f-a4cf-0d0fbb1b11ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-----+-----+\n",
      "|     Datum| Kunde| Produkt|Menge|Preis|\n",
      "+----------+------+--------+-----+-----+\n",
      "|2023-01-01|KundeA|Produkt1|    2| 10.0|\n",
      "|2023-01-02|KundeA|Produkt1|    1| 10.0|\n",
      "+----------+------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"Kunde\") == \"KundeA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f282015-33a7-4176-a9e9-39b397be4654",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Gruppieren von Daten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdd0137-883c-4df5-a6cf-69d2b9654869",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| Kunde|sum(Preis)|\n",
      "+------+----------+\n",
      "|KundeA|      20.0|\n",
      "|KundeB|      15.0|\n",
      "|KundeC|      20.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"Kunde\").sum(\"Preis\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d828b536-f4cd-4b93-8457-3ba6cbde7ee3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Daten rausschreiben und einladen\n",
    "\n",
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b6a0f2-ab6c-4e25-a3ed-19e7d090e845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dbutils.fs.rm(\"dbfs:/FileStore/demos/pypsark-demos/sales.csv\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/demos/pypsark-demos/sales.delta\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea9d04b-d058-4fbb-9fc5-24310631426d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-----+-----+\n",
      "|     Datum| Kunde| Produkt|Menge|Preis|\n",
      "+----------+------+--------+-----+-----+\n",
      "|2023-01-01|KundeA|Produkt1|    2| 10.0|\n",
      "|2023-01-01|KundeB|Produkt2|    1| 15.0|\n",
      "|2023-01-02|KundeA|Produkt1|    1| 10.0|\n",
      "|2023-01-02|KundeC|Produkt3|    3| 20.0|\n",
      "+----------+------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"dbfs:/FileStore/demos/pypsark-demos/\"\n",
    "df.write.csv(path+'sales.csv', header=True)\n",
    "spark.read.csv(path+'sales.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb6478e8-5866-4f72-ad18-bc4bcfd09b19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4acc0473-5fef-49f6-ae22-4c89b3948ab7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-----+-----+\n",
      "|     Datum| Kunde| Produkt|Menge|Preis|\n",
      "+----------+------+--------+-----+-----+\n",
      "|2023-01-02|KundeC|Produkt3|    3| 20.0|\n",
      "|2023-01-01|KundeA|Produkt1|    2| 10.0|\n",
      "|2023-01-01|KundeB|Produkt2|    1| 15.0|\n",
      "|2023-01-02|KundeA|Produkt1|    1| 10.0|\n",
      "+----------+------+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").option(\"overwrite\", \"true\").save(path+'sales.delta')\n",
    "spark.read.format(\"delta\").load(path+'sales.delta').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71ec540-56d2-4a4b-8ffd-7658dc4883d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Arbeiten mit SQL-Befehlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a6d579-2139-485d-aa83-d6fa6df58848",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed2f96a3-8921-4fe3-9809-b3becfbd8ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pandas User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ad16ad-9693-47d5-9f0e-352609107f07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d1c386-2d5d-41c8-980e-c01b6259dede",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------+\n",
      "| Kunde|Preis|DoppelterPreis|\n",
      "+------+-----+--------------+\n",
      "|KundeA| 10.0|          20.0|\n",
      "|KundeB| 15.0|          30.0|\n",
      "|KundeA| 10.0|          20.0|\n",
      "|KundeC| 20.0|          40.0|\n",
      "+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"float\")\n",
    "def double_price(s: pd.Series) -> pd.Series:\n",
    "    return s * 2\n",
    "\n",
    "spark.udf.register(\"double_price_udf\", double_price)\n",
    "spark.sql(\"SELECT Kunde , Preis, double_price_udf(Preis) as DoppelterPreis FROM tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e327d12a-4ddb-45d8-ade2-7b8afdb8ca45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_beginner_tutorial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
