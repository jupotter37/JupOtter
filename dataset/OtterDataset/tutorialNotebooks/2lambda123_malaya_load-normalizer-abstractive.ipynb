{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Normalizer HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "This tutorial is available as an IPython notebook at [Malaya/example/normalizer-abstractive](https://github.com/huseinzol05/Malaya/tree/master/example/normalizer-abstractive).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Results generated using stochastic methods.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 s, sys: 3.56 s, total: 7.54 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import malaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available HuggingFace models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size (MB)</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>SacreBLEU Verbose</th>\n",
       "      <th>Suggested length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mesolitica/finetune-noisy-translation-t5-tiny-bahasa-cased-v2</th>\n",
       "      <td>139</td>\n",
       "      <td>60.000967</td>\n",
       "      <td>77.9/63.9/54.6/47.7 (BP = 1.000 ratio = 1.036 ...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mesolitica/finetune-noisy-translation-t5-small-bahasa-cased-v4</th>\n",
       "      <td>242</td>\n",
       "      <td>64.062582</td>\n",
       "      <td>80.1/67.7/59.1/52.5 (BP = 1.000 ratio = 1.042 ...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mesolitica/finetune-noisy-translation-t5-base-bahasa-cased-v2</th>\n",
       "      <td>892</td>\n",
       "      <td>64.583819</td>\n",
       "      <td>80.2/68.1/59.8/53.2 (BP = 1.000 ratio = 1.048 ...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Size (MB)       BLEU  \\\n",
       "mesolitica/finetune-noisy-translation-t5-tiny-b...       139  60.000967   \n",
       "mesolitica/finetune-noisy-translation-t5-small-...       242  64.062582   \n",
       "mesolitica/finetune-noisy-translation-t5-base-b...       892  64.583819   \n",
       "\n",
       "                                                                                    SacreBLEU Verbose  \\\n",
       "mesolitica/finetune-noisy-translation-t5-tiny-b...  77.9/63.9/54.6/47.7 (BP = 1.000 ratio = 1.036 ...   \n",
       "mesolitica/finetune-noisy-translation-t5-small-...  80.1/67.7/59.1/52.5 (BP = 1.000 ratio = 1.042 ...   \n",
       "mesolitica/finetune-noisy-translation-t5-base-b...  80.2/68.1/59.8/53.2 (BP = 1.000 ratio = 1.048 ...   \n",
       "\n",
       "                                                   Suggested length  \n",
       "mesolitica/finetune-noisy-translation-t5-tiny-b...              256  \n",
       "mesolitica/finetune-noisy-translation-t5-small-...              256  \n",
       "mesolitica/finetune-noisy-translation-t5-base-b...              256  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.normalizer.abstractive.available_huggingface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HuggingFace model\n",
    "\n",
    "```python\n",
    "def huggingface(\n",
    "    model: str = 'mesolitica/finetune-noisy-translation-t5-small-bahasa-cased-v4',\n",
    "    force_check: bool = True,\n",
    "    use_rules_normalizer: bool = True,\n",
    "    kenlm_model: str = 'bahasa-wiki-news',\n",
    "    stem_model: str = 'noisy',\n",
    "    segmenter: Callable = None,\n",
    "    text_scorer: Callable = None,\n",
    "    replace_augmentation: bool = True,\n",
    "    minlen_speller: int = 2,\n",
    "    maxlen_speller: int = 12,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load HuggingFace model to abstractive text normalizer.\n",
    "    text -> rules based text normalizer -> abstractive.\n",
    "    To skip rules based text normalizer, set `use_rules_normalizer=False`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: str, optional (default='mesolitica/finetune-noisy-translation-t5-small-bahasa-cased-v4')\n",
    "        Check available models at `malaya.normalizer.abstractive.available_huggingface()`.\n",
    "    force_check: bool, optional (default=True)\n",
    "        Force check model one of malaya model.\n",
    "        Set to False if you have your own huggingface model.\n",
    "    use_rules_normalizer: bool, optional(default=True)\n",
    "    kenlm_model: str, optional (default='bahasa-wiki-news')\n",
    "        the model trained on `malaya.language_model.kenlm(model = 'bahasa-wiki-news')`,\n",
    "        but you can use any kenlm model from `malaya.language_model.available_kenlm`.\n",
    "        Also you can pass as None to skip spelling correction but still apply rules normalizer.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "    stem_model: str, optional (default='noisy')\n",
    "        the model trained on `malaya.stem.deep_model(model = 'noisy'),\n",
    "        but you can use any stemmer model from `malaya.stem.available_model`.\n",
    "        Also you can pass as None to skip stemming but still apply rules normalizer.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "    segmenter: Callable, optional (default=None)\n",
    "        segmenter function to segment text, read more at https://malaya.readthedocs.io/en/stable/load-normalizer.html#Use-segmenter\n",
    "        during training session, we use `malaya.segmentation.huggingface()`.\n",
    "        It is save to set as None.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "    text_scorer: Callable, optional (default=None)\n",
    "        text scorer to validate upper case.\n",
    "        during training session, we use `malaya.language_model.kenlm(model = 'bahasa-wiki-news')`.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "    replace_augmentation: bool, optional (default=True)\n",
    "        Use replace norvig augmentation method. Enabling this might generate bigger candidates, hence slower.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "    minlen_speller: int, optional (default=2)\n",
    "        minimum length of word to check spelling correction.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "    maxlen_speller: int, optional (default=12)\n",
    "        max length of word to check spelling correction.\n",
    "        This parameter will be ignored if `use_rules_normalizer=False`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: malaya.torch_model.huggingface.Normalizer\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 02:15:54.625947: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-23 02:15:54.634908: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-23 02:15:54.634927: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: husein-MS-7D31\n",
      "2023-02-23 02:15:54.634930: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: husein-MS-7D31\n",
      "2023-02-23 02:15:54.634984: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.161.3\n",
      "2023-02-23 02:15:54.634997: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.161.3\n",
      "2023-02-23 02:15:54.634999: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.161.3\n"
     ]
    }
   ],
   "source": [
    "model_default = malaya.normalizer.abstractive.huggingface(model = 'mesolitica/finetune-noisy-translation-t5-small-bahasa-cased-v3',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = malaya.normalizer.abstractive.huggingface(model = 'mesolitica/finetune-noisy-translation-t5-small-bahasa-cased-v3',\n",
    "                                                  use_rules_normalizer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Saya tidak suka awak']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_default.generate(['ak tk suka hg'], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Saya tidak suka awak']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(['ak tk suka hg'], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dia berada di lorong dalam tetapi mahu masuk ke kanan']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_default.generate(['Dia kat lane dalam tapi nk masuk kanan'], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dia berada di lorong dalam tetapi mahu masuk ke kanan']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(['Dia kat lane dalam tapi nk masuk kanan'], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@mesyceres Haha syok nak keluarkan semuanya. Selamat hari raya aidilfitri.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_default.generate(['@mesyceres Haha ayookk keluarkan semuanya. Bentar lagi Idul Fitri .'], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@mesyceres Haha jom keluarkan semuanya. Selamat Hari Raya Aidilfitri.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(['@mesyceres Haha ayookk keluarkan semuanya. Bentar lagi Idul Fitri .'], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hai kawan-kawan! Saya perasan semalam & hari ini ramai yang dapat biskut ni kan? Jadi hari ini saya ingin berkongsi beberapa post mortem kumpulan pertama kami:']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_default.generate(['Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:'],\n",
    "              max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hai kawan-kawan! Saya perasan semalam & hari ini ramai yang dapat biskut ni kan? Jadi hari ini saya ingin berkongsi beberapa post mortem kumpulan pertama kami:']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(['Hi guys! I noticed semalam & harini dah ramai yang dapat cookies ni kan. So harini i nak share some post mortem of our first batch:'],\n",
    "              max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tak guna budak zaman sekarang ni, nak cuci baju pun kena belajar kat toktok.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Punyalah tak guna bebudak zaman sekarang ni sampaikan gosok baju pun kena belajar kat tiktok.'\n",
    "model_default.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tak guna budak zaman sekarang ni, nak gosok baju pun kena belajar kat tiktok.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seronok juga bila adik beradik dah kahwin / kahwin']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Lonely jugak ye when your siblings are married/ getting married'\n",
    "model_default.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seronok juga bila adik beradik dah kahwin/ kahwin']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rasa janggal bila tengok kerajaan cepat buat kerja semasa bencana. Dengan mesin mesin yang ada, anda sebenarnya boleh. Ini adalah permulaan yang baik.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Rasa awkward bila tengok kerajaan laju buat kerja time bencana. With the machineries yang ada, sebenarnya boleh je. This is a good start.'\n",
    "model_default.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rasa janggal bila tengok kerajaan cepat buat kerja semasa bencana. Dengan mesin yang ada, anda sebenarnya boleh. Ini adalah permulaan yang baik.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kalau boleh elakkan ada perkataan macam ni dalam resume. Bukan sebab apa, sebelum kita pergi fasa temuduga, dekat resume kita sendiri dah bagi impression yang baik menunjukkan siapa kita sebenarnya.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'kalau boleh elakkan ada perkataan macam ni dalam resume. Bukan sebab apa, sebelum kita ke fasa interview, dekat resume tu sendiri kita dah kene bagi good impression menunjukkan siapa diri kita yang sebenarnya'\n",
    "model_default.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kalau boleh elakkan ada perkataan macam ni dalam resume. Bukan sebab apa, sebelum kita pergi fasa temuduga, dekat resume kita sendiri dah bagi impression yang baik menunjukkan siapa kita sebenarnya.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sudah-Sudah, cik... berehatlah dari politik agar tidak terus dibenci oleh orang dengan kenyataan yang pelik dan mengelirukan...']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Udah2 lah ayoh cik...berehatlah dari politik agar tidak berterusan dibenci orang dgn kenyataan yg pelik dan mengelirukan...'\n",
    "model_default.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ayuh cik... berehatlah dari politik agar tidak terus dibenci oleh orang dengan kenyataan yang pelik dan mengelirukan...']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate([s], max_length = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good thing about HuggingFace\n",
    "\n",
    "In `generate` method, you can do greedy, beam, sampling, nucleus decoder and so much more, read it at https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dah ada, puan... berehatlah dari politik supaya tidak terus membenci orang dengan kenyataan yang pelik dan mengelirukan...',\n",
       " 'Ayuh cik... berehatlah dari politik agar tidak terus dibenci orang dengan kenyataan pelik dan mengelirukan...',\n",
       " 'Dah macam tu cik... rehatlah dari politik agar tidak terus dibenci oleh orang dengan kenyataan yang pelik dan mengelirukan...']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Udah2 lah ayoh cik...berehatlah dari politik agar tidak berterusan dibenci orang dgn kenyataan yg pelik dan mengelirukan...'\n",
    "model.generate([s], max_length = 256, do_sample=True, top_k=100, top_p=0.95, temperature=0.7,\n",
    "                         num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Itu sahaja, puan... berehatlah dari politik agar tidak terus dibenci oleh orang dengan kenyataan pelik dan mengelirukan...',\n",
       " 'Ayuh cik... rehatlah dari politik agar tidak terus dibenci oleh rakyat dengan kenyataan yang pelik dan mengelirukan...',\n",
       " 'Ayuh cik... berehatlah dari politik agar tidak terus dibenci oleh orang dengan kenyataan yang pelik dan mengelirukan...']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate([s], max_length = 256, do_sample=True, penalty_alpha=0.6, top_k=4, temperature = 0.7,\n",
    "                         num_return_sequences=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
