{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfOXmvzADfpr"
   },
   "source": [
    "# Assignment 3: Image Colourization [50 Pt]\n",
    "\n",
    "In this assignment, you will build models to perform image colourization. That is, given a greyscale image, we wish to predict the colour at each pixel. Image colourization is a difficult problem for many reasons, one of which being that it is ill-posed: for a single greyscale image, there can be multiple, equally valid colourings.\n",
    "\n",
    "To keep the training time manageable we will use the CIFAR-10 data set, which consists of images of size 32x32 pixels. For most of the questions we will use a subset of the dataset. The data loading script is included with the notebooks, and should download automatically the first time it is loaded.\n",
    "\n",
    "We will be starting with a convolutional autoencoder and tweaking it along the way to improve our perforamnce. Then as a second part of the assignment we will compare the autoencoder approach to conditional generative adversarial networks (cGANs).\n",
    "\n",
    "In the process, you are expected to learn to:\n",
    "\n",
    "1. Clean and process the dataset and create greyscale images.\n",
    "2. Implement and modify an autoencoder architecture.\n",
    "3. Tune the hyperparameters of an autoencoder.\n",
    "4. Implement skip connections and other techniques to improve performance.\n",
    "5. Implement a cGAN and compare with an autoencoder.\n",
    "6. Improve on the cGAN by trying one of several techniques to enhance training.\n",
    "\n",
    "*This assignment is based on an assignment developed by Prof. Lisa Zhang and Prof. Jimmy Ba.*\n",
    "\n",
    "### What to submit\n",
    "\n",
    "Submit an HTML file containing all your code, outputs, and write-up\n",
    "from parts A and B. Please take extra effort to make your answers and submissions readable. Do not display unnecessary outputs, only the ones that are important for supporting your answers.\n",
    "\n",
    "You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
    "\n",
    "**Do not submit any other files produced by your code.**\n",
    "\n",
    "Include a link to your colab file in your submission.\n",
    "\n",
    "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbnrp2ig1pps"
   },
   "source": [
    "## Colab Link\n",
    "\n",
    "Include a link to your Colab file here. If you would like the TA to look at your\n",
    "Colab file in case your solutions are cut off, **please make sure that your Colab\n",
    "file is publicly accessible at the time of submission**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qgfuc_PO0_nO"
   },
   "outputs": [],
   "source": [
    "# # TO BE COMPLETED\n",
    "\n",
    "# http://"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFMdtipUPNdu"
   },
   "source": [
    "# PART A - Autoencoder\n",
    "In this part we will construct and compare different autoencoder models for the image colourization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BIpGwANoQOg"
   },
   "source": [
    "#### Helper code\n",
    "\n",
    "Provided are some helper functions for loading and preparing the data. Note that you will need to use the Colab GPU for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTF1TQObE6DG"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Colourization of CIFAR-10 Horses via classification.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import scipy.misc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wE_dtMgIh0SJ"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Setup working directory\n",
    "######################################################################\n",
    "%mkdir -p /content/a3/\n",
    "%cd /content/a3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piDmAsqFG0gU"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Helper functions for loading data\n",
    "######################################################################\n",
    "# adapted from\n",
    "# https://github.com/fchollet/keras/blob/master/keras/datasets/cifar10.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def get_file(fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"):\n",
    "    datadir = os.path.join(cache_dir)\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    if untar:\n",
    "        untar_fpath = os.path.join(datadir, fname)\n",
    "        fpath = untar_fpath + \".tar.gz\"\n",
    "    else:\n",
    "        fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    print(\"File path: %s\" % fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(\"Downloading data from\", origin)\n",
    "\n",
    "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    if untar:\n",
    "        if not os.path.exists(untar_fpath):\n",
    "            print(\"Extracting file.\")\n",
    "            with tarfile.open(fpath) as archive:\n",
    "                archive.extractall(datadir)\n",
    "        return untar_fpath\n",
    "\n",
    "    if extract:\n",
    "        _extract_archive(fpath, datadir, archive_format)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def load_batch(fpath, label_key=\"labels\"):\n",
    "    \"\"\"Internal utility for parsing CIFAR data.\n",
    "    # Arguments\n",
    "        fpath: path the file to parse.\n",
    "        label_key: key for label data in the retrieve\n",
    "            dictionary.\n",
    "    # Returns\n",
    "        A tuple `(data, labels)`.\n",
    "    \"\"\"\n",
    "    f = open(fpath, \"rb\")\n",
    "    if sys.version_info < (3,):\n",
    "        d = pickle.load(f)\n",
    "    else:\n",
    "        d = pickle.load(f, encoding=\"bytes\")\n",
    "        # decode utf8\n",
    "        d_decoded = {}\n",
    "        for k, v in d.items():\n",
    "            d_decoded[k.decode(\"utf8\")] = v\n",
    "        d = d_decoded\n",
    "    f.close()\n",
    "    data = d[\"data\"]\n",
    "    labels = d[label_key]\n",
    "\n",
    "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def load_cifar10(transpose=False):\n",
    "    \"\"\"Loads CIFAR10 dataset.\n",
    "    # Returns\n",
    "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    dirname = \"cifar-10-batches-py\"\n",
    "    origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    path = get_file(dirname, origin=origin, untar=True)\n",
    "\n",
    "    num_train_samples = 50000\n",
    "\n",
    "    x_train = np.zeros((num_train_samples, 3, 32, 32), dtype=\"uint8\")\n",
    "    y_train = np.zeros((num_train_samples,), dtype=\"uint8\")\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        fpath = os.path.join(path, \"data_batch_\" + str(i))\n",
    "        data, labels = load_batch(fpath)\n",
    "        x_train[(i - 1) * 10000 : i * 10000, :, :, :] = data\n",
    "        y_train[(i - 1) * 10000 : i * 10000] = labels\n",
    "\n",
    "    fpath = os.path.join(path, \"test_batch\")\n",
    "    x_test, y_test = load_batch(fpath)\n",
    "\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "    if transpose:\n",
    "        x_train = x_train.transpose(0, 2, 3, 1)\n",
    "        x_test = x_test.transpose(0, 2, 3, 1)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7fti3cryStt"
   },
   "outputs": [],
   "source": [
    "# Download CIFAR dataset\n",
    "m = load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2TWonhyn0FK"
   },
   "source": [
    "## Part 1 Data Preparation [4 pt]\n",
    "\n",
    "To start off run the above code to load the CIFAR dataset and then work through the following questions/tasks.\n",
    "\n",
    "### Part (i) [2pt EXPLORATORY] - load dataset\n",
    "Verify that the dataset has loaded correctly. How many samples do we have? How is the data organized?\n",
    "\n",
    "\n",
    "The CIFAR-10 dataset, which is a collection of 60,000 32x32 color images in 10 different classes. The load_cifar10 function returns a tuple containing two tuples:\n",
    "\n",
    "> The first tuple contains the training data:\n",
    "> * `x_train`: a NumPy array of shape (50000, 3, 32, 32) containing the training images. Each image is represented as a 3x32x32 array, where the 3 corresponds to the RGB channels, and the 32x32 corresponds to the height and width of the image.\n",
    "> * `y_train`: a NumPy array of shape (50000, 1) containing the labels for the training images. Each label is an integer from 0 to 9, corresponding to one of the 10 classes\n",
    "\n",
    "> The second tuple contains the test data:\n",
    "> * `x_test`: a NumPy array of shape (10000, 3, 32, 32) containing the test images.\n",
    "> * `y_test`: a NumPy array of shape (10000, 1) containing the labels for the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6bV7uvsh9Sj"
   },
   "outputs": [],
   "source": [
    "type(m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[1][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDqVs87xpA40"
   },
   "source": [
    "### Part (ii) [2pt EXPLORATORY] - select only horses\n",
    "\n",
    "Provided below is sample code to preproces the data to select only images of horses, which will help to simplify the goals of the assignment. The function also converts the colour images to greyscale to create our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-900ROTMlSPd"
   },
   "outputs": [],
   "source": [
    "# select a single category.\n",
    "HORSE_CATEGORY = 7\n",
    "\n",
    "# convert colour images into greyscale\n",
    "def process(xs, ys, max_pixel=256.0, downsize_input=False):\n",
    "    \"\"\"\n",
    "    Pre-process CIFAR10 images by taking only the horse category,\n",
    "    shuffling, and have colour values be bound between 0 and 1\n",
    "\n",
    "    Args:\n",
    "      xs: the colour RGB pixel values\n",
    "      ys: the category labels\n",
    "      max_pixel: maximum pixel value in the original data\n",
    "    Returns:\n",
    "      xs: value normalized and shuffled colour images\n",
    "      grey: greyscale images, also normalized so values are between 0 and 1\n",
    "    \"\"\"\n",
    "    xs = xs / max_pixel\n",
    "    xs = xs[np.where(ys == HORSE_CATEGORY)[0], :, :, :]\n",
    "    npr.shuffle(xs)\n",
    "\n",
    "    grey = np.mean(xs, axis=1, keepdims=True)\n",
    "\n",
    "    if downsize_input:\n",
    "        downsize_module = nn.Sequential(\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "        )\n",
    "        xs_downsized = downsize_module.forward(torch.from_numpy(xs).float())\n",
    "        xs_downsized = xs_downsized.data.numpy()\n",
    "        return (xs, xs_downsized)\n",
    "    else:\n",
    "        return (xs, grey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHSeX3ADsQu5"
   },
   "source": [
    "The provided get_batch function creates a dataloader (or function) to batch the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fNm3w_15C_9"
   },
   "outputs": [],
   "source": [
    "# dataloader for batching samples\n",
    "\n",
    "def get_batch(x, y, batch_size):\n",
    "    \"\"\"\n",
    "    Generated that yields batches of data\n",
    "\n",
    "    Args:\n",
    "      x: input values\n",
    "      y: output values\n",
    "      batch_size: size of each batch\n",
    "    Yields:\n",
    "      batch_x: a batch of inputs of size at most batch_size\n",
    "      batch_y: a batch of outputs of size at most batch_size\n",
    "    \"\"\"\n",
    "    N = np.shape(x)[0]\n",
    "    assert N == np.shape(y)[0]\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch_x = x[i : i + batch_size, :, :, :]\n",
    "        batch_y = y[i : i + batch_size, :, :, :]\n",
    "        yield (batch_x, batch_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjPTh0sjsgkV"
   },
   "source": [
    "Run the above helper code and call the appropriate fucntion to verify and visualize that we are able to generate different batches of data with the correct class of images (i.e., horses).\n",
    "\n",
    "Write code to visualize 5 train and test/val images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7zubylhiipE"
   },
   "outputs": [],
   "source": [
    "# code to load different batches of horse dataset\n",
    "\n",
    "print(\"Loading data...\")\n",
    "(x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "\n",
    "print(\"Transforming data...\")\n",
    "train_rgb, train_grey = process(x_train, y_train)\n",
    "test_rgb, test_grey = process(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wek0NfrHjNWE"
   },
   "outputs": [],
   "source": [
    "# shape of data and labels before selection\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Qpj_1-yjfM-"
   },
   "outputs": [],
   "source": [
    "# shape of training data\n",
    "print('Training Data: ', train_rgb.shape, train_grey.shape)\n",
    "# shape of testing data\n",
    "print('Testing Data: ', test_rgb.shape, test_grey.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHBOzMJPpNbq"
   },
   "source": [
    "Load Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfGh9pZOpP3r"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, AvgPool2d, Upsample\n",
    "\n",
    "# (Include the helper functions and load_cifar10 function here)\n",
    "\n",
    "# Load CIFAR dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "\n",
    "# Preprocess training images\n",
    "(xs_train, grey_train) = process(x_train, y_train)\n",
    "\n",
    "# Preprocess test images\n",
    "(xs_test, grey_test) = process(x_test, y_test)\n",
    "# Define a function to plot images\n",
    "def plot_images(images, titles, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 5))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < len(images):\n",
    "            img = images[i].transpose(1, 2, 0)  # Transpose the image\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot 5 training images\n",
    "plot_images(xs_train[:5], ['Horse'] * 5, 1, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUg_-oB_o9Wf"
   },
   "source": [
    "Write code to visualize 5 train/test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bGPAI4Go_q_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot 5 test images\n",
    "plot_images(xs_test[:5], ['Horse'] * 5, 1, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAqGXV0iK1G9"
   },
   "source": [
    "## Part 2 Image Colourization as Regression [4 pt]\n",
    "\n",
    "There are many ways to frame the problem of image colourization as a machine learning problem. One naive approach is to frame it as a regression problem, where we build a model to predict the RGB intensities at each pixel given the greyscale input. In this case, the outputs are continuous, and so squared error can be used to train the model.\n",
    "\n",
    "In this section, you will be the training neural networks using cloud GPUs. Run the helper code and answer the questions that follow.\n",
    "\n",
    "#### Helper Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hNqfK1AktAE"
   },
   "source": [
    "Regression Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOG_sFloK9gs"
   },
   "outputs": [],
   "source": [
    "class RegressionCNN(nn.Module):\n",
    "    def __init__(self, kernel, num_filters):\n",
    "        # first call parent's initialization function\n",
    "        super().__init__()\n",
    "        padding = kernel // 2\n",
    "\n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),)\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),)\n",
    "\n",
    "        self.rfconv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),)\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),)\n",
    "        self.finalconv = nn.Conv2d(3, 3, kernel_size=kernel, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.downconv1(x)\n",
    "        out = self.downconv2(out)\n",
    "        out = self.rfconv(out)\n",
    "        out = self.upconv1(out)\n",
    "        out = self.upconv2(out)\n",
    "        out = self.finalconv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nXIHUDdkbe_"
   },
   "source": [
    "Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def get_torch_vars(xs, ys, gpu=False, mps_device=None):\n",
    "    \"\"\"\n",
    "    Helper function to convert numpy arrays to pytorch tensors.\n",
    "    If GPU is used, move the tensors to GPU.\n",
    "\n",
    "    Args:\n",
    "      xs (float numpy tensor): greyscale input\n",
    "      ys (int numpy tensor): rgb as labels\n",
    "      gpu (bool): whether to move pytorch tensor to GPU\n",
    "      mps_device (torch.device): device for MPS backend\n",
    "    Returns:\n",
    "      Variable(xs), Variable(ys)\n",
    "    \"\"\"\n",
    "    xs = torch.from_numpy(xs).float()\n",
    "    ys = torch.from_numpy(ys).float()\n",
    "    if gpu and mps_device is not None:\n",
    "        xs = xs.to(mps_device)\n",
    "        ys = ys.to(mps_device)\n",
    "    return Variable(xs), Variable(ys)\n",
    "\n",
    "\n",
    "def train(args, gen=None):\n",
    "\n",
    "    # Numpy random seed\n",
    "    npr.seed(args.seed)\n",
    "\n",
    "    # Save directory\n",
    "    save_dir = \"outputs/\" + args.experiment_name\n",
    "\n",
    "    # LOAD THE MODEL\n",
    "    if gen is None:\n",
    "        Net = globals()[args.model]\n",
    "        gen = Net(args.kernel, args.num_filters)\n",
    "\n",
    "    # LOSS FUNCTION\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(gen.parameters(), lr=args.learn_rate)\n",
    "\n",
    "    # DATA\n",
    "    print(\"Loading data...\")\n",
    "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "\n",
    "    print(\"Transforming data...\")\n",
    "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
    "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
    "\n",
    "    # Create the outputs folder if not created already\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    print(\"Beginning training ...\")\n",
    "    if args.gpu:\n",
    "        if torch.backends.mps.is_available():\n",
    "            mps_device = torch.device(\"mps\")\n",
    "            gen.to(mps_device)\n",
    "        else:\n",
    "            print(\"MPS device not available. Falling back to CPU.\")\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train the Model\n",
    "        gen.train()  # Change model to 'train' mode\n",
    "        losses = []\n",
    "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
    "            images, labels = get_torch_vars(xs, ys, args.gpu, mps_device if args.gpu else None)\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = gen(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "        train_losses.append(np.mean(losses))\n",
    "\n",
    "        # Validate the Model\n",
    "        gen.eval()  # Change model to 'eval' mode\n",
    "        val_losses = []\n",
    "        for i, (xs, ys) in enumerate(get_batch(test_grey, test_rgb, args.batch_size)):\n",
    "            images, labels = get_torch_vars(xs, ys, args.gpu, mps_device if args.gpu else None)\n",
    "            outputs = gen(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.data.item())\n",
    "\n",
    "        valid_losses.append(np.mean(val_losses))\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{args.epochs}], '\n",
    "              f'Training Loss: {train_losses[-1]:.4f}, '\n",
    "              f'Validation Loss: {valid_losses[-1]:.4f}')\n",
    "\n",
    "        if args.plot:\n",
    "            visual(images, labels, outputs, args.gpu, 1)\n",
    "\n",
    "    # Plot the training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppqKboqz-zX-"
   },
   "source": [
    "Training visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbL_Vlao-yau"
   },
   "outputs": [],
   "source": [
    "# visualize 5 train/test images\n",
    "def visual(img_grey, img_real, img_fake, gpu = 0, flag_torch = 0):\n",
    "\n",
    "  if gpu:\n",
    "    img_grey = img_grey.cpu().detach()\n",
    "    img_real = img_real.cpu().detach()\n",
    "    img_fake = img_fake.cpu().detach()\n",
    "\n",
    "  if flag_torch:\n",
    "    img_grey = img_grey.numpy()\n",
    "    img_real = img_real.numpy()\n",
    "    img_fake = img_fake.numpy()\n",
    "\n",
    "  if flag_torch == 2:\n",
    "    img_real = np.transpose(img_real[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n",
    "    img_fake = np.transpose(img_fake[:, :, :, :, :], [0, 4, 2, 3, 1]).squeeze()\n",
    "\n",
    "  #correct image structure\n",
    "  img_grey = np.transpose(img_grey[:5, :, :, :], [0, 2, 3, 1]).squeeze()\n",
    "  img_real = np.transpose(img_real[:5, :, :, :], [0, 2, 3, 1])\n",
    "  img_fake = np.transpose(img_fake[:5, :, :, :], [0, 2, 3, 1])\n",
    "\n",
    "  for i in range(5):\n",
    "      ax = plt.subplot(3, 5, i + 1)\n",
    "      ax.imshow(img_grey[i], cmap='gray')\n",
    "      ax.axis(\"off\")\n",
    "      ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "      ax.imshow(img_real[i])\n",
    "      ax.axis(\"off\")\n",
    "      ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "      ax.imshow(img_fake[i])\n",
    "      ax.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTZWiuxMjQTB"
   },
   "source": [
    "Main training loop for regression CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KCy7KexsLgl"
   },
   "source": [
    "### Part (i) [2pt EXPLORATORY] - Describe model\n",
    "Describe the model RegressionCNN. How many convolution layers does it have? What are the filter sizes and number of filters at each layer?\n",
    "\n",
    "> The network has 6 convolutional layers.\n",
    "\n",
    "```python\n",
    "{'gpu': True,\n",
    " 'valid': False,\n",
    " 'checkpoint': '',\n",
    " 'colours': './data/colours/colour_kmeans24_cat7.npy',\n",
    " 'model': 'RegressionCNN',\n",
    " 'kernel': 3,\n",
    " 'num_filters': 32,\n",
    " 'learn_rate': 0.001,\n",
    " 'batch_size': 100,\n",
    " 'epochs': 25,\n",
    " 'seed': 0,\n",
    " 'plot': True,\n",
    " 'experiment_name': 'colourization_cnn',\n",
    " 'visualize': False,\n",
    " 'downsize_input': False}\n",
    "```\n",
    "> With the provided arguments (`args`), the specified kernel = 3 and num_filters = 32. Given this, the filter sizes and number of filters at each convolutional layer in the RegressionCNN model would be as follows:\n",
    "\n",
    "```python\n",
    "     self.downconv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),)\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),)\n",
    "\n",
    "        self.rfconv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),)\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),)\n",
    "        self.finalconv = nn.Conv2d(3, 3, kernel_size=kernel, padding=padding)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_nGLJ4B4rYC"
   },
   "source": [
    "\n",
    "> * `downconv1`: kernel size = 3, 1 input channel, 32 output channels.\n",
    "> * `downconv2`: kernel size = 3, 32 input channels, 64 output channels.\n",
    "> * `rfconv`: kernel size = 3, 64 input channels, 64 output channels.\n",
    "> * `upconv1`: kernel size = 3, 64 input channels, 32 output channels.\n",
    "> * `upconv2`: kernel size = 3, 32 input channels, 3 output channels.\n",
    "> * `finalconv`: kernel size = 3, 3 input channels, 3 output channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXTM_iITgED9"
   },
   "source": [
    "### Part (ii) [2pt EXPLORATORY] - Run Regression Training Code\n",
    "Run the regression training code (should run without errors). This will generate some images. How many epochs are we training the CNN model in the given setting?\n",
    "\n",
    "> 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZHc_eStGAQz"
   },
   "outputs": [],
   "source": [
    "#Main training loop for CNN\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"RegressionCNN\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5hSmTLxgEWG"
   },
   "source": [
    "### Part (iii) [1pt RESULTS] - Retrain with more/less epochs\n",
    "Re-train a couple of new models using a different number of training epochs. You may train each new models in a new code cell by copying and modifying the code from the last notebook cell. Comment on how the results (output images, training loss) change as we increase or decrease the number of epochs.\n",
    "> Note that for every epoch we have three rows of images:\n",
    "> * The first row displays the grayscale input images (`img_grey`).\n",
    "> * The second row displays the real, ground truth color images (`img_real`).\n",
    "> * The third row of images corresponds to the color images generated by the model based on the grayscale input images (`img_fake`). These generated images can be compared to the ground truth color images in the second row to see how well the model is performing.\n",
    "\n",
    "> As we increase the number of epochs, the model has more opportunities to learn from the trainin data, which will likely result in a decrease in training loss. However, after a certain point, the model may start to overfit to the training data.\n",
    "> On the other hand, if you decrease the number of epochs, the model will have fewer opportunities to learn from the training data, which may result in a higher training loss. However, it also means that the model is less likely to overfit to the training data.\n",
    "> As for the output images, as you increase the number of epochs, we see that the output images to become more accurate representations of the ground truth images, since the model has more opportunities to learn from the training data. Conversely, as you decrease the number of epochs, the output images to are less accurate representations of the ground truth images (more pixelated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UUyIcgd55cO"
   },
   "outputs": [],
   "source": [
    "#Main training loop for CNN\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"RegressionCNN\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 10,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_24GBrs5uNx"
   },
   "outputs": [],
   "source": [
    "#Main training loop for CNN\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"RegressionCNN\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 50,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1DCIkJq0rMo"
   },
   "source": [
    "### Part (iv) [1pt MODEL]\n",
    "\n",
    "Modify the convolutional neural network to include additional convolutional layers. Specifically, add an additional convolutional layer with the same number of filters as the previous layer and the same kernel size. Ensure that the appropriate parameters are modified (e.g. input and output channels, padding) to accommodate this change.\n",
    "\n",
    "\n",
    "> In the case of the additional layers I've added, there are no changes required in the input/output channels or padding because these additional layers are internal to the respective blocks (downconv1, downconv2, and upconv1).\n",
    "\n",
    "> * For `downconv1`, the additional layer takes as input the number of filters from the previous layer (`num_filters`), and outputs the same number of filters (`num_filters`).\n",
    "> * For `downconv2`, the additional layer takes as input the number of filters from the previous layer (`num_filters*2`), and outputs the same number of filters (`num_filters*2`).\n",
    "> * For `upconv1`, the additional layer takes as input the number of filters from the previous layer (`num_filters`), and outputs the same number of filters (`num_filters`).\n",
    "\n",
    "> In each case, since the input and output channels are the same for the additional layer, there is no need to adjust the padding. The padding remains the same as it was for the existing layers in the block, which is `kernel // 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the kernel needs to be restarted here so that there is no namespace clashing with `RegressionCNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdoXIKRk0rMp"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "class RegressionCNN_modified(nn.Module):\n",
    "    def __init__(self, kernel, num_filters):\n",
    "        # first call parent's initialization function\n",
    "        super().__init__()\n",
    "        padding = kernel // 2\n",
    "\n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),  # New layer\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),)\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),  # New layer\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),)\n",
    "\n",
    "        self.rfconv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),  # New layer\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),)\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, 3, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),)\n",
    "        self.finalconv = nn.Conv2d(3, 3, kernel_size=kernel, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.downconv1(x)\n",
    "        out = self.downconv2(out)\n",
    "        out = self.rfconv(out)\n",
    "        out = self.upconv1(out)\n",
    "        out = self.upconv2(out)\n",
    "        out = self.finalconv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"RegressionCNN_modified\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb248qFXhq1m"
   },
   "source": [
    "## Part 3 Skip Connections [8 pt]\n",
    "A skip connection in a neural network is a connection which skips one or more layer and connects to a later layer. In this section we will be incorporating skip connections to improve on our model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "babFMNf_h9UM"
   },
   "source": [
    "### Part (i) [3pt MODEL] - Redefine Network\n",
    "Add a skip connection from the first layer to the last, second layer to the second last, etc.\n",
    "That is, the final convolution should have both the output of the previous layer and the initial greyscale input as input. This type of skip-connection architecture and is called a \"UNet\". Following the CNN class that you have completed, complete the __init__ and forward methods of the UNet class.\n",
    "Hint: You will need to use the function torch.cat to add skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsipREOi1r7F"
   },
   "outputs": [],
   "source": [
    "# #complete the code\n",
    "\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Useful parameters\n",
    "#         stride = 2\n",
    "#         padding = kernel // 2\n",
    "#         output_padding = 1\n",
    "\n",
    "#         ############### YOUR CODE GOES HERE ###############\n",
    "#         ###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         ############### YOUR CODE GOES HERE ###############\n",
    "#         ###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Useful parameters\n",
    "        stride = 2\n",
    "        padding = kernel // 2\n",
    "        output_padding = 1\n",
    "\n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_in_channels, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.rfconv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_filters*4, num_filters, kernel_size=kernel, stride=stride, padding=padding, output_padding=output_padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_filters*2, num_colours, kernel_size=kernel, stride=stride, padding=padding, output_padding=output_padding),\n",
    "            nn.BatchNorm2d(num_colours),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.finalconv = nn.Conv2d(num_colours, num_colours, kernel_size=kernel, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.downconv1(x)\n",
    "        x2 = self.downconv2(x1)\n",
    "        x3 = self.rfconv(x2)\n",
    "        x4 = self.upconv1(torch.cat([x3, x2], 1))\n",
    "        x5 = self.upconv2(torch.cat([x4, x1], 1))\n",
    "        out = self.finalconv(x5)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VzFtk3hh9UN"
   },
   "source": [
    "### Part (ii) [2pt RESULTS] -  batch size of 100\n",
    "Train the \"UNet\" model for the same amount of epochs as the previous CNN and plot the training curve using a batch size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-vGs7qHndmY"
   },
   "outputs": [],
   "source": [
    "# Main training loop for UNet\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"UNet\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PWcZWk5TBqN"
   },
   "source": [
    "**How does the result compare to the previous model? Did skip connections improve the validation loss? Did the skip connections improve the output qualitatively? How? Give at least two reasons why skip connections might improve the performance of our CNN models.**\n",
    "\n",
    "**How does the result compare to the previous model? Did skip connections improve the validation loss?**\n",
    "> The skip connections did improve the validation loss. \n",
    "> * The last validation loss after epoch 25 was 0.0044 (skip connection case) whereas the last validation loss at epoch 25 without skip connections was 0.0095.\n",
    "\n",
    "**Did the skip connections improve the output qualitatively? How? Give at least two reasons why skip connections might improve the performance of our CNN models.**\n",
    "* As the network becomes deeper, gradients calculated during backpropagation can become extremely small (vanish) due to multiple multiplications. This makes it hard for the weights to update and can stall the training process. **Skip connections allow the gradient to be directly backpropagated to earlier layers, preventing it from diminishing entirely.**\n",
    "*  By allowing feature maps from earlier layers to be combined directly with those in deeper layers, skip connections help networks leverage both low-level features (like edges) and high-level features (like object parts) simultaneously. This can be particularly useful in tasks like image segmentation where fine details matter.\n",
    "*  Without skip connections, training very deep networks can be challenging due to the vanishing and exploding gradient problems. Skip connections enable the construction and training of much deeper architectures, like the ResNet-152, which would be difficult to train otherwise.\n",
    "*   In architectures like U-Net, skip connections allow features from downsampling layers to be fused with features from upsampling layers. This ensures that the spatial information lost during downsampling is reintroduced during upsampling, resulting in sharper and more accurate output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5B6i_U56aAF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6fl2PcliJTd"
   },
   "source": [
    "### Part (iii) [2pt RESULTS] - different mini batch sizes\n",
    "Re-train a few more \"UNet\" models using different mini batch sizes with a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mini batch size of 50 - val loss 0.0043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWF83JjpiJTd"
   },
   "outputs": [],
   "source": [
    "# Main training loop for UNet\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"UNet\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 50,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FyGDuy2TKxG"
   },
   "source": [
    "Describe the effect of batch sizes on the training/validation loss, and the final image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46LC0Ecq6hV5"
   },
   "outputs": [],
   "source": [
    "# Main training loop for UNet\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"UNet\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate':0.001,\n",
    "    \"batch_size\": 10,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "args.update(args_dict)\n",
    "cnn = train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe the effect of batch sizes on the training/validation loss, and the final image output.**\n",
    "\n",
    "> The lower the batch size, the lower the validation error we observed. However, it took significantly longer to train.\n",
    "> Qualitatively speaking, the colors used by the colorization look more opaque than those of larger batch sizes.\n",
    "> Smaller batch sizes can result in more diverse colorization results due to the increased noise in the gradient updates. This noise can act as a form of regularization, helping the model explore a wider range of colorization possibilities. The increased regularization from the noisy updates might help the model generalize better to new, unseen images, potentially leading to better colorization results on the validation and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCjRpOlEuuJj"
   },
   "source": [
    "# PART B - Conditional GAN\n",
    "\n",
    "In this second half of the assignment we will construct a conditional generative adversarial network for our image colourization task. This second half of the assignment should be started after the lecture on generative adversarial networks (GANs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNgohSYXiWnD"
   },
   "source": [
    "## Part 1 Conditional GAN [10 Pt]\n",
    "\n",
    "To start we will be modifying the previous sample code to construct and train a conditional GAN. Later you will have the opportunity to explore other modifications that can be made to our architecture to achieve the best results on image colourization model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-TzCQBe_6Uk"
   },
   "source": [
    "### Part (i) [2pt MODEL] - Implement Generator\n",
    "Modify the provided training code to implement a generator. Then test to verify it works on the desired input\n",
    "\n",
    "Hint: you can reuse your earlier autoencoder models here to act as a generator, if you do so, the architecture will be more like the pix2pix architecture (see https://phillipi.github.io/pix2pix/ for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRB4eJQIFFAD"
   },
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Useful parameters\n",
    "#         stride = 2\n",
    "#         padding = kernel // 2\n",
    "#         output_padding = 1\n",
    "\n",
    "#         ############### YOUR CODE GOES HERE ###############\n",
    "#         ###################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         ############### YOUR CODE GOES HERE ###############\n",
    "#         ###################################################\n",
    "\n",
    "\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Useful parameters\n",
    "        stride = 2\n",
    "        padding = kernel // 2\n",
    "        output_padding = 1\n",
    "\n",
    "        self.downconv1 = nn.Sequential(\n",
    "            nn.Conv2d(num_in_channels, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.downconv2 = nn.Sequential(\n",
    "            nn.Conv2d(num_filters, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.rfconv = nn.Sequential(\n",
    "            nn.Conv2d(num_filters*2, num_filters*2, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_filters*4, num_filters, kernel_size=kernel, stride=stride, padding=padding, output_padding=output_padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_filters, num_filters, kernel_size=kernel, padding=padding),\n",
    "            nn.BatchNorm2d(num_filters),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_filters*2, num_colours, kernel_size=kernel, stride=stride, padding=padding, output_padding=output_padding),\n",
    "            nn.BatchNorm2d(num_colours),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.finalconv = nn.Conv2d(num_colours, num_colours, kernel_size=kernel, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.downconv1(x)\n",
    "        x2 = self.downconv2(x1)\n",
    "        x3 = self.rfconv(x2)\n",
    "        # print(x3.shape, x2.shape)\n",
    "        # print(x2.shape)\n",
    "        x4 = self.upconv1(torch.cat([x3, x2], 1))\n",
    "        x5 = self.upconv2(torch.cat([x4, x1], 1))\n",
    "        out = self.finalconv(x5)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test generator architecture\n",
    "G = Generator(3,32)\n",
    "img_greyscale = torch.randn([50,1,32,32])\n",
    "img_fake = G(img_greyscale)\n",
    "print(img_fake.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Main training loop for GAN\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"Generator\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate': 0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_gan\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "gen = train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVJKZVo56vHe"
   },
   "source": [
    "Write code to test the generator is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TpgRI8uYieh"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o4lkOgF_3A2"
   },
   "source": [
    "### Part (ii) [2pt MODEL] - Implement Discriminator\n",
    "Modify the provided training code to implement a discriminator. Then test to verify it works on the desired input.\n",
    "\n",
    "Hint: You should build a simple CNN model to act as a discriminator for real and fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHvxX5jrF0Pw"
   },
   "outputs": [],
   "source": [
    "# # discriminator code\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Useful parameters\n",
    "#         stride = 2\n",
    "#         padding = kernel // 2\n",
    "#         output_padding = 1\n",
    "\n",
    "#         ############### YOUR CODE GOES HERE ###############\n",
    "#         ###################################################\n",
    "\n",
    "\n",
    "#     def forward(self, x, img_greyscale):\n",
    "\n",
    "#         ############### YOUR CODE GOES HERE ###############\n",
    "#         ###################################################\n",
    "\n",
    "\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, kernel, num_filters, num_colours=3, num_in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Useful parameters\n",
    "        stride = 2\n",
    "        padding = kernel // 2\n",
    "        output_padding = 1\n",
    "\n",
    "        ############### YOUR CODE GOES HERE ###############\n",
    "        self.conv1 = nn.Conv2d(num_colours + 1, num_filters, kernel, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size=kernel, stride=stride, padding=padding)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.fc1 = nn.Linear(num_filters * 4 * 8 * 8, 1)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        ###################################################\n",
    "\n",
    "    def forward(self, x, img_greyscale):\n",
    "        ############### YOUR CODE GOES HERE ###############\n",
    "        x = torch.cat((x, img_greyscale), 1)\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.sigmoid(self.fc1(x))\n",
    "        ###################################################\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i_R5LKu6570"
   },
   "source": [
    "Write code to test that the discriminator is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test discriminator architecture\n",
    "img_colored = torch.randn([50,3,32,32])\n",
    "img_greyscale = torch.randn([50,1,32,32])\n",
    "D = Discriminator(3,32)\n",
    "print(D(img_colored, img_greyscale).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xaw5C6w9Wrlx"
   },
   "outputs": [],
   "source": [
    "def train_discriminator(args, disc=None, gen=None):\n",
    "    # ... (previous code remains the same)\n",
    "  \n",
    "    # Numpy random seed\n",
    "    np.random.seed(args.seed)\n",
    "  \n",
    "    # Save directory\n",
    "    save_dir = \"outputs/\" + args.experiment_name\n",
    "  \n",
    "    # LOAD THE MODEL\n",
    "    if disc is None:\n",
    "        Disc = globals()[args.model]\n",
    "        disc = Disc(args.kernel, args.num_filters)\n",
    "  \n",
    "    # LOSS FUNCTION\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(disc.parameters(), lr=args.learn_rate)\n",
    "  \n",
    "    # DATA\n",
    "    print(\"Loading data...\")\n",
    "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "  \n",
    "    print(\"Transforming data...\")\n",
    "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
    "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
    "  \n",
    "    # Create the outputs folder if not created already\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "  \n",
    "    print(\"Beginning training ...\")\n",
    "    if args.gpu:\n",
    "        if torch.backends.mps.is_available():\n",
    "            mps_device = torch.device(\"mps\")\n",
    "            disc.to(mps_device)\n",
    "        else:\n",
    "            print(\"MPS device not available. Falling back to CPU.\")\n",
    "  \n",
    "    start = time.time()\n",
    "  \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train the Model\n",
    "        disc.train()  # Change model to 'train' mode\n",
    "        losses = []\n",
    "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
    "            images, labels = get_torch_vars(xs, ys, args.gpu, mps_device if args.gpu else None)\n",
    "            \n",
    "            # Generate fake images\n",
    "            fake_images = gen(images).detach()  # detach to avoid backpropagation to generator\n",
    "            real_labels = Variable(torch.ones(images.size(0), 1)).to(images.device)\n",
    "            fake_labels = Variable(torch.zeros(images.size(0), 1)).to(images.device)\n",
    "            \n",
    "            # Forward + Backward + Optimize for real images\n",
    "            optimizer.zero_grad()\n",
    "            outputs = disc(labels, images)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            real_loss.backward()\n",
    "            \n",
    "            # Forward + Backward + Optimize for fake images\n",
    "            outputs = disc(fake_images, images)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            fake_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = real_loss + fake_loss\n",
    "            losses.append(loss.data.item())\n",
    "  \n",
    "        train_losses.append(np.mean(losses))\n",
    "  \n",
    "        # Validate the Model\n",
    "        disc.eval()  # Change model to 'eval' mode\n",
    "        val_losses = []\n",
    "        for i, (xs, ys) in enumerate(get_batch(test_grey, test_rgb, args.batch_size)):\n",
    "            images, labels = get_torch_vars(xs, ys, args.gpu, mps_device if args.gpu else None)\n",
    "            \n",
    "            # Generate fake images\n",
    "            fake_images = gen(images).detach()  # detach to avoid backpropagation to generator\n",
    "            real_labels = Variable(torch.ones(images.size(0), 1)).to(images.device)\n",
    "            fake_labels = Variable(torch.zeros(images.size(0), 1)).to(images.device)\n",
    "            \n",
    "            # Compute loss for real images\n",
    "            outputs = disc(labels, images)\n",
    "            real_loss = criterion(outputs, real_labels)\n",
    "            \n",
    "            # Compute loss for fake images\n",
    "            outputs = disc(fake_images, images)\n",
    "            fake_loss = criterion(outputs, fake_labels)\n",
    "            \n",
    "            loss = real_loss + fake_loss\n",
    "            val_losses.append(loss.data.item())\n",
    "  \n",
    "        valid_losses.append(np.mean(val_losses))\n",
    "  \n",
    "        print(f'Epoch [{epoch+1}/{args.epochs}], '\n",
    "              f'Training Loss: {train_losses[-1]:.4f}, '\n",
    "              f'Validation Loss: {valid_losses[-1]:.4f}')\n",
    "        \n",
    "        # Visualization\n",
    "        if args.visualize:\n",
    "            # Sample a batch of images, labels, and generated images\n",
    "            xs, ys = next(iter(get_batch(test_grey, test_rgb, 5)))\n",
    "            images, labels = get_torch_vars(xs, ys, args.gpu, mps_device if args.gpu else None)\n",
    "            fake_images = gen(images).detach()\n",
    "            visual(images, labels, fake_images, gpu=args.gpu)\n",
    "  \n",
    "    # Plot the training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    return disc\n",
    "\n",
    "# Main training loop for GAN\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"Discriminator\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'learn_rate': 0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_gan\",\n",
    "    \"visualize\": True,\n",
    "    \"downsize_input\": False,\n",
    "}\n",
    "\n",
    "args.update(args_dict)\n",
    "disc = train_discriminator(args, gen=gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM8DcScdiWnD"
   },
   "source": [
    "### Part (iii) [2pt MODEL] - implement conditional GAN\n",
    "Modify the provided training code to implement the training needed for a conditional GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMPQPSykNTwi"
   },
   "outputs": [],
   "source": [
    "def get_torch_vars(xs, ys, gpu=False):\n",
    "    xs = torch.from_numpy(xs).float()\n",
    "    ys = torch.from_numpy(ys).long()\n",
    "    if gpu:\n",
    "        device = torch.device(\"mps\")\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "    return Variable(xs), Variable(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "def get_torch_vars(xs, ys, device=None):\n",
    "    \"\"\"\n",
    "    Helper function to convert numpy arrays to pytorch tensors.\n",
    "    If GPU or MPS is used, move the tensors to the specified device.\n",
    "\n",
    "    Args:\n",
    "      xs (float numpy tensor): greyscale input\n",
    "      ys (int numpy tensor): categorical labels\n",
    "      device (torch.device): the device to move the pytorch tensor to\n",
    "    Returns:\n",
    "      Variable(xs), Variable(ys)\n",
    "    \"\"\"\n",
    "    xs = torch.from_numpy(xs).float()\n",
    "    ys = torch.from_numpy(ys).float()\n",
    "    if device is not None:\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "    return Variable(xs), Variable(ys)\n",
    "\n",
    "def train(args, cnn=None):\n",
    "    # Set the maximum number of threads to prevent crash in Teaching Labs\n",
    "    # TODO: necessary?\n",
    "    torch.set_num_threads(5)\n",
    "    # Numpy random seed\n",
    "    npr.seed(args.seed)\n",
    "\n",
    "    # Save directory\n",
    "    save_dir = \"outputs/\" + args.experiment_name\n",
    "\n",
    "    # INPUT CHANNEL\n",
    "    num_in_channels = 1 if not args.downsize_input else 3\n",
    "\n",
    "    # LOAD THE MODEL\n",
    "    if cnn is None:\n",
    "        Net = globals()[args.model]\n",
    "        cnn = Net(args.kernel, args.num_filters)\n",
    "        discriminator = Discriminator(args.kernel, args.num_filters)\n",
    "\n",
    "    # LOSS FUNCTION\n",
    "    criterion = nn.BCELoss()\n",
    "    g_optimizer = torch.optim.Adam(cnn.parameters(), lr=args.lr)\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=args.lr)\n",
    "\n",
    "    # DATA\n",
    "    print(\"Loading data...\")\n",
    "    (x_train, y_train), (x_test, y_test) = load_cifar10()\n",
    "\n",
    "    print(\"Transforming data...\")\n",
    "    train_rgb, train_grey = process(x_train, y_train, downsize_input=args.downsize_input)\n",
    "    test_rgb, test_grey = process(x_test, y_test, downsize_input=args.downsize_input)\n",
    "\n",
    "    # Create the outputs folder if not created already\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    device = torch.device(\"mps\") if args.use_mps else torch.device(\"cuda\" if args.gpu and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Beginning training ...\")\n",
    "    cnn.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train the Model\n",
    "        cnn.train()\n",
    "        discriminator.train()\n",
    "        losses = []\n",
    "\n",
    "        for i, (xs, ys) in enumerate(get_batch(train_grey, train_rgb, args.batch_size)):\n",
    "            images, labels = get_torch_vars(xs, ys, device)\n",
    "\n",
    "            #--->ADDED 5\n",
    "            img_grey = images\n",
    "            img_real = labels\n",
    "            batch_size = args.batch_size\n",
    "\n",
    "            # discriminator training\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            D_real = discriminator(img_real, img_grey)\n",
    "            labels = torch.zeros(batch_size).to(device)\n",
    "            d_real_loss = criterion(D_real.squeeze(), labels)\n",
    "\n",
    "            img_fake = cnn(img_grey)\n",
    "            D_fake = discriminator(img_fake, img_grey)\n",
    "            labels = torch.ones(batch_size).to(device)\n",
    "            d_fake_loss = criterion(D_fake.squeeze(), labels)\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # generator training\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            img_fake = cnn(img_grey)\n",
    "            D_fake = discriminator(img_fake, img_grey)\n",
    "            labels = torch.zeros(batch_size).to(device)\n",
    "            g_loss = criterion(D_fake.squeeze(), labels)\n",
    "\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        # print and visualize\n",
    "        print(epoch, g_loss.cpu().detach(), d_loss.cpu().detach())\n",
    "        visual(images, img_real, img_fake, args.gpu, 1)\n",
    "\n",
    "    return cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop for GAN\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"Generator\",  \n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 32,\n",
    "    'lr': 0.001,\n",
    "    \"batch_size\": 100,\n",
    "    \"epochs\": 25,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": True,\n",
    "    \"experiment_name\": \"colourization_gan\",\n",
    "    \"visualize\": True,\n",
    "    \"downsize_input\": False,\n",
    "    \"device\": torch.device(\"mps\"),\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"use_mps\": True,  # Add this line\n",
    "}\n",
    "\n",
    "\n",
    "args.update(args_dict)\n",
    "gan = train(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KnN4Bc8iv_Y"
   },
   "source": [
    "### Part (iv) [2pt RESULTS] - train cGAN and compare against autoencoder \n",
    "Train a conditional GAN for image colourization and comment on how the results compare to the autoencoder architectures from part 1.\n",
    "\n",
    "> The colors that the network explores are definitely more diverse with the cGAN but the edges have become unclear. The results are also unstable. The colors change at a rather random pattern with different tones with little consistency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttrDict()\n",
    "args_dict = {\n",
    "    \"gpu\": True,\n",
    "    \"valid\": False,\n",
    "    \"checkpoint\": \"\",\n",
    "    \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "    \"model\": \"Generator\",\n",
    "    \"kernel\": 3,\n",
    "    \"num_filters\": 16,\n",
    "    'lr':0.001,\n",
    "    \"batch_size\": 50,\n",
    "    \"epochs\": 100,\n",
    "    \"seed\": 0,\n",
    "    \"plot\": False,\n",
    "    \"experiment_name\": \"colourization_cnn\",\n",
    "    \"visualize\": False,\n",
    "    \"downsize_input\": False,\n",
    "    \"use_mps\": True, \n",
    "}\n",
    "args.update(args_dict)\n",
    "cnn = train(args)\n",
    "\n",
    "#batch size of 50 with 100 epochs seamed to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNPFFIZgXhAd"
   },
   "outputs": [],
   "source": [
    "# args = AttrDict()\n",
    "# args_dict = {\n",
    "#     \"gpu\": True,\n",
    "#     \"valid\": False,\n",
    "#     \"checkpoint\": \"\",\n",
    "#     \"colours\": \"./data/colours/colour_kmeans24_cat7.npy\",\n",
    "#     \"gen_model\": \"Generator\",\n",
    "#     \"disc_model\": \"Discriminator\",\n",
    "#     \"kernel\": 3,\n",
    "#     \"num_filters\": 16,\n",
    "#     'learn_rate': 0.001,\n",
    "#     \"batch_size\": 50,\n",
    "#     \"epochs\": 100,\n",
    "#     \"seed\": 0,\n",
    "#     \"plot\": False,\n",
    "#     \"experiment_name\": \"colourization_cgan\",\n",
    "#     \"visualize\": False,\n",
    "#     \"downsize_input\": False,\n",
    "# }\n",
    "# args.update(args_dict)\n",
    "\n",
    "# # Assuming `Generator` and `Discriminator` are the names of your generator and discriminator classes\n",
    "# gen = Generator(args.num_filters, args.kernel)\n",
    "# disc = Discriminator(args.num_filters, args.kernel)\n",
    "\n",
    "# gen, disc = train(args, gen, disc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZYvaepWbBDQ"
   },
   "source": [
    "### Part (v) [2pt DISCUSSION] - conditional GAN vs pix2pix\n",
    "\n",
    "There are two ways to implement the conditional GAN. The first is a standard conditional GAN for which the generator is a decoder with an input that is random noise with a conditional. The second is the pix2pix architecture where the generator is a UNet architecture with a greyscale image input.\n",
    "\n",
    "Briefly describe the pros and cons of the two different approaches in regards to the image colourization task.\n",
    "\n",
    "Limit your answer to no more than 100 words.\n",
    "\n",
    "#### Standard Conditional GAN\n",
    "\n",
    "**Pros:**\n",
    "* Simple architecture: The **generator is just a decoder that takes in random noise and a condition**, which makes the model easier to implement and train.\n",
    "* Flexibility: Since the input is random noise, the generator can learn to generate a wide range of colorized images for a given greyscale image.\n",
    "\n",
    "**Cons:**\n",
    "* Lack of precision: The generated images may not be as precise or accurate as the ones generated by a pix2pix architecture since the input is random noise.\n",
    "* If a Conditional GAN experiences mode collapse, it may generate similar colorized images for different grayscale input images, even if the ground truth colorized images are different. This would indicate that the generator is not effectively using the grayscale image as a condition to guide the colorization process.\n",
    "\n",
    "#### Pix2Pix Architecture\n",
    "\n",
    "**Pros:**\n",
    "* Precision: The generator takes in a greyscale image as input, which allows it to generate more precise and accurate colorized images.\n",
    "Better handling of complex scenes: The UNet architecture of the generator can handle complex scenes and fine details better than a standard conditional GAN.\n",
    "\n",
    "**Cons:**\n",
    "* Complexity: The architecture is more complex and resource intensive. \n",
    "* Potential for overfitting: Since the generator takes in a greyscale image as input, it may overfit and not generalize well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W04U6MhOjezG"
   },
   "source": [
    "## Part 2 New Data [5 Pt]\n",
    "\n",
    "### Part (i) [3pt RESULTS]\n",
    "\n",
    "Retrieve sample pictures from online and demonstrate how well your best model performs. Provide all your code, including any image processing that you need to perform, and provide outputs on all the greyscale images you tested. Your outputs should show the greyscale images, ground truth, and model generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_wYmbcI7ima"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwXjXf3gO3u1"
   },
   "source": [
    "### Part (ii) [2pt DISCUSSION]\n",
    "\n",
    "Summarize the qualitative performance of your best model on image colourization of new data and provide some reasons why it performs the way it does. Are there certain types of images on which your model works better than on others? Explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HazQG5YvO7eY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jda8pXEQ4HKa"
   },
   "source": [
    "## Part 3 Exploration [6 Pt]\n",
    "\n",
    "At this point we have trained a few different generative models for our image colourization task with varying results. What makes this work exciting is that there many other approaches we could take. In this part of the assignment you are asked to consider some modifications you could make to improve the results and provide some comments on why those approaches could be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvHWPRKv0rNK"
   },
   "source": [
    "### Part (i) [2pt DISCUSSION]\n",
    "\n",
    "We've seen several times in this course how pretrained models could be used to improve performance on classification tasks. Do you think they could be beneficial in improving our results on the image colourization tasks? If so, would it be helpful to implement them in the discriminative and/or generative networks? Why or why not?\n",
    "\n",
    "Limit your answers to no more than 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wN0ln5E7f0v"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Vys9KXtMAHI"
   },
   "source": [
    "### Part (ii) [2pt DISCUSSION]\n",
    "The CIFAR10 images are 32x32 pixels in resolution. Do you believe that using similar images with a resolution of 250x250 would lead to better results. Why or why not?\n",
    "\n",
    "Limit your answers to no more than 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Si2QAzSx7gxr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k71ooYz_WB_T"
   },
   "source": [
    "### Part (iii) [2pt DISCUSSION]\n",
    "\n",
    "A colour space is a choice of mapping of colours into three-dimensional coordinates. Some colours could be close together in one colour space, but further apart in others. The RGB colour space is probably the most familiar to you, the model used in in our regression colourization example computes squared error in RGB colour space. But, most state of the art colourization models\n",
    "do not use RGB colour space. How could using the RGB colour space be problematic? Your answer should relate how human perception of colour is different than the squared distance. You may use the Wikipedia article on colour space to help you answer the question.\n",
    "\n",
    "Limit your answers to no more than 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAmJyDlMWF0-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4-9G8bB1WK4"
   },
   "source": [
    "# PART C (Optional) - Bonus Challenge!\n",
    "\n",
    "This is an optional exercise for those that finish the assignment early and would like to perform a deeper exploration of the concepts introduced in the assignment.\n",
    "\n",
    "In Part A we constructed an autoencoder, specifically a UNet architecture with skip connections to learn to colourize images. In Part B completed the same task using conditional GANs. For this bonus challenge we will explore several other techniques that could be used to improve the model's performance on image colourization. A great tutorial on some of these different approaches can be found in a <a href=\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\">blog post by Moein Shariatnia</a>. It is highly recommended that you read the article before completing the following tasks:\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Modify your conditional GAN architecture to incorporate a pretrained ResNet model to speed up and improve the generative abilities for image colourization.\n",
    "2. Modify your conditional GAN architecture to incorporate the patch discriminator trained on local regions in the images.\n",
    "3. Modify your conditional GAN architecture to use higher resolution images of upto 256 x 256 resolution and compare the performance. Summarize how performance changes with increasing resolution of images. Hint: for this task you will need to use another dataset, perhaps COCO, or ImageNet.\n",
    "4. Modify your conditional GAN architecture to incorporate lab colour space image data representation instead of RGB data. Compare the performance of your model using RGB vs lab colour space data.\n",
    "\n",
    "Bonus marks will be provided based on the number of tasks completed and how well they are completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond a maximum assignment grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgILVilnggKT"
   },
   "outputs": [],
   "source": [
    "# TO BE COMPLETED\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvZVSNFw1Zj4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "PROVIDE YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYwI4RmFS2RB"
   },
   "source": [
    "### Saving to HTML\n",
    "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
    "\n",
    "(1) download your ipynb file by clicking on File->Download.ipynb\n",
    "\n",
    "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
    "\n",
    "(3) run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TrsqdNgS5ex"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "jupyter nbconvert --to html /content/A3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuXhlFlPTY7F"
   },
   "source": [
    "(4) the html file will be available for download in the temporary Google Colab storage\n",
    "\n",
    "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKmW9v0413iX"
   },
   "source": [
    "# Assignment Grading Rubric\n",
    "The grading of the assignment will be based on the following categories:\n",
    "\n",
    "(1) **10 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
    "\n",
    "(2) **10 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
    "\n",
    "(3) **10 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
    "\n",
    "(4) **10 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
    "\n",
    "(5) **10 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
    "\n",
    "(6) **10 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
    "\n",
    "\n",
    "\n",
    "**TOTAL GRADE = _____ of 50 Pts**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
