{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c25a1b-a4e0-41fe-8cef-9e03203125f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Twitter Image Classification, Recreation, and Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f253-5662-4b4e-a6dd-9612ce1d881c",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook uses vision-language models to classify Twitter images and diffusion models to recreate and edit them.\n",
    "\n",
    "If you are using AWS SageMaker, ensure you are using the [Conda PyTorch kernel](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) that is specifically optimized for PyTorch-based deep learning tasks.\n",
    "\n",
    "Please check [LBSocial](www.lbsocial.net)  on how to collect Twitter data. \n",
    "\n",
    "\n",
    "## Visoan-Language Model\n",
    "A [Vision-Language Model (VLM)](https://huggingface.co/blog/vlms) is an AI model that integrates visual and textual information, enabling it to understand and generate insights from both images and text. It combines computer vision (to recognize objects and scenes) with natural language processing, allowing tasks like generating image captions, answering questions about images, and creating visuals from text prompts. \n",
    "\n",
    "## Diffusion Model\n",
    "A [diffusion model](https://huggingface.co/blog/Esmail-AGumaan/diffusion-models) is a generative AI model that creates images by reversing a process of adding noise. It learns to transform random noise into detailed images by progressively removing noise over many steps. Starting with pure noise, the model gradually refines each step until it forms a complete, high-quality image. This approach is highly effective for text-to-image generation and image editing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61eb4d-965a-474c-bc2e-c5bcc4da2526",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up a Database and API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d5bea-cf3e-4eb8-ad23-0a128f979e65",
   "metadata": {},
   "source": [
    "Create a [MongoDB](www.mongodb.com) cluster and store the connection string in a safe place, such as AWS Secrets Manager. \n",
    "- key name: `connection_string`\n",
    "- key value: <`the connection string`>, you need to type the password\n",
    "- secret name: `mongodb`\n",
    "\n",
    "\n",
    "You also need to purchase and your [oepnai](https://openai.com/) api key in AWS Secrets Manager:\n",
    "- key name: `api_key`\n",
    "- key value: <`your openai api key`>\n",
    "- secret name: `openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67194e-60e9-44f2-a800-1da75b809fae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python Libraries\n",
    "\n",
    "- pymongo: manage the MongoDB database\n",
    "- openai: use the VLM model and diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0541cd55-a63a-4ea7-b109-64e638f68058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.54.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (4.6.2.post1)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.54.4-py3-none-any.whl (389 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading jiter-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Installing collected packages: jiter, dnspython, distro, pymongo, openai\n",
      "Successfully installed distro-1.9.0 dnspython-2.7.0 jiter-0.7.1 openai-1.54.4 pymongo-4.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75f24d-4e08-471f-8ab2-5e1c062eabe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Secrets Manager Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9219e77-f395-4ec7-a81c-12d02c91c889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "    return json.loads(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6ed4e-b00d-4bdc-8465-81acbda700c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Python Libraries and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb73366-d206-417a-b34d-38ef4f9d3658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "openai_api_key  = get_secret('openai')['api_key']\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "\n",
    "mongodb_connect = get_secret('mongodb')['connection_string']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a33234-b6ec-48ea-9260-5796568b2648",
   "metadata": {},
   "source": [
    "## Connect to the MongoDB cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69718bb7-74d4-40a4-a970-34f7973e1321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mongo_client = MongoClient(mongodb_connect)\n",
    "db = mongo_client.demo # use or create a database named demo\n",
    "tweet_collection = db.tweet_sample #use or create a collection named tweet_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf88c5-ce0a-4663-9975-808baf3ec3be",
   "metadata": {},
   "source": [
    "## Extract Twitter Data\n",
    "Filter the Tweets you are interested in. You can use MongoDB Compass to help you write the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118129f5-8cdc-4dcf-a4ce-370b39a9eece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter={\n",
    "    'tweet.entities.urls.images': {\n",
    "        '$exists': True\n",
    "    }\n",
    "}\n",
    "project={\n",
    "    'tweet.entities.urls.images': 1, \n",
    "    'tweet.id': 1\n",
    "}\n",
    "result = tweet_collection.find(\n",
    "  filter=filter,\n",
    "  projection=project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d9cd40-acbf-44a5-b6ab-ff213330dab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "for tweet in result:\n",
    "    for url in tweet['tweet']['entities']['urls']:\n",
    "        if 'images' in url:\n",
    "            tweet_data.append({'tweet_id':tweet['tweet']['id'],\n",
    "                               'image_url':url['images'][1]['url'] # 150*150 image \n",
    "                             })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "047a7297-a966-41f9-bebc-6338737d23f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  0\n"
     ]
    }
   ],
   "source": [
    "print('Number of images: ',len(tweet_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05d66d-bbdc-4457-8a21-320f2d73a2fb",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "- `get_image_from_url`: Retrieve the image object from a URL.\n",
    "- `display_image`: Display an image in Python.\n",
    "- `image_to_bytes`: Convert an image to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12eff974-2fcb-43ab-96d3-afee9eb79baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "def get_image_from_url(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    # print(response)\n",
    "    if response.status_code == 200 :\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97c6fc9-17ee-4727-9aff-12eb145c61a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(image):\n",
    "    if image:\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ded5ec-ca95-4a23-bd6a-9aa9a1a10890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def image_to_bytes(image_obj):\n",
    "    buffered = BytesIO()\n",
    "    image_obj.save(buffered, format=\"PNG\")\n",
    "    return buffered.getvalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68202e14-6cc3-428c-bac2-cf0cff4ad328",
   "metadata": {},
   "source": [
    "## Classify Twitter Image\n",
    "\n",
    "Many OpenAI [models](https://platform.openai.com/docs/models) can handle both images and text, allowing you to choose the most suitable model for your needs.\n",
    "\n",
    "This function summarizes each Twitter image and extracts entities into a JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00dc02d4-7607-47e2-82b7-c0a0548c803b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vison_model = 'gpt-4o-mini'\n",
    "temperature=0\n",
    "\n",
    "def describe_image(image_url):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=vison_model,\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"\"\"Analyze the image included in the tweet.\n",
    "                                        Briefly describe the content of the image and extract entities from it.\n",
    "                                        Organize the response in a JSON document:\n",
    "                                        Place the description in the <img_desc> key.\n",
    "                                        List the entity type and entity value in the <img_entities> key.\n",
    "                                        Do not wrap the JSON code in JSON markers.\"\"\"},\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\"url\": image_url,},\n",
    " \n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "\n",
    "      ],\n",
    "    temperature=temperature\n",
    "     )\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a129f898-fc00-4583-be35-5a020bf4e972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31432135bc0e4a6cb9c51b11b742ad10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tweet in tqdm(tweet_data):\n",
    "    \n",
    "    try:\n",
    "        image_result =json.loads(describe_image(tweet['image_url']))\n",
    "        tweet['image_desc']=image_result['img_desc']\n",
    "        tweet_collection.update_one(\n",
    "                {'tweet.id':tweet['tweet_id']},\n",
    "                {\"$set\":{'tweet.img_desc':image_result['img_desc'], \n",
    "                        'tweet.img_entities':image_result['img_entities']}}\n",
    "        )\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa623de-cf61-42ec-8a36-7af7c58a28ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize the extracted items in MongoDB:\n",
    "\n",
    "Create charts using MongoDB Charts:\n",
    "- Apply the filter: `{'tweet.img_desc': {$ne: null}}` to ensure only tweets with image descriptions are included.\n",
    "- Bar Chart: Use a bar chart to show the values extracted from the images.\n",
    "- Bar Chart: Use another bar chart to display the entity types.\n",
    "- Heatmap: Create a heatmap to show the relationship between the entity type and entity name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb0137-6715-4a8c-a806-e9e9fcb85680",
   "metadata": {},
   "source": [
    "### View the image and description\n",
    "Randomly select an image, display it along with its description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6766d4fb-adf2-488a-95a0-8d4bacce773f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tweet \u001b[38;5;241m=\u001b[39m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_desc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_randbelow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tweet =random.choice(tweet_data)\n",
    "\n",
    "try:\n",
    "    print(tweet['image_desc'])\n",
    "    display_image(get_image_from_url(tweet['image_url']))\n",
    "except:\n",
    "    print(\"no image desc\")\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547e15d-ae4d-4646-bfb5-36f0c37cd9a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Images\n",
    "For image generation, you can choose between DALL·E 2 or DALL·E 3. Both models currently support different parameters. Note that only DALL·E 2 can be used to create variations or edit images. Please check the OpenAI [API documentation](https://platform.openai.com/docs/guides/images) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b680f4-c774-4556-a421-36b2c2e760f7",
   "metadata": {},
   "source": [
    "### Create New Image\n",
    "Generate a new image based on the provided image description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80269afc-6fc6-4298-83c2-351c1c0d5a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_model=\"dall-e-2\"  \n",
    "\n",
    "\n",
    "def create_new_img(prompt):\n",
    "\n",
    "    resposne = client.images.generate(\n",
    "    model=image_model,\n",
    "    prompt=prompt,\n",
    "    n=1,\n",
    "    size = \"256x256\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(resposne.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0b553d9-d980-4bee-ae20-5f2f597bd882",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_image \u001b[38;5;241m=\u001b[39m create_new_img(\u001b[43mtweet\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_desc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m display_image(get_image_from_url(new_image))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet' is not defined"
     ]
    }
   ],
   "source": [
    "new_image = create_new_img(tweet['image_desc'])\n",
    "\n",
    "display_image(get_image_from_url(new_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb289f14-1768-4c28-8922-315850c48095",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Variant\n",
    "Upload an existing Twitter image and create a variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db8ebab0-3a9c-4383-9ff9-ade79f14a7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_var_img(image_url):\n",
    "    img_obj = get_image_from_url(image_url)\n",
    "    img_byt = image_to_bytes(img_obj)\n",
    "    response = client.images.create_variation(\n",
    "        image=img_byt,\n",
    "        n=1,\n",
    "        size=\"256x256\",\n",
    "\n",
    "        )\n",
    "    return (response.data[0].url)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7e64856-95cd-45aa-b67c-f69f1f758729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m var_image \u001b[38;5;241m=\u001b[39m create_var_img(\u001b[43mtweet\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m display_image(get_image_from_url(var_image))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet' is not defined"
     ]
    }
   ],
   "source": [
    "var_image = create_var_img(tweet['image_url'])\n",
    "\n",
    "display_image(get_image_from_url(var_image))\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b001719-eb90-4c18-814c-c24453ab96e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Edit Image\n",
    "Upload Twitter images, mask the desired area, and edit the masked part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdbebd08-c953-4027-adba-b5c005f2a937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_img_edit(image, img_mask, prompt):\n",
    "\n",
    "    response = client.images.edit(\n",
    "      model=\"dall-e-2\",\n",
    "      image=image_to_bytes(image),\n",
    "      mask=image_to_bytes(img_mask),\n",
    "      prompt=prompt,\n",
    "      n=1,\n",
    "      size=\"256x256\"\n",
    "    )\n",
    "\n",
    "    return (response.data[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c361fc9-85c0-476f-a577-c443104f8a5b",
   "metadata": {},
   "source": [
    "### Create Mask with PyTorch\n",
    "The following code is provided by ChatGPT with the prompt `\"Segment image and create masks.\"` \n",
    "\n",
    "You can also use foundational models to segment images, such as Meta’s [SAM (Segment Anything Model)](https://ai.meta.com/sam2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "837cad51-db31-4de1-8b57-a9f3f4d867d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m image \u001b[38;5;241m=\u001b[39m get_image_from_url(\u001b[43mtweet\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Replace with your image loading method\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define transformations (no resizing since images are 150x150)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     14\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweet' is not defined"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "# Load the image\n",
    "image = get_image_from_url(tweet['image_url'])  # Replace with your image loading method\n",
    "\n",
    "# Define transformations (no resizing since images are 150x150)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained DeepLabV3 model\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Perform segmentation\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "\n",
    "# Convert output to predicted classes\n",
    "output_predictions = output.argmax(0).byte().numpy()\n",
    "\n",
    "\n",
    "# Create a transparent RGBA mask with fully transparent non-segmented areas\n",
    "height, width = output_predictions.shape\n",
    "mask = np.zeros((height, width, 4), dtype=np.uint8)  # 4 channels for RGBA\n",
    "\n",
    "# Define a color for segmented areas (e.g., semi-transparent red)\n",
    "color = [255, 0, 0, 180]  # Red with 70% opacity\n",
    "mask[output_predictions > 0] = color  # Apply color to segmented areas only\n",
    "\n",
    "# Convert mask to a PIL Image\n",
    "mask_image = Image.fromarray(mask, mode=\"RGBA\")\n",
    "\n",
    "# Overlay mask onto the contrast-enhanced original image\n",
    "image_with_mask = Image.alpha_composite(image.convert(\"RGBA\"), mask_image)\n",
    "\n",
    "# Display the original image and transparent mask overlay\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\" Image\")\n",
    "\n",
    "axs[1].imshow(image_with_mask)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Image Transparent Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4d045-4600-4256-b7d0-e884db28f0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_edit = create_img_edit(image,mask_image,'in the middle of a lake with yellow ducks')\n",
    "\n",
    "display_image(\n",
    "    get_image_from_url(img_edit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865863f2-15be-4984-bedb-f1a6626704f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Respoisbile AI\n",
    "Many image model providers update their security measures to foster responsible AI usage. For example, AWS adds watermarks to images generated by Amazon Titan. OpenAI has updated its safety features, including monitoring for abuse, access control, and the identification of images created by DALL·E 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7f519-eda0-4def-b321-bf9fdeba6968",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Amazon Web Services, Inc. *“Watermark Detection for Amazon Titan Image Generator Now Available in Amazon Bedrock.”* Accessed November 7, 2024. [https://aws.amazon.com/about-aws/whats-new/2024/04/watermark-detection-amazon-titan-image-generator-bedrock/](https://aws.amazon.com/about-aws/whats-new/2024/04/watermark-detection-amazon-titan-image-generator-bedrock/).\n",
    "\n",
    "- Esmail Atta Gumaan. *“Diffusion Models.”* Hugging Face (blog). Accessed November 7, 2024. https://huggingface.co/blog/Esmail-AGumaan/diffusion-models.\n",
    "\n",
    "- Merve Noyan and Edward Beeching. *“Vision Language Models Explained.”* Hugging Face (blog). Accessed November 6, 2024. [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n",
    "\n",
    "- *“OpenAI Safety Practices.”* Accessed November 7, 2024. [https://openai.com/index/openai-safety-update/](https://openai.com/index/openai-safety-update/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd95c31-9122-443f-bce7-de15ec8cca5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
