{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Notebook developend based on the starter notebook of https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n'''\ntry:\n    #import tensorflow_addons as tfa\nexcept:\n    !pip install --upgrade tensorflow==2.12 tensorflow-addons==0.19\n    !pip install --upgrade keras\n    !pip install keras==2.10.0  # Adjust the version if needed\n    #import tensorflow_addons as tfa\n'''\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n    \nprint(tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-19T03:31:33.174341Z","iopub.execute_input":"2024-08-19T03:31:33.174900Z","iopub.status.idle":"2024-08-19T03:31:33.186945Z","shell.execute_reply.started":"2024-08-19T03:31:33.174857Z","shell.execute_reply":"2024-08-19T03:31:33.185256Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of replicas: 1\n2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load in the data \nWe want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords.  Make sure You added the competition dataset as input of the notebook if you run it on Kaggle.\n![right panel of Kaggle](https://github.com/hhosseinian/GenerativeAI_Paintings/blob/main/Repo_images/Dataset_Loading.png)","metadata":{}},{"cell_type":"code","source":"gcs_path = kaggle_datasets.get_gcs_path()\n\nprint(gcs_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T03:37:38.486475Z","iopub.execute_input":"2024-08-19T03:37:38.486952Z","iopub.status.idle":"2024-08-19T03:37:38.892043Z","shell.execute_reply.started":"2024-08-19T03:37:38.486916Z","shell.execute_reply":"2024-08-19T03:37:38.890789Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"gs://kds-e6530a16838444df6eb6571a40e295de32cbc86e9da60de02f9cd083\n","output_type":"stream"}]},{"cell_type":"code","source":"MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nprint('Monet TFRecord Files:', len(MONET_FILENAMES))\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\nprint('Photo TFRecord Files:', len(PHOTO_FILENAMES))","metadata":{"execution":{"iopub.status.busy":"2024-08-19T03:37:44.219504Z","iopub.execute_input":"2024-08-19T03:37:44.220158Z","iopub.status.idle":"2024-08-19T03:37:44.275571Z","shell.execute_reply.started":"2024-08-19T03:37:44.220109Z","shell.execute_reply":"2024-08-19T03:37:44.273653Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MONET_FILENAMES \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;28mstr\u001b[39m(\u001b[43mGCS_PATH\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/monet_tfrec/*.tfrec\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonet TFRecord Files:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(MONET_FILENAMES))\n\u001b[1;32m      4\u001b[0m PHOTO_FILENAMES \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;28mstr\u001b[39m(GCS_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/photo_tfrec/*.tfrec\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'GCS_PATH' is not defined"],"ename":"NameError","evalue":"name 'GCS_PATH' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the function to extract the image from the files.","metadata":{}},{"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_monet = next(iter(monet_ds))\nexample_photo = next(iter(photo_ds))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize a photo example and a Monet example.","metadata":{}},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('Photo')\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(example_monet[0] * 0.5 + 0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Build the generator\n\nWe'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our downsample and upsample methods.\n\nThe downsample, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n\nWe'll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\n# Change the path below based on the path you set to unzip training/testing data in cell 1 above.\nkey_pts_frame = pd.read_csv(unzip_Path+'/training_frames_keypoints.csv')\n\nn = 0\nimage_name = key_pts_frame.iloc[n, 0]\nkey_pts = key_pts_frame.iloc[n, 1:].to_numpy()\nkey_pts = key_pts.astype('float').reshape(-1, 2)\n\nprint('Image name: ', image_name)\nprint('Landmarks shape: ', key_pts.shape)\nprint('First 4 key pts: {}'.format(key_pts[:4]))","metadata":{},"execution_count":null,"outputs":[]}]}