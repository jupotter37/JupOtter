{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to implement a simple part-of-speech tagger based on recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the task of labelling words (tokens) with [parts of speech](https://en.wikipedia.org/wiki/Part_of_speech). To give an example, consider the  sentence *Parker hates parsnips*. In this sentence, the word *Parker* should be labelled as a proper noun (a noun that is the name of a person), *hates* should be labelled as a verb, and *parsnips* should be labelled as a (common) noun. Part-of-speech tagging is an essential ingredient of many state-of-the-art natural language understanding systems.\n",
    "\n",
    "Part-of-speech tagging can be cast as a supervised machine learning problem where the gold-standard data consists of sentences whose words have been manually annotated with parts of speech. For the present assignment you will be using a corpus built over the source material of the [English Web Treebank](https://catalog.ldc.upenn.edu/ldc2012t13), consisting of approximately 16,000&nbsp;sentences with 254,000&nbsp;tokens. The corpus has been released by the [Universal Dependencies Project](http://universaldependencies.org).\n",
    "\n",
    "To make it easier to compare systems, the gold-standard data has been split into three parts: training, development (validation), and test. The following code uses three functions from the helper module `utils` (provided with this assignment) to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 12543\n",
      "Number of sentences in the development data: 2002\n",
      "Number of sentences in the test data: 2077\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, InputLayer, Bidirectional,TimeDistributed\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "training_data = list(utils.read_training_data())\n",
    "print('Number of sentences in the training data: {}'.format(len(training_data)))\n",
    "\n",
    "development_data = list(utils.read_development_data())\n",
    "print('Number of sentences in the development data: {}'.format(len(development_data)))\n",
    "\n",
    "test_data = list(utils.read_test_data())\n",
    "print('Number of sentences in the test data: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a Python perspective, each of the data sets is a list of what we shall refer to as *tagged sentences*. A tagged sentence, in turn, is a list of pairs $(w,t)$, where $w$ is a word token and $t$ is the word&rsquo;s POS tag. Here is an example from the training data to show you how this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'There', b'PRON'),\n",
       " (b'has', b'AUX'),\n",
       " (b'been', b'VERB'),\n",
       " (b'talk', b'NOUN'),\n",
       " (b'that', b'SCONJ'),\n",
       " (b'the', b'DET'),\n",
       " (b'night', b'NOUN'),\n",
       " (b'curfew', b'NOUN'),\n",
       " (b'might', b'AUX'),\n",
       " (b'be', b'AUX'),\n",
       " (b'implemented', b'VERB'),\n",
       " (b'again', b'ADV'),\n",
       " (b'.', b'PUNCT')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see part-of-speech tags such as `VERB` for verb, `NOUN` for noun, and `ADV` for adverb. If you are interested in learning more about the tag set used in the gold-standard data, you can have a look at the documentation of the [Universal POS tags](http://universaldependencies.org/u/pos/all.html). However, you do not need to understand the meaning of the POS tags to solve this assignment; you can simply treat them as labels drawn from a finite set of alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to build a part-of-speech tagger based on a recurrent neural network architecture, to train this tagger on the provided training data, and to evaluate its performance on the test data. To tune the hyperparameters of the network, you can use the provided development (validation) data.\n",
    "\n",
    "### Network architecture\n",
    "\n",
    "The proposed network architecture for your tagger is a sequential model with three layers, illustrated below: an embedding, a bidirectional LSTM, and a softmax layer. The embedding turns word indexes (integers representing words) into fixed-size dense vectors which are then fed into the bidirectional LSTM. The output of the LSTM at each position of the sentence is passed to a softmax layer which predicts the POS tag for the word at that position.\n",
    "\n",
    "![System architecture](architecture.png)\n",
    "\n",
    "To implement the network architecture, you will use [Keras](https://keras.io), a high-level neural network library for Python. Keras comes with an extensive online documentation, and reading the relevant parts of this documentation will be essential when working on this assignment. We suggest to start with the tutorial [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/). We also suggest to have a look at concrete examples, such as  [imdb_lstm.py](https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py).\n",
    "\n",
    "### Pre-processing the data\n",
    "\n",
    "Before you can start to implement the network architecture as such, you will have to bring the tagged sentences from the gold-standard data into a form that can be used with the network. At its core, this involves encoding each word and each tag as an index into a finite set (a non-negative integer), which can be done for example via a Python dictionary. Here is some code to illustrate the basic idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the training data: 19674\n",
      "Number of unique tags in the training data: 18\n",
      "{b'PAD123': 0, b'PROPN': 1, b'PUNCT': 2, b'ADJ': 3, b'NOUN': 4, b'VERB': 5, b'DET': 6, b'ADP': 7, b'AUX': 8, b'PRON': 9, b'PART': 10, b'SCONJ': 11, b'NUM': 12, b'ADV': 13, b'CCONJ': 14, b'X': 15, b'INTJ': 16, b'SYM': 17}\n"
     ]
    }
   ],
   "source": [
    "# Construct a simple index for words\n",
    "\n",
    "w2i = dict()\n",
    "t2i = dict()\n",
    "w2i[b'UNW123'] = 1\n",
    "w2i[b'PAD123'] = 0\n",
    "t2i[b'PAD123'] = 0\n",
    "\n",
    "for tagged_sentence in training_data:\n",
    "    for word, tag in tagged_sentence:\n",
    "        if word not in w2i:\n",
    "            w2i[word] = len(w2i)    # assign next available index\n",
    "        if tag not in t2i:\n",
    "            t2i[tag] = len(t2i)    # assign next available index\n",
    "print('Number of unique words in the training data: {}'.format(len(w2i)))\n",
    "print('Number of unique tags in the training data: {}'.format(len(t2i)))\n",
    "#print(t2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have indexes for the words and the tags, you can construct the input and the gold-standard output tensor required to train the network.\n",
    "\n",
    "**Constructing the input tensor.** The input tensor should be of shape $(N, n)$ where $N$ is the total number of sentences in the training data and $n$ is the length of the longest sentence. Note that Keras requires all sequences in an input tensor to have the same length, which means that you will have to pad all sequences to that length. You can use the helper function `pad_sequences` for this, which by default will front-pad sequences with the value&nbsp;0. It is essential then that you do not use this special padding value as the index of actual words.\n",
    "\n",
    "**Constructing the gold-standard output tensor.** The gold-standard output tensor should be of shape $(N, n, T)$ where $T$ is the number of unique tags in the training data, plus one to cater for the special padding value. The additional dimension corresponds to the fact that the softmax layer of the network will output one $T$-dimensional vector for each position of an input sentence. To construct the gold-standard version of this vector, you can use the helper function `to_categorical`, which will produce a &lsquo;one-hot vector&rsquo; for a given tag index.\n",
    "\n",
    "### Constructing the network\n",
    "\n",
    "To implement the network architecture, you need to find and instantiate the relevant building blocks from the Keras library. Note that Keras layers support a large number of optional parameters; use the default values unless you have a good reason not to. Two mandatory parameters that you will have to specify are the dimensionality of the embedding and the dimensionality of the output of the LSTM layer. The following values are reasonable starting points:\n",
    "\n",
    "* dimensionality of the embedding: 100\n",
    "* dimensionality of the output of the bidirectional LSTM layer: 100\n",
    "\n",
    "You will also have to choose an appropriate loss function. For training we recommend the Adam optimiser.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The last problem that you will have to solve is to write code to evaluate the trained tagger. The most widely-used evaluation measure for part-of-speech tagging is per-word accuracy, which is the percentage of words to which the tagger assigns the correct tag (according to the gold standard). Implementing this metric should be straightforward. However, make sure that you remove (or ignore) the special padding value when you compute the tagging accuracy.\n",
    "\n",
    "**The performance goal for this assignment is to build a tagger that achieves a development set accuracy of at least 90%.**\n",
    "\n",
    "**Unknown words.** One problem that you will encounter during evaluation is that the development data contains words that you did not see (and did not add to your index) during training. The simplest solution to this problem is to reserve a special index for &lsquo;the unknown word&rsquo; which the network can use whenever it encounters an unknown word. When you go for this strategy, the size of your index will be equal to the number of unique words in the training data plus&nbsp;2 &ndash; one extra for the unknown word, and one for the padding symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton code\n",
    "\n",
    "The following skeleton code provides you with a starting point for your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = Sequential() \n",
    "        self.n = 1\n",
    "\n",
    "    def train(self, training_data):\n",
    "        # Pre-process the training data\n",
    "        # Construct the network, add layers, compile, and fit\n",
    "        self.n= len(max(training_data, key=len))\n",
    "        N = len(training_data)\n",
    "        input_size = np.zeros((N,self.n),dtype='int32')\n",
    "        output_size = np.zeros((N,self.n),dtype='int32')\n",
    "\n",
    "        iter = 0\n",
    "        for tagged_sentence in training_data:\n",
    "            train_sentences_num, train_tags_num = [], []\n",
    "            for word, tag in tagged_sentence:\n",
    "                try:\n",
    "                    train_sentences_num.append(w2i[word])\n",
    "                except KeyError:\n",
    "                    train_sentences_num.append(w2i[b'UNW123'])\n",
    "        \n",
    "                train_tags_num.append(t2i[tag])\n",
    "            train_sentences_num_paded = sequence.pad_sequences([train_sentences_num], maxlen = self.n) \n",
    "            train_tags_num_padded = sequence.pad_sequences([train_tags_num], maxlen = self.n) \n",
    "            input_size[iter,:] = train_sentences_num_paded\n",
    "            output_size[iter,:] = train_tags_num_padded\n",
    "            iter = iter + 1\n",
    "        output_size = keras.utils.to_categorical(output_size,num_classes=len(t2i)) \n",
    "\n",
    "        self.model.add(InputLayer(input_shape=(self.n, )))\n",
    "        self.model.add(Embedding(len(w2i), 100,mask_zero= True))\n",
    "        self.model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "        self.model.add(Dense(len(t2i),activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        self. model.fit(input_size, output_size, batch_size=32, epochs=4)\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, gold_data):\n",
    "        # Compute the accuracy of the tagger relative to the gold data\n",
    "        input_size_eval = [] \n",
    "        output_size_eval = []\n",
    "\n",
    "        for tagged_sentence_eval in gold_data:\n",
    "            train_sentences_num_eval, train_tags_num_eval = [], []\n",
    "            for word, tag in tagged_sentence_eval:\n",
    "                try:\n",
    "                    train_sentences_num_eval.append(w2i[word])\n",
    "                except KeyError:\n",
    "                    train_sentences_num_eval.append(w2i[b'UNW123'])\n",
    "        \n",
    "                train_tags_num_eval.append(t2i[tag])\n",
    "            input_size_eval.append(train_sentences_num_eval)\n",
    "            output_size_eval.append(train_tags_num_eval)\n",
    "        train_sentences_num_paded_eval = sequence.pad_sequences(input_size_eval, maxlen = self.n) \n",
    "        train_tags_num_padded_eval = sequence.pad_sequences(output_size_eval, maxlen = self.n) \n",
    "        output_size_eval = keras.utils.to_categorical(train_tags_num_padded_eval,num_classes=len(t2i)) \n",
    "        acc = self.model.evaluate(train_sentences_num_paded_eval, output_size_eval)\n",
    "        return  acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the tagger is supposed to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 159, 100)          1967400   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 159, 100)          60400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 159, 18)           1818      \n",
      "=================================================================\n",
      "Total params: 2,029,618\n",
      "Trainable params: 2,029,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aminghazanfari/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "12543/12543 [==============================] - 51s 4ms/step - loss: 0.1106 - accuracy: 0.6891\n",
      "Epoch 2/4\n",
      "12543/12543 [==============================] - 50s 4ms/step - loss: 0.0213 - accuracy: 0.9427\n",
      "Epoch 3/4\n",
      "12543/12543 [==============================] - 50s 4ms/step - loss: 0.0120 - accuracy: 0.9658\n",
      "Epoch 4/4\n",
      "12543/12543 [==============================] - 48s 4ms/step - loss: 0.0085 - accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "tagger = Tagger()\n",
    "tagger.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002/2002 [==============================] - 1s 729us/step\n",
      "Accuracy on development data: 91.5%\n",
      "2077/2077 [==============================] - 2s 736us/step\n",
      "Accuracy on test data: 91.58%\n"
     ]
    }
   ],
   "source": [
    "acc = tagger.evaluate(development_data)\n",
    "print('Accuracy on development data: {:.1%}'.format(acc[1]))\n",
    "acc_test = tagger.evaluate(test_data)\n",
    "print('Accuracy on test data: {:.2%}'.format(acc_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit this assignment by emailing this notebook to Marco. Your notebook should include all your code, and should be runnable by Marco without further modification. It should also include a short text with your reflections on this assignment. What did you find particularly surprising or hard? What have you learned from this assignment? You can paste your text into the box below. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reflections\n",
    "\n",
    "Dear Marco,\n",
    "\n",
    "This lab was much easier than lab 1 for me especially after your comment about the possibility of using Mask_zero option in the embedding layer which made everything straightforward. Using Keras was much easier than implementing everything from scratch. \n",
    "\n",
    "with epoch equal to 3, I tried different batch size and with 100 I have got:\n",
    "\n",
    "\n",
    "Accuracy on development data: 70.83%\n",
    "\n",
    "Accuracy on test data: 70.67%\n",
    "\n",
    "When I decreased the size to 50 the results for accuracy got improved to:\n",
    "\n",
    "\n",
    "Accuracy on development data: 90.19%\n",
    "\n",
    "\n",
    "Accuracy on test data: 90.59%\n",
    "\n",
    "Finally in order to get even better accuracy, I tried 32:\n",
    "\n",
    "\n",
    "Accuracy on development data: 90.53%\n",
    "\n",
    "\n",
    "Accuracy on test data: 90.60%\n",
    "\n",
    "\n",
    "\n",
    "and increasing the epoch to 4 I have the results as follows:\n",
    "\n",
    "\n",
    "Accuracy on development data: 91.5%\n",
    "\n",
    "\n",
    "Accuracy on test data: 91.58%\n",
    "\n",
    "\n",
    "Best Regards,\n",
    "Amin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
