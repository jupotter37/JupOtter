{"cells":[{"metadata":{"_uuid":"282d1320a2c0480afeaede3fa9aeae65b5bbaac7"},"cell_type":"markdown","source":"# Introduction\nLower back pain is an annoyance that is familiar to many of us today. Whether it is an issue with the spine or a developing habit of bad posture, there are many potential causes. \n\nThis notebook explores and extracts insight from the Lower Back Pain Symptoms Dataset. The dataset contains 12 features and 1 binary class representing whether or not the patient experienced lower back pain. The features are all measurements and parameters of the spine. **Therefore, the problem of interest is whether back pain can be predicted from measurements of spine parameters for instance from a spine x-ray.  **\n\nThroughout this notebook, we go through some initial data exploration as well as some background research into the anatomy of our back. We then apply a variety of models to the data and evaluate the performance of each model.  \n\nThis notebook is a good tutorial for beginners looking to expand to working on new datasets. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom IPython.display import Image,display\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Let us begin by loading in the dataset into a Pandas dataframe and take a peek into what the dataset contains."},{"metadata":{"trusted":true,"_uuid":"5882e646d412ce30737995e57cf592f8ff719df2"},"cell_type":"code","source":"#Column names ~ 12 features, 1 binary class\ncols = ['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius',\\\n        'degree_spondylolisthesis','pelvic_slope','direct_tilt','thoracic_slope','cervical_tilt',\\\n        'sacrum_angle','scoliosis_slope','normality']\n\n#Read in data using column names, remove last column\ndata = pd.read_csv('../input/Dataset_spine.csv',header=0,names=cols,usecols=range(0,13))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ebfa43654adfefa94067b71a7a0e3bd1dc7dc38"},"cell_type":"code","source":"#Initial data stats\nprint(\"Number of examples: \",data.shape[0])\nprint(\"Number of features: \",data.shape[1]-1)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faad04bcf446c7947ebdf7f18b61314534e22603"},"cell_type":"markdown","source":"Finally, let's check for any missing data. It looks like we are good (y)."},{"metadata":{"trusted":true,"_uuid":"1bb54c57496469076c44cc23cc88ef21925809f9"},"cell_type":"code","source":"#Check for missing values \ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cedda6ae03cc536875af346e568f6b7f1111158d"},"cell_type":"markdown","source":"# Understanding the Features\nA deeper dive into the meaning of the features reveals some interesting and important information. All of these features represent different parameters of the back, specifically broken down into the cervical, thoracic, lumbar, and sacral areas of the spine as depicted in the first image below [1]. Degree spondylolisthesis and scoliosis are the only two features that can be spinal disorders. About half of these features (the pelvic and sacral features) are parameters in the pelvis and sacrum area which has been found to have a significant impact on the rest of the spine according to literature.\n\n1. pelvic_incidence (degrees) - pelvic parameter,sum of two complementary angles: pelvic tilt and sacral slope, fixed for a given patient [2]\n\n2. pelvic_tilt (degrees) - pelvic parameter \n\n3. lumbar_lordosis_angle (degrees) - curvature of spine (lordosis) in lumbar region,proportional to sacral slope \n4. sacral_slope (degrees) - pelvic parameter\n5. pelvic_radius -\n6. degree_spondylolisthesis (degrees) - a spinal disorder in which a bone (vertebra) slips forward onto the bone below it,  possibly slip angle, negative values might come from how this angle is measured\n7. pelvic_slope - \n8. direct_tilt - \n9. thoracic_slope (degrees) - curvature of spine in thoracic region\n10. cervical_tilt (degrees) - tilt in cervical region of spine \n11. sacrum_angle - \n12. scoliosis_slope (degree) - possibly cobbs angle to measure degree of scoliosis \n13. normality\n\nNote: Some of the features did not yield any results in my search. While the name of these features are descriptive, I am definitely suspicious of these parameters representing similar information.\n\n<img src=\"https://d11q7g6vqo5ah4.cloudfront.net/areas-of-the-spine-w320.jpg\" width='30%' ></img>\n<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/aa0d743d5ea374086e83cd07e1cd0f55c3a97b37/2-Figure1-1.png\" width='30%'></img>"},{"metadata":{"_uuid":"f759fa034616e8942f6e5d90e253c46316b80d92"},"cell_type":"markdown","source":"One interesting piece of information that an initial search of the feature definitions produced is that pelvic incidence is just the sum of the pelvic tilt and sacral slope. Let's double check this."},{"metadata":{"trusted":true,"_uuid":"e272bb11867fa739c805457f9904621a440a44ef"},"cell_type":"code","source":"#Make a copy of the original data for understanding features\ndata_f = data.copy()\n\n#Verify PI = PT + SS\ndata_f['pelvic_tilt + sacral_slope'] = data_f['pelvic_tilt']+data_f['sacral_slope']\ncols = ['pelvic_incidence','pelvic_tilt + sacral_slope']\ndata_f[cols].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ab899983bb18f65d67d02eddb3e14f3c8d8187b"},"cell_type":"markdown","source":"# Data Exploration\nNow that we have a better understanding of what the features mean, we move onto exploring relationships within the data.\n\nFirst, we look at the class variable. We see that there are about twice as many abnormal examples as normal ones. While it is not a perfect balance, at least the dataset is not severly unbalanced."},{"metadata":{"trusted":true,"_uuid":"c012d02a0d7dbbf72f76933428a2f45ddb3e4fb0"},"cell_type":"code","source":"#Distribution for binary class variable \nprint(data['normality'].value_counts())\nsns.countplot(x=\"normality\",data=data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e1465dd5bc5800aadbe9cc9966d1060f45ceac"},"cell_type":"markdown","source":"Next, we take a look at the correlation between the features visualized in a heatmap. The top left corner is very interesting at first glance. However, much of this is expected because we have already found out that pelvic incidence, pelvic tilt, and sacral slope are related. Based on literature, the pelvic and sacral parameters also have a significant impact on the spinal parameters [2]. \n\nIt is interesting that only the top left corner is heavily correlated. I originally hypothesized that most of the features were measuring very similar parameters. However, this correlation heatmap would suggest otherwise. These uncorrelated features could be very important in distiguishing back pain from the unique information they provide."},{"metadata":{"trusted":true,"_uuid":"7eb35721b9a941935d002833c3fa99b2808d88a0"},"cell_type":"code","source":"#Visualize heatmap of correlations\ncorr_mtx = data.corr()\nfig, ax = plt.subplots()\nsns.heatmap(corr_mtx, square=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbebaa3e9382e19c08810a734e81080eea5de35a"},"cell_type":"markdown","source":"Let us now zoom into the top left corner and visualize how these features are correlated.\n\nThere are evidently some outliers by inspection from the pairplot. "},{"metadata":{"trusted":true,"_uuid":"3757eeaf763450ca56dfcb1410ce6417840e10ea"},"cell_type":"code","source":"#Pair plot showing pairwise relationships between features that are somewhat correlated from heatmap \nuse_cols = ['pelvic_incidence','pelvic_tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius',\\\n            'degree_spondylolisthesis','normality']\nsns.pairplot(data[use_cols],size=2,hue='normality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e1e13970eaa60a068111000400dd78e2c8945c8"},"cell_type":"markdown","source":"# Baseline Models\nNow that we have a feel for the dataset and some initial insight, let's go ahead and use the raw data to create a few baseline models. We will start by just looking at accuracy as the evaluation metric. Namely, we will experiment with the following models:\n\n    1. K-Nearest-Neighbor\n    2. Naive Bayes\n    3. Logistic Regression\n    4. Support Vector Classifier\n    5. Random Forest Classifier\n    \nFirst, let's make a copy of our original data and replace the categorical class variable with a numerical binary number. Abnormal will be 1 and normal will be 0."},{"metadata":{"trusted":true,"_uuid":"81f36ef84fddf03f56bf15ca4bbd10df10f715af"},"cell_type":"code","source":"#Convert categorical class to numerical binary\nX = data.copy()\nX = pd.get_dummies(X)\nX.drop(X.columns[-1], axis=1, inplace=True)\nX.rename(columns = {X.columns[-1]:'normality'},inplace=True)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bccd05e9c40030bbcc9f31b8b47cfb7fea9693dc"},"cell_type":"markdown","source":"In addition, we split the data into a training and test set using a 60/40 split and visualize the distribution of the resulting split."},{"metadata":{"trusted":true,"_uuid":"717d5c0cdc48ecf5bc8267ee685964baf5c65293"},"cell_type":"code","source":"#Split train/test test\nX_train,X_test,y_train,y_test = train_test_split(X.drop(X.columns[-1], axis=1),X['normality'],test_size=0.4,random_state=0)\nprint('X_train: ',X_train.shape)\nprint('X_test: ',X_test.shape)\nprint('y_train: ',y_train.shape)\nprint('y_test: ',y_test.shape)\n\nfig, ax =plt.subplots(1,2)\nsns.countplot(y_train,ax=ax[0])\nax[0].set_title('y_train')\nsns.countplot(y_test,ax=ax[1])\nax[1].set_title('y_test')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d1a6b7c5d419598791e9f4f092eff2488ffed9d"},"cell_type":"markdown","source":"Now, on to fitting models.\n\n### K-NN"},{"metadata":{"trusted":true,"_uuid":"8bf8c7471095dc6f2bf7d21c2c57b11cb246459e"},"cell_type":"code","source":"#Fit Nearest Neighbor model\nmodel = KNeighborsClassifier(n_neighbors=3)\nclf = model.fit(X_train,y_train)\n\ntrain_score = clf.score(X_train, y_train)\ntest_score  = clf.score(X_test, y_test)\n\nprint(\"Nearest Neighbor Model: \")\nprint (\"Training Score: {}\\nTest Score: {}\" .format(train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6727f926db645bf6b519befb74f5b3e47e3001ad"},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"929b027464755aa3a9810ba97e012915bfeda43f"},"cell_type":"code","source":"#Fit Naive Bayes model\nmodel = GaussianNB()\nclf = model.fit(X_train,y_train)\n\ntrain_score = clf.score(X_train, y_train)\ntest_score  = clf.score(X_test, y_test)\n\nprint(\"Naive Bayes Model: \")\nprint (\"Training Score: {}\\nTest Score: {}\" .format(train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c41778c8a252973c4a69d67418ebe22970207ac"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"f9dae31fb759c70afee3de0aac2d545694a7ec41"},"cell_type":"code","source":"#Fit logistic regression model \nmodel = LogisticRegression(random_state=0)\nclf = model.fit(X_train,y_train)\n\ntrain_score = clf.score(X_train, y_train)\ntest_score  = clf.score(X_test, y_test)\n\nprint(\"Logistic Regression Model: \")\nprint (\"Training Score: {}\\nTest Score: {}\" .format(train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d905c8bbd45899b8e578cda2429d457e0ad7b145"},"cell_type":"markdown","source":"### SVM"},{"metadata":{"trusted":true,"_uuid":"02a9e1f29da46854df9b97863dc19dd159229754"},"cell_type":"code","source":"#Fit SVM model - does not do as good probably bc SVM are better when there are alot of features \nmodel = LinearSVC(random_state=0)\nclf = model.fit(X_train,y_train)\n\ntrain_score = clf.score(X_train, y_train)\ntest_score  = clf.score(X_test, y_test)\n\nprint(\"SVM Model:\")\nprint (\"Training Score: {}\\nTest Score: {}\" .format(train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9096a8cbfd3085301f7034491584b9b9f1c107d3"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true,"_uuid":"6d68a613d240f54e82d046ebb33eb6c7c9669082"},"cell_type":"code","source":"#Fit Random Forest Model\nmodel = RandomForestRegressor(max_depth=5,n_estimators=30,random_state=0)\nclf = model.fit(X_train,y_train)\n\ntrain_score = clf.score(X_train, y_train)\ntest_score  = clf.score(X_test, y_test)\n\nprint(\"Random Forest Model:\")\nprint (\"Training Score: {}\\nTest Score: {}\" .format(train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"267f94f33c04c68dd6556247558c88853eba1ae9"},"cell_type":"markdown","source":"### Model Performance\nOverall, the K-NN with K = 3 results in the best performance with a test accuracy of ~87%. Not bad for an initial model with raw data. Logistic Regression also provides decent results with a test accuracy of ~81%.\n\nNaive Bayes is somewhat unstable because it is based off the assumptions that the features are independent whereas half of these features are highly correlated. Nevertheless, we are able to achieve a test accuracy of ~75%. Similarly, our SVC model achieves a test accuracy of ~77%. However, SVMs tend to perform better when there are many features.\n\nFinally, the random forest classifier seems to perform the worst with a test accuracy of ~46%. From the training accuracy of ~88%, we can see that the model is overfitting to the data. "},{"metadata":{"_uuid":"52b93372058d7c5c780ee7dc162746828b7ac441"},"cell_type":"markdown","source":"### What's next:\n1. Feature engineering. Feature selection? http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n2. Outlier removal?\n3. What are the most important variables? \n4. Scoliosis Regression\n5. Model tuning & selection"},{"metadata":{"_uuid":"717077f4c2e10c6b2b642dd1110d2e313744b266"},"cell_type":"markdown","source":"# Resources\n[1] Spine Anatomy - https://managebackpain.com/articles/spine-anatomy\n\n[2] Pelvic parameters: origin and significance - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3175921/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}