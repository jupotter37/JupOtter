{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WqlmAQMRjdX"
      },
      "source": [
        "# **Machine Learning - Assignment 7**\n",
        "\n",
        "*These lab assignments are new in the Machine and Deep Learning course. We'd like to hear what you think!*\n",
        "\n",
        "*Please post any feedback you have on Brightspace. Thanks!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uP97XspRjdc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-6-QwU6Rjdc"
      },
      "source": [
        "## Introduction - Complexity in Machine Learning\n",
        "\n",
        "In this assignment, you will learn about the support vector classifier and how to kernelize a nearest mean classifier. The support vector classifier is a powerful tool for classification tasks, and it is based on the idea of finding the hyperplane that maximizes the margin between the classes. The kernelized nearest mean classifier is a simple classifier that can be used to classify data points based on their distance to the mean of each class.\n",
        "\n",
        "**Prerequisites:**\n",
        "* Basic working knowledge of multivariate statistics and linear algebra\n",
        "* Basic knowledge of Python and Numpy. Recommended tutorial for Python and Numpy [here](https://cs231n.github.io/python-numpy-tutorial/).\n",
        "\n",
        "**Learning objectives:**\n",
        "* Should know the fundament of the support vector classifier\n",
        "* Should be able to kernelize a nearest mean classifier\n",
        "\n",
        "**Exercises types:**\n",
        "* **Pen \\& Paper** - Some exercises will ask you to write down mathematical derivations, calculations, explanations, or simple plots and representations. You can perform these exercises on paper or using a LaTeX editor.\n",
        "* **Coding** - Some exercises will ask you to write Python code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgk8LOjQRjdd"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzFjep0eRjde",
        "outputId": "ed3f41b1-4a0f-4fed-b439-9db95da4de33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'prtools'...\n",
            "remote: Enumerating objects: 1119, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 1119 (delta 264), reused 259 (delta 252), pack-reused 839 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1119/1119), 1.97 MiB | 23.99 MiB/s, done.\n",
            "Resolving deltas: 100% (747/747), done.\n",
            "Obtaining file:///content/prtools\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prtools==1.2.1) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prtools==1.2.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prtools==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->prtools==1.2.1) (1.16.0)\n",
            "Installing collected packages: prtools\n",
            "  Running setup.py develop for prtools\n",
            "Successfully installed prtools-1.2.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Necessary libraries\n",
        "\n",
        "!git clone https://github.com/DMJTax/prtools.git\n",
        "!pip install -e prtools\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahmVM7QwRjdg"
      },
      "source": [
        "## 1 - The Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcjvXif5Rjdg"
      },
      "source": [
        "### **Exercise 1.1** (Pen & Paper)\n",
        "\n",
        "Consider the following 2D two-class data set. Class one contains two points:\n",
        "$\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$. Class two has a single data point: $\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ifMkGYlRjdh"
      },
      "source": [
        "(a) Determine the classifier that maximizes the margin on this classification problem, using a graphical/geometrical reasoning (probably you cannot do the minimization of the support vector error function by hand). How many support vector are obtained?."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ZtLSl4Rjdh"
      },
      "source": [
        "(b) Shift the first point above, $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, to $\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$. How does the new maximum margin classifier look? What happened to the number of support vectors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ6vsGvlRjdh"
      },
      "source": [
        "### **Exercise 1.2** (Pen & Paper)\n",
        "\n",
        "Demonstrate, possibly graphically/geometrically, that the support vector classifier is sensitive to feature scaling. Hint: this can be done in 2D based on a training set of size 3 and a single test point.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uprp_Vd5Rjdh"
      },
      "source": [
        "### **Exercise 1.3** (Pen & Paper)\n",
        "\n",
        "Study, again, small data sets in 2D, for instance those from 5.1(a) and 5.2, or generate one yourself and compare the solution of the fisherc classifier to those obtained by means of an SVM. In what cases do they differ? Do you see the pattern?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6P3FitRjdi"
      },
      "source": [
        "## 2 - The Nonlinear Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHRJAXnRjdi"
      },
      "source": [
        "### **Exercise 2.1** (Pen & Paper)\n",
        "\n",
        "(a) Assume we have two objects, represented by 1-dimensional feature vectors $x$ and $\\chi$. Find a feature mapping $\\phi$ that leads to the inner product $\\exp(- (x - \\chi)^2)$. Hints: expand the term $-(x - \\chi)^2$ and write $\\exp(2x\\chi)$ as a series based on the Taylor series of the exponential.\n",
        "\n",
        "(b) What is the dimensionality of the space that $\\phi$ maps a 1-dimensional feature vector to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d8X59ZORjdi"
      },
      "source": [
        "### **Exercise 2.2** (Pen & Paper)\n",
        "\n",
        "(a) Express the squared distance to any class mean in terms of regular inner products between the test point $x$ and, say, the $N_C$ samples $x^C_{i}$ from class $C$.\n",
        "\n",
        "(b) Kernelize the nearest mean classifier by means of the Gaussian kernel, $K(x, \\chi) = \\exp\\left(-\\frac{\\|x - \\chi\\|^2}{2\\sigma^2}\\right)$. Can you show that this boils down to something like a Parzen classifier? You may limit yourself to the two-class case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut9gpvcTRjdi"
      },
      "source": [
        "### **Exercise 2.3** (Coding)\n",
        "The function svc can be used to both construct linear and non- linear support vector machines. The following kernels K are defined:\n",
        "\n",
        "* 'linear' -> linear kernel (default)\n",
        "* 'poly' ->  polynomial kernel with degree par\n",
        "* 'rbf' -> RBF or Gaussian kernel with width pa\n",
        "\n",
        "To define the kernel in svc using the prtools library, supply a second input argument with a list of kernel type, kernel parameter, and tradeoff parameter C: svc(a,(kernel type,par,C)).\n",
        "\n",
        "(a) On a = gendatb([20,20]), train an svc with a ’rbf’ kernel, i.e., the Gaussian kernel, for kernel widths that vary from fairly small (0.1?) to fairly large (10?). Check with a large (enough) independent banana test set how the performance varies for the different choices of kernel widths.\n",
        "\n",
        "(b) How does the kernel width of parzenc relate to the width of the radial basis function?\n",
        "\n",
        "(c) Why can the svc, potentially, perform much faster at test time than the Parzen classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUQ30g0FRjdj"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}