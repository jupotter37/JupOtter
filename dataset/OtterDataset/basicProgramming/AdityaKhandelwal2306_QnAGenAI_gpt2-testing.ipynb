{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name=\"gpt2\"):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        )\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        model.to(device)\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model and tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, options: str, correct_answer: str, explanation: str) -> str:\n",
    "    try:\n",
    "        return f\"\"\"\n",
    "        Original Question: {question}\n",
    "        Options: {options}\n",
    "        Correct Answer: {correct_answer}\n",
    "        Explanation: {explanation}\n",
    "\n",
    "        Generate 4 distinct questions based on the original question. For each question:\n",
    "        - Provide 4 options (one correct, three incorrect).\n",
    "        - Clearly label the correct answer and provide an explanation.\n",
    "        Output format:\n",
    "        1. Question: ...\n",
    "        Options: a) ... b) ... c) ... d) ...\n",
    "        Correct Answer: ...\n",
    "        Explanation: ...\n",
    "        \"\"\"\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error building prompt: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distinct_questions(model, tokenizer, prompt: str) -> List[str]:\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs.get(\"attention_mask\"),\n",
    "                max_length=512,\n",
    "                num_return_sequences=4,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error generating distinct questions: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generated_output(generated_text: str) -> List[dict]:\n",
    "    try:\n",
    "        questions = []\n",
    "        blocks = generated_text.split(\"1. Question:\")[1:]\n",
    "        for block in blocks:\n",
    "            lines = block.strip().split(\"\\n\")\n",
    "            question = lines[0].replace(\"Question:\", \"\").strip()\n",
    "            options_line = next((line for line in lines if line.startswith(\"Options:\")), None)\n",
    "            correct_answer_line = next((line for line in lines if line.startswith(\"Correct Answer:\")), None)\n",
    "            explanation_line = next((line for line in lines if line.startswith(\"Explanation:\")), None)\n",
    "\n",
    "            if options_line and correct_answer_line and explanation_line:\n",
    "                options = options_line.replace(\"Options:\", \"\").strip()\n",
    "                correct_answer = correct_answer_line.replace(\"Correct Answer:\", \"\").strip()\n",
    "                explanation = explanation_line.replace(\"Explanation:\", \"\").strip()\n",
    "\n",
    "                questions.append({\n",
    "                    \"Generated Question\": question,\n",
    "                    \"Options\": options,\n",
    "                    \"Correct Answer\": correct_answer,\n",
    "                    \"Explanation\": explanation,\n",
    "                })\n",
    "        return questions\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error parsing generated output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_questions(input_data: pd.DataFrame, model, tokenizer) -> pd.DataFrame:\n",
    "    try:\n",
    "        required_columns = [\"Degree\", \"Role\", \"Section\", \"Proficiency Level\", \"Question\", \"Options\", \"Correct Answer\", \"Explanation\"]\n",
    "        for column in required_columns:\n",
    "            if column not in input_data.columns:\n",
    "                raise ValueError(f\"Missing required column: {column}\")\n",
    "\n",
    "        expanded_questions = []\n",
    "        for _, row in input_data.iterrows():\n",
    "            try:\n",
    "                prompt = build_prompt(\n",
    "                    row[\"Question\"], row[\"Options\"], row[\"Correct Answer\"], row[\"Explanation\"]\n",
    "                )\n",
    "                generated_texts = generate_distinct_questions(model, tokenizer, prompt)\n",
    "                for generated_text in generated_texts:\n",
    "                    parsed_questions = parse_generated_output(generated_text)\n",
    "                    for parsed_question in parsed_questions:\n",
    "                        expanded_questions.append({\n",
    "                            \"Degree\": row[\"Degree\"],\n",
    "                            \"Role\": row[\"Role\"],\n",
    "                            \"Section\": row[\"Section\"],\n",
    "                            \"Proficiency Level\": row[\"Proficiency Level\"],\n",
    "                            **parsed_question,\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {row.to_dict()}: {e}\")\n",
    "\n",
    "        return pd.DataFrame(expanded_questions)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error expanding questions: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset: Dataset, tokenizer, max_length: int = 512):\n",
    "    try:\n",
    "        def tokenize_function(examples):\n",
    "            inputs = tokenizer(\n",
    "                examples[\"Question\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            targets = tokenizer(\n",
    "                examples[\"Correct Answer\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "            return inputs\n",
    "\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        return tokenized_dataset.train_test_split(test_size=0.2)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error preprocessing dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(input_data: pd.DataFrame, tokenizer, model, output_dir: str):\n",
    "      try:\n",
    "          dataset = Dataset.from_pandas(input_data)\n",
    "          tokenized_dataset = preprocess_dataset(dataset, tokenizer)\n",
    "          data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "  \n",
    "          training_args = TrainingArguments(\n",
    "              output_dir=output_dir,\n",
    "              num_train_epochs=3,\n",
    "              per_device_train_batch_size=8,\n",
    "              per_device_eval_batch_size=8,\n",
    "              warmup_steps=500,\n",
    "              weight_decay=0.01,\n",
    "              logging_dir=\"./logs\",\n",
    "              logging_steps=10,\n",
    "              evaluation_strategy=\"epoch\",\n",
    "              fp16=torch.cuda.is_available(),\n",
    "          )\n",
    "  \n",
    "          trainer = Trainer(\n",
    "              model=model,\n",
    "              train_dataset=tokenized_dataset[\"train\"],\n",
    "              eval_dataset=tokenized_dataset[\"test\"],\n",
    "              tokenizer=tokenizer,\n",
    "              data_collator=data_collator,\n",
    "              args=training_args,\n",
    "          )\n",
    "  \n",
    "          trainer.train()\n",
    "          trainer.save_model(output_dir)\n",
    "          print(f\"Model fine-tuned and saved to {output_dir}\")\n",
    "      except Exception as e:\n",
    "          raise RuntimeError(f\"Error fine-tuning model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degree</th>\n",
       "      <th>Role</th>\n",
       "      <th>Section</th>\n",
       "      <th>Proficiency Level</th>\n",
       "      <th>Question</th>\n",
       "      <th>Options</th>\n",
       "      <th>Correct Answer</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B.Tech in Computer Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Computational Skills</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>What is the primary purpose of the pandas grou...</td>\n",
       "      <td>['To sort data', 'To split data into groups', ...</td>\n",
       "      <td>To split data into groups</td>\n",
       "      <td>The groupby() function splits the data into gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B.Tech in Computer Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Core Programming</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Which time complexity represents binary search?</td>\n",
       "      <td>['O(n)', 'O(log n)', 'O(n log n)', 'O(1)']</td>\n",
       "      <td>O(log n)</td>\n",
       "      <td>Binary search repeatedly divides the search sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B.Tech in Computer Science</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analysis</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>In a dataset with outliers which visualization...</td>\n",
       "      <td>['Simple line plot', 'Box plot with whiskers',...</td>\n",
       "      <td>Box plot with whiskers</td>\n",
       "      <td>Box plots show median, quartiles, and outliers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B.Sc. in Mathematics</td>\n",
       "      <td>Risk Analyst</td>\n",
       "      <td>Core Mathematical Subjects</td>\n",
       "      <td>Beginner</td>\n",
       "      <td>What is the variance of a constant?</td>\n",
       "      <td>['1', 'The constant value', '0', 'Undefined']</td>\n",
       "      <td>0</td>\n",
       "      <td>The variance measures spread around the mean. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B.Sc. in Mathematics</td>\n",
       "      <td>Risk Analyst</td>\n",
       "      <td>Applied Mathematics</td>\n",
       "      <td>Medium</td>\n",
       "      <td>In Value at Risk (VaR) calculation what confid...</td>\n",
       "      <td>['90%', '95%', '99%', '99.9%']</td>\n",
       "      <td>99%</td>\n",
       "      <td>99% is the standard confidence level for VaR i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Degree          Role                     Section  \\\n",
       "0  B.Tech in Computer Science  Data Analyst        Computational Skills   \n",
       "1  B.Tech in Computer Science  Data Analyst            Core Programming   \n",
       "2  B.Tech in Computer Science  Data Analyst               Data Analysis   \n",
       "3        B.Sc. in Mathematics  Risk Analyst  Core Mathematical Subjects   \n",
       "4        B.Sc. in Mathematics  Risk Analyst         Applied Mathematics   \n",
       "\n",
       "  Proficiency Level                                           Question  \\\n",
       "0          Beginner  What is the primary purpose of the pandas grou...   \n",
       "1            Medium    Which time complexity represents binary search?   \n",
       "2          Advanced  In a dataset with outliers which visualization...   \n",
       "3          Beginner                What is the variance of a constant?   \n",
       "4            Medium  In Value at Risk (VaR) calculation what confid...   \n",
       "\n",
       "                                             Options  \\\n",
       "0  ['To sort data', 'To split data into groups', ...   \n",
       "1         ['O(n)', 'O(log n)', 'O(n log n)', 'O(1)']   \n",
       "2  ['Simple line plot', 'Box plot with whiskers',...   \n",
       "3      ['1', 'The constant value', '0', 'Undefined']   \n",
       "4                     ['90%', '95%', '99%', '99.9%']   \n",
       "\n",
       "              Correct Answer  \\\n",
       "0  To split data into groups   \n",
       "1                   O(log n)   \n",
       "2     Box plot with whiskers   \n",
       "3                          0   \n",
       "4                        99%   \n",
       "\n",
       "                                         Explanation  \n",
       "0  The groupby() function splits the data into gr...  \n",
       "1  Binary search repeatedly divides the search sp...  \n",
       "2  Box plots show median, quartiles, and outliers...  \n",
       "3  The variance measures spread around the mean. ...  \n",
       "4  99% is the standard confidence level for VaR i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "file_path = \"../placement-questions-excel.csv\"\n",
    "input_data = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45/45 [00:00<00:00, 1635.74 examples/s]\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer. If Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\akhan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 36\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15\n",
      "  Number of trainable parameters = 124439808\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 5/15 [01:59<03:14, 19.50s/it]The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer. If Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 8\n",
      "                                              \n",
      " 33%|███▎      | 5/15 [02:05<03:14, 19.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.257019996643066, 'eval_runtime': 5.4028, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 0.37, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [04:05<01:47, 21.40s/it]The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer. If Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6686, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 10/15 [04:10<01:47, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.881877899169922, 'eval_runtime': 5.3767, 'eval_samples_per_second': 1.674, 'eval_steps_per_second': 0.372, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:46<00:00, 18.17s/it]The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer. If Role, Options, Degree, Question, Section, Explanation, Proficiency Level, Correct Answer are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9\n",
      "  Batch size = 8\n",
      "                                               \n",
      "100%|██████████| 15/15 [05:51<00:00, 18.17s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 15/15 [05:52<00:00, 23.47s/it]\n",
      "Saving model checkpoint to fine_tuned_gpt2_model\n",
      "Configuration saved in fine_tuned_gpt2_model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.235095977783203, 'eval_runtime': 5.4073, 'eval_samples_per_second': 1.664, 'eval_steps_per_second': 0.37, 'epoch': 3.0}\n",
      "{'train_runtime': 352.1129, 'train_samples_per_second': 0.307, 'train_steps_per_second': 0.043, 'train_loss': 9.48508809407552, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in fine_tuned_gpt2_model\\pytorch_model.bin\n",
      "tokenizer config file saved in fine_tuned_gpt2_model\\tokenizer_config.json\n",
      "Special tokens file saved in fine_tuned_gpt2_model\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned and saved to fine_tuned_gpt2_model\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model_dir = \"fine_tuned_gpt2_model\"\n",
    "tuned_model=fine_tune_model(input_data, tokenizer, model, output_dir=fine_tuned_model_dir)\n",
    "tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.expand_questions(input_data: pandas.core.frame.DataFrame, model, tokenizer) -> pandas.core.frame.DataFrame>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_questions = expand_questions(input_data, model, tokenizer)\n",
    "expanded_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded questions saved to expanded_questions.csv\n"
     ]
    }
   ],
   "source": [
    "expanded_questions.to_csv(\"expanded_questions.csv\", index=False)\n",
    "print(\"Expanded questions saved to expanded_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
