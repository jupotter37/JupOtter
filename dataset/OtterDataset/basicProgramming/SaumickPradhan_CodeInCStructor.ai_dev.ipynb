{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m290.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m323.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.14.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m380.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m691.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m430.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m305.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.6-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m243.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m416.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m218.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, sniffio, pydantic-core, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.2.0 certifi-2023.11.17 distro-1.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 idna-3.6 openai-1.7.2 pydantic-2.5.3 pydantic-core-2.14.6 sniffio-1.3.0 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      4\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-e31CdGNZra62pEFq31H6T3BlbkFJqi4xRZSPzpfrsAPEVNQb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a coding assignment creator, assignment grader, feedback provider and coding tutorial platform.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello ChatGPT, does this work?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a coding assignment creator, assignment grader, feedback provider and coding tutorial platform.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello ChatGPT, does this work?\"}\n",
    "  ],\n",
    "  temperature=0.5,\n",
    "  max_tokens=10\n",
    "  )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Coding Tutorial Chatbot!\n"
     ]
    },
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_solution)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m code_solution \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_solution)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mgenerate_code\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_code\u001b[39m(query):\n\u001b[1;32m      6\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode a solution for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-davinci-003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'Ysk-e31CdGNZra62pEFq31H6T3BlbkFJqi4xRZSPzpfrsAPEVNQb'\n",
    "\n",
    "def generate_code(query):\n",
    "    prompt = f\"Code a solution for: {query}\\n\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=4\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to Coding Tutorial Chatbot!\")\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        code_solution = generate_code(user_query)\n",
    "        print(\"Chatbot:\", code_solution)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exercise():\n",
    "    # Prompt for exercise generation\n",
    "    prompt = \"Generate a unique Python coding exercise related to \"\n",
    "\n",
    "    # Request completion from OpenAI GPT-3.5 Turbo\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=200,  # Adjust as needed\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract and return the generated exercise\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-3.3.2 requests-2.31.0 urllib3-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to OpenAI API...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 92\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# with open('embeddings.json', 'r') as f:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#     embeddings_dict = json.load(f)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# print(f\"Annoy index loaded: {annoy_index.get_n_items()} items\")\u001b[39;00m\n\u001b[1;32m     89\u001b[0m conversation_history \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a coding Teaching Assistant to teach python programming to first year college students. You will review student submissions and provide feedback to their response without givng just a hint.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m---> 92\u001b[0m \u001b[43minteract_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is my code: pRint(dsds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36minteract_with_gpt\u001b[0;34m(user_input, conversation_history)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending request to OpenAI API...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# app.logger.info(\"Sending request to OpenAI API...\")\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mdata, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived response from OpenAI API\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# app.logger.info(\"Received response from OpenAI\")\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def interact_with_gpt(user_input, conversation_history):\n",
    "    context = \"\"\n",
    "    # most_similar_files = [] \n",
    "    # if embeddings_lookup_enabled:\n",
    "    #     app.logger.info(f\"Searching in embeddings for user input: {user_input}\")\n",
    "    #     processed_user_input = preprocess_code(user_input, 'py')\n",
    "    #     user_input_embeddings = generate_embeddings_for_code(processed_user_input)\n",
    "    #     most_similar_files = find_most_similar_files(annoy_index, index_map, user_input_embeddings)\n",
    "\n",
    "    #     for file_path in most_similar_files:\n",
    "    #         with open(file_path, 'r') as f:\n",
    "    #             code_snippet = f.read()[:500]  # Limit the length of the code snippet\n",
    "    #             context += f\"\\n\\n--- {file_path} ---\\n{code_snippet}\\n\"\n",
    "    #     relevant_code_files = ', '.join([f\"{i+1}. {file}\" for i, file in enumerate(most_similar_files)])\n",
    "    #     conversation_history.append({\"role\": \"user\", \"content\": f\"Relevant code files: {relevant_code_files}\"})\n",
    "    # else:\n",
    "    #     app.logger.info(f\"Embeddings lookup disabled for user input: {user_input}\")\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": context})\n",
    "\n",
    "    # token_usage = calculate_token_count(conversation_history)\n",
    "    # token_limit = 8000\n",
    "\n",
    "    # if token_usage >= token_limit:\n",
    "    #     app.logger.warning(\"Token limit reached. Clearing conversation history and asking user to rephrase the query.\")\n",
    "    #     conversation_history.clear()\n",
    "    #     conversation_history.append({\"role\": \"system\", \"content\": \"You are a GPT-4 coding assistant helping with code.\"})\n",
    "    #     assistant_response = \"Warning: Conversation history cleared due to reaching the token limit. Please rephrase your query.\"\n",
    "    #     conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    #     return {\"response\": assistant_response, \"token_usage\": 0}\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 120,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "    }\n",
    "\n",
    "    print(\"Sending request to OpenAI API...\")\n",
    "    # app.logger.info(\"Sending request to OpenAI API...\")\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", json=data, headers=headers)\n",
    "    print(\"Received response from OpenAI API\")\n",
    "    # app.logger.info(\"Received response from OpenAI\")\n",
    "                    \n",
    "    response_data = response.json()\n",
    "\n",
    "    print(\"Conversation history:\", conversation_history)\n",
    "    print(\"Response data:\", response_data)\n",
    "\n",
    "    if response_data.get(\"choices\") and response_data[\"choices\"][0].get(\"message\"):\n",
    "        assistant_response = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        # # Add the content of the most similar files to the response dictionary\n",
    "        # most_similar_file_contents = []\n",
    "        # for file_path in most_similar_files:\n",
    "        #     with open(file_path, 'r') as f:\n",
    "        #         file_content = f.read()\n",
    "        #         most_similar_file_contents.append(file_content)\n",
    "\n",
    "        #         print(\"Assistant response:\", assistant_response)  # Debug print\n",
    "        #         result = {\"response\": assistant_response, \"token_usage\": token_usage, \"most_similar_files\": most_similar_files, \"most_similar_file_contents\": most_similar_file_contents}\n",
    "        #         return result\n",
    "\n",
    "        result = {\"response\": assistant_response}#, \"token_usage\": token_usage, \"most_similar_files\": most_similar_files, \"most_similar_file_contents\": most_similar_file_contents}\n",
    "        return result     \n",
    "\n",
    "\n",
    "\n",
    "# with open('embeddings.json', 'r') as f:\n",
    "#     embeddings_dict = json.load(f)\n",
    "\n",
    "# with open('index_map.json', 'r') as f:\n",
    "#     index_map = json.load(f)\n",
    "\n",
    "# annoy_index = AnnoyIndex(768, 'angular')  # 768 is the embedding dimension\n",
    "# annoy_index.load('embeddings.ann')\n",
    "\n",
    "# print(f\"Annoy index loaded: {annoy_index.get_n_items()} items\")\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a coding Teaching Assistant to teach python programming to first year college students. You will review student submissions and provide feedback to their response without givng just a hint.\"}]\n",
    "\n",
    "\n",
    "interact_with_gpt(\"Here is my code: pRint(dsds')\", conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     72\u001b[0m topic_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Python function that calculates the factorial of a given number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m solutions \u001b[38;5;241m=\u001b[39m generate_solutions(questions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mgenerate_assignment\u001b[0;34m(topic_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate three Python coding questions based on the following topic description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtopic_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate questions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m question1 \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate second question\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/CodeInCStructor.ai/.conda/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize OpenAI API with your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_assignment(topic_description):\n",
    "    prompt = f\"Create three Python coding questions based on the following topic description:\\n\\n{topic_description}\\n\\n1)\"\n",
    "    \n",
    "    # Generate questions\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"2)\"]\n",
    "    )\n",
    "    \n",
    "    question1 = response.choices[0].text.strip()\n",
    "    \n",
    "    # Generate second question\n",
    "    prompt += f\"\\n\\n2)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"3)\"]\n",
    "    )\n",
    "    question2 = response.choices[0].text.strip()\n",
    "    \n",
    "    # Generate third question\n",
    "    prompt += f\"\\n\\n3)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"gpt-3.5-turbo\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"4)\"]\n",
    "    )\n",
    "    question3 = response.choices[0].text.strip()\n",
    "    \n",
    "    return [question1, question2, question3]\n",
    "\n",
    "def generate_solutions(questions):\n",
    "    solutions = []\n",
    "    for question in questions:\n",
    "        # Generate solutions for each question\n",
    "        prompt = f\"Write a solution for the following Python coding question:\\n\\n{question}\\n\\nSolution:\"\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"gpt-3.5-turbo\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"\\n\\n\"]\n",
    "        )\n",
    "        solutions.append(response.choices[0].text.strip())\n",
    "    return solutions\n",
    "\n",
    "# Example usage\n",
    "topic_description = \"Write a Python function that calculates the factorial of a given number.\"\n",
    "questions = generate_assignment(topic_description)\n",
    "solutions = generate_solutions(questions)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "\n",
    "print(\"\\nGenerated Solutions:\")\n",
    "for i, solution in enumerate(solutions):\n",
    "    print(f\"Solution {i+1}: {solution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'chatCompletion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     72\u001b[0m topic_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a Python function that calculates the factorial of a given number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m solutions \u001b[38;5;241m=\u001b[39m generate_solutions(questions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mgenerate_assignment\u001b[0;34m(topic_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate three Python coding questions based on the following topic description:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtopic_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Generate questions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchatCompletion\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     11\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-davinci-003\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     13\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     14\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     16\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     17\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     18\u001b[0m     stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2)\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m question1 \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate second question\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'chatCompletion'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize OpenAI API with your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_assignment(topic_description):\n",
    "    prompt = f\"Create three Python coding questions based on the following topic description:\\n\\n{topic_description}\\n\\n1)\"\n",
    "\n",
    "    # Generate questions\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"2)\"]\n",
    "    )\n",
    "\n",
    "    question1 = response.choices[0].text.strip()\n",
    "\n",
    "    # Generate second question\n",
    "    prompt += f\"\\n\\n2)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"3)\"]\n",
    "    )\n",
    "    question2 = response.choices[0].text.strip()\n",
    "\n",
    "    # Generate third question\n",
    "    prompt += f\"\\n\\n3)\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=150,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"4)\"]\n",
    "    )\n",
    "    question3 = response.choices[0].text.strip()\n",
    "\n",
    "    return [question1, question2, question3]\n",
    "\n",
    "def generate_solutions(questions):\n",
    "    solutions = []\n",
    "    for question in questions:\n",
    "        # Generate solutions for each question\n",
    "        prompt = f\"Write a solution for the following Python coding question:\\n\\n{question}\\n\\nSolution:\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=100,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"\\n\\n\"]\n",
    "        )\n",
    "        solutions.append(response.choices[0].text.strip())\n",
    "    return solutions\n",
    "\n",
    "# Example usage\n",
    "topic_description = \"Write a Python function that calculates the factorial of a given number.\"\n",
    "questions = generate_assignment(topic_description)\n",
    "solutions = generate_solutions(questions)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "\n",
    "print(\"\\nGenerated Solutions:\")\n",
    "for i, solution in enumerate(solutions):\n",
    "    print(f\"Solution {i+1}: {solution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested loops\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m solutions \u001b[38;5;241m=\u001b[39m generate_solutions(topic)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Questions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mgenerate_questions\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m      7\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs a Teaching Assistant for CS 101, I need to generate three well-formatted Python coding questions based on the topic \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m query(payload)\n\u001b[0;32m---> 11\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m questions\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"As a Teaching Assistant for CS 101, I need to generate three well-formatted Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    questions = response[\"generated_text\"].split(\"\\n\\n\")\n",
    "    return questions\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "Question 1: You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'nested loops'.\n",
      "Question 2: Topic: Nested Loops\n",
      "Question 3: Question:\n",
      "Question 4: Write a Python program to implement a nested loop using two for-loops (one for each nesting) hanging inside two other for-loops. Iterate over the numbers 1 to 10 with both loops. Use the formula `sum = n * ( n+1 ) / 2` to calculate the sum-of-integers in each nested loop iteration. Print the sum for each nested loop.\n",
      "\n",
      "Solutions:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "# Question 1\n",
      "def factorial(n):\n",
      "    if not n:\n",
      "        return 1\n",
      "    else:\n",
      "        fact = 1\n",
      "        for i in range(n, 0, -1):\n",
      "            fact *= i\n",
      "        return fact\n",
      "\n",
      "# Call the function to check if it's working\n",
      "print(factorial(5))\n",
      "\n",
      "# Question 2\n",
      "def find_kth_smaller_than_n(n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'nested loops'.\n",
      "Topic: Nested Loops\n",
      "Question:\n",
      "Write a Python program that uses nested loops to calculate the sum of the squares of the first 10 positive integers.\n",
      "Answer:\n",
      "```python\n",
      "# Initialize the sum of squares\n",
      "sum_of_squares = 0\n",
      "# Outer loop for the first 10 positive integers\n",
      "for i in range(1, 11):\n",
      "    # In\n",
      "\n",
      "Solution:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "1. Write a Python program that uses nested loops to print the multiplication table for numbers 1 through 10.\n",
      "\n",
      "```python\n",
      "for i in range(1, 11):\n",
      "    for j in range(1, 11):\n",
      "        print(f\"{i} x {j} = {i*j}\")\n",
      "    print()\n",
      "```\n",
      "\n",
      "2. Write a Python program that uses nested loops to find the sum of\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    # Placeholder implementation\n",
    "    # This function should make a request to the API and return the response\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(question)\n",
    "\n",
    "print(\"\\nSolution:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Try again or ask for a hint.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide hint or feedback based on the student's solution\n",
    "        # You can implement your own logic here based on the requirements\n",
    "        # For simplicity, let's assume providing a generic feedback\n",
    "        return False, \"Your solution is incorrect. Try again or ask for a hint.\"\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Here's a hint: \n",
      "  if n == 0\n",
      "    1\n",
      "  else\n",
      "    n * factorial(n-1)\n",
      "  end\n",
      "end\n",
      "\n",
      "def factorial_iterative(n)\n",
      "  result = 1\n",
      "  while n > 0\n",
      "    result *= n\n",
      "    n -= 1\n",
      "  end\n",
      "  result\n",
      "end\n",
      "\n",
      "def factorial_recursive(n)\n",
      "  if n == 0\n",
      "    1\n",
      "  else\n",
      "    n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide hint using OpenChat API\n",
    "        hint = generate_hint(student_solution)\n",
    "        return False, \"Your solution is incorrect. Here's a hint: \" + hint\n",
    "\n",
    "def generate_hint(student_solution):\n",
    "    # Generate hint based on the student's solution using the OpenChat API\n",
    "    # We'll simply append the student's solution with a request for a hint\n",
    "    hint_request = \"Can you provide a hint for improving my solution?\\n\\n\" + student_solution\n",
    "    hint_output = query({\"inputs\": hint_request})\n",
    "    hint = hint_output[0]['generated_text']\n",
    "    return hint\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number.\n",
      "\n",
      "The factorial of a number is the product of all positive integers less than or equal to that number. For example, the factorial of 5 is 5*4*3*2*1 = 120.\n",
      "\n",
      "We need a function that takes an integer as input and returns the factorial of that number.\n",
      "\n",
      "Here is a Python code that calculates the factorial of a number:\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "Your solution is incorrect. Here's a hint based on your response:\n",
      "Can you provide a hint for improving my solution?\n",
      "\n",
      "def factorial():\n",
      "    n = 1\n",
      "    for i in range(1, 6):\n",
      "        n *= i\n",
      "    return n\n",
      "\n",
      "def factorial_iterative(n):\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "\n",
      "def factorial_recursive(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n *\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide tailored hint using OpenChat API\n",
    "        hint = generate_hint(student_solution)\n",
    "        return False, \"Your solution is incorrect. Here's a hint based on your response:\\n\" + hint\n",
    "\n",
    "def generate_hint(student_solution):\n",
    "    # Generate a tailored hint based on the student's solution using the OpenChat API\n",
    "    # We'll use the student's solution to ask for a hint\n",
    "    hint_request = \"Can you provide a hint for improving my solution?\\n\\n\" + student_solution\n",
    "    hint_output = query({\"inputs\": hint_request})\n",
    "    # Extract the hint from the response\n",
    "    hint = hint_output[0]['generated_text']\n",
    "    return hint\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /Users/saumickpradhan/Desktop/CodeInCStructor.ai/.conda/bin/python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question for students:\n",
      "Can you please let us know more details about your Write a function to calculate the factorial of a number. mathematical problem to be solved?\n",
      "\n",
      "Any original mathematical problems that might require us to help with our \"Write a function to calculate the factorial of a number\" ability?\n",
      "\n",
      "I have a question to ask about our \"Write a function to calculate the factorial of a number\" capability.\n",
      "\n",
      "We would appreciate it if you could clarify your inquiry regards to \"Write a function to calculate the factorial of a number\" in more detail.\n",
      "\n",
      "Please be more specific in\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feedback)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m student_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your solution: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 5. Platform checks student's attempt\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m is_correct, feedback \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_solution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_solution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(feedback)\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mcheck_solution\u001b[0;34m(student_solution, correct_solution)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCongratulations! Your solution is correct.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Provide a tailored hint based on the student's solution\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     hint \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_hint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_solution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour solution is incorrect. Here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a hint: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m hint\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mgenerate_hint\u001b[0;34m(student_solution)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_hint\u001b[39m(student_solution):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Generate a tailored hint based on the student's solution\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# We'll use OpenChat API to generate a hint based on the student's response\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     hint_output \u001b[38;5;241m=\u001b[39m query({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: student_solution})\n\u001b[0;32m---> 32\u001b[0m     hint \u001b[38;5;241m=\u001b[39m \u001b[43mhint_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hint\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def generate_question(description):\n",
    "    # Generate question based on the description using the OpenChat API\n",
    "    output = query({\n",
    "        \"inputs\": \"Can you please let us know more details about your \" + description,\n",
    "    })\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = output[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "def check_solution(student_solution, correct_solution):\n",
    "    # Check if student's solution matches the correct solution\n",
    "    if student_solution == correct_solution:\n",
    "        return True, \"Congratulations! Your solution is correct.\"\n",
    "    else:\n",
    "        # Provide a tailored hint based on the student's solution\n",
    "        hint = generate_hint(student_solution)\n",
    "        return False, \"Your solution is incorrect. Here's a hint: \" + hint\n",
    "\n",
    "def generate_hint(student_solution):\n",
    "    # Generate a tailored hint based on the student's solution\n",
    "    # We'll use OpenChat API to generate a hint based on the student's response\n",
    "    hint_output = query({\"inputs\": student_solution})\n",
    "    hint = hint_output[0]['generated_text']\n",
    "    return hint\n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # 1. Professor inputs description\n",
    "    professor_description = \"Write a function to calculate the factorial of a number.\"\n",
    "    # 2. Platform generates question\n",
    "    question = generate_question(professor_description)\n",
    "    print(\"Question for students:\")\n",
    "    print(question)\n",
    "    # 3. Platform already has the question and answer stored\n",
    "    correct_solution = \"def factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\"\n",
    "    # 4. Student provides attempt/solution\n",
    "    student_solution = input(\"Enter your solution: \")\n",
    "    # 5. Platform checks student's attempt\n",
    "    is_correct, feedback = check_solution(student_solution, correct_solution)\n",
    "    print(feedback)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'nested loops'.\n",
      "Research the given topic and prepare a pool of multiple-choice questions and their correct answers that test students' understanding of the topic. Here is the list of questions and their correct answers:\n",
      "Question 1:\n",
      "What is the starting index of a for loop using ascending order?\n",
      "A) 0\n",
      "B) -1\n",
      "C) 1\n",
      "D) None\n",
      "Answer: A) 0\n",
      "Question 2:\n",
      "What is the purpose\n",
      "\n",
      "Solution:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "Solution:\n",
      "\n",
      "    Question 1:\n",
      "    Given the following nested loops:\n",
      "\n",
      "    inner = [[1, 2, 3], [4, 5, 6]]\n",
      "\n",
      "    outer = [['inner'], ['inner2'], ['inner3']]\n",
      "\n",
      "    Write a Python program that prints the list of names in ascending order.\n",
      "\n",
      "    Output:  inner2, inner, inner3 (The order can differ)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    # Placeholder implementation\n",
    "    # This function should make a request to the API and return the response\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(question)\n",
    "\n",
    "print(\"\\nSolution:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "Question 1: You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'nested loops'.\n",
      "Question 2: Research the given topic and prepare a pool of multiple-choice questions and their correct answers that test students' understanding of the topic. Here is the list of questions and their correct answers:\n",
      "Question 3: Question 1:\n",
      "What is the starting index of a for loop using ascending order?\n",
      "A) 0\n",
      "B) -1\n",
      "C) 1\n",
      "D) None\n",
      "Question 4: Answer: A) 0\n",
      "Question 5: Question 2:\n",
      "What is the purpose\n",
      "\n",
      "Solutions:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "Solution:\n",
      "\n",
      "    Question 1:\n",
      "    Given the following nested loops:\n",
      "\n",
      "    inner = [[1, 2, 3], [4, 5, 6]]\n",
      "\n",
      "    outer = [['inner'], ['inner2'], ['inner3']]\n",
      "\n",
      "    Write a Python program that prints the list of names in ascending order.\n",
      "\n",
      "    Output:  inner2, inner, inner3 (The order can differ)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate a well-formatted Python coding question based on the topic and give its answer'{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")\n",
    "\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(solutions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "Question 1: # Nested Loops\n",
      "Question 2: Nested loops are loops inside of loops. They are applicable when you want to perform the same, or similar, actions repeatedly and iteratively, but still want to take alternate steps based on conditions or index positions.\n",
      "Question 3: In Python, nested loops can be a very powerful tool when sorting, searching, and iterative computation.\n",
      "\n",
      "Solutions:\n",
      "Provide solutions for Python coding questions based on the topic 'nested loops'.\n",
      "\n",
      "##### Q1) Combining Nested Loops\n",
      "-------------------------\n",
      "Combined output for the following code snippets - \n",
      "\n",
      "```python\n",
      "# Snippet 1:\n",
      "for i in range(6):\n",
      "    print('*' * i)\n",
      "\n",
      "# Snippet 2:\n",
      "for j in range(6, 0, -1):\n",
      "    print('*' * j)\n",
      "\n",
      "# Snippet 3:\n",
      "for i\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "\n",
    "def generate_questions(topic):\n",
    "    prompt = f\"You are a Teaching Assistant chat bot for CS 101. I will enter the topic I want to test the students on. You need to generate three well-formatted Python coding questions based on the topic: {topic}.\"\n",
    "    payload = {\"inputs\": prompt}\n",
    "    response = query(payload)\n",
    "    generated_text = response[0][\"generated_text\"]\n",
    "    return generated_text.split(\"\\n\\n\")[1:4]  # Get the first three generated questions\n",
    "def generate_solutions(topic):\n",
    "    payload = {\n",
    "        \"inputs\": f\"Provide solutions for Python coding questions based on the topic '{topic}'.\"\n",
    "    }\n",
    "    response = query(payload)\n",
    "    solutions = response[0][\"generated_text\"]\n",
    "    return solutions\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "topic = \"nested loops\"\n",
    "questions = generate_questions(topic)\n",
    "solutions = generate_solutions(topic)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Question {i}: {question}\")\n",
    "\n",
    "print(\"\\nSolutions:\")\n",
    "print(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.4/387.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from openai==0.28) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from requests>=2.20->openai==0.28) (1.26.15)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/saumickpradhan/tensorflow-test/env/lib/python3.10/site-packages (from aiohttp->openai==0.28) (22.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.13.3\n",
      "    Uninstalling openai-1.13.3:\n",
      "      Successfully uninstalled openai-1.13.3\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 openai-0.28.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Python Programming Question Generator!\n",
      "\n",
      "Choose an option:\n",
      "1. Train the engine\n",
      "2. Generate questions\n",
      "3. Exit\n",
      "Engine trained successfully.\n",
      "\n",
      "Choose an option:\n",
      "1. Train the engine\n",
      "2. Generate questions\n",
      "3. Exit\n",
      "Generated question: A tuple is a data structure in Python that is similar to a list, but is immutable, meaning it cannot be changed once it is created. Tuples are defined by enclosing values in parentheses, separated by commas. Tuples are often used to store related pieces of data together, and can be accessed by index just like lists. However, because tuples are immutable, they are often used in situations where the data should not be changed.\n",
      "\n",
      "Choose an option:\n",
      "1. Train the engine\n",
      "2. Generate questions\n",
      "3. Exit\n",
      "Exiting program...\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import traceback\n",
    "# import openai\n",
    "# # from program_generator_constants import ENGINE, JSON_LOG_FILE, DEBUG_CONSOLE_LOG\n",
    "# # from program_generator_utils import save_to_file, create_log_dir, wrap_content, show_message\n",
    "\n",
    "\n",
    "# class ProgramGeneratorEngine:\n",
    "#     def __init__(self):\n",
    "#         self.response = None\n",
    "#         self.q_nos = None\n",
    "#         self.file_path = None\n",
    "#         self.req_log = None\n",
    "#         openai.api_key = \"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\"\n",
    "#         self.engine = 'gpt-3.5-turbo'\n",
    "\n",
    "#     def train_engine(self):\n",
    "#         training_prompts = [\n",
    "#             {\"role\": \"system\", \"content\": \"Let's learn to code in Java and Python programming languages.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Generate 1 Beginner level programming question(s) in Python Tuples\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Write a Python program that creates a tuple of your favorite fruits and \"\n",
    "#                                              \"prints out each fruit in the tuple using a for loop.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Generate 1 Beginner level programming question(s) in Python Strings\"}\n",
    "#         ]\n",
    "#         params = {'model': self.engine,\n",
    "#                   'messages': training_prompts,\n",
    "#                   'temperature': 0.6,\n",
    "#                   'max_tokens': 200\n",
    "#                   }\n",
    "#         # try:\n",
    "#         training_response = openai.ChatCompletion.create(**params)\n",
    "#             # if DEBUG_CONSOLE_LOG:\n",
    "#             #     print(training_response)\n",
    "#         process_finish_reason(training_response.choices[0]['finish_reason'])\n",
    "#         return True\n",
    "#         # except openai.error.OpenAIError as error:\n",
    "#         #     print(f\"OpenAI Error: {error}\")\n",
    "#         #     return False\n",
    "#         # except Exception as error:\n",
    "#         #     print(f\"Unknown error sending initial OpenAI training data: {error}\")\n",
    "#         #     traceback.print_exc()\n",
    "#         #     return False\n",
    "\n",
    "#     def set_gpt_params(self, file_path, q_nos, req_log):\n",
    "#         self.file_path = file_path\n",
    "#         self.q_nos = q_nos\n",
    "#         self.req_log = req_log\n",
    "\n",
    "#     def get_max_tokens(self):\n",
    "#         return min((self.q_nos + 1) * 50, 500)\n",
    "\n",
    "#     def generate_questions(self, prompt):\n",
    "#         tokens = self.get_max_tokens()\n",
    "#         params = {'model': self.engine,\n",
    "#                   'messages': [{\"role\": \"user\",\n",
    "#                                 \"content\": prompt}],\n",
    "#                   'temperature': 0.6,\n",
    "#                   'max_tokens': tokens\n",
    "#                   }\n",
    "#         # try:\n",
    "#         self.response = openai.ChatCompletion.create(**params)\n",
    "#         # except openai.error.OpenAIError as error:\n",
    "#         #     print(f\"OpenAI Error: {error}\")\n",
    "#         #     return False\n",
    "#         # except Exception as error:\n",
    "#         #     print(f\"Error sending OpenAI request: {error}\")\n",
    "#         #     traceback.print_exc()\n",
    "\n",
    "#         if not self.response:\n",
    "#             print(\"GPT None response\")\n",
    "#             return False\n",
    "\n",
    "#         # try:\n",
    "#         #     self.parse_json_log()\n",
    "#         # except Exception as error:\n",
    "#         #     print(f\"Error parsing JSON response: {error}\")\n",
    "#         #     traceback.print_exc()\n",
    "\n",
    "#         process_finish_reason(self.response.choices[0]['finish_reason'])\n",
    "#         questions = self.response.choices[0]['message']['content']\n",
    "\n",
    "#         return questions\n",
    "#         # return save_to_file(self.file_path, wrap_content(questions))\n",
    "\n",
    "#     # def parse_json_log(self):\n",
    "#     #     create_log_dir()\n",
    "#     #     json_log = json.dumps(json.loads(str(self.response)), indent=4)\n",
    "#     #     req_log = wrap_content(self.req_log)\n",
    "#     #     out_file_path = f\"Destination file: {self.file_path}\"\n",
    "#     #     try:\n",
    "#     #         with open(JSON_LOG_FILE, 'a+') as fObj:\n",
    "#     #             fObj.write(f\"{req_log}\\n{json_log}\\n{out_file_path}\\n\\n\")\n",
    "#     #     except FileNotFoundError:\n",
    "#     #         print(\"Response log file not found\")\n",
    "#     #     except Exception as error:\n",
    "#     #         print(f\"Response log file Error: {error}\")\n",
    "#     #         traceback.print_exc()\n",
    "\n",
    "\n",
    "# def process_finish_reason(reason):\n",
    "#     if reason == 'length':\n",
    "#         print(\"Response possibly truncated due to max token limit\")\n",
    "#         #show_message(\"Warning\", \"Response possibly truncated due to max token limit\")\n",
    "#     elif reason == 'content_filter':\n",
    "#         print(\"Response omitted due to content flag\")\n",
    "#         #show_message(\"Warning\", \"Response omitted due to content flag\")\n",
    "#     elif reason == 'null':\n",
    "#         print(\"API response still in progress\")\n",
    "#         #show_message(\"Warning\", \"API response still in progress\")\n",
    "#     else:\n",
    "#         return\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     generator = ProgramGeneratorEngine()\n",
    "\n",
    "#     print(\"Welcome to the Python Programming Question Generator!\")\n",
    "#     while True:\n",
    "#         print(\"\\nChoose an option:\")\n",
    "#         print(\"1. Train the engine\")\n",
    "#         print(\"2. Generate questions\")\n",
    "#         print(\"3. Exit\")\n",
    "\n",
    "#         choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "#         if choice == \"1\":\n",
    "#             if generator.train_engine():\n",
    "#                 print(\"Engine trained successfully.\")\n",
    "#             else:\n",
    "#                 print(\"Failed to train the engine. Please check the logs for details.\")\n",
    "#         elif choice == \"2\":\n",
    "#             prompt = input(\"Enter the programming topic: \")\n",
    "#             file_path = input(\"Enter the file path to save questions: \")\n",
    "#             q_nos = int(input(\"Enter the number of questions to generate: \"))\n",
    "#             req_log = input(\"Enter any additional information for logging: \")\n",
    "\n",
    "#             generator.set_gpt_params(file_path, q_nos, req_log)\n",
    "#             if generator.generate_questions(prompt):\n",
    "#                 print(f\"Questions generated successfully and saved to {file_path}.\")\n",
    "#             else:\n",
    "#                 print(\"Failed to generate questions. Please check the logs for details.\")\n",
    "#         elif choice == \"3\":\n",
    "#             print(\"Exiting program...\")\n",
    "#             break\n",
    "#         else:\n",
    "#             print(\"Invalid choice. Please enter a valid option.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "import openai\n",
    "\n",
    "class ProgramGeneratorEngine:\n",
    "    def __init__(self):\n",
    "        self.response = None\n",
    "        self.q_nos = None\n",
    "        self.file_path = None\n",
    "        self.req_log = None\n",
    "        openai.api_key = \"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\"\n",
    "        self.engine = 'gpt-3.5-turbo'\n",
    "\n",
    "    def train_engine(self):\n",
    "        # training_prompts = [\n",
    "        #     {\"role\": \"system\", \"content\": \"You are a coding (programming) assignments generator for python beginners\"},\n",
    "        #     {\"role\": \"user\", \"content\": \"Generate 1 Beginner level programming question(s) in Python Tuples\"},\n",
    "        #     {\"role\": \"assistant\", \"content\": \"Write a Python program that creates a tuple of your favorite fruits and \"\n",
    "        #                                      \"prints out each fruit in the tuple using a for loop.\"},\n",
    "        #     {\"role\": \"user\", \"content\": \"Generate 1 Beginner level programming question(s) in Python Strings\"}\n",
    "        # ]\n",
    "\n",
    "        training_prompts = [{\"role\": \"system\", \"content\": \"You are a coding (programming) assignments generator for python beginners. Questions will be generated in natural language\"}]\n",
    "        \n",
    "        params = {'model': self.engine,\n",
    "                  'messages': training_prompts,\n",
    "                  'temperature': 0.6,\n",
    "                  'max_tokens': 100\n",
    "                  }\n",
    "        training_response = openai.ChatCompletion.create(**params)\n",
    "        process_finish_reason(training_response.choices[0]['finish_reason'])\n",
    "        return True\n",
    "\n",
    "    def set_gpt_params(self, file_path, q_nos, req_log):\n",
    "        self.file_path = file_path\n",
    "        self.q_nos = q_nos\n",
    "        self.req_log = req_log\n",
    "\n",
    "    def get_max_tokens(self):\n",
    "        return min((self.q_nos + 1) * 50, 500)\n",
    "\n",
    "    def generate_questions(self, prompt):\n",
    "        if self.q_nos is None:\n",
    "            print(\"Error: Number of questions is not set.\")\n",
    "            return False\n",
    "\n",
    "        tokens = self.get_max_tokens()\n",
    "        params = {'model': self.engine,\n",
    "                'messages': [{\"role\": \"user\",\n",
    "                                \"content\": prompt}],\n",
    "                'temperature': 0.6,\n",
    "                'max_tokens': tokens\n",
    "                }\n",
    "        self.response = openai.ChatCompletion.create(**params)\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response\")\n",
    "            return False\n",
    "\n",
    "        process_finish_reason(self.response.choices[0]['finish_reason'])\n",
    "        question = self.response.choices[0]['message']['content']\n",
    "\n",
    "        return question\n",
    "\n",
    "\n",
    "def process_finish_reason(reason):\n",
    "    if reason == 'length':\n",
    "        print(\"Response possibly truncated due to max token limit\")\n",
    "    elif reason == 'content_filter':\n",
    "        print(\"Response omitted due to content flag\")\n",
    "    elif reason == 'null':\n",
    "        print(\"API response still in progress\")\n",
    "    else:\n",
    "        return\n",
    "\n",
    "\n",
    "def main():\n",
    "    generator = ProgramGeneratorEngine()\n",
    "\n",
    "    print(\"Welcome to the Python Programming Question Generator!\")\n",
    "    while True:\n",
    "        print(\"\\nChoose an option:\")\n",
    "        print(\"1. Train the engine\")\n",
    "        print(\"2. Generate questions\")\n",
    "        print(\"3. Exit\")\n",
    "\n",
    "        choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            if generator.train_engine():\n",
    "                print(\"Engine trained successfully.\")\n",
    "            else:\n",
    "                print(\"Failed to train the engine. Please check the logs for details.\")\n",
    "        elif choice == \"2\":\n",
    "            prompt = input(\"Enter the programming topic: \")\n",
    "            q_nos = int(input(\"Enter the number of questions to generate: \"))\n",
    "            \n",
    "            generator.set_gpt_params(None, q_nos, None)  # Set file_path and req_log to None for now\n",
    "            generated_question = generator.generate_questions(prompt)\n",
    "            if generated_question:\n",
    "                print(f\"Generated question: {generated_question}\")\n",
    "            else:\n",
    "                print(\"Failed to generate question. Please check the logs for details.\")\n",
    "        elif choice == \"3\":\n",
    "            print(\"Exiting program...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a valid option.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Python Programming Question Generator!\n",
      "\n",
      "Choose an option:\n",
      "1. Train the engine\n",
      "2. Generate questions\n",
      "3. Exit\n",
      "Engine trained successfully.\n",
      "\n",
      "Choose an option:\n",
      "1. Train the engine\n",
      "2. Generate questions\n",
      "3. Exit\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid choice. Please enter a valid option.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 96\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m q_nos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the number of questions to generate: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     95\u001b[0m generator\u001b[38;5;241m.\u001b[39mset_gpt_params(\u001b[38;5;28;01mNone\u001b[39;00m, q_nos, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Set file_path and req_log to None for now\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m generated_question \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_question:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 59\u001b[0m, in \u001b[0;36mProgramGeneratorEngine.generate_questions\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     56\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Extract conversation history from the response\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m conversation_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history\u001b[38;5;241m.\u001b[39mextend(conversation_messages)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m question\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "class ProgramGeneratorEngine:\n",
    "    def __init__(self):\n",
    "        self.response = None\n",
    "        self.q_nos = None\n",
    "        self.file_path = None\n",
    "        self.req_log = None\n",
    "        self.engine = 'gpt-3.5-turbo'\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def train_engine(self):\n",
    "        training_prompts = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a coding (programming) assignments generator for python beginners. Questions will be generated in natural language\"}\n",
    "        ]\n",
    "        \n",
    "        params = {\n",
    "            'model': self.engine,\n",
    "            'messages': training_prompts,\n",
    "            'temperature': 0.6,\n",
    "            'max_tokens': 100\n",
    "        }\n",
    "\n",
    "        training_response = openai.ChatCompletion.create(**params)\n",
    "        process_finish_reason(training_response.choices[0]['finish_reason'])\n",
    "        return True\n",
    "\n",
    "    def set_gpt_params(self, file_path, q_nos, req_log):\n",
    "        self.file_path = file_path\n",
    "        self.q_nos = q_nos\n",
    "        self.req_log = req_log\n",
    "\n",
    "    def get_max_tokens(self):\n",
    "        return min((self.q_nos + 1) * 50, 500)\n",
    "\n",
    "    def generate_questions(self, prompt):\n",
    "        if self.q_nos is None:\n",
    "            print(\"Error: Number of questions is not set.\")\n",
    "            return False\n",
    "\n",
    "        tokens = self.get_max_tokens()\n",
    "        params = {\n",
    "            'model': self.engine,\n",
    "            'messages': self.conversation_history + [{\"role\": \"user\", \"content\": prompt}],\n",
    "            'temperature': 0.6,\n",
    "            'max_tokens': tokens\n",
    "        }\n",
    "\n",
    "        self.response = openai.ChatCompletion.create(**params)\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response\")\n",
    "            return False\n",
    "\n",
    "        process_finish_reason(self.response.choices[0]['finish_reason'])\n",
    "        question = self.response.choices[0]['message']['content']\n",
    "\n",
    "        # Extract conversation history from the response\n",
    "        conversation_messages = self.response.choices[0]['message']['content']['messages']\n",
    "        self.conversation_history.extend(conversation_messages)\n",
    "        \n",
    "        return question\n",
    "\n",
    "def process_finish_reason(reason):\n",
    "    if reason == 'length':\n",
    "        print(\"Response possibly truncated due to max token limit\")\n",
    "    elif reason == 'content_filter':\n",
    "        print(\"Response omitted due to content flag\")\n",
    "    elif reason == 'null':\n",
    "        print(\"API response still in progress\")\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    generator = ProgramGeneratorEngine()\n",
    "\n",
    "    print(\"Welcome to the Python Programming Question Generator!\")\n",
    "    while True:\n",
    "        print(\"\\nChoose an option:\")\n",
    "        print(\"1. Train the engine\")\n",
    "        print(\"2. Generate questions\")\n",
    "        print(\"3. Exit\")\n",
    "\n",
    "        choice = input(\"Enter your choice (1/2/3): \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            if generator.train_engine():\n",
    "                print(\"Engine trained successfully.\")\n",
    "            else:\n",
    "                print(\"Failed to train the engine. Please check the logs for details.\")\n",
    "        elif choice == \"2\":\n",
    "            prompt = input(\"Enter the programming topic: \")\n",
    "            q_nos = int(input(\"Enter the number of questions to generate: \"))\n",
    "            prompt = \"the topic is \" + prompt\n",
    "            generator.set_gpt_params(None, q_nos, None)  # Set file_path and req_log to None for now\n",
    "            generated_question = generator.generate_questions(prompt)\n",
    "            if generated_question:\n",
    "                print(f\"Generated question: {generated_question}\")\n",
    "            else:\n",
    "                print(\"Failed to generate question. Please check the logs for details.\")\n",
    "        elif choice == \"3\":\n",
    "            print(\"Exiting program...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a valid option.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_api_key_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "    return api_key\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def interact_with_gpt(user_input, conversation_history):\n",
    "    context = \"\"\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": context})\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 1200,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "    }\n",
    "\n",
    "    print(\"Sending request to OpenAI API...\")\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", json=data, headers=headers)\n",
    "    print(\"Received response from OpenAI API\")\n",
    "                    \n",
    "    response_data = response.json()\n",
    "\n",
    "    print(\"Conversation history:\", conversation_history)\n",
    "    print(\"Response data:\", response_data)\n",
    "\n",
    "    if response_data.get(\"choices\") and response_data[\"choices\"][0].get(\"message\"):\n",
    "        assistant_response = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        result = {\"response\": assistant_response}\n",
    "        return result\n",
    "\n",
    "# with open('index_map.json', 'r') as f:\n",
    "#     index_map = json.load(f)\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a coding (programming) assignments generator for python beginners. Questions will be generated in natural language\"}]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to OpenAI API...\n",
      "Received response from OpenAI API\n",
      "Assistant: Sure! Here is a coding assignment for you:\n",
      "\n",
      "**Question:**\n",
      "Write a Python program that creates a list of numbers from 1 to 10, and then prints each number in the list on a new line.\n",
      "\n",
      "**Expected Output:**\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "\n",
      "Please write the code to solve this problem.\n",
      "Sending request to OpenAI API...\n",
      "Received response from OpenAI API\n",
      "Assistant: Sure! Here is a coding assignment for you:\n",
      "\n",
      "**Question:**\n",
      "Write a Python program to implement a simple linked list. Your program should have a Node class with two attributes: data and next. Implement the following methods in the LinkedList class:\n",
      "1. `__init__`: Initializes an empty linked list.\n",
      "2. `add_node`: Adds a new node with the given data to the end of the linked list.\n",
      "3. `print_list`: Prints the data of all nodes in the linked list.\n",
      "\n",
      "Test your implementation by creating a linked list, adding some nodes, and printing the list.\n",
      "\n",
      "Please\n",
      "Sending request to OpenAI API...\n",
      "Received response from OpenAI API\n",
      "Assistant: Sure! Here is a coding assignment for you:\n",
      "\n",
      "**Question:**\n",
      "Write a Python program that takes a list of numbers as input and creates a new list where each element is the square of the corresponding element in the input list. Use the `map` function to achieve this.\n",
      "\n",
      "**Example:**\n",
      "Input: [1, 2, 3, 4, 5]\n",
      "Output: [1, 4, 9, 16, 25]\n",
      "\n",
      "Please write the code to solve this problem using the `map` function.\n",
      "Sending request to OpenAI API...\n",
      "Received response from OpenAI API\n",
      "Assistant: It seems like you haven't requested a specific topic for a coding assignment. Would you like a coding assignment on a specific topic or concept in Python?\n",
      "Sending request to OpenAI API...\n",
      "Received response from OpenAI API\n",
      "Assistant: It seems like you haven't provided any specific topic for a coding assignment. If you have a specific topic or concept in Python that you'd like a coding assignment on, please let me know!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import openai\n",
    "\n",
    "def get_api_key_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "    return api_key\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def interact_with_gpt():\n",
    "    conversation_history = [{\"role\": \"system\", \"content\": \"You are a coding (programming) assignments generator for python beginners. Questions will be generated in natural language\"}]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting program...\")\n",
    "            break\n",
    "\n",
    "        response = generate_response(user_input, conversation_history)\n",
    "        if response:\n",
    "            print(\"Assistant:\", response)\n",
    "\n",
    "def generate_response(user_input, conversation_history):\n",
    "    context = \"\"\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": context})\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 120,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "    }\n",
    "\n",
    "    print(\"Sending request to OpenAI API...\")\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", json=data, headers=headers)\n",
    "    print(\"Received response from OpenAI API\")\n",
    "                    \n",
    "    response_data = response.json()\n",
    "\n",
    "    if response_data.get(\"choices\") and response_data[\"choices\"][0].get(\"message\"):\n",
    "        assistant_response = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        return assistant_response\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interact_with_gpt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate coding assignment question related to data structures and algorithms for the topic: tuples\n",
      "b'[{\"generated_text\":\"Generate coding assignment question related to data structures and algorithms for the topic: tuples - - ordered collection of elements\\\\n\\\\nA question related to the topic of ordered collection of elements, such as tuples, can be:\\\\n\\\\n\\\\nDesign a class \\\\\"Tuple\\\\\" using List whose elements must be of the same type and supports the basic operations on tuples like, appending an element, getting the sum of elements, getting the max element from the tuples using the following functions :\\\\n*/\\\\n\\\\n\\\\nimport java.io.BufferedReader;\\\\nimport java.io\"}]'\n",
      "Generate coding assignment question related to data structures and algorithms for the topic: tuples - - ordered collection of elements\n",
      "\n",
      "A question related to the topic of ordered collection of elements, such as tuples, can be:\n",
      "\n",
      "\n",
      "Design a class \"Tuple\" using List whose elements must be of the same type and supports the basic operations on tuples like, appending an element, getting the sum of elements, getting the max element from the tuples using the following functions :\n",
      "*/\n",
      "\n",
      "\n",
      "import java.io.BufferedReader;\n",
      "import java.io\n"
     ]
    }
   ],
   "source": [
    "##### DONE @ ###\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat-3.5-0106\"\n",
    "API_TOKEN = \"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\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "def generate_questions(topic):\n",
    "    inputs = f\"Generate coding assignment question related to data structures and algorithms for the topic: {topic}\"\n",
    "    print(inputs)\n",
    "    payload = {\n",
    "        \"inputs\": inputs,\n",
    "        \"max_length\": 200\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    print(response.content)  # Print response content\n",
    "    questions = response.json()[0][\"generated_text\"]\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "topic = \"tuples\"\n",
    "questions = generate_questions(topic)\n",
    "print(questions)\n",
    "# for i, question in enumerate(questions, start=1):\n",
    "#     print(f\"Question {i}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to OpenAI API...\n",
      "Received response from OpenAI API\n",
      "Conversation history: [{'role': 'system', 'content': 'You are a assignments (questions) generator for python. I will give a topic for you to generate a problem based on that topic.'}, {'role': 'user', 'content': 'tuples'}, {'role': 'assistant', 'content': ''}]\n",
      "Response data: {'id': 'chatcmpl-91JEDmuXhJBupBfkR0JEmeg1ucV9I', 'object': 'chat.completion', 'created': 1710099269, 'model': 'gpt-3.5-turbo-0125', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Write a function `multiply_elements` that takes in a tuple of numbers as input and returns a new tuple where each element is the square of the corresponding element in the input tuple.\\n\\nExample:\\n```\\ninput_tuple = (1, 2, 3, 4)\\noutput_tuple = multiply_elements(input_tuple)\\nprint(output_tuple)  # Output should be (1, 4, 9, 16)\\n```'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 44, 'completion_tokens': 85, 'total_tokens': 129}, 'system_fingerprint': 'fp_4f0b692a78'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'Write a function `multiply_elements` that takes in a tuple of numbers as input and returns a new tuple where each element is the square of the corresponding element in the input tuple.\\n\\nExample:\\n```\\ninput_tuple = (1, 2, 3, 4)\\noutput_tuple = multiply_elements(input_tuple)\\nprint(output_tuple)  # Output should be (1, 4, 9, 16)\\n```'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### DONE ####################\n",
    "\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_api_key_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        api_key = f.read().strip()\n",
    "    return api_key\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def interact_with_gpt(user_input, conversation_history):\n",
    "    context = \"\"\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": context})\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": conversation_history,\n",
    "        \"max_tokens\": 120,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "    }\n",
    "\n",
    "    print(\"Sending request to OpenAI API...\")\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", json=data, headers=headers)\n",
    "    print(\"Received response from OpenAI API\")\n",
    "                    \n",
    "    response_data = response.json()\n",
    "\n",
    "    print(\"Conversation history:\", conversation_history)\n",
    "    print(\"Response data:\", response_data)\n",
    "\n",
    "    if response_data.get(\"choices\") and response_data[\"choices\"][0].get(\"message\"):\n",
    "        assistant_response = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        result = {\"response\": assistant_response}\n",
    "        return result\n",
    "\n",
    "# with open('index_map.json', 'r') as f:\n",
    "#     index_map = json.load(f)\n",
    "user_input = 'tuples'\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"You are a assignments (questions) generator for python. I will give a topic for you to generate a problem based on that topic.\"}]\n",
    "\n",
    "interact_with_gpt(user_input, conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizer\n",
      "  Downloading tokenizer-3.4.3-py2.py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m931.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizer-3.4.3-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizer\n",
      "Successfully installed tokenizer-3.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Here is a question based on tuples:\n",
      "\n",
      "Question: Write a Python function that takes in a tuple of integers and returns the sum of all the elements in the tuple.\n",
      "\n",
      "Example:\n",
      "Input: (1, 2, 3, 4)\n",
      "Output: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import openai\n",
    "import requests\n",
    "import json\n",
    "import traceback\n",
    "# from program_generator_utils import wrap_content, process_finish_reason, save_to_file, show_message\n",
    "\n",
    "class ProgramGeneratorEngine:\n",
    "    def __init__(self):\n",
    "        self.response = None\n",
    "        self.q_nos = None\n",
    "        self.file_path = None\n",
    "        self.req_log = None\n",
    "        self.engine = \"gpt-3.5-turbo\"  # Assuming 'gpt-3.5-turbo' as default engine\n",
    "\n",
    "        # Set your OpenAI API key here\n",
    "        openai.api_key = \"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\"\n",
    "\n",
    "    def set_gpt_params(self, q_nos):\n",
    "        # self.file_path = file_path\n",
    "        self.q_nos = q_nos\n",
    "        # self.req_log = req_log\n",
    "\n",
    "    def get_max_tokens(self):\n",
    "        return min((self.q_nos + 1) * 50, 500)\n",
    "\n",
    "    def generate_questions(self, prompt):\n",
    "        tokens = self.get_max_tokens()\n",
    "        params = {'model': self.engine,\n",
    "                  'messages': [{\"role\": \"user\", \"content\": prompt}],\n",
    "                #   'temperature': 0.3,\n",
    "                  'max_tokens': 1000\n",
    "                  }\n",
    "        try:\n",
    "            self.response = openai.ChatCompletion.create(**params)\n",
    "        except openai.error.OpenAIError as error:\n",
    "            print(f\"OpenAI Error: {error}\")\n",
    "            return False\n",
    "        except Exception as error:\n",
    "            print(f\"Error sending OpenAI request: {error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response\")\n",
    "            return False\n",
    "\n",
    "        # try:\n",
    "        #     self.parse_json_log()\n",
    "        # except Exception as error:\n",
    "        #     print(f\"Error parsing json response: {error}\")\n",
    "        #     traceback.print_exc()\n",
    "\n",
    "        # process_finish_reason(self.response.choices[0]['finish_reason'])\n",
    "        questions = self.response.choices[0]['message']['content']\n",
    "\n",
    "        # return save_to_file(self.file_path, wrap_content(questions))\n",
    "        return questions\n",
    "\n",
    "    # def parse_json_log(self):\n",
    "    #     json_log = json.dumps(json.loads(str(self.response)), indent=4)\n",
    "    #     req_log = wrap_content(self.req_log)\n",
    "    #     out_file_path = \"Destination file: {}\".format(self.file_path)\n",
    "    #     try:\n",
    "    #         with open('response_log.json', 'a+') as fObj:\n",
    "    #             fObj.write(\"{}\\n{}\\n{}\\n\\n\".format(req_log, json_log, out_file_path))\n",
    "    #     except FileNotFoundError:\n",
    "    #         print(\"Response log file not found\")\n",
    "    #     except Exception as error:\n",
    "    #         print(f\"Response log file Error: {error}\")\n",
    "    #         traceback.print_exc()\n",
    "\n",
    "    # def wrap_content(paragraph):\n",
    "    #     lines = paragraph.splitlines(True)\n",
    "    #     wrapped_para = \"\"\n",
    "    #     for line in lines:\n",
    "    #         if len(line) > 100:\n",
    "    #             wrapped_para += wrap_line(line) + \"\\n\"\n",
    "    #         else:\n",
    "    #             wrapped_para += line\n",
    "    #     return wrapped_para\n",
    "\n",
    "# Initialize ProgramGeneratorEngine instance\n",
    "program_generator = ProgramGeneratorEngine()\n",
    "\n",
    "# Example usage:\n",
    "user_input = \"You are a assignments (questions) generator for python. I will give a topic for you to generate a problem based on that topic. The topic is: tuples\"\n",
    "# file_path = \"output_questions.txt\"\n",
    "# req_log = \"Request Log: Generating questions for Python Tuples\"\n",
    "\n",
    "# Set parameters\n",
    "program_generator.set_gpt_params(1)\n",
    "\n",
    "# Generate questions\n",
    "result = program_generator.generate_questions(user_input)\n",
    "\n",
    "# if result:\n",
    "print(result)\n",
    "# else:\n",
    "#     print(\"Failed to generate questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Here is a tuple-related assignment for you:\n",
      "\n",
      "**Assignment: Tuple Operations**\n",
      "\n",
      "1. Create a tuple named `student_scores` with the following data: 85, 92, 78, 90, 88.\n",
      "2. Print the third element of the tuple `student_scores`.\n",
      "3. Add a new score of 95 to the tuple `student_scores`.\n",
      "4. Create a new tuple named `student_names` with the following data: \"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\".\n",
      "5. Combine the `student_scores` tuple and `student_names` tuple into a new tuple named `student_data`.\n",
      "6. Print the length of the `student_data` tuple.\n",
      "7. Check if \"Alice\" is in the `student_data` tuple.\n",
      "8. Find the maximum and minimum score from the `student_scores` tuple.\n",
      "9. Print the `student_data` tuple in reverse order.\n",
      "\n",
      "Have fun practicing tuple operations!\n"
     ]
    }
   ],
   "source": [
    "######### FINAL ######\n",
    "\n",
    "import openai\n",
    "import traceback\n",
    "\n",
    "class ProgramGeneratorEngine:\n",
    "    def __init__(self, api_key):\n",
    "        self.response = None\n",
    "        self.q_nos = None\n",
    "        self.file_path = None\n",
    "        self.req_log = None\n",
    "        self.engine = \"gpt-3.5-turbo\"  # Assuming 'gpt-3.5-turbo' as default engine\n",
    "        self.training_prompt = [{\"role\": \"system\", \"content\":\"You are an assignments (questions) generator for python. I will give a topic for you to generate a problem based on that topic.\"}]\n",
    "\n",
    "        # Set your OpenAI API key here\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def set_gpt_params(self, q_nos):\n",
    "        self.q_nos = q_nos\n",
    "\n",
    "    def get_max_tokens(self):\n",
    "        return min((self.q_nos + 1) * 50, 500)\n",
    "\n",
    "    def generate_questions(self, prompt):\n",
    "        tokens = self.get_max_tokens()\n",
    "        params = {'model': self.engine,\n",
    "                  'messages': self.training_prompt + [{\"role\": \"user\", \"content\": prompt}],\n",
    "                  'max_tokens': 1000\n",
    "                  }\n",
    "        try:\n",
    "            self.response = openai.ChatCompletion.create(**params)\n",
    "        except openai.error.OpenAIError as error:\n",
    "            print(f\"OpenAI Error: {error}\")\n",
    "            return False\n",
    "        except Exception as error:\n",
    "            print(f\"Error sending OpenAI request: {error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response\")\n",
    "            return False\n",
    "\n",
    "        questions = self.response.choices[0]['message']['content']\n",
    "\n",
    "        return questions\n",
    "\n",
    "# Usage Example:\n",
    "api_key = \"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\"  # Replace with your OpenAI API key\n",
    "program_generator = ProgramGeneratorEngine(api_key)\n",
    "\n",
    "user_input = \"The topic is: tuples\"\n",
    "\n",
    "# Set parameters\n",
    "program_generator.set_gpt_params(1)\n",
    "\n",
    "# Generate questions\n",
    "result = program_generator.generate_questions(user_input)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n",
      "[{'role': 'system', 'content': 'You are an assignments generator for coding questions in python. You wil generate only coding questions based on a given topic. Provide only the question. Provide only 1 test case'}, {'role': 'user', 'content': 'The topic is: tuples'}]\n",
      "Question: Write a Python function named `reverse_tuple` that takes a tuple as input and returns a new tuple with the elements reversed.\n",
      "\n",
      "Example:\n",
      "Input: (1, 2, 3, 4, 5)\n",
      "Output: (5, 4, 3, 2, 1)\n",
      "[{'role': 'system', 'content': 'You are an assignments generator for coding questions in python. You wil generate only coding questions based on a given topic. Provide only the question. Provide only 1 test case'}, {'role': 'user', 'content': 'The topic is: tuples'}, {'role': 'user', 'content': 'The topic is: tuples'}, {'role': 'assistant', 'content': 'Question: Write a Python function named `reverse_tuple` that takes a tuple as input and returns a new tuple with the elements reversed.\\n\\nExample:\\nInput: (1, 2, 3, 4, 5)\\nOutput: (5, 4, 3, 2, 1)'}]\n"
     ]
    }
   ],
   "source": [
    "# import openai\n",
    "# import traceback\n",
    "\n",
    "# class ProgramGeneratorEngine:\n",
    "#     def __init__(self, api_key):\n",
    "#         self.response = None\n",
    "#         self.q_nos = None\n",
    "#         self.file_path = None\n",
    "#         self.req_log = None\n",
    "#         self.engine = \"gpt-3.5-turbo\"  # Assuming 'gpt-3.5-turbo' as default engine\n",
    "#         self.training_prompt = [{\"role\": \"system\", \"content\":\"You are an assignments generator for coding questions in python. You wil generate coding questions based on a given topic.\"}]\n",
    "#         self.conversation_history = []\n",
    "\n",
    "#         # Set your OpenAI API key here\n",
    "#         openai.api_key = api_key\n",
    "\n",
    "#     def train_model(self):\n",
    "#         tokens = 1000\n",
    "#         params = {'model': self.engine,\n",
    "#                   'messages': self.training_prompt,\n",
    "#                   'max_tokens': tokens\n",
    "#                   }\n",
    "#         try:\n",
    "#             self.response = openai.ChatCompletion.create(**params)\n",
    "#         except openai.error.OpenAIError as error:\n",
    "#             print(f\"OpenAI Error: {error}\")\n",
    "#             return False\n",
    "#         except Exception as error:\n",
    "#             print(f\"Error sending OpenAI request: {error}\")\n",
    "#             traceback.print_exc()\n",
    "\n",
    "#         if not self.response:\n",
    "#             print(\"GPT None response during training\")\n",
    "#             return False\n",
    "\n",
    "#         return True\n",
    "\n",
    "#     def set_gpt_params(self, q_nos):\n",
    "#         self.q_nos = q_nos\n",
    "\n",
    "#     # def get_max_tokens(self):\n",
    "#     #     return min((self.q_nos + 1) * 50, 500)\n",
    "\n",
    "#     def generate_questions(self, prompt):\n",
    "#         tokens = 1000\n",
    "#         messages = self.conversation_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "#         params = {'model': self.engine,\n",
    "#                   'messages': messages,\n",
    "#                   'max_tokens': tokens\n",
    "#                   }\n",
    "#         try:\n",
    "#             self.response = openai.ChatCompletion.create(**params)\n",
    "#         except openai.error.OpenAIError as error:\n",
    "#             print(f\"OpenAI Error: {error}\")\n",
    "#             return False\n",
    "#         except Exception as error:\n",
    "#             print(f\"Error sending OpenAI request: {error}\")\n",
    "#             traceback.print_exc()\n",
    "\n",
    "#         if not self.response:\n",
    "#             print(\"GPT None response during question generation\")\n",
    "#             return False\n",
    "\n",
    "#         assistant_response = self.response.choices[0]['message']['content']\n",
    "#         self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "#         return assistant_response\n",
    "\n",
    "# # Usage Example:\n",
    "# api_key = \"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\"  # Replace with your OpenAI API key\n",
    "# program_generator = ProgramGeneratorEngine(api_key)\n",
    "\n",
    "# # Train the model\n",
    "# training_result = program_generator.train_model()\n",
    "# if training_result:\n",
    "#     print(\"Model trained successfully!\")\n",
    "# else:\n",
    "#     print(\"Failed to train the model.\")\n",
    "\n",
    "# # Generate questions\n",
    "# user_input = \"The topic is: tuples\"\n",
    "# program_generator.set_gpt_params(1)\n",
    "# result = program_generator.generate_questions(user_input)\n",
    "# print(result)\n",
    "# print(conversation_history)\n",
    "\n",
    "\n",
    "######### FINAL ######\n",
    "\n",
    "import openai\n",
    "import traceback\n",
    "\n",
    "class ProgramGeneratorEngine:\n",
    "    def __init__(self, api_key):\n",
    "        self.response = None\n",
    "        self.q_nos = None\n",
    "        self.file_path = None\n",
    "        self.req_log = None\n",
    "        self.engine = \"gpt-3.5-turbo\"  # Assuming 'gpt-3.5-turbo' as default engine\n",
    "        #self.training_prompt = [{\"role\": \"system\", \"content\":\"You are an assignments generator for coding questions in python. You wil generate coding questions based on a given topic.\"}]\n",
    "        self.conversation_history =  [{\"role\": \"system\", \"content\":\"You are an assignments generator for coding questions in python. You wil generate only coding questions based on a given topic. Provide only the question. Provide only 1 test case\"}]\n",
    "\n",
    "        # Set your OpenAI API key here\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def train_model(self):\n",
    "        tokens = 300\n",
    "        params = {'model': self.engine,\n",
    "                  'messages': self.conversation_history,\n",
    "                  'max_tokens': tokens\n",
    "                  }\n",
    "        try:\n",
    "            self.response = openai.ChatCompletion.create(**params)\n",
    "        except openai.error.OpenAIError as error:\n",
    "            print(f\"OpenAI Error: {error}\")\n",
    "            return False\n",
    "        except Exception as error:\n",
    "            print(f\"Error sending OpenAI request: {error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response during training\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def set_gpt_params(self, q_nos):\n",
    "        self.q_nos = q_nos\n",
    "\n",
    "    def generate_questions(self, prompt):\n",
    "        tokens = 1000\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        print(self.conversation_history)\n",
    "        messages = self.conversation_history \n",
    "        params = {'model': self.engine,\n",
    "                  'messages': messages,\n",
    "                  'max_tokens': tokens,\n",
    "                  'temperature': 0.3\n",
    "                  }\n",
    "        try:\n",
    "            self.response = openai.ChatCompletion.create(**params)\n",
    "        except openai.error.OpenAIError as error:\n",
    "            print(f\"OpenAI Error: {error}\")\n",
    "            return False\n",
    "        except Exception as error:\n",
    "            print(f\"Error sending OpenAI request: {error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response during question generation\")\n",
    "            return False\n",
    "\n",
    "        assistant_response = self.response.choices[0]['message']['content']\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        return assistant_response\n",
    "\n",
    "# Usage Example:\n",
    "api_key = \"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\"  # Replace with your OpenAI API key\n",
    "program_generator = ProgramGeneratorEngine(api_key)\n",
    "\n",
    "# Train the model\n",
    "training_result = program_generator.train_model()\n",
    "if training_result:\n",
    "    print(\"Model trained successfully!\")\n",
    "else:\n",
    "    print(\"Failed to train the model.\")\n",
    "\n",
    "# Generate questions\n",
    "user_input = \"The topic is: tuples\"\n",
    "program_generator.set_gpt_params(1)\n",
    "result = program_generator.generate_questions(user_input)\n",
    "print(result)\n",
    "print(program_generator.conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### FINAL FLASK HTML #######\n",
    "from flask import Flask, render_template, request\n",
    "import openai\n",
    "import traceback\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "class ProgramGeneratorEngine:\n",
    "    def __init__(self, api_key):\n",
    "        self.response = None\n",
    "        self.q_nos = None\n",
    "        self.file_path = None\n",
    "        self.req_log = None\n",
    "        self.engine = \"gpt-3.5-turbo\"  # Assuming 'gpt-3.5-turbo' as default engine\n",
    "        self.conversation_history = [{\"role\": \"system\", \"content\":\"You are an assignments generator for coding questions in python. You will generate only coding questions based on a given topic. Provide only the question. Provide only 1 test case\"}]\n",
    "\n",
    "        # Set your OpenAI API key here\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def train_model(self):\n",
    "        tokens = 300\n",
    "        params = {'model': self.engine,\n",
    "                  'messages': self.conversation_history,\n",
    "                  'max_tokens': tokens\n",
    "                  }\n",
    "        try:\n",
    "            self.response = openai.ChatCompletion.create(**params)\n",
    "        except openai.error.OpenAIError as error:\n",
    "            print(f\"OpenAI Error: {error}\")\n",
    "            return False\n",
    "        except Exception as error:\n",
    "            print(f\"Error sending OpenAI request: {error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response during training\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def set_gpt_params(self, q_nos):\n",
    "        self.q_nos = q_nos\n",
    "\n",
    "    def generate_questions(self, prompt, difficulty_level):\n",
    "        tokens = 1000\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Adjust the difficulty level based on user input\n",
    "        difficulty_modifier = \"\"\n",
    "        if difficulty_level.lower() == \"easy\":\n",
    "            difficulty_modifier = \"easy\"\n",
    "        elif difficulty_level.lower() == \"medium\":\n",
    "            difficulty_modifier = \"medium\"\n",
    "        elif difficulty_level.lower() == \"hard\":\n",
    "            difficulty_modifier = \"hard\"\n",
    "\n",
    "        messages = self.conversation_history + [{\"role\": \"user\", \"content\": f\"Difficulty: {difficulty_modifier}\"}]\n",
    "        \n",
    "        params = {'model': self.engine,\n",
    "                  'messages': messages,\n",
    "                  'max_tokens': tokens,\n",
    "                  'temperature': 0.3\n",
    "                  }\n",
    "        try:\n",
    "            self.response = openai.ChatCompletion.create(**params)\n",
    "        except openai.error.OpenAIError as error:\n",
    "            print(f\"OpenAI Error: {error}\")\n",
    "            return False\n",
    "        except Exception as error:\n",
    "            print(f\"Error sending OpenAI request: {error}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        if not self.response:\n",
    "            print(\"GPT None response during question generation\")\n",
    "            return False\n",
    "\n",
    "        assistant_response = self.response.choices[0]['message']['content']\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        return assistant_response\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        topic = request.form['topic']\n",
    "        difficulty_level = request.form['difficulty']\n",
    "        \n",
    "        result = program_generator.generate_questions(topic, difficulty_level)\n",
    "        return render_template('index.html', result=result)\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Usage Example:\n",
    "api_key = \"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\"  # Replace with your OpenAI API key\n",
    "program_generator = ProgramGeneratorEngine(api_key)\n",
    "\n",
    "# Train the model\n",
    "training_result = program_generator.train_model()\n",
    "if training_result:\n",
    "    print(\"Model trained successfully!\")\n",
    "else:\n",
    "    print(\"Failed to train the model.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# <!DOCTYPE html>\n",
    "# <html lang=\"en\">\n",
    "# <head>\n",
    "#     <meta charset=\"UTF-8\">\n",
    "#     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "#     <title>Question Generator</title>\n",
    "# </head>\n",
    "# <body>\n",
    "#     <h1>Question Generator</h1>\n",
    "#     <form method=\"post\">\n",
    "#         <label for=\"topic\">Enter Topic:</label>\n",
    "#         <input type=\"text\" id=\"topic\" name=\"topic\"><br><br>\n",
    "#         <label for=\"difficulty\">Select Difficulty:</label>\n",
    "#         <select id=\"difficulty\" name=\"difficulty\">\n",
    "#             <option value=\"easy\">Easy</option>\n",
    "#             <option value=\"medium\">Medium</option>\n",
    "#             <option value=\"hard\">Hard</option>\n",
    "#         </select><br><br>\n",
    "#         <input type=\"submit\" value=\"Generate Question\">\n",
    "#     </form>\n",
    "#     {% if result %}\n",
    "#     <h2>Generated Question:</h2>\n",
    "#     <p>{{ result }}</p>\n",
    "#     {% endif %}\n",
    "# </body>\n",
    "# </html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
