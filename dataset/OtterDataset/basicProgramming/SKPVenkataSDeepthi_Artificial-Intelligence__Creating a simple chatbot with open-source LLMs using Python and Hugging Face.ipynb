{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe5ba3f-7402-4d44-90ba-3d9e47de2c09",
   "metadata": {},
   "source": [
    "Imagine having a personal assistant who can talk to you anytime, answer your questions, and even engage in a fun conversation. That’s what a chatbot does!\n",
    "\n",
    "# What is a Chatbot?\n",
    "* A chatbot is like a virtual buddy you chat with online. Think about when you contact customer service for help—often, you’re talking to a chatbot first. It listens to your question and tries to give you the best response. For example:\n",
    "\n",
    "> You ask: \"What’s the weather today?\"\n",
    "\n",
    "> It replies: \"Today, it’s sunny and 75°F!\"\n",
    "\n",
    "* The chatbot understands your input, figures out what you need, and gives you a meaningful reply.\n",
    "\n",
    "# How Does a Chatbot Work?\n",
    "Think of a chatbot as a machine with a \"brain.\" This brain consists of two key parts:\n",
    "\n",
    "1. Transformer: A super-smart organizer that breaks down your message and organizes it for the chatbot to understand.\n",
    "2. Language Model (LLM - Large Language Model): A language expert that helps the chatbot make sense of your words and generate a human-like reply.\n",
    "\n",
    "* Example: A Library Assistant\n",
    "Let’s say you walk into a library and ask the assistant, “Do you have any books on space?”\n",
    "\n",
    "> Input Processing: The assistant hears your request, understands the words \"books\" and \"space,\" and knows what you're looking for.\n",
    "\n",
    "> Understanding Context: The assistant remembers what kind of books the library has (space-related topics) and finds something relevant.\n",
    "\n",
    "> Generating Response: The assistant tells you, “Yes, we have several books on space in the science section.”\n",
    "\n",
    "> Conversation Continues: You might ask follow-up questions like, \"Can I borrow them?\" and the assistant answers based on your needs.\n",
    "\n",
    "This is exactly how a chatbot works! It listens to your input, understands it using its brain, and replies appropriately.\n",
    "\n",
    "# What’s Inside the Chatbot Brain?\n",
    "1. Transformers: The Organizer\n",
    "A transformer is like a librarian sorting through books or words in this case. When you ask a chatbot a question, the transformer splits your sentence into smaller pieces, called tokens.\n",
    "\n",
    "Example:\n",
    "> Your message: “Tell me about the weather.”\n",
    "\n",
    "> Tokens: [“Tell,” “me,” “about,” “the,” “weather.”]\n",
    "\n",
    "By breaking it down, the transformer can focus on the meaning of each word and its relationship to others.\n",
    "\n",
    "2. Large Language Models (LLMs): The Language Expert\n",
    "The LLM is like a super-knowledgeable librarian who has read thousands of books and knows a lot about the world. It understands the patterns of language and generates human-like responses.\n",
    "\n",
    "Example:\n",
    "> You say: “What’s your favorite food?”\n",
    "\n",
    "> The chatbot replies: “I’m just a program, but I hear pizza is a popular choice!”\n",
    "\n",
    "\n",
    "# What Makes a Chatbot Smart?\n",
    "Chatbots become smart because they learn from a massive amount of text data, such as books, articles, and conversations. This process helps them:\n",
    "1. Understand context: Knowing that “Can you tell me about apples?” is different from “Can you tell me about Apple (the company)?”\n",
    "2. Generate meaningful responses: Giving useful answers instead of random gibberish.\n",
    "\n",
    "# How Do We Build One?\n",
    "## Step 1: Using Hugging Face\n",
    "To build our chatbot, we’ll use a tool called Hugging Face, which is like an all-in-one toolkit for creating chatbots. Hugging Face makes it easier to:\n",
    "\n",
    "1. Use pre-built language models.\n",
    "2. Process messages quickly.\n",
    "3. Get human-like replies.\n",
    "\n",
    "* Think of Hugging Face as a \"shortcut\" that gives us everything we need to build a chatbot without starting from scratch.\n",
    "\n",
    "## Step 2: Installing the Right Tools\n",
    "To build our chatbot, we’ll use a Python library called transformers, which Hugging Face provides. It’s like getting a pre-packed toolbox with all the tools for assembling a chatbot.\n",
    "\n",
    "* We’ll also use other helpful Python tools to write the program and test it. Once we have everything ready, we can start building!\n",
    "\n",
    "# Why Is This Important?\n",
    "Chatbots are becoming part of our daily lives. From virtual assistants like Siri and Alexa to customer service bots, they save time and make tasks easier. Learning how they work not only helps us understand technology better but also gives us the power to build something that can solve real-world problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ebe71-5a96-4346-b5de-db504772756a",
   "metadata": {},
   "source": [
    "# About Hugging Face\n",
    "\n",
    "# What is Hugging Face?\n",
    "Hugging Face is like a friendly hub for creating intelligent programs that can understand and generate human-like text. It started as a chatbot company but has grown into one of the most popular platforms for Natural Language Processing (NLP) and Machine Learning (ML).\n",
    "\n",
    "In simpler terms, Hugging Face provides tools that help us build applications like chatbots, text summarizers, translators, and more without having to create everything from scratch. It's like having a ready-made box of ingredients and recipes to quickly prepare a dish, where the dish is your AI application.\n",
    "\n",
    "# What Does Hugging Face Offer?\n",
    "\n",
    "## 1. Transformers Library\n",
    "The Transformers library is Hugging Face’s most famous tool. It’s like a collection of pre-trained “brains” for your AI application. These brains are powerful models that can handle various language-related tasks, such as:\n",
    "\n",
    "1. Chatbots: Engage in conversations with users.\n",
    "2. Text Summarization: Condense long articles into a few lines.\n",
    "3. Translation: Convert text from one language to another.\n",
    "4. Sentiment Analysis: Determine whether a sentence expresses positive or negative emotions.\n",
    "5. Question Answering: Answer questions based on given text.\n",
    "\n",
    "Example:\n",
    "> You ask the chatbot, \"What’s the capital of France?\"\n",
    "\n",
    "> It uses the model to reply, \"Paris.\"\n",
    "\n",
    "## 2. Pre-Trained Models\n",
    "Hugging Face offers pre-trained models, which are like AI systems that already know a lot about the world because they've been trained on tons of data. You don’t need to teach them from scratch—they’re ready to use with just a little tweaking for your specific needs.\n",
    "\n",
    "Some famous models available in Hugging Face are:\n",
    "1. GPT (Generative Pre-trained Transformer): Great for generating text or building chatbots.\n",
    "2. BERT (Bidirectional Encoder Representations from Transformers): Ideal for understanding text, such as finding answers in a paragraph.\n",
    "3. T5 (Text-to-Text Transfer Transformer): Can handle any NLP task by converting it into a text input-output format.\n",
    "\n",
    "## 3. Hugging Face Hub\n",
    "The Hugging Face Hub is like an app store for AI models. Developers from all over the world upload their pre-trained models here, which you can use for free or fine-tune for your own project. \n",
    "\n",
    "Example:\n",
    "If you need a model for sentiment analysis, you can simply search for it on the Hub, download it, and start using it.\n",
    "\n",
    "## 4. Tokenizers\n",
    "A tokenizer is a tool that breaks text into smaller pieces called tokens (words or parts of words). Hugging Face provides efficient tokenizers that prepare text data in a format the models can understand. \n",
    "\n",
    "Example:\n",
    "> Text: \"I love Aviation.\"\n",
    "\n",
    "> Tokens: [“I,” “love,” “Aviation,” “.”]\n",
    "\n",
    "## 5. Pipelines\n",
    "Hugging Face makes using models easy with pipelines. A pipeline is like a shortcut—just tell it what you need, and it handles everything behind the scenes. \n",
    "\n",
    "> CODE:\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love Hugging Face!\")\n",
    "print(result)\n",
    "\n",
    "> OUTPUT: [{'label': 'POSITIVE', 'score': 0.99}]\n",
    "\n",
    "# Why is Hugging Face So Popular?\n",
    "1. Ease of Use: Hugging Face is beginner-friendly, with lots of pre-built tools. You can get started with just a few lines of Python code.\n",
    "\n",
    "2. Flexibility: It supports many types of models and tasks. You can customize pre-trained models to fit your specific needs which is called fine-tuning.\n",
    "\n",
    "3. Open-Source: Hugging Face is open-source, meaning anyone can use it for free and contribute to improving it. This has created a strong community of developers and researchers.\n",
    "\n",
    "4. Time-Saving: Instead of spending weeks training a model from scratch, you can start with a pre-trained model and adapt it to your task in hours or days.\n",
    "\n",
    "5. Support for State-of-the-Art Models: Hugging Face always stays updated with the latest research, so you get access to cutting-edge AI models.\n",
    "\n",
    "# How is Hugging Face Used in Real Life?\n",
    "Hugging Face has practical applications across many industries.\n",
    "1. Healthcare: \n",
    "* Chatbots powered by Hugging Face help patients book appointments or get information about symptoms.\n",
    "* Text models analyze medical reports to find patterns or trends.\n",
    "  \n",
    "2. Customer Support\n",
    "* Companies use chatbots built with Hugging Face to handle FAQs, reducing the need for human support agents.\n",
    "\n",
    "3. Education\n",
    "* AI tutors can answer students’ questions, grade essays, or even provide explanations for complex topics.\n",
    "\n",
    "4. Social Media\n",
    "* Analyze tweets or posts to understand public sentiment about a brand or event.\n",
    "\n",
    "5. Content Creation\n",
    "* Generate marketing slogans, blog posts, or creative content automatically.\n",
    "\n",
    "## Why Should we Use Hugging Face?\n",
    "Hugging Face simplifies NLP tasks, making it accessible to developers of all skill levels.\n",
    "1. Quick Results: You can get a working chatbot or other AI application up and running in hours.\n",
    "2. Accessible Documentation: Hugging Face has excellent guides and examples to help you.\n",
    "3. Community Support: If you’re stuck, there’s a massive community to answer questions or share tips.\n",
    "4. Building a Chatbot with Hugging Face\n",
    "\n",
    "## To create a chatbot, we’ll:\n",
    "1. Install the Hugging Face transformers library.\n",
    "2. Use a pre-trained model from Hugging Face Hub.\n",
    "4. Write a Python program to process input and generate responses.\n",
    "5. By the end, we’ll have a functional chatbot that understands and responds like a human!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1576f139-4c5c-49e1-bc04-ead4b11027ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the required libraries \n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d586b23-071e-4e0d-bb3b-9b196b865fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the required tools from transformers library \n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcf3a1-ba5e-4c47-98e0-486501976974",
   "metadata": {},
   "source": [
    "# Step 3: Choosing a model for chatbot\n",
    "\n",
    "Choosing the right model is like selecting the right tool for a specific job. Not all models are created equal, and each one has its strengths depending on the task you want to accomplish.\n",
    "\n",
    "# What Are LLMs and Why Do They Matter?\n",
    "A Large Language Model (LLM) is the \"brain\" of a chatbot. It helps the chatbot understand what you're saying and respond in a way that makes sense. Different LLMs are trained to do different things based on the kind of data they've seen during training. Think of them like specialized professionals:\n",
    "* A chef knows how to make amazing food but might not be the best at fixing a car.\n",
    "* A mechanic can fix your car but won’t know how to cook a gourmet meal.\n",
    "\n",
    "Similarly, you choose an LLM based on what you want your chatbot to do.\n",
    "\n",
    "# Types of Models and Their Use Cases\n",
    "1. Text Generation: \"The Creative Storyteller\"\n",
    "If you want your chatbot to have creative and engaging conversations, choose a model like GPT-2 or GPT-3. These models are great at generating natural and coherent text.\n",
    "\n",
    "Example:\n",
    "You’re building a chatbot for customer support. It needs to respond in a friendly and creative way to keep the user engaged.\n",
    "Or imagine you’re creating a chatbot that tells bedtime stories to kids. A text generation model would be perfect for spinning imaginative tales.\n",
    "\n",
    "2. Sentiment Analysis: \"The Emotional Detective\"\n",
    "Models like BERT or RoBERTa are great at understanding the emotional tone behind a message—whether it’s happy, sad, or angry.\n",
    "\n",
    "Example:\n",
    "You run an online store and want to analyze customer reviews. A sentiment analysis model can help you figure out if customers are satisfied with your product or frustrated.\n",
    "A chatbot powered by this model could adjust its tone, offering cheerful responses to positive messages or apologizing if a customer is upset.\n",
    "\n",
    "3. Named Entity Recognition (NER): \"The Information Extractor\"\n",
    "Some models specialize in pulling out specific information from text, like names, dates, or places. BERT, GPT-2, or RoBERTa work well for this task.\n",
    "\n",
    "Example:\n",
    "You’re building a travel chatbot. When a user says, “I want to book a flight from New York to Paris next Friday,” the model can identify the cities (“New York” and “Paris”) and the date (“next Friday”).\n",
    "Another use case could be creating a chatbot for job applications that extracts names, email addresses, and company names from a user’s message.\n",
    "\n",
    "4. Question Answering: \"The Knowledgeable Expert\"\n",
    "For answering questions accurately, models like BERT, GPT-2, or XLNet are a great fit.\n",
    "\n",
    "Example:\n",
    "You’re developing a chatbot that acts as a personal assistant. When the user asks, “What’s the weather in Dallas today?” the model can answer based on weather data you provide.\n",
    "Or, imagine you’re building a bot to answer trivia questions for a fun quiz game.\n",
    "\n",
    "5. Language Translation: \"The Multilingual Genius\"\n",
    "Need to translate between languages? Models like MarianMT or T5 are your go-to options.\n",
    "\n",
    "Example:\n",
    "You’re building a chatbot for travelers that translates messages from English to French.\n",
    "A multilingual chatbot for international businesses could translate conversations in real-time, helping people communicate easily.\n",
    "\n",
    "# Other Considerations When Choosing a Model\n",
    "Before picking a model, there are some practical things to think about:\n",
    "\n",
    "1. Licensing: Can You Use It Freely?\n",
    "Some models are open-source, meaning you can use them without worrying about restrictions. Others might require you to pay or follow certain rules,like, facebook/blenderbot-400M-distill is a great open-source model we’ll use in this example.\n",
    "\n",
    "2. Model Size: How Big Is Too Big?\n",
    "Larger models, like GPT-3, can be more powerful but require more computing power to run. Smaller models, like facebook/blenderbot-400M-distill, are faster and more efficient but may have limitations.\n",
    "\n",
    "Example:\n",
    "If you’re running the chatbot on your laptop, you’ll want a smaller, lightweight model that won’t slow down your system.\n",
    "\n",
    "3. Training Data: Does It Match Your Needs?\n",
    "Some models are trained on general knowledge, while others are trained on specific topics e.g., medical or legal text. Choose one that aligns with your chatbot’s purpose.\n",
    "\n",
    "Example:\n",
    "\n",
    "For a general-purpose chatbot, GPT-2 or GPT-3 might be fine.\n",
    "For a legal advice bot, you’d want a model trained on legal documents.\n",
    "\n",
    "4. Performance: How Accurate Is It?\n",
    "Accuracy is crucial. Test the model on your specific use case to see how well it works. You might also need to fine-tune it (train it a little more with your data).\n",
    "\n",
    "# Where to Explore Models?\n",
    "To find the perfect model for your chatbot, you can explore Hugging Face’s library of pre-trained models:\n",
    "> [Hugging Face Models](https://huggingface.co/models)\n",
    "\n",
    "# facebook/blenderbot-400M-distill\n",
    "For this tutorial, we’ll use the facebook/blenderbot-400M-distill model. Why?\n",
    "1. It’s open-source (free to use).\n",
    "2. It’s lightweight and runs fast.\n",
    "3. It’s well-suited for building simple conversational chatbots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d854b1e3-583a-40d3-9f81-c87bc078b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/blenderbot-400M-distill\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d86edd-65c0-4d4d-991d-0e82228f23ac",
   "metadata": {},
   "source": [
    "# Step 4: Fetch the Model and Initialize a Tokenizer\n",
    "\n",
    "In this step, we’re going to fetch a pre-trained language model (like the brain of our chatbot) and a tokenizer (the translator between human language and the chatbot’s brain). Both are essential to help the chatbot understand and respond in human-like language.\n",
    "\n",
    "When you run the code for the first time, the Hugging Face library will download the required files from their servers to your computer. The files include the \"brain\" of the chatbot and the tokenizer. But once downloaded, they are stored locally, so you don’t need to download them again.\n",
    "\n",
    "# What’s a Model?\n",
    "A model is like the \"brain\" of the chatbot. It’s a powerful program that has been trained on tons of text data (like books, websites, and conversations) to understand and generate text.\n",
    "\n",
    "Example:\n",
    "Imagine the chatbot’s brain has read thousands of novels, so now it can predict what a human might say in response to something like \"Hello, how are you?\" It might respond with, \"I’m good! How about you?\"\n",
    "\n",
    "# What’s a Tokenizer?\n",
    "A tokenizer is like a translator between human language and the model’s brain. Humans write in words, but the model’s brain understands language in the form of smaller pieces called tokens.\n",
    "\n",
    "Example:\n",
    "Think of the tokenizer as a chef cutting vegetables before cooking. If the model is the \"chef\" (the one who creates responses), the tokenizer is the knife, breaking down the ingredients (words) into smaller, manageable pieces (tokens).\n",
    "\n",
    "Example:\n",
    "> Sentence: \"I love pizza!\"\n",
    "\n",
    "> Tokens: [\"I\", \"love\", \"pizza\", \"!\"]\n",
    "\n",
    "Tokens are like the building blocks that the model uses to understand what we’re saying.\n",
    "\n",
    "# Why Is This Important?\n",
    "Without the tokenizer, the model wouldn’t understand your input. And without the model, the tokenizer would just chop up words without generating a meaningful response. They work as a team to make the chatbot intelligent and human-like.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfe2b54-7c63-4ca7-9017-9cf5697b4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (download on first run and reference local installation for consequent runs)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a86554-ec7f-4367-b5be-94522f31e149",
   "metadata": {},
   "source": [
    "# Step 5: Step 5: Chat with the Model\n",
    "\n",
    "Now that we have set up everything, it's time to start chatting with our chatbot! In this step, we will interact with the chatbot, and the chatbot will respond based on the conversation history. \n",
    "\n",
    "\n",
    "# What Happens in This Step?\n",
    "To make the chatbot give relevant responses, we need to keep track of what we say and what the chatbot says. This is where conversation history comes into play. It's like keeping a diary of everything that happens in the chat.\n",
    "\n",
    "We will also follow these steps to make sure the chatbot can understand and respond properly to what we ask:\n",
    "\n",
    "1. Store Conversation History: We need a place to keep track of what we and the chatbot have said. For now, we will use a list (like a simple notebook) to keep all messages.\n",
    "2. Encode the Conversation: We will prepare the conversation history to send it to the model. This is like getting all the pages from the notebook and preparing them for the chatbot to read.\n",
    "3. Ask the User for a Question: The chatbot will ask for a prompt, which is your question or statement.\n",
    "4. Tokenize the Prompt: We will break down what the user types into pieces (tokens), so the chatbot can understand it better.\n",
    "5. Generate a Response: The model will come up with a response based on both the user’s question and the previous conversation.\n",
    "6. Decode the Response: After the model generates a response, we convert it back into normal text for you to read.\n",
    "7. Update the Conversation History: We add this new exchange (your question and the chatbot's response) to the conversation history so that the chatbot remembers it.\n",
    "\n",
    "## Step 5.1: Keeping Track of Conversation History\n",
    "## Why is Conversation History Important?\n",
    "Think about how you have a conversation with someone. If you ask a question and they give you an answer, you usually remember the previous part of the conversation when you ask your next question. A chatbot needs to do the same to have a meaningful chat with you. Without knowing what was said before, the chatbot might give irrelevant or confusing answers.\n",
    "\n",
    "Example:\n",
    "> You: \"Hi, how are you?\"\n",
    "\n",
    "> Chatbot: \"I'm good, thank you for asking! How about you?\"\n",
    "\n",
    "> You: \"I’m doing great!\"\n",
    "\n",
    "> Chatbot: \"Glad to hear that!\"\n",
    "\n",
    "Notice that in the second message, the chatbot is referring to the earlier part of the conversation (\"How about you?\"). This is possible because the chatbot remembers what was said before, and that's where conversation history comes in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c27f6e0-d371-45f6-8380-5b15fe799590",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0419a0-7fe7-4207-b41c-bff2afe28485",
   "metadata": {},
   "source": [
    "## Step 5.2: Encoding the Conversation History\n",
    "\n",
    "In this step, we will prepare the conversation history before sending it to the model so that it can remember what has been said so far. Think of this as making sure the chatbot knows everything you've talked about before it gives its next answer.\n",
    "\n",
    "# Why Do We Need to Encode the Conversation History?\n",
    "When you talk to a person, they remember what you said earlier in the conversation, right? Similarly, for a chatbot to give meaningful responses, it needs to \"remember\" previous things you've said. This way, the chatbot can refer back to earlier parts of the conversation and provide relevant answers.\n",
    "\n",
    "Example:\n",
    "> You: \"What’s your favorite color?\"\n",
    "\n",
    "> Chatbot: \"I love the color blue!\"\n",
    "\n",
    "> You: \"Why do you like blue?\"\n",
    "\n",
    "> Chatbot: \"Because it's the color of the sky and the ocean.\"\n",
    "\n",
    "In this example, the chatbot knows you're asking about why it likes blue because it remembers your earlier question about its favorite color.\n",
    "\n",
    "To make this work with a machine, we need to pass the conversation history as one continuous string of text. This helps the model understand the context for generating its next response.\n",
    "\n",
    "# How Do We Encode the Conversation History?\n",
    "In Python, we can use a built-in function to encode the conversation history into one long string, with each message separated by a special symbol. This is what the model needs to understand the context.\n",
    "\n",
    "* Conversation History as a String: We will take the list of messages (conversation history) and combine them into one single string.\n",
    "\n",
    "* Why Use '\\n' (Newline Character)?: To make it easy for the model to read, we need to separate each message with a newline character '\\n'. This will look like each message being on a new line\n",
    "\n",
    "# Why Is This Important?\n",
    "By encoding the conversation history into a string, we give the model the context it needs to understand the conversation and generate an appropriate response. Without this context, the model wouldn't know what the previous questions and answers were, so it wouldn't be able to make sense of the next input you give it.\n",
    "\n",
    "So, this encoding step is a simple but powerful way to make sure the chatbot can hold a meaningful and coherent conversation with you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e63da37-442e-4e79-9753-30c2e59ce07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_string = \"\\n\".join(conversation_history)\n",
    "history_string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d3e8e-ff01-45cb-9b57-6ff4d7df1892",
   "metadata": {},
   "source": [
    "## Step 5.3: Fetch prompt from user\n",
    "\n",
    "Befor we start building a simple terminal chatbot, let us keep this input for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d358123d-c382-474e-a763-948d70d97385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, how are you doing?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text =\"hello, how are you doing?\"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f230ca-3ae8-4b01-af1c-4eda996843d7",
   "metadata": {},
   "source": [
    "## Step 5.4: Tokenization of User Prompt and Chat History \n",
    "\n",
    "# What Is Encoding?\n",
    "After we’ve broken down our text into tokens, the next step is to convert these tokens into numerical values so that the model can process them. Just like how computers understand numbers and not text, we need to transform words and tokens into a format that the computer can interpret.\n",
    "\n",
    "Example: \n",
    "> The word \"hello\" might become the number 45, \"world\" might become 123, and so on.\n",
    "\n",
    "This is the encoding process, where each token gets a unique number (or vector).\n",
    "\n",
    "# How Does Tokenization and Encoding Work Together?\n",
    "When you're interacting with a chatbot, you type out your questions or messages, and the chatbot needs to understand them to generate a response. For this, we tokenize your sentence and encode it into numbers. This allows the machine to process the information efficiently.\n",
    "\n",
    "Example: \n",
    "Tokenization and Encoding in Action\n",
    "\n",
    "> You: \"What's your name?\"\n",
    "\n",
    "First, the sentence is tokenized into smaller pieces:\n",
    "\n",
    "> Tokens: [\"What's\", \"your\", \"name\", \"?\"]\n",
    "\n",
    "Now, each of these tokens needs to be converted into numerical form, so the computer can understand them. This is encoding. The model will convert each token into a number.\n",
    "\n",
    "Example:\n",
    "\n",
    "> \"What's\" might be converted into the number 2345\n",
    "\n",
    "> \"your\" might be converted into the number 678\n",
    "\n",
    "> \"name\" might be converted into the number 1342\n",
    "\n",
    "> \"?\" might be converted into the number 99\n",
    "\n",
    "\n",
    "So after encoding, we have a list of numbers instead of words:\n",
    "\n",
    "> Encoded tokens: [2345, 678, 1342, 99]\n",
    "\n",
    "# Using the encode_plus Method\n",
    "In NLP, we usually use a special tool called a tokenizer to automatically handle the tokenization and encoding process. The tokenizer has a handy method called encode_plus that helps us convert both the input (your prompt or question) and the conversation history (previous exchanges) into tokens and numerical values.\n",
    "\n",
    "# Why Use encode_plus?\n",
    "The encode_plus method is part of a library called Transformers. It helps make sure that the input is ready to be processed by the model.\n",
    "\n",
    "# Tokenizing and Encoding the Text:\n",
    "* It breaks your sentence (or conversation history) into tokens (words or parts of words).\n",
    "* It converts those tokens into numbers that the model understands.\n",
    "\n",
    "# What’s Special About encode_plus?:\n",
    "* It not only performs the basic tokenization and encoding, but also prepares the input in a way that’s efficient and optimized for processing by the model.\n",
    "* It can handle multiple types of inputs, making it versatile.\n",
    "\n",
    "\n",
    "\"What's your name?\" is tokenized into smaller chunks like [\"What's\", \"your\", \"name\", \"?\"].\n",
    "Then, each of those tokens is turned into a number, ready for the model to process.\n",
    "add_special_tokens=True adds special tokens that the model might need, like a beginning-of-sequence token.\n",
    "return_tensors='pt' formats the data into a tensor, which is a special structure used in machine learning models.\n",
    "\n",
    "# Why Do We Need to Encode the History and Prompt Together?\n",
    "In many chatbot applications, we don’t just need the current user input but also the history of the conversation to give context to the model. This helps the chatbot provide more meaningful responses.\n",
    "\n",
    "Like, if you ask the chatbot \"What is your name?\" and then later ask \"Where do you live?\", the chatbot needs to remember your first question to give a relevant response. So, we combine the conversation history and the latest user input, tokenize and encode everything together.\n",
    "\n",
    "Step-by-Step Breakdown: \n",
    "Let’s say you have a chatbot conversation like this:\n",
    "\n",
    "> You: \"Hi, how are you?\"\n",
    "\n",
    "> Chatbot: \"I’m good, thanks for asking!\"\n",
    "\n",
    "> You: \"What’s your name?\"\n",
    "\n",
    "Now, we want the chatbot to remember the earlier part of the conversation while it processes your latest question (\"What’s your name?\").\n",
    "\n",
    "We’ll encode the entire conversation as one string:\n",
    "\n",
    "> \"Hi, how are you?\\nI’m good, thanks for asking!\\nWhat’s your name?\"\n",
    "\n",
    "This conversation history will be tokenized and encoded into numerical form, so the model understands it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a59fa9c-3d96-4d27-8207-6ee75b8c06dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1710,   86,   19,  544,  366,  304,  929,   38]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f5079-201b-4f2b-9f52-bcac7e104ea7",
   "metadata": {},
   "source": [
    "* In doing so, we've now created a Python `dictionary` which contains special keywords that allow the model to properly reference its contents. To learn more about tokens and their associated pretrained vocabulary files, you can explore the pretrained_vocab_files_map attribute. This attribute provides a mapping of pretrained models to their corresponding vocabulary files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114d94da-c50b-4717-bd25-789f50d38bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pretrained_vocab_files_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6009ab7-5d77-4964-82c8-2326fd7589b9",
   "metadata": {},
   "source": [
    "## Step 5.5: Generating Output from the Model\n",
    "\n",
    "Now that we've prepared everything — the chatbot’s conversation history and current input — it’s time for the chatbot to respond! Just like how you’d ask a question and expect an answer, the model needs to be given everything it needs to generate the right response.\n",
    "\n",
    "# What is the generate() Function?\n",
    "In a chatbot, the generate() function is like giving the model everything it needs (conversation history and current input), and then it produces a response based on that information.\n",
    "\n",
    "Imagine you ask a chatbot, “What’s the weather today?” The generate() function takes that question, looks at any past conversations you’ve had (if applicable), and then comes up with the most appropriate response, like “It’s sunny today.”\n",
    "\n",
    "# How Does the generate() Function Work?\n",
    "Here’s how it works in a simple, non-technical way:\n",
    "\n",
    "* You Provide Inputs:\n",
    "\n",
    "You type your question or statement (this is your current input).\n",
    "The chatbot looks at past conversations to understand the context better.\n",
    "\n",
    "# The Model Does the Heavy Lifting:\n",
    "The generate() function uses the inputs (current input + conversation history) to figure out what response would make the most sense.\n",
    "It doesn't just spit out any answer randomly; instead, it generates an answer that fits based on everything it’s \"learned\" from the conversation.\n",
    "\n",
    "# The Output is Generated:\n",
    "The chatbot then gives you a response based on all this input. The output is usually in the form of a sentence or multiple sentences.\n",
    "\n",
    "# What’s in the generate() Function?\n",
    "The generate() function takes a number of inputs to create the final response. When we say inputs, we are talking about both:\n",
    "\n",
    "1. The conversation history (everything that has been said so far).\n",
    "2. The new message or question you ask.\n",
    "\n",
    "In Python programming, we call these inputs \"keyword arguments\" (also known as kwargs). This simply means we pass all the necessary information to the function in a way that the function can understand, process, and respond.\n",
    "\n",
    "Example of input (kwargs):\n",
    "The model.generate() function can take conversation history (as a string of text), user input, and some additional settings like the maximum length of the response or temperature (a setting that controls how random or creative the response is).\n",
    "\n",
    "# Why Use generate()?\n",
    "The generate() function is helpful because:\n",
    "1. It takes care of all the complex tasks behind the scenes: understanding the input, processing the data, and generating the response.\n",
    "2. It ensures that the model responds correctly, considering the context of your conversation.\n",
    "\n",
    "Step-by-Step Example:\n",
    "Let’s walk through an example with a chatbot:\n",
    "\n",
    "> You say: \"Hi, how are you?\"\n",
    "\n",
    "The chatbot's history is now: [\"Hi, how are you?\"]\n",
    "\n",
    "> Chatbot replies: \"I’m good, thanks!\"\n",
    "\n",
    "The chatbot's history now is: [\"Hi, how are you?\", \"I’m good, thanks!\"]\n",
    "\n",
    "> You ask: \"What’s your name?\"\n",
    "\n",
    "The chatbot’s history now is: [\"Hi, how are you?\", \"I’m good, thanks!\", \"What’s your name?\"]\n",
    "\n",
    "Now, the chatbot needs to generate a response:\n",
    "\n",
    "The model looks at the entire conversation history (what has been said before) and the most recent question (\"What’s your name?\").\n",
    "It uses the generate() function to create a response. It might respond with something like: \"I’m a chatbot, I don’t have a name, but you can call me whatever you like!\"\n",
    "\n",
    "This process involves the generate() function taking your input, understanding the conversation history, and coming up with an appropriate response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8526f4-d7d0-4839-b1f6-fdc75f6625a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  281,  476,  929,  731,   21,  281,  632,  929,  712,  731,   21,\n",
       "          855,  366,  304,   38,  946,  304,  360,  463, 5459, 7930,   38,    2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baf9d5-2e72-4e6c-b53b-9a7c67fccca3",
   "metadata": {},
   "source": [
    "## Step 5.6: Decoding the Output\n",
    "\n",
    "Now that we have the model's response, it's not quite in the form we want yet. The response is still in a tokenized format, meaning the model has given us a collection of \"tokens\" (small pieces or chunks of the response), not a complete sentence in plain text.\n",
    "\n",
    "To make sense of the model’s output and turn it into something we can read, we need to decode the tokens back into words, just like a puzzle where we piece together the individual parts to form a whole.\n",
    "\n",
    "This process of converting tokens back into words or sentences is called \"detokenization\" or \"reconstruction\".\n",
    "\n",
    "# Why Do We Need to Decode?\n",
    "1. Tokenization: The model works with tokens (chunks of text, like words or subwords), which it uses to understand the language. It breaks down everything into small pieces for better processing.\n",
    "\n",
    "2. Model Output: After generating a response, the model gives us these tokens, not the readable text.\n",
    "\n",
    "3. Detokenization: We need to rebuild the text from the tokens so we can see the response in plain, understandable words.\n",
    "\n",
    "# What is tokenizer.decode()?\n",
    "The decode() method is a special function that translates the tokens back into readable text.\n",
    "\n",
    "When we pass tokens to this method, it reconstructs the text from the tokens and gives us the final output, like a sentence you can read and understand.\n",
    "\n",
    "Example Breakdown: \n",
    "1. Model Generates Tokens\n",
    "Let’s say you asked the chatbot, “What’s your name?”\n",
    "\n",
    "The model processes your question, generates a response, and gives us the output in tokens. \n",
    "\n",
    "2. We Need to Decode the Tokens\n",
    "Now, we want to convert these tokens back into the actual sentence so we can read it. That’s where tokenizer.decode() comes in.\n",
    "\n",
    "3. Decoded Output\n",
    "After decoding, the output might look something like this:\n",
    "\n",
    "> \"I am a chatbot. You can call me whatever you like!\"\n",
    "\n",
    "# Why Does This Matter?\n",
    "Imagine trying to have a conversation with a robot that speaks in numbers and codes. That’s how the model works! But it’s not very helpful to us in this form, so we decode those numbers into words we can understand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b776d57-9242-45a3-8093-a03377fa749e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well. I am doing very well. How are you? Do you have any hobbies?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb3a59-825d-460d-bad3-d25aa4f6593b",
   "metadata": {},
   "source": [
    "We've successfully had an interaction with our chatbot! We've given it a prompt, and we received its response.\n",
    "\n",
    "Now, all that's left to do is to update our conversation history, so that we may pass it with the next iteration.\n",
    "\n",
    "### Step 5.7: Update Conversation History\n",
    "\n",
    "All we need to do here is add both the input and response to `conversation_history` in plaintext.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24ad501c-fc0f-4b76-817b-df15e4ed3a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello, how are you doing?',\n",
       " \"I'm doing well. I am doing very well. How are you? Do you have any hobbies?\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history.append(input_text)\n",
    "conversation_history.append(response)\n",
    "conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62670a5d-bf51-40cb-9dd0-42c788a5b877",
   "metadata": {},
   "source": [
    "# Step 6: Repeat\n",
    "\n",
    "We have gone through all the steps of interacting with your chatbot. Now, we can put everything in a loop and run a whole conversation! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6ecb1ce-4c65-466b-9a89-36d01ffa96ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Hi, How are you doing..?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm great! I'm doing great as well, thanks for asking. What hobbies do you have?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I do arts and crafts... and you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love crafts as well! I love to sew and crochet. What kind of crafts do you do?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I do calligraphy and paper arts..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to make jewelry. I've been doing it since I was a little girl.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Goodbye! It was nice chatting with you.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Create conversation history string\n",
    "    history_string = \"\\n\".join(conversation_history)\n",
    "\n",
    "    # Get the input data from the user\n",
    "    input_text = input(\"> \")\n",
    "\n",
    "    # If input matches \"goodbye\" or similar, end the conversation\n",
    "    if input_text.lower() in ['goodbye', 'exit', 'bye', 'end']:\n",
    "        print(\"Bot: Goodbye! It was nice chatting with you.\")\n",
    "        break\n",
    "\n",
    "    # Tokenize the input text and history\n",
    "    inputs = tokenizer.encode_plus(\n",
    "    history_string, input_text, \n",
    "    return_tensors=\"pt\", \n",
    "    max_length=128,  # Ensures truncation to 128 tokens\n",
    "    truncation=True,  # Automatically truncates the input if it exceeds max_length\n",
    "    padding=\"max_length\"  # Pads the input if it's shorter than max_length\n",
    ")\n",
    "\n",
    "\n",
    "    # Generate the response from the model\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    print(response)\n",
    "\n",
    "    # Add interaction to conversation history\n",
    "    conversation_history.append(input_text)\n",
    "    conversation_history.append(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394b5e9-a8a9-4b0e-a67f-219714321a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
