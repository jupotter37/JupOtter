{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2740feb770>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "import random\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "\n",
    "SUMMARY_NUM_WORDS = 600\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "0\n",
      "<torch.cuda.device object at 0x7f268783c610>\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# Load the vtt_data.csv file\n",
    "# filter only use 'large' files\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "podcast_data = []\n",
    "row_num = 0\n",
    "with open('vtt_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in reader:\n",
    "        row_num += 1\n",
    "        \n",
    "        if row_num == 1:\n",
    "            continue\n",
    "            \n",
    "        filename = row[5]\n",
    "        if not filename.endswith(\"_large.vtt\"):\n",
    "            continue\n",
    "\n",
    "        podcast = {    \n",
    "            \"episode_index\": row[0],    \n",
    "            \"guest\": row[1],\n",
    "            \"episode_name\": row[2],\n",
    "            \"host_name\": row[3],\n",
    "            \"episode_number\": row[4],\n",
    "            \"transcript\": row[6],\n",
    "            \"duration\": row[7],\n",
    "        }\n",
    "        podcast_data.append(podcast)\n",
    "#         break\n",
    "\n",
    "print(len(podcast_data))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_title_text_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'text': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'text': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'text': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'text': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_titles_stage_1(keypoints_text):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title.\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Text\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in keypoints_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_text_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(text_array):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "    # Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "    openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "    return np.array(openai_embed.embed_documents(text_array))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "    \n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_summary(summary):\n",
    "    eval_prompt_template = \"\"\"\n",
    "    Rewrite the given summary to improve readability.\n",
    "    Use transitional words or phrases at the beginning of paragraphs if necessary.\n",
    "    Remove the reference of 'podcast' in the rewritten summary.\n",
    "    The rewritten summary should have 300-400 words.\n",
    "\n",
    "    Here is the data:\n",
    "    {summary}\n",
    "\n",
    "    Return your answer in the following format:\n",
    "    REWRITTEN_SUMMARY\n",
    "    \"\"\"\n",
    "    \n",
    "    eval_prompt = PromptTemplate(template=eval_prompt_template, input_variables=[\"summary\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = eval_prompt)\n",
    "\n",
    "    eval_input_data = [\n",
    "        {\n",
    "            'summary': summary    \n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    map_llm_chain_input = eval_input_data\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "    print()\n",
    "    print(\"RRR given summary\")\n",
    "    print(summary)\n",
    "    print(\"RRR rewritten summary\")\n",
    "    print(map_llm_chain_results)\n",
    "    return map_llm_chain_results[0]['text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "  ...\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "#   map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "#     {text}\n",
    "\n",
    "#     CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Write a 175-200 word summary of the following topic of a podcast:\n",
    "      {text}\n",
    "\n",
    "      CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "\n",
    "  print(f\"RRRRRR summary_num_words: {summary_num_words}\")\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following podcast, removing irrelevant information. \n",
    "  \n",
    "  Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'texts': [stage_1_outputs[chunk_id]['text'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['texts_concat'] = ' '.join(topic_data['texts'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['texts_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "  title_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  print(\"RRRRR titles:\")\n",
    "  for title in titles:\n",
    "    print(title)\n",
    "\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "  reduce_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "\n",
    "  final_summary = rewrite_summary(final_summary)\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '4', '5', '6', '7', '9', '10', '11', '13', '14', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '28', '30', '31', '32', '34', '35', '36', '38', '40', '41', '42', '43', '44', '47', '48', '49', '50', '52', '53', '56', '57', '60', '61', '62', '65', '66', '68', '69', '70', '71', '72', '73', '74', '75', '76', '79', '80', '81', '83', '86', '89', '90', '91', '92', '93', '94', '95', '97', '98', '99', '103', '104', '106', '108', '109', '110', '111', '113', '114', '115', '118', '119', '120', '122', '126', '129', '130', '131', '132', '133', '139', '141', '144', '146', '147', '148', '151', '153', '155', '157', '160', '168', '173', '177', '181', '183', '186', '187', '188', '190', '193', '195', '206', '208', '209', '213', '215', '217', '218', '219', '221', '222', '224', '225', '235', '241', '246', '247', '250', '252', '257', '258', '261', '266', '271', '280', '294', '299', '302', '306', '307', '309', '322', '325']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "# Filter out and keep only techincal podcasts\n",
    "f = open('./summarized_dataset/check_is_techincal_podcast.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "check_is_technical_podcast = json.load(f)\n",
    " \n",
    "is_techincal_episode_numbers = []\n",
    "\n",
    "for podcast in check_is_technical_podcast:\n",
    "    is_technical = podcast['is_technical']\n",
    "    if is_technical == \"yes\":\n",
    "        is_techincal_episode_numbers.append(podcast['episode_number'])\n",
    "        \n",
    "print(is_techincal_episode_numbers)\n",
    "print(len(is_techincal_episode_numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(chunks_text, show_log=False):\n",
    "  \n",
    "  print(f'extract_keypoints start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"\n",
    "  Extract the key points out of the give text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in a list, with new line separating each key point.\n",
    "  There is no limit on the number of key points in your list\n",
    "  Each key point starts with '<->' and ends with a '.'\n",
    "  Here is the format of the list: \n",
    "  <-> key point 1\n",
    "  <-> key point 2\n",
    "  <-> key point 3\n",
    "  ...\n",
    "\n",
    "  KEY_POINTS:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "#   if show_log:   \n",
    "#       print(\"map_llm_chain_results:\")\n",
    "#       print(map_llm_chain_results)\n",
    "    \n",
    "  keypoints = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log:\n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"keypoints:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "            \n",
    "      result_keypoints = result['text'].split('<->')\n",
    "      result_keypoints = [k.strip() for k in result_keypoints if k.strip()]\n",
    "      keypoints.append({'text':result_keypoints})\n",
    " \n",
    "  print(f'extract_keypoints done time {datetime.now()}')\n",
    "  return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_questions(chunks_text, show_log=False):\n",
    "  print(f'remove_questions start time: {datetime.now()}')\n",
    "\n",
    "  map_prompt_template = \"\"\"\n",
    "  Your jon is to read through the given text and remove sentences that are asking a question.\n",
    "  Remove all the sentences that end with a question mark '?'.\n",
    "  Here is the given text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer as text with sentences that are question removed.\n",
    "\n",
    "  QUESTIONS_REMOVED_TEXT:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-1106')\n",
    "    \n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  print(\"remove_questions map_llm_chain_results:\")\n",
    "#   print(map_llm_chain_results)\n",
    "  print(f'remove_questions done time {datetime.now()}')\n",
    " \n",
    "  processed_chunks = []\n",
    "  for i, result in enumerate(map_llm_chain_results):\n",
    "      if show_log: \n",
    "          print(\"chunks:\")\n",
    "          print(chunks_text[i])\n",
    "          print(\"question removed chunks:\")\n",
    "          print(result['text'])\n",
    "          print(\"-------\")\n",
    "      processed_chunks.append({'text':result['text']})\n",
    "\n",
    "  return processed_chunks   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments[i])\n",
    "    segment_words = segments[i].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "    chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "    chunk_text = ' '.join(chunk['text'].tolist())\n",
    "    \n",
    "    chunks.append({\n",
    "      'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "      'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "      'text': chunk_text,\n",
    "      'num_words': len(chunk_text.split(' '))\n",
    "    })\n",
    "    \n",
    "  chunks_df = pd.DataFrame(chunks)\n",
    "  return chunks_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_questions start time: 2024-03-18 09:09:24.622120\n",
      "remove_questions map_llm_chain_results:\n",
      "remove_questions done time 2024-03-18 09:13:34.292140\n",
      "chunks_text len: 81\n",
      "extract_keypoints start time: 2024-03-18 09:13:34.292285\n",
      "extract_keypoints done time 2024-03-18 09:15:30.096558\n",
      "Start time: 2024-03-18 09:15:30.096762\n",
      "Stage 1 done time 2024-03-18 09:17:27.478677\n",
      "RR stage_1_outputs:\n",
      "[{'title': 'The Impact of TensorFlow on the Tech Industry ', 'text': 'Rajat Manga is an engineer and director of Google, leading the TensorFlow team. TensorFlow is an open source library at the center of much of the work in deep learning. It is now an ecosystem of tools for the deployment of machine learning in various platforms. There is a big emphasis on growing a passionate community of developers. TensorFlow 2.0 is now in alpha and is being developed by a large team of engineers at Google Brain. The decision to open source TensorFlow is a definitive moment in the tech industry, inspiring many companies to open source their code.'}, {'title': 'The Impact of Open Sourcing TensorFlow ', 'text': 'The decision to open source TensorFlow is a definitive moment in the tech industry. Open innovation can be successful and inspire many companies to open source their code. The conversation is part of the Artificial Intelligence podcast. Rajat Manga was involved with Google Brain since its start in 2011 with Jeff Dean. The proprietary machine learning library turned into TensorFlow in 2014, the open source library. The idea of deep learning was interesting back then.'}, {'title': 'The Promise of Deep Learning ', 'text': 'The idea of deep learning was interesting and intriguing in some ways. Deep learning had shown some very promising and early results. Scaling the compute and data resulted in better performance. The first year or two involved proving out the belief in deep learning. Early wins were achieved in the first year. There are two early wins.'}, {'title': 'The Birth and Mission of Google Brain ', 'text': 'Google Brain was born around neural networks and focused on deep learning from the beginning. Early wins included collaboration with the speech research team and the success of the \"cat paper\" on images. The mission of Google Brain was to scale deep learning.'}, {'title': 'The Rise of Machine Learning in Industry and Academia ', 'text': 'Machine learning is now at the core of the entire company and is growing in that direction. Google has been doing machine learning for a long time, and as they scaled it up, they showed that it was possible and impactful. Real products started to use machine learning, such as speech and image recognition. Academia also started to show interest in deep learning and pushed for more research in the field.'}, {'title': 'The Rise of Deep Learning and Open Innovation in 2014 ', 'text': \"Academia and external push for deep learning in 2014. Recognition of the potential growth of deep learning. Decision to open source TensorFlow as a significant moment in software engineering. Google's decision to lead the world in open innovation. Embracing open innovation as a powerful concept.\"}, {'title': 'The Impact of Open Source on Research and Innovation ', 'text': 'The decision to go open source with a lot of IP was a significant moment in time. The initial idea came from Jeff, who was a big proponent of open innovation. The research group was focused on sharing their research to push the state of the art forward. Deep learning and machine learning have grown rapidly due to sharing research. The next step was to consider how software could help with sharing research. Existing libraries like Tiano and Torch were already available for this purpose.'}, {'title': 'The Importance of Software for Development Speed ', 'text': 'The need for software to help with the speed of development. Existing libraries such as Tiano and Torch were done by academia and had a different level. Google had done lots of software internally and published papers, leading to successful open source projects. The tech built by Google was considered better than existing tech like Hadoop. Google Cloud was not providing their tech, but offering H base APIs on top of Bigtable.'}, {'title': \"Google's Contributions to Bigtable and TensorFlow \", 'text': 'Google is providing H base APIs on top of Bigtable to help push a good standard forward. TensorFlow is an open source library that can be used anywhere. Google Cloud ensures lots of integrations with everything else and works really well with TensorFlow. The interviewee is leading the TensorFlow effort. The interviewee discusses the history and timeline of the TensorFlow project.'}, {'title': 'The Evolution of TensorFlow ', 'text': 'TensorFlow effort led by the speaker. The incredible ecosystem surrounding TensorFlow. The timeline of TensorFlow development, from starting in 2014 to open sourcing in 2015. The fast pace of development in deep learning.'}, {'title': 'Decision to Open Source Project and Support for Mobile and GPU ', 'text': 'The decision to open source the project was made in late 2014. Consideration was given to running the project at large scale in the data center and supporting different kinds of hardware, including GPUs. There was a focus on running models on mobile and supporting customization of code. The design of the project included support for running models on mobile devices. The project was being designed to support deployment and running of models on mobile phones.'}, {'title': 'Title ', 'text': 'Running Machine Learning Algorithms on Mobile PhonesText '}, {'title': 'Influences on Design Decisions in Machine Learning Framework Development ', 'text': 'The design decisions were influenced by the parallel development of internal beliefs and external libraries such as Theano, Torch, Lua, Caffe, and possibly JNR. The team looked at a number of libraries and discussed ideas around whether to have a graph or not. Key decisions were made based on limitations seen in prior beliefs and the fast-moving research environment.'}, {'title': 'Key Decisions in Transitioning to TensorFlow 2.0 ', 'text': 'The key decisions included the need for flexibility in research and hardware changes. Moving towards TensorFlow 2.0 and eager execution were important graph decisions. The decision to hide the graph a little bit was influenced by the less intuitive nature of graph development. The origin of the decision came from the prior disbelief having a graph-like structure, but in a simpler form.'}, {'title': 'The Use of Graphs for Production Deployment ', 'text': 'The graph was initially a simple straight line thing, more like a cafe. The decision to use a graph came from the need to deploy a lot of stuff in production. The team experimented with using Python without a graph, but it made deployment more complicated. The focus on production was a key factor in the decision to use a graph. The team did not anticipate the high number of downloads, 41 million.'}, {'title': 'The Impact of Open Sourcing on Deep Learning ', 'text': 'The unexpected popularity of the product with 41 million downloads. The recognition of the need for the product from a research perspective. The potential for future growth and enabling more people to use the product. The impact of open sourcing on the growth of deep learning. The incredible amount of attention from a global population of developers after open sourcing.'}, {'title': 'The Impact of Open Sourcing on a Deep Learning Project ', 'text': 'The project received an incredible amount of attention from a global population of developers after open sourcing. There is now good documentation, an ecosystem of tools, a community, a blog, and a YouTube channel for the project. The project is very community driven. The documentation for the project was a huge step up from academic projects. Deep learning shifted from being a research thing to something more accessible and practical.'}, {'title': 'The Evolution of Deep Learning for Developers ', 'text': 'Deep learning shifted from being a research thing to something that developers could use for interesting projects. The focus shifted from just researchers to also include stability and deployment for users. Planning for version 1.0 involved addressing the needs for stability, deployment, and documentation. The release of version 1.0 led to increased interest and support from enterprises. Subsequent releases focused on meeting the needs of enterprise users.'}, {'title': 'Enterprise Adoption of Product Post 1.0 Release ', 'text': 'The excitement around the enterprise adoption of the product post 1.0 release. The initial interest from researchers, hobbyists, and early adopters before 1.0 release. The pressure for stability from enterprises before the 1.0 release. The importance of understanding what enterprises want in the midst of product development.'}, {'title': 'Popular Models in Machine Learning ', 'text': 'Inception and ResNet 50 are still widely used by many people, even though they are a few years old. Some users prioritize stability and simplicity over the latest performance or quality improvements. Providing stability and simplicity allows more people to access the technology. The research crowd is interested in exploring new and advanced models such as RNNs and transformers.'}, {'title': 'Evolution of Deep Learning Models ', 'text': 'Deep learning models are evolving from RNNs to transformers, and now need to combine with RL and GANs. The boundary of state-of-the-art in deep learning is shifting and pushing. Older deep learning models from two or three years ago are still very usable by many people. The stability of past deep learning models makes it easier for many people to use them.'}, {'title': 'Common Uses of Data Predictions ', 'text': 'The most common case for hobbyists is to make data predictions. Enterprises want to make predictions on their data using regression models, linear models, or deep learning. The focus for enterprises is on structured data and making predictions. The best of enterprise probably just has a very specific use case for data predictions.'}, {'title': 'Perspectives on Structured Data, Deep Learning, and TensorFlow Extended ', 'text': \"The audience's perspective on structured data and deep learning may vary. Enterprise with a large dataset can benefit from deep learning. The importance of stability and simplicity in the entire pipeline of TensorFlow Extended. The need for repetitive model training in various industries such as immigration and insurance.\"}, {'title': 'The Impact of Machine Learning on Legal Data Organization ', 'text': 'Machine learning entering the legal realm is a common question in various disciplines such as immigration and insurance. Companies often have old school data organization methods, which hinders the readiness and digitization of data. There are various questions and needs related to making machine learning work, ranging from basic requirements to expert support. The lack of digitized data presents an interesting challenge for the implementation of machine learning in legal and other disciplines.'}, {'title': 'Title ', 'text': 'The Importance of Organized Data and Starting with Basic Models in Machine LearningText '}, {'title': 'Making TensorFlow More Accessible with Keras ', 'text': 'Start with the basics and improve from there. Keras made TensorFlow more accessible. Francois started the Keras project before he was at Google. Keras was initially on top of Tiano and then on top of TensorFlow. There were enough similarities between Keras and TensorFlow that Francois decided to integrate Keras with TensorFlow.'}, {'title': 'The Development of TensorFlow Interface by Tiano ', 'text': \"The creation of the interface and integration of TensorFlow as a backend was done by Tiano. Tiano's decision to create the interface and integrate TensorFlow was made before he joined Google. Tiano was initially working on research ideas and doing Keras as a side project at Google. Tiano's research work and papers demonstrate his skills as a great researcher. Tiano's work on the API was well-received by the community.\"}, {'title': 'Integration of Keras into TensorFlow ', 'text': 'Keras was integrated into TensorFlow in a deep way. Keras is the recommended way for a beginner to interact with TensorFlow 2.0. The integration of Keras into TensorFlow was initially planned for a quarter but has now been ongoing for two years. The decision to integrate Keras into TensorFlow was made after realizing the positive reception of Keras API. The researcher who developed Keras was invited to join the team and work on the integration.'}, {'title': 'The Decision to Focus on Keras as the Standard API for Integration ', 'text': 'The decision to focus on Keras was a bold one after considering multiple APIs. The goal was to integrate Keras as much as possible and simplify the process for users. The community was confused about which API to use, leading to the need for a standard choice. The decision to focus on Keras 2.0 was based on simplifying and picking one standard API. Keras was the clear choice based on community preference.'}, {'title': 'Choosing Keras for Version 2.0 ', 'text': 'The decision to simplify and pick Keras as the main framework for version 2.0. Keras was chosen because it was popular and had many great features. The integration of Keras into TensorFlow was surprising but ultimately empowering. The goal is to make it easier for developers and align with the same vision as the team. Python has Guido van Rossum, who held the position of.'}, {'title': 'The Importance of Leadership in Open Source Projects ', 'text': 'Python has Guido van Rossum, who held the position of benevolent dictator for life. Successful open source projects like TensorFlow need one person who makes a final decision. TensorFlow Dev Summit was successful with new features being incorporated. Martin Wick has driven a lot of open source stuff and APIs.'}, {'title': 'Efforts to Grow and Scale the Open Source Community ', 'text': 'Martin Wick has driven a lot of open source stuff and APIs. Regular design reviews are conducted. Efforts have been made to open up to the community and add transparency. More processes are being set in place, such as RFCs and special interest groups, to grow the community and scale it. The ecosystem is at a scale where a lone decision maker is not sufficient. Andrej Karpathy first did ComNetJS, allowing training of neural networks in the browser using JavaScript. TensorFlow.js is making training neural networks in the browser a serious and legitimate thing.'}, {'title': 'TensorFlow for Machine Learning ', 'text': 'TensorFlow.js allows training and networking in the browser using JavaScript. TensorFlow Extended and TensorFlow Lite are also available for different platforms. The goal is to enable machine learning in various ways, including support for deep learning and other exciting developments in ML.'}, {'title': 'Title ', 'text': 'Advancing Machine Learning Research and Integration into Real ProductsText '}, {'title': 'The Expansion of Machine Learning to Mobile and Embedded Devices ', 'text': 'ML and training are no longer limited to workstations, data centers, or the cloud. Machine learning is now running on phones and tiny chips. The goal is to get machine learning on every device with compute capability. The ecosystem for machine learning has grown to cover more devices over time. There is a focus on pushing the boundaries and building more tooling to help with machine learning.'}, {'title': 'Exploring TensorFlow Libraries for Research and Production ', 'text': 'TensorBoard is the first tool mentioned for learning the training piece and the effects of TensorFlow extended. There are lots of libraries being built on top of TensorFlow for research and production purposes. Some libraries like TensorFlow agents and TensorFlow probability started as research tools but are now being used in production. Libraries have come from within Google and from the community to address different needs and goals. The goal is to enable the community to build and use different pieces for their specific needs.'}, {'title': 'Title ', 'text': 'Empowering Community Collaboration in TensorFlow 2.0Text '}, {'title': 'Challenges in Integrating TensorFlow.js and Deep Learning JS ', 'text': 'The skepticism about the difficulty of integrating TensorFlow.js or deep learning JS into the ecosystem. The technical difficulty of the project, especially in integrating into the ecosystem. The numerous challenges that have been and still need to be overcome in the technical side. The learning and iteration process over the last few years. The importance of making things look easy for the end user, despite the complexity behind the scenes. The anticipation of more challenges ahead with the addition of more devices.'}, {'title': 'Challenges in End User Experience and System Evolution ', 'text': 'The end user experience should be easy, but there are many complexities behind it. There are challenges ahead with the increasing number of devices and hardware perspective. There is ongoing work on compiler stuff and APIs to address these challenges. TensorFlow started as a monolithic system and is still largely monolithic despite the addition of tools around it. The system is rapidly evolving and difficult to change and modify.'}, {'title': 'Challenges of Adapting to the Rapid Evolution of TensorFlow ', 'text': 'The system is four years old and still rapidly evolving, making it difficult to change and modify. Many people rely on TensorFlow in their applications, creating technical debt and responsibility for previous versions to still work. TensorFlow 2.0 breaks some backward compatibility, but the conversion seems pretty straightforward. The challenge is to change and modify the system without slowing down its evolution.'}, {'title': 'The Importance of Balancing Backward Compatibility and New Changes in TensorFlow 2.0 ', 'text': \"2.0 breaks some back compatibility, but not too much. The conversion seems pretty straightforward. It's a tricky balance between compatibility and new changes. Production systems rely on TensorFlow, so compatibility is important. There is a huge cost associated with maintaining compatibility. It's a trade off between slowing certain things down and bringing overall value.\"}, {'title': 'Importance of Designing with a Clean Slate in Mind ', 'text': \"Designing with a clean slate in mind is important when starting new things. Making compromises occasionally is necessary, but designing with a clean slate is crucial. The overall value of bringing in new changes is much bigger, even if it may slow certain things down. It's important to consider the impact on future individuals joining the team when making changes. Responsibility in the idea stage is crucial for successful implementation of new ideas.\"}, {'title': 'The Significance of Embracing New Ideas and Competition in Research and Development ', 'text': 'The importance of starting with a clean slate and not worrying about past ideas. The idea of putting past ideas behind and focusing on new ones. The preference for TensorFlow over PyTorch in the research group. The recognition of competition between TensorFlow and PyTorch. The enjoyment of different ideas and competition in research and development.'}, {'title': 'Comparison of TensorFlow and PyTorch ', 'text': \"TensorFlow focused on both research and production, while PyTorch focused solely on research. PyTorch prioritized ease of use over speed, while TensorFlow prioritized speed and production readiness. Both TensorFlow and PyTorch learned from each other's approaches and made improvements based on previous technologies. PyTorch built on technologies like JNR to enhance its capabilities.\"}, {'title': 'The Importance of Eager Execution in Project Development ', 'text': 'The benefit of seeing what had come before and exploring different kinds of spaces. Building on things like JNR and previous experiences. Competition is interesting and led to the development of eager execution. Revisiting the idea of eager execution multiple times before deciding to add it. It took a while to get all the components of eager execution together. Eager execution is a powerful addition to the project.'}, {'title': 'The Evolution of TensorFlow and Its Impact on Keras and Eager Execution ', 'text': \"It might have taken longer. TensorFlow is finally in the way they did. It's doing some incredible work last couple years. Making it easily accessible to Keras, eager execution. 2.0 is, and with all the things that we've talked about. There are lots of other things that it enables us to do and that we're excited about. Here are these really clean APIs.\"}, {'title': 'The Benefits of Clean APIs ', 'text': 'The clean APIs enable new possibilities and excitement. The APIs allow for performance optimization and cleaner code. There are plans to explore new spaces and capabilities after version 2.0.'}, {'title': 'Exploring Future Developments in TensorFlow ', 'text': 'TensorFlow allows for exploration of other spaces behind the scenes in future versions. Restructuring the monolithic thing into more pieces and making it more modular is important for other organizations and ecosystem. The TensorFlow organization at GitHub has lots of repositories, including the core one with the execution engine, key backends for CPUs and GPUs, and work for distributed stuff.'}, {'title': 'Improving Scalability and Customization in TensorFlow ', 'text': 'TensorFlow has the execution engine and key backends for CPUs and GPUs. It has the capability to do distributed work. There are some interfaces for splitting the core components, but they are not very clean. Clean interfaces for running on custom clusters with custom networking would be ideal. Clean separation of components would help in supporting more interesting things and scaling better. Enabling individual developers and organizations to evolve and push on things independently allows for better scalability. The hope is to enable the ecosystem to scale better for both individual developers and organizations.'}, {'title': 'The Growing Influence of TensorFlow in Major Corporations ', 'text': 'TensorFlow allows for independent scaling. Major corporations like Pepsi are already using TensorFlow. Some companies, like hardware vendors and IBM, are involved in special interest groups for TensorFlow. Autonomous vehicle companies are also interested in TensorFlow.'}, {'title': 'The Growth of TensorFlow ', 'text': \"TensorFlow has been downloaded 41 million times, with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. There is a sweet spot of timing for new things, including what's needed and how it grows. TensorFlow has grown not just because it was a good tool, but also because it grew with what was needed.\"}, {'title': 'Importance of Timing, Growth, and Community Engagement in Open Source Projects ', 'text': 'Timing and growth are important factors in the development of tools like TensorFlow. Listening to the community and being open to external contributions is crucial for the success of an open source project. Transparency and communication with the community are essential for the success of an open source project.'}, {'title': 'The Role of Community in Project Growth ', 'text': 'Community aspects play a significant role in the process. Small projects may find it easier to implement processes with fewer developers. As a project grows, more processes, documentation, and tools become necessary. The growth of TensorFlow is fueled by people building and implementing cool and useful architectures on GitHub.'}, {'title': 'Transitioning to Newer Versions of TensorFlow ', 'text': \"There may be a partitioning like there is with Python 2 and 3, with a code base in the older versions of TensorFlow not being as compatible easily. Working hard to make the transition to newer versions very easy. Lots of tooling discussed at the developer summit and continued investment in that tooling. Pushing hard to make the transition to newer versions very smooth. People want to move to the new thing because they see the value, not just because it's new. Expecting to see a shift towards the new version as people start to see the value.\"}, {'title': 'Rapid Advancements in Deep Learning and Future Expectations ', 'text': 'People want a really good thing and the value will be seen in the next few months. The field is moving rapidly, which will help in doing more things and new things will happen in 2.x. Change is expected to happen, but the basics of deep learning, such as convolution models, will probably still be around in some form in five years.'}, {'title': 'The Future of Deep Learning and TensorFlow ', 'text': 'The basics of deep learning, such as convolution models, are likely to still be around in some form in five years. Directionally, there is a focus on combining eager execution and graphs to make programming more natural. Swift for TensorFlow is taking a similar approach to make programming more natural. In five years, there is an expectation to see more development in the area of combining eager execution and graphs. The uncertainty of whether hardware accelerators will remain the same in the future. The uncertainty of whether training with four bits will be possible in the future.'}, {'title': 'Advancements in Hardware Accelerators and TensorFlow Evolution ', 'text': 'In five years, there is an expectation to see more in the area of hardware accelerators and training with four bits instead of 32 bits. The evolution of TPU and TensorFlow are coevolving and learning from each other and from the community and applications. Efforts have been made to make TensorFlow as accessible and easy to use as possible, especially for beginners. Beginners want to be able to take some image model without caring about the specific model.'}, {'title': 'Making Image Model Training Accessible ', 'text': \"Beginners want to be able to use simple image models for training or transfer learning. Providing easy access to pre-trained models decreases the time to start. Different levels of users have different needs, from simple pre-built models to custom layer creation. The focus is on making it easy for users to use the models, regardless of what's inside the box.\"}, {'title': 'The Versatility and Impact of TensorFlow ', 'text': 'There is a whole spectrum of activities that can be done with TensorFlow, from beginners to advanced users. Pre-trained models provided by TensorFlow can significantly decrease the time needed to start a project. TensorFlow has made it easier for beginners to achieve their goals. High schoolers are now able to do a lot with TensorFlow, which is both amazing and terrifying. Google has a management aspect to its role with TensorFlow, leading a large number of developers and people.'}, {'title': 'The Power of Team Cohesion in TensorFlow Development ', 'text': 'TensorFlow is a cutting edge technology. Cohesion across the team is important for delivering well. Individual engineers can only do so much, but can achieve a lot more together. The product of what the team generates is larger than the individual contributions.'}, {'title': 'The Importance of Teamwork and Culture in Productivity ', 'text': \"The product of what the team generates is way larger than the whole or the individual put together. Having all team members work together and the culture of the team itself is important. Hiring good people is important, but they also have to care about what they're building and be motivated for the right kind of things. Having a somewhat unified vision of where the team wants to go is important. Google is a bottom-up organization, but it's important to combine that with a mix of direction as the organization grows.\"}, {'title': 'The Importance of Research and Team Dynamics in the Work Environment ', 'text': \"Research is important for the product and ecosystem. It's important to have a clear direction and not be all over the place. There are tensions and complexities in the work environment. Individual superstars play a significant role in the work, but can also create tensions within a team. The mission of superstars is important, even at Google.\"}, {'title': 'Challenges of Superstars in Team Dynamics ', 'text': 'Superstars can sometimes be against the dynamic of a team and cause tensions. The mission of the project in TensorFlow is beautiful and exciting. Google values getting people who care and have the same culture. The project allows for lots of people to do different things and grow. It is important for superstars to work well with the team.'}, {'title': 'Refining the Hiring Process at Google ', 'text': 'Hiring process at Google has been refined over the last 20 years. Core technical skills are important, but motivation also matters in hiring engineers. Productivity at Google is about the team, not just individual superstars. Superstars need to work well with the team across Google. Value addition is important, but not at the cost of hurting the team.'}, {'title': 'The Importance of Motivation in the Hiring Process ', 'text': \"The hiring process has been helpful in maintaining consistency. Motivation is important in addition to technical skills for long term success. Alignment of motivation with the team's goals is crucial for success. Motivation is important at every level, not just for senior positions. Lack of motivation can hinder success, regardless of intelligence.\"}, {'title': \"Importance of Puzzle Solving and Culture Fit in Google's Interview Process \", 'text': 'Puzzle solving ability and problem solving ability are important skills. The determination of whether a person has a strong internal drive and passion for their work is also considered. Culture fit is a significant part of the interview process at Google. Different projects and teams may have varying cultures and requirements. For example, TensorFlow has its own unique culture and requirements.'}, {'title': 'Understanding Project Culture at Google ', 'text': 'There are various kinds of projects and different kinds of things. The culture varies across projects and teams at Google. TensorFlow is a fast moving project and requires people comfortable with that pace. There is a focus on ensuring that things work really well and not cutting corners. Finding the right fit for different project requirements is important. The core culture at Google includes engineering excellence and a fun environment.'}, {'title': 'The Challenge of Making Difficult Decisions ', 'text': 'Difficult things are fun when you solve them. Striking a fine balance across different aspects is key to success. Making hard decisions, such as saying no to certain things, is a part of the process. The speed versus perfection and community involvement are important considerations. Both quick and thoughtful decisions can be difficult.'}, {'title': 'Making Decisions Under Time Constraints ', 'text': \"Making hard decisions quickly due to lack of time. Making hard decisions with time to think about them. The Dev Summit came together incredibly. Balancing the value of deadlines and urgency with getting the right things together. Emphasizing the importance of having something that's good and works well over perfection. The team did a great job in putting things together.\"}, {'title': 'Importance of Agile Development ', 'text': \"Urgency to get the right things together. Focus on key things that are important and figure out how much of it's important. Developing in the open, both internally and externally, everything's available to everybody. Regular releases at a regular cadence. Iterating and improving on things, even if they aren't fully ready.\"}, {'title': 'Importance of Quick Iteration and Improvement in Software Development ', 'text': \"Quick iteration and improvement is important. It's okay to release experimental versions for feedback. Pressure to make TensorFlow 2.0 stable. Comparison with WordPress 5.0 updates. Focus on quick cycle and iteration rather than strict deadlines.\"}, {'title': 'Title ', 'text': 'NodeX Release and Future PlansText '}, {'title': 'App Development Update ', 'text': 'The app has 41 million downloads for 1.0 X. The focus is on polishing and putting together features. The release is planned for the next few months or next quarter. The goal is to get it right and not rush the release. The speaker has experience leading a team at Google on search ads. Ads can connect people to the things they want and need, but at their worst, they can be annoying.'}, {'title': 'The Power of Ads and Machine Learning in Personalized User Experience ', 'text': 'Ads can connect people to the things they want and need, but at their worst, they can be annoying and ruin the user experience. Machine learning has the opportunity to shine in connecting users to personalized data and mapping to their actual wants and needs. Search ads are seen as an extension of what search is trying to do, which is to make information and make the user experience better.'}, {'title': 'The Role of Search Ads in Making Information Accessible ', 'text': \"Search ads are an extension of what search is trying to do, which is to make the world's information accessible. The goal of search ads is to align with what the users need and provide minimum quality level before showing the ad. Advertising is a key part of the web and has become a core part of search and many other search engines. The model of advertising has been around for ages and has been adapted to the web.\"}, {'title': 'The Importance of Ad Revenue in the Online World ', 'text': 'Adapting to the web and becoming a core part of search engines across the world. Striking a balance between showing valuable ads and avoiding annoying ones. The need for monetization to provide services like search and websites. The significance of ad revenue for businesses like Google. The reluctance of internet users to pay for services.'}, {'title': 'The Shift Towards Paid Services on the Web ', 'text': 'The model funds businesses like Google, which is a significant revenue stream. Advertisements, when done well, can be useful and not annoying. There is a transition towards more paid services across the web. People are willing to pay for services they see value in, such as Netflix and paid apps.'}, {'title': 'The Shift Towards Willingness to Pay for Content ', 'text': 'People are willing to pay for content they see value in, such as Netflix and newspaper websites. There is a shift towards more people being willing to pay for content, compared to a few years ago. The speaker sees a change in themselves and those around them in terms of willingness to pay for content. The speaker is hopeful for a transition to a mixed model where content can be tried for free with ads, but also has a clear revenue model.'}, {'title': 'The Accessibility and Power of TensorFlow for Students ', 'text': 'TensorFlow open source can be run on desktop and phone, making it more accessible and powerful. Cloud platforms like Colab make it easy for students to get started without any installation needed. The power in the hands of students is a lot more due to the increasing power of desktops and phones.'}, {'title': 'Getting Started with Machine Learning and TensorFlow using Colab ', 'text': 'Colab makes it easy to get started with machine learning and TensorFlow. No installation is needed to use Colab, making it very convenient. Colab is a free service, making it great for beginners to play and explore. However, with free services, there are limitations on what you can do. Beginners should start by visiting the TensorFlow website and exploring tutorials and guides. Colab allows users to get started with machine learning without any installation.'}, {'title': 'Title ', 'text': 'No Installation Needed for Getting StartedText '}]\n",
      "generating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done gen embeddings.\n",
      "num_topics: 8\n",
      "get topics 2024-03-18 09:17:28.383079 ...\n",
      "Best SD: 1.7638342073763937, Best iteration: 26\n",
      "done get topics 2024-03-18 09:17:29.404344.\n",
      "Stage 2 start time 2024-03-18 09:17:29.404363\n",
      "RRRRRR summary_num_words: 600\n",
      "RRRRR titles:\n",
      "1. The Impact of TensorFlow on the Tech Industry and Open Source Innovation\n",
      "2. Key Decisions and Evolution in Transitioning to TensorFlow 2.0\n",
      "3. Evolution and Impact of Deep Learning Models in Machine Learning\n",
      "4. Development and Integration of Keras in TensorFlow\n",
      "5. Challenges and Expansion of TensorFlow in Machine Learning\n",
      "6. The Evolution and Influence of TensorFlow in Research and Development\n",
      "7. Growth, Community Engagement, and Future Expectations of TensorFlow\n",
      "8. Importance of Team Dynamics and Culture in Google's Project Development\n",
      "9. The Power of Ads, Machine Learning, and the Shift Towards Paid Services in Software Development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bizon/anaconda3/envs/w210_podcast_ollama/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRR given summary\n",
      "In the Artificial Intelligence podcast, Rajat Manga, director of Google's TensorFlow team, discusses the development and impact of the open source library on the tech industry. He explains how Google Brain's early successes in deep learning led to the creation of TensorFlow in 2014 and its open sourcing in 2015. This decision was a significant moment in software engineering, inspiring open innovation and the growth of deep learning. Google's mission to scale deep learning has led to the integration of machine learning into real products and increased interest in the field within academia. The interviewee emphasizes the importance of open innovation and the role of software in sharing research and speeding up development. He also highlights the integration of TensorFlow with Google Cloud and the passionate community of developers surrounding the library. Overall, the conversation provides insight into the history and timeline of the TensorFlow project, as well as the rapid pace of development in deep learning.\n",
      "\n",
      "The podcast discusses the continued use of older deep learning models like Inception and ResNet 50, as many users prioritize stability and simplicity over the latest performance improvements. The research community is exploring new and advanced models such as RNNs and transformers, and the boundary of state-of-the-art in deep learning is shifting. Enterprises focus on making predictions with structured data using regression models, linear models, or deep learning. The importance of stability and simplicity in the entire pipeline of TensorFlow Extended is highlighted, and the need for repetitive model training in industries like immigration and insurance is discussed. The podcast also addresses the challenges of implementing machine learning in disciplines like law due to the lack of digitized data and the importance of organized data and starting with basic models in machine learning.\n",
      "\n",
      "The podcast discusses the integration of Keras into TensorFlow, making it more accessible for beginners. The decision to focus on Keras 2.0 was based on community preference and the goal was to simplify and pick one standard API. The integration of Keras into TensorFlow was surprising but ultimately empowering, aligning with the team's vision. The podcast also touches on the need for a single decision maker in successful open source projects like TensorFlow, as well as efforts to open up to the community and add transparency. The ecosystem is at a scale where a lone decision maker is not sufficient. Additionally, the podcast mentions the development of ComNetJS by Andrej Karpathy, allowing training of neural networks in the browser using JavaScript, and the success of TensorFlow.js in making training neural networks in the browser a serious and legitimate thing.\n",
      "\n",
      "The podcast discusses the advancements in machine learning, particularly the availability of TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for different platforms. The goal is to enable machine learning on various devices with compute capability, including phones and tiny chips. The podcast also addresses the challenges and complexities of integrating machine learning into the ecosystem, as well as the evolution and technical debt associated with TensorFlow. The discussion emphasizes the importance of making the end user experience easy, despite the complexities behind the scenes, and the trade-off between maintaining compatibility and introducing new changes in TensorFlow 2.0. Overall, the podcast highlights the ongoing work and challenges in advancing machine learning research and integration into real products.\n",
      "\n",
      "The podcast discusses the importance of starting with a clean slate and not being tied down by past ideas, as well as the competition between TensorFlow and PyTorch in the research and development field. It highlights the different focuses of the two platforms, with TensorFlow catering to both research and production, while PyTorch prioritizes ease of use over speed. The podcast also explores the benefits of competition and learning from each other's approaches, as well as the development of eager execution in TensorFlow. It emphasizes the power of clean APIs and the plans to explore new spaces and capabilities after version 2.0. The podcast also delves into the restructuring of TensorFlow into more modular pieces to support better scalability and enable individual developers and organizations to evolve independently. It mentions the involvement of major corporations and autonomous vehicle companies in using TensorFlow, as well as the interest of hardware vendors and IBM in special interest groups for the platform.\n",
      "\n",
      "TensorFlow has been downloaded 41 million times and has seen significant growth with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The timing and growth of the project have been crucial to its success, as it has evolved to meet the needs of the community. Transparency, communication, and community involvement are essential for open source projects like TensorFlow. As the project grows, more processes, documentation, and tools become necessary. The development of TensorFlow is fueled by people building and implementing architectures on GitHub. The project is working to make the transition to newer versions smooth and is focused on combining eager execution and graphs to make programming more natural. There is uncertainty about the future of hardware accelerators and training with four bits, but there is an expectation to see more development in these areas in the next five years. TensorFlow has made it easier for beginners to use pre-trained models and achieve their goals, and there is a whole spectrum of activities that can be done with the platform. Google plays a significant role in managing TensorFlow and leading the large community of developers and users.\n",
      "\n",
      "The podcast discusses the hiring process at Google and the importance of motivation, culture fit, and teamwork in addition to technical skills. It emphasizes the need for motivation at every level and the alignment of motivation with the team's goals. The culture and requirements vary across different projects and teams at Google, with a focus on engineering excellence and a fun environment. The podcast also highlights the importance of making hard decisions, balancing speed and perfection, and considering the impact of changes on future team members. It emphasizes the value of cohesion across the team and the need for superstars to work well with the team. The hiring process at Google has been refined to prioritize motivation and teamwork over individual technical skills, and the overall productivity is about the team, not just individual superstars.\n",
      "\n",
      "The podcast discusses the importance of quick iteration and improvement, and the release of experimental versions for feedback. There is pressure to make TensorFlow 2.0 stable, with a comparison to WordPress 5.0 updates. The focus is on quick cycle and iteration rather than strict deadlines. The NodeX release and future plans are also mentioned. The speaker, with experience leading a team at Google on search ads, discusses the potential of machine learning in connecting users to personalized data and mapping to their wants and needs. The significance of ad revenue for businesses like Google is highlighted, as well as the shift towards more people being willing to pay for content. The accessibility and power of TensorFlow open source, as well as the convenience of using cloud platforms like Colab for machine learning, are also discussed. The podcast emphasizes the ease of getting started with machine learning using Colab, but also mentions the limitations of free services and encourages beginners to explore tutorials and guides on the TensorFlow website.\n",
      "RRR rewritten summary\n",
      "[{'text': \"The discussion with Rajat Manga, director of Google's TensorFlow team, provides insight into the development and impact of the open source library on the tech industry. The decision to open source TensorFlow in 2015 was a significant moment in software engineering, inspiring open innovation and the growth of deep learning. The interviewee emphasizes the importance of open innovation and the role of software in sharing research and speeding up development. He also highlights the integration of TensorFlow with Google Cloud and the passionate community of developers surrounding the library. The conversation provides insight into the history and timeline of the TensorFlow project, as well as the rapid pace of development in deep learning.\\n\\nThe continued use of older deep learning models like Inception and ResNet 50 is prioritized by many users for stability and simplicity. The research community is exploring new and advanced models such as RNNs and transformers, and the boundary of state-of-the-art in deep learning is shifting. Enterprises focus on making predictions with structured data using regression models, linear models, or deep learning. The importance of stability and simplicity in the entire pipeline of TensorFlow Extended is highlighted, and the need for repetitive model training in industries like immigration and insurance is discussed. The challenges of implementing machine learning in disciplines like law due to the lack of digitized data and the importance of organized data and starting with basic models in machine learning are also addressed.\\n\\nThe integration of Keras into TensorFlow has made it more accessible for beginners. The decision to focus on Keras 2.0 was based on community preference and the goal was to simplify and pick one standard API. The podcast also touches on the need for a single decision maker in successful open source projects like TensorFlow, as well as efforts to open up to the community and add transparency. The ecosystem is at a scale where a lone decision maker is not sufficient. Additionally, the podcast mentions the development of ComNetJS by Andrej Karpathy, allowing training of neural networks in the browser using JavaScript, and the success of TensorFlow.js in making training neural networks in the browser a serious and legitimate thing.\\n\\nThe advancements in machine learning, particularly the availability of TensorFlow.js, TensorFlow Extended, and TensorFlow Lite for different platforms, are discussed. The goal is to enable machine learning on various devices with compute capability, including phones and tiny chips. The challenges and complexities of integrating machine learning into the ecosystem, as well as the evolution and technical debt associated with TensorFlow, are also addressed. The discussion emphasizes the importance of making the end user experience easy, despite the complexities behind the scenes, and the trade-off between maintaining compatibility and introducing new changes in TensorFlow 2.0. The podcast highlights the ongoing work and challenges in advancing machine learning research and integration into real products.\\n\\nThe importance of starting with a clean slate and not being tied down by past ideas, as well as the competition between TensorFlow and PyTorch in the research and development field, is discussed. It highlights the different focuses of the two platforms, with TensorFlow catering to both research and production, while PyTorch prioritizes ease of use over speed. The podcast also explores the benefits of competition and learning from each other's approaches, as well as the development of eager execution in TensorFlow. It emphasizes the power of clean APIs and the plans to explore new spaces and capabilities after version 2.0. The podcast also delves into the restructuring of TensorFlow into more modular pieces to support better scalability and enable individual developers and organizations to evolve independently.\\n\\nTensorFlow has been downloaded 41 million times and has seen significant growth with 50,000 commits, almost 10,000 pull requests, and 1,800 contributors. The timing and growth of the project have been crucial to its success, as it has evolved to meet the needs of the community. Transparency, communication, and community involvement are essential for open source projects like TensorFlow. The development of TensorFlow is fueled by people building and implementing architectures on GitHub. The project is working to make the transition to newer versions smooth and is focused on combining eager execution and graphs to make programming more natural. There is uncertainty about the future of hardware accelerators and training with four bits, but there is an expectation to see more development in these areas in the next five years. TensorFlow has made it easier for beginners to use pre-trained models and achieve their goals, and there is a whole spectrum of activities that can be done with the platform. Google plays a significant role in managing TensorFlow and leading the large community of developers and users.\\n\\nThe hiring process at Google and the importance of motivation, culture fit, and teamwork in addition to technical skills are discussed. The culture and requirements vary across different projects and teams at Google, with a focus on engineering excellence and a fun environment. The podcast also highlights the importance of making hard decisions, balancing speed and perfection, and considering the impact of changes on future team members. It emphasizes the value of cohesion across the team and the need for superstars to work well with the team. The hiring process at Google has been refined to prioritize motivation and teamwork over individual technical skills, and the overall productivity is about the team, not just individual superstars.\\n\\nThe importance of quick iteration and improvement, and the release of experimental versions for feedback, is discussed. There is pressure to make TensorFlow 2.0 stable, with a comparison to WordPress 5.0 updates. The focus is on quick cycle and iteration rather than strict deadlines. The NodeX release and future plans are also mentioned. The speaker, with experience leading a team at Google on search ads, discusses the potential of machine learning in connecting users to personalized data and mapping to their wants and needs. The significance of ad revenue for businesses like Google is highlighted, as well as the shift towards more people being willing to pay for content. The accessibility and power of TensorFlow open source, as well as the convenience of using cloud platforms like Colab for machine learning, are also discussed. The podcast emphasizes the ease of getting started with machine learning using Colab, but also mentions the limitations of free services and encourages beginners to explore tutorials and guides on the TensorFlow website.\"}]\n",
      "Stage 2 done time 2024-03-18 09:18:39.175105\n",
      "stage_2_titles: len: 9\n",
      "['1. The Impact of TensorFlow on the Tech Industry and Open Source Innovation', '2. Key Decisions and Evolution in Transitioning to TensorFlow 2.0', '3. Evolution and Impact of Deep Learning Models in Machine Learning', '4. Development and Integration of Keras in TensorFlow', '5. Challenges and Expansion of TensorFlow in Machine Learning', '6. The Evolution and Influence of TensorFlow in Research and Development', '7. Growth, Community Engagement, and Future Expectations of TensorFlow', \"8. Importance of Team Dynamics and Culture in Google's Project Development\", '9. The Power of Ads, Machine Learning, and the Shift Towards Paid Services in Software Development']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "    \n",
    "podcast_summary = []\n",
    "\n",
    "for podcast in podcast_data:\n",
    "    \n",
    "    if not podcast['episode_number'] in is_techincal_episode_numbers:\n",
    "        #print(f\"episode {podcast['episode_number']} is not technical. skip\")\n",
    "        continue\n",
    "    \n",
    "    if int(podcast['episode_number']) != 22:    \n",
    "        #print(f\"episode {podcast['episode_number']} already processed. skip\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, #900\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    chunks_text = text_splitter.split_text(podcast['transcript'])\n",
    "    \n",
    "    \n",
    "#     segments = podcast['transcript'].split('.')\n",
    "#     # Put the . back in\n",
    "#     segments = [segment + '.' for segment in segments]\n",
    "#     # Further split by comma\n",
    "#     segments = [segment.split(',') for segment in segments]\n",
    "#     # Flatten\n",
    "#     segments = [item for sublist in segments for item in sublist]\n",
    "\n",
    "#     sentences = create_sentences(segments, MIN_WORDS=20, MAX_WORDS=80)\n",
    "#     chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "#     chunks_text = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    chunks_text = remove_questions(chunks_text)\n",
    "    \n",
    "#     continue\n",
    "    \n",
    "    print(f\"chunks_text len: {len(chunks_text)}\")\n",
    "    keypoints = extract_keypoints(chunks_text)\n",
    "    \n",
    "#     print(\"RRR keypoints\")\n",
    "#     for keypoint in keypoints:\n",
    "#         print(keypoint)\n",
    "        \n",
    "#     continue\n",
    "    \n",
    "    # Run Stage 1 Summarizing\n",
    "    stage_1_outputs = assign_titles_stage_1(keypoints)['stage_1_outputs']\n",
    "    \n",
    "    print(\"RR stage_1_outputs:\")\n",
    "    print(stage_1_outputs)\n",
    "    \n",
    "#     break\n",
    "    \n",
    "    # Split the titles and summaries\n",
    "    stage_1_keypoints = [e['text'] for e in stage_1_outputs]\n",
    "#     stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "    num_1_chunks = len(stage_1_keypoints)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"generating embeddings...\")\n",
    "    keypoint_embeds = generate_embeddings(stage_1_keypoints)\n",
    "    #title_embeds = generate_embeddings(stage_1_titles) # not used\n",
    "    print(\"done gen embeddings.\")\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    keypoint_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    keypoint_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "      for col in range(row, num_1_chunks):\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity = 1- cosine(keypoint_embeds[row], keypoint_embeds[col])\n",
    "        keypoint_similarity_matrix[row, col] = similarity\n",
    "        keypoint_similarity_matrix[col, row] = similarity\n",
    "        \n",
    "#     time.sleep(10)    \n",
    "    \n",
    "    # Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "    num_topics = min(int(num_1_chunks / 4), 8)\n",
    "    \n",
    "    print(f\"num_topics: {num_topics}\")\n",
    "    print(f\"get topics {datetime.now()} ...\")\n",
    "    topics_out = get_topics(keypoint_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "    print(f\"done get topics {datetime.now()}.\")\n",
    "#     chunk_topics = topics_out['chunk_topics']\n",
    "    topics = topics_out['topics']\n",
    "    \n",
    "#     print(f\"topics: {len(topics)}\")\n",
    "#     for topic in topics:\n",
    "#         print(topic)\n",
    "        \n",
    "#     print(f\"chunk_topics: {len(chunk_topics)}\")\n",
    "#     for c_topic in chunk_topics:\n",
    "#         print(c_topic)        \n",
    "        \n",
    "#     continue    \n",
    "    \n",
    "#     # Plot a heatmap of this array\n",
    "#     plt.figure(figsize = (10, 4))\n",
    "#     plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "#     # Draw vertical black lines for every 1 of the x-axis \n",
    "#     for i in range(1, len(chunk_topics)):\n",
    "#       plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "    \n",
    "    # Query LLM to get a summarized title for each topic_data\n",
    "#     out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 600) #250)\n",
    "    out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = SUMMARY_NUM_WORDS)\n",
    "    \n",
    "    \n",
    "    stage_2_outputs = out['stage_2_outputs']\n",
    "    stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "    \n",
    "    print(f\"stage_2_titles: len: {len(stage_2_titles)}\")\n",
    "    print(stage_2_titles)\n",
    "    \n",
    "    stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "    final_summary = out['final_summary']\n",
    "    \n",
    "    summarized_podcast = {\n",
    "        \"episode_number\": podcast['episode_number'],\n",
    "        \"title_and_summary_array\": stage_2_outputs,\n",
    "        \"final_summary\": final_summary\n",
    "    }\n",
    "    \n",
    "    with open(f\"./summarized_dataset/podcast_summaries_openai_gpt35turbo_{podcast['episode_number']}_v3_stage3_extractkeypoints.json\", \"w\") as outfile: \n",
    "        json.dump(summarized_podcast, outfile)\n",
    "\n",
    "#     time.sleep(20)\n",
    "#     break\n",
    "    \n",
    "# print(podcast_summary)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
