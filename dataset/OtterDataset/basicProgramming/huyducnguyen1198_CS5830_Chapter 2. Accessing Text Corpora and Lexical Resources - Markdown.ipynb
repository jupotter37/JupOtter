{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d1c63e",
   "metadata": {},
   "source": [
    "# Chapter 2. Accessing Text Corpora and Lexical Resources\n",
    "\n",
    "Practical work in Natural Language Processing typically uses large bodies of linguistic data, or corpora. The goal of this chapter is to answer the following questions:\n",
    "\n",
    "<ol>\n",
    "<li>What are some useful text corpora and lexical resources, and how can we access them with Python?</li>\n",
    "\n",
    "<li>Which Python constructs are most helpful for this work?</li>\n",
    "\n",
    "<li>How do we avoid repeating ourselves when writing Python code?</li>\n",
    "</ol>\n",
    "\n",
    "This chapter continues to present programming concepts by example, in the context of a linguistic processing task. We will wait until later before exploring each Python construct systematically. Don’t worry if you see an example that contains something unfamiliar; simply try it out and see what it does, and—if you’re game—modify it by substituting some part of the code with a different text or word. This way you will associate a task with a programming idiom, and learn the hows and whys later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062432",
   "metadata": {},
   "source": [
    "## Accessing Text Corpora\n",
    "\n",
    "As just mentioned, a text corpus is a large body of text. Many corpora are designed to contain a careful balance of material in one or more genres. We examined some small text collections in Chapter 1, such as the speeches known as the US Presidential Inaugural Addresses. This particular corpus actually contains dozens of individual texts—one per address—but for convenience we glued them end-to-end and treated them as a single text. \n",
    "Chapter 1 also used various predefined texts that we accessed by typing from book import *. However, since we want to be able to work with other texts, this section examines a variety of text corpora. We’ll see how to select individual texts, and how to work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb689a6",
   "metadata": {},
   "source": [
    "### Gutenberg Corpus\n",
    "\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. We begin by getting the Python interpreter to load the NLTK package, then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> import nltk<br>\n",
    ">>> nltk.corpus.gutenberg.fileids()\n",
    "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
    "'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
    "'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
    "'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
    "'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
    "'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
    "</div>\n",
    "\n",
    "Let’s pick out the first of these texts—Emma by Jane Austen—and give it a short name, emma, then find out how many words it contains:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')<br>\n",
    ">>> len(emma)<br>\n",
    "192427\n",
    "</div>\n",
    "\n",
    "***\n",
    "    Note\n",
    "\n",
    "        In Computing with Language: Texts and Words, we showed how you could carry out concordancing \n",
    "        of a text such as text1 with the command text1.concordance(). However, this assumes that you \n",
    "        are using one of the nine texts obtained as a result of doing from nltk.book import *. Now \n",
    "        that you have started examining data from nltk.corpus, as in the previous example, you have \n",
    "        to employ the following pair of statements to perform concordancing and other tasks from \n",
    "        Computing with Language: Texts and Words:\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))<br>\n",
    ">>> emma.concordance(\"surprize\")\n",
    "</div>\n",
    "\n",
    "When we defined emma, we invoked the words() function of the gutenberg object in NLTK’s corpus package. But since it is cumbersome to type such long names all the time, Python provides another version of the import statement, as follows:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import gutenberg<br>\n",
    ">>> gutenberg.fileids()<br>\n",
    "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]<br>\n",
    ">>> emma = gutenberg.words('austen-emma.txt')\n",
    "</div>\n",
    "\n",
    "Let’s write a short program to display other information about each text, by looping over all the values of fileid corresponding to the gutenberg file identifiers listed earlier and then computing statistics for each text. For a compact output display, we will make sure that the numbers are all integers, using int().\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> for fileid in gutenberg.fileids():<br>\n",
    "...     num_chars = len(gutenberg.raw(fileid)) 1<br>\n",
    "...     num_words = len(gutenberg.words(fileid))<br>\n",
    "...     num_sents = len(gutenberg.sents(fileid))<br>\n",
    "...     num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))<br>\n",
    "...     print int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab), \n",
    "        fileid<br>\n",
    "...<br>\n",
    "4 21 26 austen-emma.txt<br>\n",
    "4 23 16 austen-persuasion.txt<br>\n",
    "4 24 22 austen-sense.txt<br>\n",
    "4 33 79 bible-kjv.txt<br>\n",
    "4 18 5 blake-poems.txt<br>\n",
    "4 17 14 bryant-stories.txt<br>\n",
    "4 17 12 burgess-busterbrown.txt<br>\n",
    "4 16 12 carroll-alice.txt<br>\n",
    "4 17 11 chesterton-ball.txt<br>\n",
    "4 19 11 chesterton-brown.txt<br>\n",
    "4 16 10 chesterton-thursday.txt<br>\n",
    "4 18 24 edgeworth-parents.txt<br>\n",
    "4 24 15 melville-moby_dick.txt<br>\n",
    "4 52 10 milton-paradise.txt<br>\n",
    "4 12 8 shakespeare-caesar.txt<br>\n",
    "4 13 7 shakespeare-hamlet.txt<br>\n",
    "4 13 6 shakespeare-macbeth.txt<br>\n",
    "4 35 12 whitman-leaves.txt\n",
    "</div>\n",
    "\n",
    "This program displays three statistics for each text: average word length, average sentence length, and the number of times each vocabulary item appears in the text on average (our lexical diversity score). Observe that average word length appears to be a general property of English, since it has a recurrent value of 4. (In fact, the average word length is really 3, not 4, since the num_chars variable counts space characters.) By contrast average sentence length and lexical diversity appear to be characteristics of particular authors.\n",
    "\n",
    "The previous example also showed how we can access the “raw” text of the book 1, not split up into tokens. The raw() function gives us the contents of the file without any linguistic processing. So, for example, len(gutenberg.raw('blake-poems.txt') tells us how many letters occur in the text, including the spaces between words. The sents() function divides the text up into its sentences, where each sentence is a list of words:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    ">>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')<br>\n",
    ">>> macbeth_sentences<br>\n",
    "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',\n",
    "'1603', ']'], ['Actus', 'Primus', '.'], ...]<br>\n",
    ">>> macbeth_sentences[1037]<br>\n",
    "['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',\n",
    "'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']<br>\n",
    ">>> longest_len = max([len(s) for s in macbeth_sentences])<br>\n",
    ">>> [s for s in macbeth_sentences if len(s) == longest_len]<br>\n",
    "[['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that',\n",
    "'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The',\n",
    "'mercilesse', 'Macdonwald', ...], ...]<br>\n",
    "</div>\n",
    "\n",
    "***\n",
    "    Note\n",
    "\n",
    "        Most NLTK corpus readers include a variety of access methods apart from words(), raw(), and sents(). \n",
    "        Richer linguistic content is available from some corpora, such as part-of-speech tags, dialogue tags, \n",
    "        syntactic trees, and so forth; we will see these in later chapters.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27993b",
   "metadata": {},
   "source": [
    "### Web and Chat Text\n",
    "\n",
    "Although Project Gutenberg contains thousands of books, it represents established literature. It is important to consider less formal language as well. NLTK’s small collection of web text includes content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, and wine reviews:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    ">>> from nltk.corpus import webtext<br>\n",
    ">>> for fileid in webtext.fileids():<br>\n",
    "...     print fileid, webtext.raw(fileid)[:65], '...'<br>\n",
    "...<br>\n",
    "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se...<br>\n",
    "grail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop...<br>\n",
    "overheard.txt White guy: So, do you have any plans for this evening? Asian girl...<br>\n",
    "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr...<br>\n",
    "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun...<br>\n",
    "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...<br>\n",
    "</div>\n",
    "\n",
    "There is also a corpus of instant messaging chat sessions, originally collected by the Naval Postgraduate School for research on automatic detection of Internet predators. The corpus contains over 10,000 posts, anonymized by replacing usernames with generic names of the form “UserNNN”, and manually edited to remove any other identifying information. The corpus is organized into 15 files, where each file contains several hundred posts collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a generic adults chatroom). The filename contains the date, chatroom, and number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered from the 20s chat room on 10/19/2006.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import nps_chat<br>\n",
    ">>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')<br>\n",
    ">>> chatroom[123]<br>\n",
    "['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',\n",
    "'I', 'can', 'look', 'in', 'a', 'mirror', '.']<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4f159",
   "metadata": {},
   "source": [
    "### Brown Corpus\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. Table 2-1 gives an example of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n",
    "\n",
    "Table 2-1. Example document for each section of the Brown Corpus\n",
    "![table 2-b](brown1.png)\n",
    "![table 2-b](brown3.png)\n",
    "\n",
    "We can access the corpus as a list of words or a list of sentences (where each sentence is itself just a list of words). We can optionally specify particular categories or files to read:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import brown<br>\n",
    ">>> brown.categories()<br>\n",
    "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',\n",
    "'science_fiction']<br>\n",
    ">>> brown.words(categories='news')<br>\n",
    "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]<br>\n",
    ">>> brown.words(fileids=['cg22'])<br>\n",
    "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]<br>\n",
    ">>> brown.sents(categories=['news', 'editorial', 'reviews'])<br>\n",
    "[['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]<br>\n",
    "</div>\n",
    "\n",
    "The Brown Corpus is a convenient resource for studying systematic differences between genres, a kind of linguistic inquiry known as stylistics. Let’s compare genres in their usage of modal verbs. The first step is to produce the counts for a particular genre. Remember to import nltk before doing the following:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import brown<br>\n",
    ">>> news_text = brown.words(categories='news')<br>\n",
    ">>> fdist = nltk.FreqDist([w.lower() for w in news_text])<br>\n",
    ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']<br>\n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> for m in modals:<br>\n",
    "...     print m + ':', fdist[m],<br>\n",
    "...<br>\n",
    "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389<br>\n",
    "    </div>\n",
    "    \n",
    "***\n",
    "    Note\n",
    "\n",
    "        Your Turn: Choose a different section of the Brown Corpus, and adapt the preceding \n",
    "        example to count a selection of wh words, such as what, when, where, who and why.\n",
    "***\n",
    "    \n",
    "Next, we need to obtain counts for each genre of interest. We’ll use NLTK’s support for conditional frequency distributions. These are presented systematically in Conditional Frequency Distributions, where we also unpick the following code line by line. For the moment, you can ignore the details and just concentrate on the output.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (genre, word)<br>\n",
    "...           for genre in brown.categories()<br>\n",
    "...           for word in brown.words(categories=genre))<br>\n",
    ">>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']<br>\n",
    ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']<br>\n",
    ">>> cfd.tabulate(conditions=genres, samples=modals)<br>\n",
    "                 can could  may might must will<br>\n",
    "           news   93   86   66   38   50  389<br>\n",
    "       religion   82   59   78   12   54   71<br>\n",
    "        hobbies  268   58  131   22   83  264<br>\n",
    "science_fiction   16   49    4   12    8   16<br>\n",
    "        romance   74  193   11   51   45   43<br>\n",
    "          humor   16   30    8    8    9   13<br>\n",
    "    </div>\n",
    "    \n",
    "Observe that the most frequent modal in the news genre is will, while the most frequent modal in the romance genre is could. Would you have predicted this? The idea that word counts might distinguish genres will be taken up again in Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08050102",
   "metadata": {},
   "source": [
    "### Reuters Corpus\n",
    "\n",
    "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and grouped into two sets, called “training” and “test”; thus, the text with fileid 'test/14826' is a document drawn from the test set. This split is for training and testing algorithms that automatically detect the topic of a document, as we will see in Chapter 6.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import reuters<br>\n",
    ">>> reuters.fileids()<br>\n",
    "['test/14826', 'test/14828', 'test/14829', 'test/14832', ...]<br>\n",
    ">>> reuters.categories()<br>\n",
    "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',<br>\n",
    "'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',<br>\n",
    "'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]<br>\n",
    "</div>\n",
    "    \n",
    "Unlike the Brown Corpus, categories in the Reuters Corpus overlap with each other, simply because a news story often covers multiple topics. We can ask for the topics covered by one or more documents, or for the documents included in one or more categories. For convenience, the corpus methods accept a single fileid or a list of fileids.\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> reuters.categories('training/9865')<br>\n",
    "['barley', 'corn', 'grain', 'wheat']<br>\n",
    ">>> reuters.categories(['training/9865', 'training/9880'])<br>\n",
    "['barley', 'corn', 'grain', 'money-fx', 'wheat']<br>\n",
    ">>> reuters.fileids('barley')<br>\n",
    "['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]<br>\n",
    ">>> reuters.fileids(['barley', 'corn'])<br>\n",
    "['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',\n",
    "'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]<br>\n",
    "</div>\n",
    "    \n",
    "Similarly, we can specify the words or sentences we want in terms of files or categories. The first handful of words in each of these texts are the titles, which by convention are stored as uppercase.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> reuters.words('training/9865')[:14]<br>\n",
    "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS',\n",
    "'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export']<br>\n",
    ">>> reuters.words(['training/9865', 'training/9880'])<br>\n",
    "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]<br>\n",
    ">>> reuters.words(categories='barley')<br>\n",
    "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]<br>\n",
    ">>> reuters.words(categories=['barley', 'corn'])<br>\n",
    "['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc0957",
   "metadata": {},
   "source": [
    "### Inaugural Address Corpus\n",
    "\n",
    "In Computing with Language: Texts and Words, we looked at the Inaugural Address Corpus, but treated it as a single text. The graph in Figure 1-2 used “word offset” as one of the axes; this is the numerical index of the word in the corpus, counting from the first word of the first address. However, the corpus is actually a collection of 55 texts, one for each presidential address. An interesting property of this collection is its time dimension:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import inaugural<br>\n",
    ">>> inaugural.fileids()<br>\n",
    "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]<br>\n",
    ">>> [fileid[:4] for fileid in inaugural.fileids()]<br>\n",
    "['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]<br>\n",
    "    </div>\n",
    "\n",
    "Notice that the year of each text appears in its filename. To get the year out of the filename, we extracted the first four characters, using fileid[:4].\n",
    "\n",
    "Let’s look at how the words America and citizen are used over time. The following code converts the words in the Inaugural corpus to lowercase using w.lower() 1, then checks whether they start with either of the “targets” america or citizen using startswith() 1. Thus it will count words such as American’s and Citizens. We’ll learn about conditional frequency distributions in Conditional Frequency Distributions; for now, just consider the output, shown in Figure 2-1.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (target, fileid[:4])<br>\n",
    "...           for fileid in inaugural.fileids()<br>\n",
    "...           for w in inaugural.words(fileid)<br>\n",
    "...           for target in ['america', 'citizen']<br>\n",
    "...           if w.lower().startswith(target)) 1<br>\n",
    ">>> cfd.plot()<br>\n",
    "    </div>\n",
    "\n",
    "Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus that begin with america or citizen are counted; separate counts are kept for each address; these are plotted so that trends in usage over time can be observed; counts are not normalized for document length.\n",
    "\n",
    "![figure 2-1](freqDist.png)\n",
    "\n",
    "Figure 2-1. Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus that begin with america or citizen are counted; separate counts are kept for each address; these are plotted so that trends in usage over time can be observed; counts are not normalized for document length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7377b5",
   "metadata": {},
   "source": [
    "### Annotated Text Corpora\n",
    "\n",
    "Many text corpora contain linguistic annotations, representing part-of-speech tags, named entities, syntactic structures, semantic roles, and so forth. NLTK provides convenient ways to access several of these corpora, and has data packages containing corpora and corpus samples, freely downloadable for use in teaching and research. Table 2-2 lists some of the corpora. For information about downloading them, see http://www.nltk.org/data. For more examples of how to access NLTK corpora, please consult the Corpus HOWTO at http://www.nltk.org/howto.\n",
    "\n",
    "![table 2-c1](corp1.png)\n",
    "![table 2-c2](corp2.png)\n",
    "![table 2-c3](corp3.png)\n",
    "![table 2-c4](corp4.png)\n",
    "![table 2-c5](corp5.png)\n",
    "![table 2-c6](corp6.png)\n",
    "\n",
    "Table 2-2. Some of the corpora and corpus samples distributed with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea9217",
   "metadata": {},
   "source": [
    "### Corpora in Other Languages\n",
    "\n",
    "NLTK comes with corpora for many languages, though in some cases you will need to learn how to manipulate character encodings in Python before using these corpora (see Text Processing with Unicode).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> nltk.corpus.cess_esp.words()<br>\n",
    "['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', ...]<br>\n",
    ">>> nltk.corpus.floresta.words()<br>\n",
    "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]<br>\n",
    ">>> nltk.corpus.indian.words('hindi.pos')<br>\n",
    "['\\xe0\\xa4\\xaa\\xe0\\xa5\\x82\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xa3',\n",
    "'\\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa4\\xbf\\xe0\\xa4\\xac\\xe0\\xa4\n",
    "\\x82\\xe0\\xa4\\xa7', ...]<br>\n",
    ">>> nltk.corpus.udhr.fileids()<br>\n",
    "['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',\n",
    "'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',\n",
    "'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...]<br>\n",
    ">>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]<br>\n",
    "[u'Saben', u'umat', u'manungsa', u'lair', u'kanthi', ...]<br>\n",
    "    </div>\n",
    "\n",
    "The last of these corpora, udhr, contains the Universal Declaration of Human Rights in over 300 languages. The fileids for this corpus include information about the character encoding used in the file, such as UTF8 or Latin1. Let’s use a conditional frequency distribution to examine the differences in word lengths for a selection of languages included in the udhr corpus. The output is shown in Figure 2-2 (run the program yourself to see a color plot). Note that True and False are Python’s built-in Boolean values.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import udhr<br>\n",
    ">>> languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (lang, len(word))<br>\n",
    "...           for lang in languages<br>\n",
    "...           for word in udhr.words(lang + '-Latin1'))<br>\n",
    ">>> cfd.plot(cumulative=True)<br>\n",
    "    </div>\n",
    "\n",
    "Cumulative word length distributions: Six translations of the Universal Declaration of Human Rights are processed; this graph shows that words having five or fewer letters account for about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\n",
    "\n",
    "![figure 2-2](langs.png)\n",
    "\n",
    "Figure 2-2. Cumulative word length distributions: Six translations of the Universal Declaration of Human Rights are processed; this graph shows that words having five or fewer letters account for about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\n",
    "\n",
    "***\n",
    "    Note\n",
    "\n",
    "    Your Turn: Pick a language of interest in udhr.fileids(), and define a variable \n",
    "    raw_text = udhr.raw(Language-Latin1). \n",
    "    Now plot a frequency distribution of the letters of the text using\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    "nltk.FreqDist(raw_text).plot().<br>\n",
    "    </div>\n",
    "\n",
    "Unfortunately, for many languages, substantial corpora are not yet available. Often there is insufficient government or industrial support for developing language resources, and individual efforts are piecemeal and hard to discover or reuse. Some languages have no established writing system, or are endangered. (See Further Reading for suggestions on how to locate language resources.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ebb45",
   "metadata": {},
   "source": [
    "### Text Corpus Structure\n",
    "\n",
    "We have seen a variety of corpus structures so far; these are summarized in Figure 2-3. The simplest kind lacks any structure: it is just a collection of texts. Often, texts are grouped into categories that might correspond to genre, source, author, language, etc. Sometimes these categories overlap, notably in the case of topical categories, as a text can be relevant to more than one topic. Occasionally, text collections have temporal structure, news collections being the most common example.\n",
    "\n",
    "NLTK’s corpus readers support efficient access to a variety of corpora, and can be used to work with new corpora. Table 2-3 lists functionality provided by the corpus readers.\n",
    "Common structures for text corpora: The simplest kind of corpus is a collection of isolated texts with no particular organization; some corpora are structured into categories, such as genre (Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other corpora represent language use over time (Inaugural Address Corpus).\n",
    "\n",
    "![figure 2-3](cats.png)\n",
    "\n",
    "Figure 2-3. Common structures for text corpora: The simplest kind of corpus is a collection of isolated texts with no particular organization; some corpora are structured into categories, such as genre (Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other corpora represent language use over time (Inaugural Address Corpus).\n",
    "\n",
    "![table 2-3a](corpFunc1.png)\n",
    "![table 2-3b](corpFunc2.png)\n",
    "![table 2-3c](corpFunc3.png)\n",
    "\n",
    "Table 2-3. Basic corpus functionality defined in NLTK: <br>\n",
    "    More documentation can be found using help(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://www.nltk.org/howto.\n",
    "\n",
    "\n",
    "We illustrate the difference between some of the corpus access methods here:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> raw = gutenberg.raw(\"burgess-busterbrown.txt\")<br>\n",
    ">>> raw[1:20]<br>\n",
    "'The Adventures of B'<br>\n",
    ">>> words = gutenberg.words(\"burgess-busterbrown.txt\")<br>\n",
    ">>> words[1:20]<br>\n",
    "['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.',\n",
    "'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster',\n",
    "'Bear']<br>\n",
    ">>> sents = gutenberg.sents(\"burgess-busterbrown.txt\")<br>\n",
    ">>> sents[1:20]<br>\n",
    "[['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as',\n",
    "'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched',\n",
    "'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', ...], ...]<br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca42be2",
   "metadata": {},
   "source": [
    "### Loading Your Own Corpus\n",
    "\n",
    "If you have a your own collection of text files that you would like to access using the methods discussed earlier, you can easily load them with the help of NLTK’s PlaintextCorpusReader. Check the location of your files on your file system; in the following example, we have taken this to be the directory /usr/share/dict. Whatever the location, set this to be the value of corpus_root 1. The second parameter of the PlaintextCorpusReader initializer 2 can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern that matches all fileids, like '[abc]/.*\\.txt' (see Regular Expressions for Detecting Word Patterns for information about regular expressions).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from nltk.corpus import PlaintextCorpusReader<br>\n",
    ">>> corpus_root = '/usr/share/dict'<br>\n",
    ">>> wordlists = PlaintextCorpusReader(corpus_root, '.*')<br>\n",
    ">>> wordlists.fileids()<br>\n",
    "['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']<br>\n",
    ">>> wordlists.words('connectives')<br>\n",
    "['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]<br>\n",
    "    </div>\n",
    "\n",
    "As another example, suppose you have your own local copy of Penn Treebank (release 3), in C:\\corpora. We can use the BracketParseCorpusReader to access this corpus. We specify the corpus_root to be the location of the parsed Wall Street Journal component of the corpus 1, and give a file_pattern that matches the files contained within its subfolders 2 (using forward slashes).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from nltk.corpus import BracketParseCorpusReader<br>\n",
    ">>> corpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\"<br>\n",
    ">>> file_pattern = r\".*/wsj_.*\\.mrg\"<br>\n",
    ">>> ptb = BracketParseCorpusReader(corpus_root, file_pattern)<br>\n",
    ">>> ptb.fileids()<br>\n",
    "['00/wsj_0001.mrg', '00/wsj_0002.mrg', '00/wsj_0003.mrg', '00/wsj_0004.mrg', ...]<br>\n",
    ">>> len(ptb.sents())<br>\n",
    "49208<br>\n",
    ">>> ptb.sents(fileids='20/wsj_2013.mrg')[19]<br>\n",
    "['The', '55-year-old', 'Mr.', 'Noriega', 'is', \"n't\", 'as', 'smooth', 'as', 'the',\n",
    "'shah', 'of', 'Iran', ',', 'as', 'well-born', 'as', 'Nicaragua', \"'s\", 'Anastasio',<br>\n",
    "'Somoza', ',', 'as', 'imperial', 'as', 'Ferdinand', 'Marcos', 'of', 'the', 'Philippines',\n",
    "'or', 'as', 'bloody', 'as', 'Haiti', \"'s\", 'Baby', Doc', 'Duvalier', '<br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4733f9",
   "metadata": {},
   "source": [
    "## Conditional Frequency Distributions\n",
    "\n",
    "We introduced frequency distributions in Computing with Language: Simple Statistics. We saw that given some list mylist of words or other items, FreqDist(mylist) would compute the number of occurrences of each item in the list. Here we will generalize this idea.\n",
    "\n",
    "When the texts of a corpus are divided into several categories (by genre, topic, author, etc.), we can maintain separate frequency distributions for each category. This will allow us to study systematic differences between the categories. In the previous section, we achieved this using NLTK’s ConditionalFreqDist data type. A conditional frequency distribution is a collection of frequency distributions, each one for a different “condition.” The condition will often be the category of the text. Figure 2-4 depicts a fragment of a conditional frequency distribution having just two conditions, one for news text and one for romance text.\n",
    "Counting words appearing in a text collection (a conditional frequency distribution).\n",
    "\n",
    "![figure 2-4](countWords.png)\n",
    "\n",
    "Figure 2-4. Counting words appearing in a text collection (a conditional frequency distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801d63c",
   "metadata": {},
   "source": [
    "### Conditions and Events\n",
    "\n",
    "A frequency distribution counts observable events, such as the appearance of words in a text. A conditional frequency distribution needs to pair each event with a condition. So instead of processing a sequence of words 1, we have to process a sequence of pairs 2:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]<br>\n",
    ">>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]<br>\n",
    "    </div>\n",
    "\n",
    "Each pair has the form (condition, event). If we were processing the entire Brown Corpus by genre, there would be 15 conditions (one per genre) and 1,161,192 events (one per word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b83215",
   "metadata": {},
   "source": [
    "### Counting Words by Genre\n",
    "\n",
    "In Accessing Text Corpora, we saw a conditional frequency distribution where the condition was the section of the Brown Corpus, and for each condition we counted words. Whereas FreqDist() takes a simple list as input, ConditionalFreqDist() takes a list of pairs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from nltk.corpus import brown<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (genre, word)<br>\n",
    "...           for genre in brown.categories()<br>\n",
    "...           for word in brown.words(categories=genre))<br>\n",
    "    </div>\n",
    "\n",
    "Let’s break this down, and look at just two genres, news and romance. For each genre 2, we loop over every word in the genre 3, producing pairs consisting of the genre and the word 1:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> genre_word = [(genre, word)<br>\n",
    "...               for genre in ['news', 'romance']<br>\n",
    "...               for word in brown.words(categories=genre)]<br>\n",
    ">>> len(genre_word)<br>\n",
    "170576<br>\n",
    "    </div>\n",
    "\n",
    "So, as we can see in the following code, pairs at the beginning of the list genre_word will be of the form ('news', word) 1, whereas those at the end will be of the form ('romance', word) 2.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> genre_word[:4]<br>\n",
    "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]<br>\n",
    ">>> genre_word[-4:]<br>\n",
    "[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')]<br>\n",
    "    </div>\n",
    "\n",
    "We can now use this list of pairs to create a ConditionalFreqDist, and save it in a variable cfd. As usual, we can type the name of the variable to inspect it 1, and verify it has two conditions 2:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> cfd = nltk.ConditionalFreqDist(genre_word)<br>\n",
    ">>> cfd<br>\n",
    "<ConditionalFreqDist with 2 conditions><br>\n",
    ">>> cfd.conditions()<br>\n",
    "['news', 'romance']<br>\n",
    "    </div>\n",
    "\n",
    "Let’s access the two conditions, and satisfy ourselves that each is just a frequency distribution:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> cfd['news']<br>\n",
    "<FreqDist with 100554 outcomes><br>\n",
    ">>> cfd['romance']<br>\n",
    "<FreqDist with 70022 outcomes><br>\n",
    ">>> list(cfd['romance'])<br>\n",
    "[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was', 'I', 'in', 'he', 'had',<br>\n",
    "'?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him',<br>\n",
    "'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', ...]<br>\n",
    ">>> cfd['romance']['could']<br>\n",
    "193<br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db91320",
   "metadata": {},
   "source": [
    "### Plotting and Tabulating Distributions\n",
    "\n",
    "Apart from combining two or more frequency distributions, and being easy to initialize, a ConditionalFreqDist provides some useful methods for tabulation and plotting.\n",
    "\n",
    "The plot in Figure 2-1 was based on a conditional frequency distribution reproduced in the following code. The condition is either of the words america or citizen 2, and the counts being plotted are the number of times the word occurred in a particular speech. It exploits the fact that the filename for each speech—for example, 1865-Lincoln.txt—contains the year as the first four characters 1. This code generates the pair ('america', '1865') for every instance of a word whose lowercased form starts with america—such as Americans—in the file 1865-Lincoln.txt.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from nltk.corpus import inaugural<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (target, fileid[:4])<br>\n",
    "...           for fileid in inaugural.fileids()<br>\n",
    "...           for w in inaugural.words(fileid)<br>\n",
    "...           for target in ['america', 'citizen']<br>\n",
    "...           if w.lower().startswith(target))<br>\n",
    "    </div>\n",
    "\n",
    "The plot in Figure 2-2 was also based on a conditional frequency distribution, reproduced in the following code. This time, the condition is the name of the language, and the counts being plotted are derived from word lengths 1. It exploits the fact that the filename for each language is the language name followed by '-Latin1' (the character encoding).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from nltk.corpus import udhr<br>\n",
    ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
    "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(<br>\n",
    "...           (lang, len(word))<br>\n",
    "...           for lang in languages<br>\n",
    "...           for word in udhr.words(lang + '-Latin1'))<br>\n",
    "    </div>\n",
    "\n",
    "In the plot() and tabulate() methods, we can optionally specify which conditions to display with a conditions= parameter. When we omit it, we get all the conditions. Similarly, we can limit the samples to display with a samples= parameter. This makes it possible to load a large quantity of data into a conditional frequency distribution, and then to explore it by plotting or tabulating selected conditions and samples. It also gives us full control over the order of conditions and samples in any displays. For example, we can tabulate the cumulative frequency data just for two languages, and for words less than 10 characters long, as shown next. We interpret the last cell on the top row to mean that 1,638 words of the English text have nine or fewer letters.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> cfd.tabulate(conditions=['English', 'German_Deutsch'],<br>\n",
    "...              samples=range(10), cumulative=True)<br>\n",
    "                  0    1    2    3    4    5    6    7    8    9<br>\n",
    "       English    0  185  525  883  997 1166 1283 1440 1558 1638<br>\n",
    "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275<br>\n",
    "    </div>\n",
    "\n",
    "***    \n",
    "    Note\n",
    "\n",
    "    Your Turn: Working with the news and romance genres from the Brown Corpus, find out which days \n",
    "    of the week are most newsworthy, and which are most romantic. Define a variable called days \n",
    "    containing a list of days of the week, i.e., ['Monday', ...]. Now tabulate the counts for these \n",
    "    words using cfd.tabulate(samples=days). Now try the same thing using plot in place of tabulate. \n",
    "    You may control the output order of days with the help of an extra parameter: conditions=['Monday', ...].\n",
    "***\n",
    "\n",
    "You may have noticed that the multiline expressions we have been using with conditional frequency distributions look like list comprehensions, but without the brackets. In general, when we use a list comprehension as a parameter to a function, like set([w.lower for w in t]), we are permitted to omit the square brackets and just write set(w.lower() for w in t). (See the discussion of “generator expressions” in Sequences for more about this.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7371d0",
   "metadata": {},
   "source": [
    "### Generating Random Text with Bigrams\n",
    "\n",
    "We can use a conditional frequency distribution to create a table of bigrams (word pairs, introduced in Computing with Language: Simple Statistics). The bigrams() function takes a list of words and builds a list of consecutive word pairs:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n",
    "...   'and', 'the', 'earth', '.']<br>\n",
    ">>> nltk.bigrams(sent)<br>\n",
    "[('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'),<br>\n",
    "('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'),<br>\n",
    "('the', 'earth'), ('earth', '.')]<br>\n",
    "    </div>\n",
    "\n",
    "In Example 2-1, we treat each word as a condition, and for each one we effectively create a frequency distribution over the following words. The function generate_model() contains a simple loop to generate text. When we call the function, we choose a word (such as 'living') as our initial context. Then, once inside the loop, we print the current value of the variable word, and reset word to be the most likely token in that context (using max()); next time through the loop, we use that word as our new context. As you can see by inspecting the output, this simple approach to text generation tends to get stuck in loops. Another method would be to randomly choose the next word from among the available words.\n",
    "\n",
    "Example 2-1. Generating random text: This program obtains all bigrams from the text of the book of Genesis, then constructs a conditional frequency distribution to record which words are most likely to follow a given word; e.g., after the word living, the most likely word is creature; the generate_model() function uses this data, and a seed word, to generate random text.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    "def generate_model(cfdist, word, num=15):<br>\n",
    "    for i in range(num):<br>\n",
    "        print word,<br>\n",
    "        word = cfdist[word].max()<br>\n",
    "  <br>\n",
    "text = nltk.corpus.genesis.words('english-kjv.txt')<br>\n",
    "bigrams = nltk.bigrams(text)<br>\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)<br>\n",
    "    </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> print cfd['living']<br>\n",
    "<FreqDist: 'creature': 7, 'thing': 4, 'substance': 2, ',': 1, '.': 1, 'soul': 1><br>\n",
    ">>> generate_model(cfd, 'living')<br>\n",
    "living creature that he said , and the land of the land of the land<br>\n",
    "    </div>\n",
    "\n",
    "Conditional frequency distributions are a useful data structure for many NLP tasks. Their commonly used methods are summarized in Table 2-4.\n",
    "    \n",
    "![table 2-4](condFreq1.png)\n",
    "![table 2-4](condFreq2.png)\n",
    "\n",
    "Table 2-4. NLTK’s conditional frequency distributions: Commonly used methods and idioms for defining, accessing, and visualizing a conditional frequency distribution of counters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620de01",
   "metadata": {},
   "source": [
    "## More Python: Reusing Code\n",
    "\n",
    "By this time you’ve probably typed and retyped a lot of code in the Python interactive interpreter. If you mess up when retyping a complex example, you have to enter it again. Using the arrow keys to access and modify previous commands is helpful but only goes so far. In this section, we see two important ways to reuse code: text editors and Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c88155",
   "metadata": {},
   "source": [
    "### Creating Programs with a Text Editor\n",
    "\n",
    "The Python interactive interpreter performs your instructions as soon as you type them. Often, it is better to compose a multiline program using a text editor, then ask Python to run the whole program at once. Using IDLE, you can do this by going to the File menu and opening a new window. Try this now, and enter the following one-line program:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "print 'Monty Python'<br>\n",
    "</div>\n",
    "\n",
    "Save this program in a file called monty.py, then go to the Run menu and select the command Run Module. (We’ll learn what modules are shortly.) The result in the main IDLE window should look like this:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> ================================ RESTART ================================<br>\n",
    ">>><br>\n",
    "Monty Python<br>\n",
    ">>><br>\n",
    "</div>\n",
    "\n",
    "You can also type from monty import * and it will do the same thing.\n",
    "\n",
    "From now on, you have a choice of using the interactive interpreter or a text editor to create your programs. It is often convenient to test your ideas using the interpreter, revising a line of code until it does what you expect. Once you’re ready, you can paste the code (minus any >>> or ... prompts) into the text editor, continue to expand it, and finally save the program in a file so that you don’t have to type it in again later. Give the file a short but descriptive name, using all lowercase letters and separating words with underscore, and using the .py filename extension, e.g., monty_python.py.\n",
    "Note\n",
    "\n",
    "Important: Our inline code examples include the >>> and ... prompts as if we are interacting directly with the interpreter. As they get more complicated, you should instead type them into the editor, without the prompts, and run them from the editor as shown earlier. When we provide longer programs in this book, we will leave out the prompts to remind you to type them into a file rather than using the interpreter. You can see this already in Example 2-1. Note that the example still includes a couple of lines with the Python prompt; this is the interactive part of the task where you inspect some data and invoke a function. Remember that all code samples like Example 2-1 are downloadable from http://www.nltk.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1383085",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Suppose that you work on analyzing text that involves different forms of the same word, and that part of your program needs to work out the plural form of a given singular noun. Suppose it needs to do this work in two places, once when it is processing some texts and again when it is processing user input.\n",
    "\n",
    "Rather than repeating the same code several times over, it is more efficient and reliable to localize this work inside a function. A function is just a named block of code that performs some well-defined task, as we saw in Computing with Language: Texts and Words. A function is usually defined to take some inputs, using special variables known as parameters, and it may produce a result, also known as a return value. We define a function using the keyword def followed by the function name and any input parameters, followed by the body of the function. Here’s the function we saw in Computing with Language: Texts and Words (including the import statement that makes division behave as expected):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> from __future__ import division<br>\n",
    ">>> def lexical_diversity(text):<br>\n",
    "...     return len(text) / len(set(text))<br>\n",
    "</div>\n",
    "\n",
    "We use the keyword return to indicate the value that is produced as output by the function. In this example, all the work of the function is done in the return statement. Here’s an equivalent definition that does the same work using multiple lines of code. We’ll change the parameter name from text to my_text_data to remind you that this is an arbitrary choice:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> def lexical_diversity(my_text_data):<br>\n",
    "...     word_count = len(my_text_data)<br>\n",
    "...     vocab_size = len(set(my_text_data))<br>\n",
    "...     diversity_score = word_count / vocab_size<br>\n",
    "...     return diversity_score<br>\n",
    "</div>\n",
    "\n",
    "Notice that we’ve created some new variables inside the body of the function. These are local variables and are not accessible outside the function. So now we have defined a function with the name lexical_diversity. But just defining it won’t produce any output! Functions do nothing until they are “called” (or “invoked”).\n",
    "\n",
    "Let’s return to our earlier scenario, and actually define a simple function to work out English plurals. The function plural() in Example 2-2 takes a singular noun and generates a plural form, though it is not always correct. (We’ll discuss functions at greater length in Functions: The Foundation of Structured Programming.)\n",
    "\n",
    "Example 2-2. A Python function: This function tries to work out the plural form of any English noun; the keyword def (define) is followed by the function name, then a parameter inside parentheses, and a colon; the body of the function is the indented block of code; it tries to recognize patterns within the word and process the word accordingly; e.g., if the word ends with y, delete the y and add ies.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "def plural(word):<br>\n",
    "    if word.endswith('y'):<br>\n",
    "        return word[:-1] + 'ies'<br>\n",
    "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:<br>\n",
    "        return word + 'es'<br>\n",
    "    elif word.endswith('an'):<br>\n",
    "        return word[:-2] + 'en'<br>\n",
    "    else:<br>\n",
    "        return word + 's'<br>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> plural('fairy')<br>\n",
    "'fairies'<br>\n",
    ">>> plural('woman')<br>\n",
    "'women'<br>\n",
    "</div>\n",
    "\n",
    "The endswith() function is always associated with a string object (e.g., word in Example 2-2). To call such functions, we give the name of the object, a period, and then the name of the function. These functions are usually known as methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0520f7",
   "metadata": {},
   "source": [
    "### Modules\n",
    "\n",
    "Over time you will find that you create a variety of useful little text-processing functions, and you end up copying them from old programs to new ones. Which file contains the latest version of the function you want to use? It makes life a lot easier if you can collect your work into a single place, and access previously defined functions without making copies.\n",
    "\n",
    "To do this, save your function(s) in a file called (say) textproc.py. Now, you can access your work simply by importing it from the file:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from textproc import plural<br>\n",
    ">>> plural('wish')<br>\n",
    "wishes<br>\n",
    ">>> plural('fan')<br>\n",
    "fen<br>\n",
    "</div>\n",
    "\n",
    "Our plural function obviously has an error, since the plural of fan is fans. Instead of typing in a new version of the function, we can simply edit the existing one. Thus, at every stage, there is only one version of our plural function, and no confusion about which one is being used.\n",
    "\n",
    "A collection of variable and function definitions in a file is called a Python module. A collection of related modules is called a package. NLTK’s code for processing the Brown Corpus is an example of a module, and its collection of code for processing all the different corpora is an example of a package. NLTK itself is a set of packages, sometimes called a library.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Caution!</b><br>\n",
    "\n",
    "If you are creating a file to contain some of your Python code, do not name your file nltk.py: it may get imported in place of the “real” NLTK package. When it imports modules, Python first looks in the current directory (folder).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f596a7c",
   "metadata": {},
   "source": [
    "## Lexical Resources\n",
    "\n",
    "A lexicon, or lexical resource, is a collection of words and/or phrases along with associated information, such as part-of-speech and sense definitions. Lexical resources are secondary to texts, and are usually created and enriched with the help of texts. For example, if we have defined a text my_text, then vocab = sorted(set(my_text)) builds the vocabulary of my_text, whereas word_freq = FreqDist(my_text) counts the frequency of each word in the text. Both vocab and word_freq are simple lexical resources. Similarly, a concordance like the one we saw in Computing with Language: Texts and Words gives us information about word usage that might help in the preparation of a dictionary. Standard terminology for lexicons is illustrated in Figure 2-5. A lexical entry consists of a headword (also known as a lemma) along with additional information, such as the part-of-speech and the sense definition. Two distinct words having the same spelling are called homonyms.\n",
    "Lexicon terminology: Lexical entries for two lemmas having the same spelling (homonyms), providing part-of-speech and gloss information.\n",
    "\n",
    "![figure 2-5](lex.png)\n",
    "\n",
    "Figure 2-5. Lexicon terminology: Lexical entries for two lemmas having the same spelling (homonyms), providing part-of-speech and gloss information.\n",
    "\n",
    "The simplest kind of lexicon is nothing more than a sorted list of words. Sophisticated lexicons include complex structure within and across the individual entries. In this section, we’ll look at some lexical resources included with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3cb52",
   "metadata": {},
   "source": [
    "### Wordlist Corpora\n",
    "\n",
    "NLTK includes some corpora that are nothing more than wordlists. The Words Corpus is the /usr/dict/words file from Unix, used by some spellcheckers. We can use it to find unusual or misspelled words in a text corpus, as shown in Example 2-3.\n",
    "\n",
    "Example 2-3. Filtering a text: This program computes the vocabulary of a text, then removes all items that occur in an existing wordlist, leaving just the uncommon or misspelled words.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    return sorted(unusual)\n",
    "    </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
    "['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary',\n",
    "'adieus', 'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham',\n",
    "'amiably', 'annamaria', 'annuities', 'apologising', 'arbour', 'archness', ...]\n",
    ">>> unusual_words(nltk.corpus.nps_chat.words())\n",
    "['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros',\n",
    "'actualy', 'adduser', 'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk',\n",
    "'agaibn', 'agurlwithbigguns', 'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', ...]\n",
    "</div>\n",
    "    \n",
    "There is also a corpus of stopwords, that is, high-frequency words such as the, to, and also that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> from nltk.corpus import stopwords\n",
    ">>> stopwords.words('english')\n",
    "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across',\n",
    "'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow',\n",
    "'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', ...]\n",
    "</div>\n",
    "    \n",
    "Let’s define a function to compute what fraction of words in a text are not in the stopwords list:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> def content_fraction(text):\n",
    "...     stopwords = nltk.corpus.stopwords.words('english')\n",
    "...     content = [w for w in text if w.lower() not in stopwords]\n",
    "...     return len(content) / len(text)\n",
    "...\n",
    ">>> content_fraction(nltk.corpus.reuters.words())\n",
    "0.65997695393285261\n",
    "    </div>\n",
    "\n",
    "Thus, with the help of stopwords, we filter out a third of the words of the text. Notice that we’ve combined two different kinds of corpus here, using a lexical resource to filter the content of a text corpus.\n",
    "A word puzzle: A grid of randomly chosen letters with rules for creating words out of the letters; this puzzle is known as “Target.”\n",
    "\n",
    "![figure 2-6](grid.png)\n",
    "\n",
    "Figure 2-6. A word puzzle: A grid of randomly chosen letters with rules for creating words out of the letters; this puzzle is known as “Target.”\n",
    "\n",
    "A wordlist is useful for solving word puzzles, such as the one in Figure 2-6. Our program iterates through every word and, for each one, checks whether it meets the conditions. It is easy to check obligatory letter 2 and length 1 constraints (and we’ll only look for words with six or more letters here). It is trickier to check that candidate solutions only use combinations of the supplied letters, especially since some of the supplied letters appear twice (here, the letter v). The FreqDist comparison method 3 permits us to check that the frequency of each letter in the candidate word is less than or equal to the frequency of the corresponding letter in the puzzle.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> puzzle_letters = nltk.FreqDist('egivrvonl')\n",
    ">>> obligatory = 'r'\n",
    ">>> wordlist = nltk.corpus.words.words()\n",
    ">>> [w for w in wordlist if len(w) >= 6 1\n",
    "...                      and obligatory in w 2\n",
    "...                      and nltk.FreqDist(w) <= puzzle_letters] 3\n",
    "['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',\n",
    "'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',\n",
    "'revolving', 'ringle', 'roving', 'violer', 'virole']\n",
    "                                                               </div>                                                               \n",
    "\n",
    "One more wordlist corpus is the Names Corpus, containing 8,000 first names categorized by gender. The male and female names are stored in separate files. Let’s find names that appear in both files, i.e., names that are ambiguous for gender:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">                                                                   \n",
    ">>> names = nltk.corpus.names\n",
    ">>> names.fileids()\n",
    "['female.txt', 'male.txt']\n",
    ">>> male_names = names.words('male.txt')\n",
    ">>> female_names = names.words('female.txt')\n",
    ">>> [w for w in male_names if w in female_names]\n",
    "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
    "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
    "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
    "    </div>\n",
    "\n",
    "It is well known that names ending in the letter a are almost always female. We can see this and some other patterns in the graph in Figure 2-7, produced by the following code. Remember that name[-1] is the last letter of name.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> cfd = nltk.ConditionalFreqDist(\n",
    "...           (fileid, name[-1])\n",
    "...           for fileid in names.fileids()\n",
    "...           for name in names.words(fileid))\n",
    ">>> cfd.plot()\n",
    "    </div>\n",
    "\n",
    "Conditional frequency distribution: This plot shows the number of female and male names ending with each letter of the alphabet; most names ending with a, e, or i are female; names ending in h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male.\n",
    "    \n",
    "![figure 2-7](morf.png)\n",
    "\n",
    "Figure 2-7. Conditional frequency distribution: This plot shows the number of female and male names ending with each letter of the alphabet; most names ending with a, e, or i are female; names ending in h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02abc50d",
   "metadata": {},
   "source": [
    "### A Pronouncing Dictionary\n",
    "\n",
    "A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word plus some properties in each row. NLTK includes the CMU Pronouncing Dictionary for U.S. English, which was designed for use by speech synthesizers.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> entries = nltk.corpus.cmudict.entries()<br>\n",
    ">>> len(entries)<br>\n",
    "127012<br>\n",
    ">>> for entry in entries[39943:39951]:<br>\n",
    "...     print entry<br>\n",
    "...<br>\n",
    "('fir', ['F', 'ER1'])<br>\n",
    "('fire', ['F', 'AY1', 'ER0'])<br>\n",
    "('fire', ['F', 'AY1', 'R'])<br>\n",
    "('firearm', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M'])<br>\n",
    "('firearm', ['F', 'AY1', 'R', 'AA2', 'R', 'M'])<br>\n",
    "('firearms', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'])<br>\n",
    "('firearms', ['F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'])<br>\n",
    "('fireball', ['F', 'AY1', 'ER0', 'B', 'AO2', 'L'])<br>\n",
    "    </div>\n",
    "\n",
    "For each word, this lexicon provides a list of phonetic codes—distinct labels for each contrastive sound—known as phones. Observe that fire has two pronunciations (in U.S. English): the one-syllable F AY1 R, and the two-syllable F AY1 ER0. The symbols in the CMU Pronouncing Dictionary are from the Arpabet, described in more detail at http://en.wikipedia.org/wiki/Arpabet.\n",
    "\n",
    "Each entry consists of two parts, and we can process these individually using a more complex version of the for statement. Instead of writing for entry in entries:, we replace entry with two variable names, word, pron 1. Now, each time through the loop, word is assigned the first part of the entry, and pron is assigned the second part of the entry:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> for word, pron in entries:<br>\n",
    "...     if len(pron) == 3:<br>\n",
    "...         ph1, ph2, ph3 = pron<br>\n",
    "...         if ph1 == 'P' and ph3 == 'T':<br>\n",
    "...             print word, ph2,<br>\n",
    "...<br>\n",
    "pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1<br>\n",
    "pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1<br>\n",
    "pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1<br>\n",
    "    </div>\n",
    "\n",
    "The program just shown scans the lexicon looking for entries whose pronunciation consists of three phones 2. If the condition is true, it assigns the contents of pron to three new variables: ph1, ph2, and ph3. Notice the unusual form of the statement that does that work 3.\n",
    "\n",
    "Here’s another example of the same for statement, this time used inside a list comprehension. This program finds all words whose pronunciation ends with a syllable sounding like nicks. You could use this method to find rhyming words.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> syllable = ['N', 'IH0', 'K', 'S']<br>\n",
    ">>> [word for word, pron in entries if pron[-4:] == syllable]<br>\n",
    "[\"atlantic's\", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',<br>\n",
    "'chetniks', \"clinic's\", 'clinics', 'conics', 'cynics', 'diasonics', \"dominic's\",<br>\n",
    "'ebonics', 'electronics', \"electronics'\", 'endotronics', \"endotronics'\", 'enix', ...]<br>\n",
    "    </div>\n",
    "\n",
    "Notice that the one pronunciation is spelled in several ways: nics, niks, nix, and even ntic’s with a silent t, for the word atlantic’s. Let’s look for some other mismatches between pronunciation and writing. Can you summarize the purpose of the following examples and explain how they work?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> [w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']<br>\n",
    "['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']<br>\n",
    ">>> sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))<br>\n",
    "['gn', 'kn', 'mn', 'pn']<br>\n",
    "    </div>\n",
    "\n",
    "The phones contain digits to represent primary stress (1), secondary stress (2), and no stress (0). As our final example, we define a function to extract the stress digits and then scan our lexicon to find words having a particular stress pattern.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> def stress(pron):<br>\n",
    "...     return [char for phone in pron for char in phone if char.isdigit()]<br>\n",
    ">>> [w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']]<br>\n",
    "['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',<br>\n",
    "'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',<br>\n",
    "'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]<br>\n",
    ">>> [w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']]<br>\n",
    "['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',<br>\n",
    "'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',<br>\n",
    "'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]<br>\n",
    "    </div>\n",
    "\n",
    "***\n",
    "    Note\n",
    "\n",
    "        A subtlety of this program is that our user-defined function stress() is invoked \n",
    "        inside the condition of a list comprehension. There is also a doubly nested for loop. \n",
    "        There’s a lot going on here, and you might want to return to this once you’ve had \n",
    "        more experience using list comprehensions.\n",
    "***\n",
    "    \n",
    "We can use a conditional frequency distribution to help us find minimally contrasting sets of words. Here we find all the p words consisting of three sounds 2, and group them according to their first and last sounds 1.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> p3 = [(pron[0]+'-'+pron[2], word)<br>\n",
    "...       for (word, pron) in entries<br>\n",
    "...       if pron[0] == 'P' and len(pron) == 3]<br>\n",
    ">>> cfd = nltk.ConditionalFreqDist(p3)<br>\n",
    ">>> for template in cfd.conditions():<br>\n",
    "...     if len(cfd[template]) > 10:<br>\n",
    "...         words = cfd[template].keys()<br>\n",
    "...         wordlist = ' '.join(words)<br>\n",
    "...         print template, wordlist[:70] + \"...\"<br>\n",
    "...<br>\n",
    "P-CH perch puche poche peach petsche poach pietsch putsch pautsch piche pet...<br>\n",
    "P-K pik peek pic pique paque polk perc poke perk pac pock poch purk pak pa...<br>\n",
    "P-L pil poehl pille pehl pol pall pohl pahl paul perl pale paille perle po...<br>\n",
    "P-N paine payne pon pain pin pawn pinn pun pine paign pen pyne pane penn p...<br>\n",
    "P-P pap paap pipp paup pape pup pep poop pop pipe paape popp pip peep pope...<br>\n",
    "P-R paar poor par poore pear pare pour peer pore parr por pair porr pier...<br>\n",
    "P-S pearse piece posts pasts peace perce pos pers pace puss pesce pass pur...<br>\n",
    "P-T pot puett pit pete putt pat purt pet peart pott pett pait pert pote pa...<br>\n",
    "P-Z pays p.s pao's pais paws p.'s pas pez paz pei's pose poise peas paiz p...<br>\n",
    "</div>\n",
    "\n",
    "Rather than iterating over the whole dictionary, we can also access it by looking up particular words. We will use Python’s dictionary data structure, which we will study systematically in Mapping Words to Properties Using Python Dictionaries. We look up a dictionary by specifying its name, followed by a key (such as the word 'fire') inside square brackets 1.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> prondict = nltk.corpus.cmudict.dict()<br>\n",
    ">>> prondict['fire'] <br>\n",
    "[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]<br>\n",
    ">>> prondict['blog'] <br>\n",
    "Traceback (most recent call last):<br>\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "KeyError: 'blog'<br>\n",
    ">>> prondict['blog'] = [['B', 'L', 'AA1', 'G']]<br>\n",
    ">>> prondict['blog']<br>\n",
    "[['B', 'L', 'AA1', 'G']]<br>\n",
    "    </div>\n",
    "\n",
    "If we try to look up a non-existent key 2, we get a KeyError. This is similar to what happens when we index a list with an integer that is too large, producing an IndexError. The word blog is missing from the pronouncing dictionary, so we tweak our version by assigning a value for this key 3 (this has no effect on the NLTK corpus; next time we access it, blog will still be absent).\n",
    "\n",
    "We can use any lexical resource to process a text, e.g., to filter out words having some lexical property (like nouns), or mapping every word of the text. For example, the following text-to-speech function looks up each word of the text in the pronunciation dictionary:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> text = ['natural', 'language', 'processing']<br>\n",
    ">>> [ph for w in text for ph in prondict[w][0]]<br>\n",
    "['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH',\n",
    "'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']<br>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27e5d1",
   "metadata": {},
   "source": [
    "### Comparative Wordlists\n",
    "\n",
    "Another example of a tabular lexicon is the comparative wordlist. NLTK includes so-called Swadesh wordlists, lists of about 200 common words in several languages. The languages are identified using an ISO 639 two-letter code.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from nltk.corpus import swadesh <br>\n",
    ">>> swadesh.fileids()<br>\n",
    "['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk',\n",
    "'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk']<br>\n",
    ">>> swadesh.words('en')<br>\n",
    "['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that',\n",
    "'here', 'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some',\n",
    "'few', 'other', 'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', ...]<br>\n",
    "    </div>\n",
    "\n",
    "We can access cognate words from multiple languages using the entries() method, specifying a list of languages. With one further step we can convert this into a simple dictionary (we’ll learn about dict() in Mapping Words to Properties Using Python Dictionaries).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> fr2en = swadesh.entries(['fr', 'en'])<br>\n",
    ">>> fr2en<br>\n",
    "[('je', 'I'), ('tu, vous', 'you (singular), thou'), ('il', 'he'), ...]<br>\n",
    ">>> translate = dict(fr2en)<br>\n",
    ">>> translate['chien']<br>\n",
    "'dog'<br>\n",
    ">>> translate['jeter']<br>\n",
    "'throw'<br>\n",
    "    </div>\n",
    "\n",
    "We can make our simple translator more useful by adding other source languages. Let’s get the German-English and Spanish-English pairs, convert each to a dictionary using dict(), then update our original translate dictionary with these additional mappings:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> de2en = swadesh.entries(['de', 'en'])    # German-English<br>\n",
    ">>> es2en = swadesh.entries(['es', 'en'])    # Spanish-English<br>\n",
    ">>> translate.update(dict(de2en))<br>\n",
    ">>> translate.update(dict(es2en))<br>\n",
    ">>> translate['Hund']<br>\n",
    "'dog'<br>\n",
    ">>> translate['perro']<br>\n",
    "'dog'<br>\n",
    "    </div>\n",
    "\n",
    "We can compare words in various Germanic and Romance languages:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']<br>\n",
    ">>> for i in [139, 140, 141, 142]:<br>\n",
    "...     print swadesh.entries(languages)[i]<br>\n",
    "...\n",
    "('say', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere')<br>\n",
    "('sing', 'singen', 'zingen', 'cantar', 'chanter', 'cantar', 'canere')<br>\n",
    "('play', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar, brincar', 'ludere')<br>\n",
    "('float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar, boiar', 'fluctuare')<br>\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea408d",
   "metadata": {},
   "source": [
    "### Shoebox and Toolbox Lexicons\n",
    "\n",
    "Perhaps the single most popular tool used by linguists for managing data is Toolbox, previously known as Shoebox since it replaces the field linguist’s traditional shoebox full of file cards. Toolbox is freely downloadable from http://www.sil.org/computing/toolbox/.\n",
    "\n",
    "A Toolbox file consists of a collection of entries, where each entry is made up of one or more fields. Most fields are optional or repeatable, which means that this kind of lexical resource cannot be treated as a table or spreadsheet.\n",
    "\n",
    "Here is a dictionary for the Rotokas language. We see just the first entry, for the word kaa, meaning “to gag”:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> from nltk.corpus import toolbox<br>\n",
    ">>> toolbox.entries('rotokas.dic')<br>\n",
    "[('kaa', [('ps', 'V'), ('pt', 'A'), ('ge', 'gag'), ('tkp', 'nek i pas'),<br>\n",
    "('dcsv', 'true'), ('vx', '1'), ('sc', '???'), ('dt', '29/Oct/2005'),<br>\n",
    "('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),<br>\n",
    "('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),<br>\n",
    "('xe', 'Apoka is gagging from food while talking.')]), ...]<br>\n",
    "    </div>\n",
    "    \n",
    "Entries consist of a series of attribute-value pairs, such as ('ps', 'V') to indicate that the part-of-speech is 'V' (verb), and ('ge', 'gag') to indicate that the gloss-into-English is 'gag'. The last three pairs contain an example sentence in Rotokas and its translations into Tok Pisin and English.\n",
    "\n",
    "The loose structure of Toolbox files makes it hard for us to do much more with them at this stage. XML provides a powerful way to process this kind of corpus, and we will return to this topic in Chapter 11.\n",
    "Note\n",
    "\n",
    "The Rotokas language is spoken on the island of Bougainville, Papua New Guinea. This lexicon was contributed to NLTK by Stuart Robinson. Rotokas is notable for having an inventory of just 12 phonemes (contrastive sounds); see http://en.wikipedia.org/wiki/Rotokas_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950d46f",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "WordNet is a semantically oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We’ll begin by looking at synonyms and how they are accessed in WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae9a8d",
   "metadata": {},
   "source": [
    "Senses and Synonyms\n",
    "\n",
    "Consider the sentence in a. If we replace the word motorcar in a with automobile, to get b, the meaning of the sentence stays pretty much the same:\n",
    "\n",
    "Example 2-4. \n",
    "\n",
    "    Benz is credited with the invention of the motorcar.\n",
    "\n",
    "    Benz is credited with the invention of the automobile.\n",
    "\n",
    "Since everything else in the sentence has remained unchanged, we can conclude that the words motorcar and automobile have the same meaning, i.e., they are synonyms. We can explore these words with the help of WordNet:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> from nltk.corpus import wordnet as wn<br>\n",
    ">>> wn.synsets('motorcar')<br>\n",
    "[Synset('car.n.01')]<br>\n",
    "    </div>\n",
    "\n",
    "Thus, motorcar has just one possible meaning and it is identified as car.n.01, the first noun sense of car. The entity car.n.01 is called a synset, or “synonym set,” a collection of synonymous words (or “lemmas”):\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> wn.synset('car.n.01').lemma_names<br>\n",
    "['car', 'auto', 'automobile', 'machine', 'motorcar']<br>\n",
    "    </div>\n",
    "\n",
    "Each word of a synset can have several meanings, e.g., car can also signify a train carriage, a gondola, or an elevator car. However, we are only interested in the single meaning that is common to all words of this synset. Synsets also come with a prose definition and some example sentences:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> wn.synset('car.n.01').definition<br>\n",
    "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'<br>\n",
    ">>> wn.synset('car.n.01').examples<br>\n",
    "['he needs a car to get to work']<br>\n",
    "    </div>\n",
    "\n",
    "Although definitions help humans to understand the intended meaning of a synset, the words of the synset are often more useful for our programs. To eliminate ambiguity, we will identify these words as car.n.01.automobile, car.n.01.motorcar, and so on. This pairing of a synset with a word is called a lemma. We can get all the lemmas for a given synset 1, look up a particular lemma 2, get the synset corresponding to a lemma 3, and get the “name” of a lemma 4:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> wn.synset('car.n.01').lemmas<br>\n",
    "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'),<br>\n",
    "Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]<br>\n",
    ">>> wn.lemma('car.n.01.automobile')<br>\n",
    "Lemma('car.n.01.automobile')\n",
    ">>> wn.lemma('car.n.01.automobile').synset<br>\n",
    "Synset('car.n.01')<br>\n",
    ">>> wn.lemma('car.n.01.automobile').name<br>\n",
    "'automobile'<br>\n",
    "    </div>\n",
    "\n",
    "Unlike the words automobile and motorcar, which are unambiguous and have one synset, the word car is ambiguous, having five synsets:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> wn.synsets('car')<br>\n",
    "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'),<br>\n",
    "Synset('cable_car.n.01')]<br>\n",
    ">>> for synset in wn.synsets('car'):<br>\n",
    "...     print synset.lemma_names<br>\n",
    "...\n",
    "['car', 'auto', 'automobile', 'machine', 'motorcar']<br>\n",
    "['car', 'railcar', 'railway_car', 'railroad_car']<br>\n",
    "['car', 'gondola']<br>\n",
    "['car', 'elevator_car']<br>\n",
    "['cable_car', 'car']<br>\n",
    "    </div>\n",
    "\n",
    "For convenience, we can access all the lemmas involving the word car as follows:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> wn.lemmas('car')<br>\n",
    "[Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'),<br>\n",
    "Lemma('car.n.04.car'), Lemma('cable_car.n.01.car')]<br>\n",
    "    </div>\n",
    "\n",
    "***\n",
    "    Note\n",
    "\n",
    "        Your Turn: Write down all the senses of the word dish that you can think of. \n",
    "        Now, explore this word with the help of WordNet, using the same operations shown earlier.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f19c8d",
   "metadata": {},
   "source": [
    "The WordNet Hierarchy\n",
    "\n",
    "WordNet synsets correspond to abstract concepts, and they don’t always have corresponding words in English. These concepts are linked together in a hierarchy. Some concepts are very general, such as Entity, State, Event; these are called unique beginners or root synsets. Others, such as gas guzzler and hatchback, are much more specific. A small portion of a concept hierarchy is illustrated in Figure 2-8.\n",
    "Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges indicate the hypernym/hyponym relation, i.e., the relation between superordinate and subordinate concepts.\n",
    "\n",
    "![figure 2-8](concept.png)    \n",
    "    \n",
    "Figure 2-8. Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges indicate the hypernym/hyponym relation, i.e., the relation between superordinate and subordinate concepts.\n",
    "\n",
    "WordNet makes it easy to navigate between concepts. For example, given a concept like motorcar, we can look at the concepts that are more specific—the (immediate) hyponyms.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> motorcar = wn.synset('car.n.01')<br>\n",
    ">>> types_of_motorcar = motorcar.hyponyms()<br>\n",
    ">>> types_of_motorcar[26]<br>\n",
    "Synset('ambulance.n.01')<br>\n",
    ">>> sorted([lemma.name for synset in types_of_motorcar for lemma in synset.lemmas])\n",
    "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon',\n",
    "'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible',\n",
    "'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car',\n",
    "'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap',\n",
    "'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover',\n",
    "'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car',\n",
    "'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer',\n",
    "'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan',\n",
    "'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car',\n",
    "'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car',\n",
    "'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon',\n",
    "'wagon']<br>\n",
    "    </div>\n",
    "\n",
    "We can also navigate up the hierarchy by visiting hypernyms. Some words have multiple paths, because they can be classified in more than one way. There are two paths between car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a vehicle and a container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> motorcar.hypernyms()<br>\n",
    "[Synset('motor_vehicle.n.01')]<br>\n",
    ">>> paths = motorcar.hypernym_paths()<br>\n",
    ">>> len(paths)<br>\n",
    "2<br>\n",
    ">>> [synset.name for synset in paths[0]]<br>\n",
    "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n",
    "'instrumentality.n.03', 'container.n.01', 'wheeled_vehicle.n.01',\n",
    "'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']<br>\n",
    ">>> [synset.name for synset in paths[1]]<br>\n",
    "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n",
    "'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01', 'wheeled_vehicle.n.01',\n",
    "'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']<br>\n",
    "    </div>\n",
    "\n",
    "We can get the most general hypernyms (or root hypernyms) of a synset as follows:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">        \n",
    ">>> motorcar.root_hypernyms()<br>\n",
    "[Synset('entity.n.01')]<br>\n",
    "    </div>\n",
    "\n",
    "***\n",
    "    Note\n",
    "\n",
    "        Your Turn: Try out NLTK’s convenient graphical WordNet browser: nltk.app.wordnet(). \n",
    "        Explore the WordNet hierarchy by following the hypernym and hyponym links.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b518cd2",
   "metadata": {},
   "source": [
    "More Lexical Relations\n",
    "    \n",
    "Hypernyms and hyponyms are called lexical relations because they relate one synset to another. These two relations navigate up and down the “is-a” hierarchy. Another important way to navigate the WordNet network is from items to their components (meronyms) or to the things they are contained in (holonyms). For example, the parts of a tree are its trunk, crown, and so on; these are the part_meronyms(). The substance a tree is made of includes heartwood and sapwood, i.e., the substance_meronyms(). A collection of trees forms a forest, i.e., the member_holonyms():\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> wn.synset('tree.n.01').part_meronyms()<br>\n",
    "[Synset('burl.n.02'), Synset('crown.n.07'), Synset('stump.n.01'),\n",
    "Synset('trunk.n.01'), Synset('limb.n.02')]<br>\n",
    ">>> wn.synset('tree.n.01').substance_meronyms()<br>\n",
    "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]<br>\n",
    ">>> wn.synset('tree.n.01').member_holonyms()<br>\n",
    "[Synset('forest.n.01')]<br>\n",
    "    </div>\n",
    "\n",
    "To see just how intricate things can get, consider the word mint, which has several closely related senses. We can see that mint.n.04 is part of mint.n.02 and the substance from which mint.n.05 is made.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    ">>> for synset in wn.synsets('mint', wn.NOUN):<br>\n",
    "...     print synset.name + ':', synset.definition<br>\n",
    "...<br>\n",
    "batch.n.02: (often followed by `of') a large number or amount or extent<br>\n",
    "mint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and\n",
    "           small mauve flowers<br>\n",
    "mint.n.03: any member of the mint family of plants<br>\n",
    "mint.n.04: the leaves of a mint plant used fresh or candied<br>\n",
    "mint.n.05: a candy that is flavored with a mint oil<br>\n",
    "mint.n.06: a plant where money is coined by authority of the government<br>\n",
    ">>> wn.synset('mint.n.04').part_holonyms()<br>\n",
    "[Synset('mint.n.02')]<br>\n",
    ">>> wn.synset('mint.n.04').substance_holonyms()<br>\n",
    "[Synset('mint.n.05')]<br>\n",
    "    </div>\n",
    "\n",
    "There are also relationships between verbs. For example, the act of walking involves the act of stepping, so walking entails stepping. Some verbs have multiple entailments:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> wn.synset('walk.v.01').entailments()<br>\n",
    "[Synset('step.v.01')]<br>\n",
    ">>> wn.synset('eat.v.01').entailments()<br>\n",
    "[Synset('swallow.v.01'), Synset('chew.v.01')]<br>\n",
    ">>> wn.synset('tease.v.03').entailments()<br>\n",
    "[Synset('arouse.v.07'), Synset('disappoint.v.01')]<br>\n",
    "    </div>\n",
    "\n",
    "Some lexical relationships hold between lemmas, e.g., antonymy:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> wn.lemma('supply.n.02.supply').antonyms()<br>\n",
    "[Lemma('demand.n.02.demand')]<br>\n",
    ">>> wn.lemma('rush.v.01.rush').antonyms()<br>\n",
    "[Lemma('linger.v.04.linger')]<br>\n",
    ">>> wn.lemma('horizontal.a.01.horizontal').antonyms()<br>\n",
    "[Lemma('vertical.a.01.vertical'), Lemma('inclined.a.02.inclined')]<br>\n",
    ">>> wn.lemma('staccato.r.01.staccato').antonyms()<br>\n",
    "[Lemma('legato.r.01.legato')]<br>\n",
    "    </div>\n",
    "\n",
    "You can see the lexical relations, and the other methods defined on a synset, using dir(). For example, try dir(wn.synset('harmony.n.02'))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73403bc8",
   "metadata": {},
   "source": [
    "Semantic Similarity\n",
    "\n",
    "We have seen that synsets are linked by a complex network of lexical relations. Given a particular synset, we can traverse the WordNet network to find synsets with related meanings. Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term such as vehicle will match documents containing specific terms such as limousine.\n",
    "\n",
    "Recall that each synset has one or more hypernym paths that link it to a root hypernym such as entity.n.01. Two synsets linked to the same root may have several hypernyms in common (see Figure 2-8). If two synsets share a very specific hypernym—one that is low down in the hypernym hierarchy—they must be closely related.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> right = wn.synset('right_whale.n.01')<br>\n",
    ">>> orca = wn.synset('orca.n.01')<br>\n",
    ">>> minke = wn.synset('minke_whale.n.01')<br>\n",
    ">>> tortoise = wn.synset('tortoise.n.01')<br>\n",
    ">>> novel = wn.synset('novel.n.01')<br>\n",
    ">>> right.lowest_common_hypernyms(minke)<br>\n",
    "[Synset('baleen_whale.n.01')]<br>\n",
    ">>> right.lowest_common_hypernyms(orca)<br>\n",
    "[Synset('whale.n.02')]<br>\n",
    ">>> right.lowest_common_hypernyms(tortoise)<br>\n",
    "[Synset('vertebrate.n.01')]<br>\n",
    ">>> right.lowest_common_hypernyms(novel)<br>\n",
    "[Synset('entity.n.01')]<br>\n",
    "    </div>\n",
    "\n",
    "Of course we know that whale is very specific (and baleen whale even more so), whereas vertebrate is more general and entity is completely general. We can quantify this concept of generality by looking up the depth of each synset:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> wn.synset('baleen_whale.n.01').min_depth()<br>\n",
    "14<br>\n",
    ">>> wn.synset('whale.n.02').min_depth()<br>\n",
    "13<br>\n",
    ">>> wn.synset('vertebrate.n.01').min_depth()<br>\n",
    "8<br>\n",
    ">>> wn.synset('entity.n.01').min_depth()<br>\n",
    "0<br>\n",
    "    </div>\n",
    "    \n",
    "Similarity measures have been defined over the collection of WordNet synsets that incorporate this insight. For example, path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found). Comparing a synset with itself will return 1. Consider the following similarity scores, relating right whale to minke whale, orca, tortoise, and novel. Although the numbers won’t mean much, they decrease as we move away from the semantic space of sea creatures to inanimate objects.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">    \n",
    ">>> right.path_similarity(minke)<br>\n",
    "0.25<br>\n",
    ">>> right.path_similarity(orca)<br>\n",
    "0.16666666666666666<br>\n",
    ">>> right.path_similarity(tortoise)<br>\n",
    "0.076923076923076927<br>\n",
    ">>> right.path_similarity(novel)<br>\n",
    "0.043478260869565216<br>\n",
    "    </div>\n",
    "\n",
    "***    \n",
    "    Note\n",
    "\n",
    "    Several other similarity measures are available; you can type help(wn) for more information. \n",
    "    NLTK also includes VerbNet, a hierarchical verb lexicon linked to WordNet. \n",
    "    It can be accessed with nltk.corpus.verbnet.\n",
    "***    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a12de1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "    A text corpus is a large, structured collection of texts. NLTK comes with many corpora, e.g., the Brown Corpus, nltk.corpus.brown.\n",
    "\n",
    "    Some text corpora are categorized, e.g., by genre or topic; sometimes the categories of a corpus overlap each other.\n",
    "\n",
    "    A conditional frequency distribution is a collection of frequency distributions, each one for a different condition. They can be used for counting word frequencies, given a context or a genre.\n",
    "\n",
    "    Python programs more than a few lines long should be entered using a text editor, saved to a file with a .py extension, and accessed using an import statement.\n",
    "\n",
    "    Python functions permit you to associate a name with a particular block of code, and reuse that code as often as necessary.\n",
    "\n",
    "    Some functions, known as “methods,” are associated with an object, and we give the object name followed by a period followed by the method name, like this: x.funct(y), e.g., word.isalpha().\n",
    "\n",
    "    To find out about some variable v, type help(v) in the Python interactive interpreter to read the help entry for this kind of object.\n",
    "\n",
    "    WordNet is a semantically oriented dictionary of English, consisting of synonym sets—or synsets—and organized into a network.\n",
    "\n",
    "    Some functions are not available by default, but must be accessed using Python’s import statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b0f23",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "Extra materials for this chapter are posted at http://www.nltk.org/, including links to freely available resources on the Web. The corpus methods are summarized in the Corpus HOWTO, at http://www.nltk.org/howto, and documented extensively in the online API documentation.\n",
    "\n",
    "Significant sources of published corpora are the Linguistic Data Consortium (LDC) and the European Language Resources Agency (ELRA). Hundreds of annotated text and speech corpora are available in dozens of languages. Non-commercial licenses permit the data to be used in teaching and research. For some corpora, commercial licenses are also available (but for a higher fee).\n",
    "\n",
    "These and many other language resources have been documented using OLAC Metadata, and can be searched via the OLAC home page at http://www.language-archives.org/. Corpora List (see http://gandalf.aksis.uib.no/corpora/sub.html) is a mailing list for discussions about corpora, and you can find resources by searching the list archives or posting to the list. The most complete inventory of the world’s languages is Ethnologue, http://www.ethnologue.com/. Of 7,000 languages, only a few dozen have substantial digital resources suitable for use in NLP.\n",
    "\n",
    "This chapter has touched on the field of Corpus Linguistics. Other useful books in this area include (Biber, Conrad, & Reppen, 1998), (McEnery, 2006), (Meyer, 2002), (Sampson & McCarthy, 2005), and (Scott & Tribble, 2006). Further readings in quantitative data analysis in linguistics are: (Baayen, 2008), (Gries, 2009), and (Woods, Fletcher, & Hughes, 1986).\n",
    "\n",
    "The original description of WordNet is (Fellbaum, 1998). Although WordNet was originally developed for research in psycholinguistics, it is now widely used in NLP and Information Retrieval. WordNets are being developed for many other languages, as documented at http://www.globalwordnet.org/. For a study of WordNet similarity measures, see (Budanitsky & Hirst, 2006).\n",
    "\n",
    "Other topics touched on in this chapter were phonetics and lexical semantics, and we refer readers to Chapters 7 and 20 of (Jurafsky & Martin, 2008)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534d12b",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "    ○ Create a variable phrase containing a list of words. Experiment with the operations described in this chapter, including addition, multiplication, indexing, slicing, and sorting.\n",
    "\n",
    "    ○ Use the corpus module to explore austen-persuasion.txt. How many word tokens does this book have? How many word types?\n",
    "\n",
    "    ○ Use the Brown Corpus reader nltk.corpus.brown.words() or the Web Text Corpus reader nltk.corpus.webtext.words() to access some sample text in two different genres.\n",
    "\n",
    "    ○ Read in the texts of the State of the Union addresses, using the state_union corpus reader. Count occurrences of men, women, and people in each document. What has happened to the usage of these words over time?\n",
    "\n",
    "    ○ Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use member_meronyms(), part_meronyms(), substance_meronyms(), member_holonyms(), part_holonyms(), and substance_holonyms().\n",
    "\n",
    "    ○ In the discussion of comparative wordlists, we created an object called translate, which you could look up using words in both German and Italian in order to get corresponding words in English. What problem might arise with this approach? Can you suggest a way to avoid this problem?\n",
    "\n",
    "    ○ According to Strunk and White’s Elements of Style, the word however, used at the start of a sentence, means “in whatever way” or “to whatever extent,” and not “nevertheless.” They give this example of correct usage: However you advise him, he will probably do as he thinks best. (http://www.bartleby.com/141/strunk3.html) Use the concordance tool to study actual usage of this word in the various texts we have been considering. See also the LanguageLog posting “Fossilized prejudices about ‘however’” at http://itre.cis.upenn.edu/~myl/languagelog/archives/001913.html.\n",
    "\n",
    "    Define a conditional frequency distribution over the Names Corpus that allows you to see which initial letters are more frequent for males versus females (see Figure 2-7).\n",
    "\n",
    "    Pick a pair of texts and study the differences between them, in terms of vocabulary, vocabulary richness, genre, etc. Can you find pairs of words that have quite different meanings across the two texts, such as monstrous in Moby Dick and in Sense and Sensibility?\n",
    "\n",
    "    Read the BBC News article: “UK’s Vicky Pollards ‘left behind’” at http://news.bbc.co.uk/1/hi/education/6173441.stm. The article gives the following statistic about teen language: “the top 20 words used, including yeah, no, but and like, account for around a third of all words.” How many word types account for a third of all word tokens, for a variety of text sources? What do you conclude about this statistic? Read more about this on LanguageLog, at http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html.\n",
    "\n",
    "    Investigate the table of modal distributions and look for other patterns. Try to explain them in terms of your own impressionistic understanding of the different genres. Can you find other closed classes of words that exhibit significant differences across different genres?\n",
    "\n",
    "    The CMU Pronouncing Dictionary contains multiple pronunciations for certain words. How many distinct words does it contain? What fraction of words in this dictionary have more than one possible pronunciation?\n",
    "\n",
    "    What percentage of noun synsets have no hyponyms? You can get all noun synsets using wn.all_synsets('n').\n",
    "\n",
    "    Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s.\n",
    "\n",
    "    Write a program to find all words that occur at least three times in the Brown Corpus.\n",
    "\n",
    "    Write a program to generate a table of lexical diversity scores (i.e., token/type ratios), as we saw in Table 1-1. Include the full set of Brown Corpus genres (nltk.corpus.brown.categories()). Which genre has the lowest diversity (greatest number of tokens per type)? Is this what you would have expected?\n",
    "\n",
    "    Write a function that finds the 50 most frequently occurring words of a text that are not stopwords.\n",
    "\n",
    "    Write a program to print the 50 most frequent bigrams (pairs of adjacent words) of a text, omitting bigrams that contain stopwords.\n",
    "\n",
    "    Write a program to create a table of word frequencies by genre, like the one given in Accessing Text Corpora for modals. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings.\n",
    "\n",
    "    Write a function word_freq() that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus.\n",
    "\n",
    "    Write a program to guess the number of syllables contained in a text, making use of the CMU Pronouncing Dictionary.\n",
    "\n",
    "    Define a function hedge(text) that processes a text and produces a new version with the word 'like' between every third word.\n",
    "\n",
    "    ● Zipf’s Law: Let f(w) be the frequency of a word w in free text. Suppose that all the words of a text are ranked according to their frequency, with the most frequent word first. Zipf’s Law states that the frequency of a word type is inversely proportional to its rank (i.e., f × r = k, for some constant k). For example, the 50th most common word type should occur three times as frequently as the 150th most common word type.\n",
    "\n",
    "        Write a function to process a large text and plot word frequency against word rank using pylab.plot. Do you confirm Zipf’s law? (Hint: it helps to use a logarithmic scale.) What is going on at the extreme ends of the plotted line?\n",
    "\n",
    "        Generate random text, e.g., using random.choice(\"abcdefg \"), taking care to include the space character. You will need to import random first. Use the string concatenation operator to accumulate characters into a (very) long string. Then tokenize this string, generate the Zipf plot as before, and compare the two plots. What do you make of Zipf’s Law in the light of this?\n",
    "\n",
    "    ● Modify the text generation program in Example 2-1 further, to do the following tasks:\n",
    "\n",
    "        Store the n most likely words in a list words, then randomly choose a word from the list using random.choice(). (You will need to import random first.)\n",
    "\n",
    "        Select a particular genre, such as a section of the Brown Corpus or a Genesis translation, one of the Gutenberg texts, or one of the Web texts. Train the model on this corpus and get it to generate random text. You may have to experiment with different start words. How intelligible is the text? Discuss the strengths and weaknesses of this method of generating random text.\n",
    "\n",
    "        Now train your system using two distinct genres and experiment with generating text in the hybrid genre. Discuss your observations.\n",
    "\n",
    "    ● Define a function find_language() that takes a string as its argument and returns a list of languages that have that string as a word. Use the udhr corpus and limit your searches to files in the Latin-1 encoding.\n",
    "\n",
    "    ● What is the branching factor of the noun hypernym hierarchy? I.e., for every noun synset that has hyponyms—or children in the hypernym hierarchy—how many do they have on average? You can get all noun synsets using wn.all_synsets('n').\n",
    "\n",
    "    ● The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has seven senses with len(wn.synsets('dog', 'n')). Compute the average polysemy of nouns, verbs, adjectives, and adverbs according to WordNet.\n",
    "\n",
    "    ● Use one of the predefined similarity measures to score the similarity of each of the following pairs of words. Rank the pairs in order of decreasing similarity. How close is your ranking to the order given here, an order that was established experimentally by (Miller & Charles, 1998): car-automobile, gem-jewel, journey-voyage, boy-lad, coast-shore, asylum-madhouse, magician-wizard, midday-noon, furnace-stove, food-fruit, bird-cock, bird-crane, tool-implement, brother-monk, lad-brother, crane-implement, journey-car, monk-oracle, cemetery-woodland, food-rooster, coast-hill, forest-graveyard, shore-woodland, monk-slave, coast-forest, lad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3603ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
