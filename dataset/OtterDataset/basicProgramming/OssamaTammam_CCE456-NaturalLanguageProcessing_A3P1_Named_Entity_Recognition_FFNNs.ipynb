{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OssamaTammam/CCE456-NaturalLanguageProcessing/blob/master/assignment_3/A3P1_Named_Entity_Recognition_FFNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 3\n",
        "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a written portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
        "\n",
        "We'll also be programming in Python, which we will assume a basic familiarity with. Python has fantastic community support and we'll be using numerous packages for machine learning (ML) and natural language processing (NLP) tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement feedforward neural networks (FFNNs) and long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset as in Assignment #2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GAWytObKh4K"
      },
      "source": [
        "## Use GPUs in Colab\n",
        "\n",
        "GPUs are not strictly necessary for this assignment, but they will accerlerate your experiments and may be helpful for your final project. To enable the free GPU provided by Colab, just go to **Edit > Notebook settings** and select **GPU** as **Hardware accelerator**. To test your GPU, run the `nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817a3b32-7f87-408f-8e85-8a1738d760df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 28 18:17:15 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0              26W /  70W |    137MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbgSdkFMMa3E"
      },
      "source": [
        "You should see a table summarizing the current status of the GPU, including the model, power consumption, GPU memory usage, etc. However, Colab does not guarantee which model you will get. We tested the assignment successfully on Tesla T4 but had some problems on other GPUs. **Please make sure you get a Tesla T4** by checking the output of `nvidia-smi`. We empirically observe that you are more likely to get a T4 **with your Princeton account**.\n",
        "\n",
        "If you are still unable to get a T4. You have 3 options:\n",
        "1. Reload the page until you get one.\n",
        "2. Proceed with other GPUs. They should work as long as Colab does not crash when executing code.\n",
        "3. Disable the GPU and use the CPU for this assignment, which is totally fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26HdExpsKTOg"
      },
      "source": [
        "## Learning PyTorch\n",
        "\n",
        "This assignment assumes basic familiarity with [PyTorch](https://pytorch.org/), which is a much needed skill for the upcoming Assignment #4, for the final project, and even beyond this course. If you aren't familiar with PyTorch, don't worry, it's a great time to start now! Here are some resources to help you get started. You may also find plenty of them online.\n",
        "\n",
        "* We will cover PyTorch in the precept on 3/18. Watch the video on Canvas if you are not able to make it.\n",
        "* [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "* [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedb8ddd-95d8-49dc-e9a6-6ef3ec7f0077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ded173-ea4c-4888-bd8f-864e777632e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8a3661-08e0-4abc-f131-03135a3b3a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Feedforward Neural Network (FFNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8698b8a5-48dd-43aa-8281-3f6f7a91aff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooQEJEafWCcd"
      },
      "source": [
        "First, we write a dataloader for loading the dataset into mini-batches used for training the model. See [torch.utils.data](https://pytorch.org/docs/stable/data.html) for how dataloaders work in PyTorch. In short, we typically need to do two things:\n",
        "1. Define a [map-style dataset](https://pytorch.org/docs/stable/data.html#map-style-datasets) by subclassing [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and overriding 3 methods: `__init__`, `__getitem__`, and `__len__`.\n",
        "1. Create a [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) by calling its constructor. We have to specify the dataset and a few hyperparameters such as batch size.\n",
        "\n",
        "Most of the work has been done by us. As a simple exercise, try to understand the code and implement `__len__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFVnIjA8KEe0"
      },
      "source": [
        "## Implement the `__len__` function below **(1 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9RGv1K0pP1bR"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FixedWindowDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a word, its NER tag (the target), and a fixed window centered around it (the input).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        datapath: str,\n",
        "        window_size: int,\n",
        "        words_vocab: Optional[Vocab] = None,\n",
        "        tags_vocab: Optional[Vocab] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.examples = []\n",
        "        START = \"<START>\"\n",
        "        END = \"<END>\"\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "\n",
        "        # Extract windows\n",
        "        for sent in sentences:\n",
        "            words = [START for _ in range(window_size)]\n",
        "            tags = [None for _ in range(window_size)]\n",
        "            for w, t in sent:\n",
        "                words.append(w)\n",
        "                tags.append(t)\n",
        "            words.extend([END for _ in range(window_size)])\n",
        "            tags.extend([None for _ in range(window_size)])\n",
        "\n",
        "            for i, t in enumerate(tags[window_size:-window_size], start=window_size):\n",
        "                self.examples.append(\n",
        "                    {\n",
        "                        \"word\": words[i],\n",
        "                        \"tag\": t,\n",
        "                        \"context\": words[i - window_size : i + window_size + 1],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        print(\"%d examples loaded.\" % len(self.examples))\n",
        "\n",
        "        # set vocabs\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[START, END, UNKNOWN]) # automatically create a vocabulary from words in dataset\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "        self.words_vocab = words_vocab\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.start_idx = self.words_vocab[START]\n",
        "        self.end_idx = self.words_vocab[END]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[]) # automatically create tags vocabulary from tags in dataset\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get the idx'th example in the dataset.\n",
        "        Convert words and the tag to indexes.\n",
        "        \"\"\"\n",
        "        example = self.examples[idx]\n",
        "        word = example[\"word\"]\n",
        "        tag = example[\"tag\"]\n",
        "        context = example[\"context\"]\n",
        "        return {\n",
        "            \"word\": word,\n",
        "            \"word_idx\": self.words_vocab[word],\n",
        "            \"tag\": tag,\n",
        "            \"tag_idx\": self.tags_vocab[tag],\n",
        "            \"context\": context,\n",
        "            \"context_idxs\": torch.tensor(\n",
        "                [self.words_vocab[w] for w in context]\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.examples)\n",
        "        # END\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F-aHcN5yJyoa"
      },
      "outputs": [],
      "source": [
        "def create_fixed_window_dataloaders(\n",
        "    batch_size: int, window_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Dict[str, Vocab]]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = FixedWindowDataset(\"eng.train\", window_size)\n",
        "    # Re-use the vocabulary of the training data\n",
        "    ds_val = FixedWindowDataset(\"eng.val\", window_size, words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train, batch_size, shuffle, drop_last=True, pin_memory=True\n",
        "    )\n",
        "    loader_val = DataLoader(ds_val, batch_size, pin_memory=True)\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODfPyQjPSmCv"
      },
      "source": [
        "Let's test our dataloader. Try to understand the output, as it will save your time later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zmt9c9svgzy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333f5d6a-5e8f-4ba4-c2cc-87bd703b4d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "203621 examples loaded.\n",
            "Loading data from eng.val\n",
            "49086 examples loaded.\n",
            "Iterating on the training data..\n",
            "{'word': ['EU', 'rejects', 'German'], 'word_idx': tensor([3, 4, 5]), 'tag': ['ORG', 'O', 'MISC'], 'tag_idx': tensor([0, 1, 2]), 'context': [['<START>', '<START>', 'EU'], ['<START>', 'EU', 'rejects'], ['EU', 'rejects', 'German'], ['rejects', 'German', 'call'], ['German', 'call', 'to']], 'context_idxs': tensor([[0, 0, 3, 4, 5],\n",
            "        [0, 3, 4, 5, 6],\n",
            "        [3, 4, 5, 6, 7]])}\n",
            "5\n",
            "torch.Size([3, 5])\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_fixed_window_dataloader() -> None:\n",
        "    loader_train, _, _ = create_fixed_window_dataloaders(\n",
        "        batch_size=3, window_size=2, shuffle=False\n",
        "    )\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "            print(len(data_batch[\"context\"]))\n",
        "            print(data_batch[\"context_idxs\"].shape)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_fixed_window_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR_aJ-FincuN"
      },
      "source": [
        "### Implement the Model **(4 points)**   \n",
        "\n",
        "Next, let's implement feedforward neural networks following the description of Problem 1 in Assignment #3.\n",
        "\n",
        "Models in PyTorch are subclasses of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You have to override `__init__` for initializing the model and `forward` for calculating the forward pass. Checkout this [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html#) if you are not sure how torch.nn.Module works.\n",
        "\n",
        "PyTorch provides a wide array of [neural network layers](https://pytorch.org/docs/stable/nn.html) as building blocks for your model. Here are some of them that may be relevant:\n",
        "* [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)\n",
        "* [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "* [torch.sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid) or [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
        "\n",
        "Note a difference with Problem 3 of Assignment #2 is that we do not apply softmax when calculatinng $\\hat{y}^{(t)}$. Instead, we leave what softmax does to the loss function [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy). For details, please see its difference with [F.nll_loss](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hx0oMgVffSD1"
      },
      "outputs": [],
      "source": [
        "class FFNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward Neural Networks for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, words_vocab: Vocab, tags_vocab: Vocab, window_size: int, d_emb: int, d_hidden: int\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize a two-layer feedforward neural network with sigmoid activation.\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `window_size`: size of the context window (w in Problem 3 of Assignment #2)\n",
        "            `d_emb`: dimension of word embeddings (D in Problem 3 of Assignment #2)\n",
        "            `d_hidden`: dimension of the hidden layer (H in Problem 3 of Assignment #2)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the hidden layer and the output layer (nn.Linear).\n",
        "        # START HERE\n",
        "        self.words_vocab = words_vocab\n",
        "        self.tags_vocab = tags_vocab\n",
        "        self.window_size = window_size\n",
        "        self.d_emb = d_emb\n",
        "        self.d_hidden = d_hidden\n",
        "\n",
        "        # Word embeddings layer\n",
        "        self.word_embeddings = nn.Embedding(len(words_vocab), d_emb)\n",
        "\n",
        "        # Hidden layer\n",
        "        self.hidden_layer = nn.Linear((2 * window_size + 1) * d_emb, d_hidden)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(d_hidden, len(tags_vocab))        # END\n",
        "\n",
        "    def forward(self, context_idxs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given the word indexes in a context window, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `context_idxs`: a batch_size x (2 * window_size + 1) tensor\n",
        "                          context_idxs[i] contains word indexes in the window of the i'th data example.\n",
        "        Return values:\n",
        "            `logits`: a batch_size x 5 tensor (\\hat{y}^{(t)} in Problem 3 of Assignment #2, without softmax)\n",
        "                    logits[i][j] is the output score (before softmax) of the i'th example for tag j.\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass of the two-layer FFNN with sigmoid hidden layer.\n",
        "        #       Do not apply softmax, since we will use F.cross_entropy as the loss function.\n",
        "        # START HERE\n",
        "        # Embedding lookup\n",
        "        embedded_context = self.word_embeddings(context_idxs)\n",
        "\n",
        "        # Flatten the embeddings\n",
        "        flattened_context = embedded_context.view(embedded_context.size(0), -1)\n",
        "\n",
        "        # Pass through hidden layer with sigmoid activation\n",
        "        hidden_output = torch.sigmoid(self.hidden_layer(flattened_context))\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.output_layer(hidden_output)\n",
        "\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyZCvfOMR7YP"
      },
      "source": [
        "Optionally, let's do a simple sanity check of your implementation. In `check_ffnn`, we load a batch of data examples and pass it through the FFNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H7jikP_1fZP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea38232-e872-42a7-cefd-6a65bf250185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "203621 examples loaded.\n",
            "Loading data from eng.val\n",
            "49086 examples loaded.\n",
            "FFNN(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (word_embeddings): Embedding(20103, 64)\n",
            "  (hidden_layer): Linear(in_features=448, out_features=128, bias=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Input tensor shape: torch.Size([3, 7])\n",
            "Output tensor shape: torch.Size([3, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_ffnn() -> None:\n",
        "  # Hyperparameters\n",
        "  batch_size = 3\n",
        "  d_emb = 64\n",
        "  d_hidden = 128\n",
        "  window_size = 3\n",
        "  # Create the dataloaders and the model\n",
        "  loader_train, _, ds_train = create_fixed_window_dataloaders(batch_size, window_size)\n",
        "  model = FFNN(ds_train.words_vocab, ds_train.tags_vocab, window_size, d_emb, d_hidden)\n",
        "  device = get_device()\n",
        "  model.to(device)\n",
        "  print(model)\n",
        "  # Get the first batch\n",
        "  data_batch = next(iter(loader_train))\n",
        "  # Move data to GPU\n",
        "  context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
        "  tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
        "  # Calculate the model\n",
        "  print(\"Input tensor shape:\", context_idxs.size())\n",
        "  logits = model(context_idxs)\n",
        "  print(\"Output tensor shape:\", logits.size())\n",
        "\n",
        "check_ffnn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PkRIOgeXIng0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0614db13-11e5-463d-8840-043750c2434a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "203621 examples loaded.\n"
          ]
        }
      ],
      "source": [
        "ds_train = FixedWindowDataset(\"eng.train\", 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0angAnEno9v"
      },
      "source": [
        "### Training and Validation  **(4 points)**   \n",
        "\n",
        "Having implemented the model, the next step is to implement functions for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vWcKwiIMjekY"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_ffnn(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the FFNN model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO:\n",
        "        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
        "        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
        "        # 3. Perform the backward pass to calculate the gradient.\n",
        "        # 4. Use the optimizer to update model parameters.\n",
        "        # Caveat: You may need to call optimizer.zero_grad(). Figure out what it does!\n",
        "        # START HERE\n",
        "\n",
        "        # logits = ...\n",
        "        # loss = ...\n",
        "\n",
        "        # Perform forward pass\n",
        "        logits = model(context_idxs)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.cross_entropy(logits, tag_idx)\n",
        "\n",
        "        # Perform backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "         # END\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        ground_truth.extend(tag_idx.tolist())\n",
        "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_ffnn(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the FFNN model.\n",
        "    Return values:\n",
        "        1. the average validation loss\n",
        "        2. validation metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            context_idxs = data_batch[\"context_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idx = data_batch[\"tag_idx\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Similar to what you did in train_ffnn, but only step 1 and 2.\n",
        "            # START HERE\n",
        "            # Perform forward pass\n",
        "            logits = model(context_idxs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = F.cross_entropy(logits, tag_idx)\n",
        "\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            ground_truth.extend(tag_idx.tolist())\n",
        "            predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_ffnn(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the FFNN model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_fixed_window_dataloaders(\n",
        "        hyperparams[\"batch_size\"], hyperparams[\"window_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = FFNN(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"window_size\"],\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_ffnn(\n",
        "            model, loader_train, optimizer, device, silent=True\n",
        "        )\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_ffnn(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbuC7sYHX3Wl"
      },
      "source": [
        "We are ready to run experiments! Let's train the model for 5 epochs, with `window_size=2`. After each epoch, we perform validation and print the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5ZTOFj9NXl65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404a3f90-0907-4dfc-fb2c-522eaf3458f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 2, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "203621 examples loaded.\n",
            "Loading data from eng.val\n",
            "49086 examples loaded.\n",
            "FFNN(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (word_embeddings): Embedding(20103, 64)\n",
            "  (hidden_layer): Linear(in_features=320, out_features=128, bias=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.1876669392008475\n",
            "Training metrics:\n",
            "\t accuracy :  0.9464095954030227\n",
            "\t f1 :  [0.7493188  0.97674134 0.69611577 0.82966288 0.79913771]\n",
            "\t average f1 :  0.8101952995599004\n",
            "\t confusion matrix :  [[  6875   2063    196    394    477]\n",
            " [   632 167580    179    611    275]\n",
            " [   230   1218   2742    166    231]\n",
            " [   257   1729     50   8872    195]\n",
            " [   351   1274    124    241   6302]]\n",
            "Validating..\n",
            "Validation loss:  0.14064293226935357\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9608849773866276\n",
            "\t f1 :  [0.79287091 0.9824944  0.77562327 0.87246722 0.87318646]\n",
            "\t average f1 :  0.859328450902065\n",
            "\t confusion matrix :  [[ 1646   434    37    64    69]\n",
            " [   74 40999    34    35    22]\n",
            " [   51   220   700    17    19]\n",
            " [   46   431     5  2196    12]\n",
            " [   85   211    22    32  1625]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.035411136355063405\n",
            "Training metrics:\n",
            "\t accuracy :  0.990008068324937\n",
            "\t f1 :  [0.94197643 0.99669786 0.92801772 0.98109981 0.95643768]\n",
            "\t average f1 :  0.960845898719238\n",
            "\t confusion matrix :  [[  9351    314     98     66    175]\n",
            " [   194 168876     81     78     57]\n",
            " [    95    197   4190     25     77]\n",
            " [    52     95     15  10901     46]\n",
            " [   158    103     62     43   7915]]\n",
            "Validating..\n",
            "Validation loss:  0.14621477136582448\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9633907835227967\n",
            "\t f1 :  [0.79418345 0.98446445 0.80392157 0.87826087 0.89335072]\n",
            "\t average f1 :  0.8708362103794389\n",
            "\t confusion matrix :  [[ 1775   308    32    54    81]\n",
            " [  185 40841    45    68    25]\n",
            " [   49   188   738    13    19]\n",
            " [  121   321     4  2222    22]\n",
            " [   90   149    10    13  1713]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.022916501436044723\n",
            "Training metrics:\n",
            "\t accuracy :  0.9930582887279596\n",
            "\t f1 :  [0.96358065 0.99722802 0.95878668 0.98244034 0.97627528]\n",
            "\t average f1 :  0.9756621942000834\n",
            "\t confusion matrix :  [[  9591    220     49     42     94]\n",
            " [   159 168904     76    111     48]\n",
            " [    53    115   4362     20     33]\n",
            " [    35    128     10  10910     25]\n",
            " [    73     82     19     19   8086]]\n",
            "Validating..\n",
            "Validation loss:  0.17830831329653543\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9615368944301838\n",
            "\t f1 :  [0.8018278  0.98239682 0.80748663 0.86907631 0.87480106]\n",
            "\t average f1 :  0.8671177228382557\n",
            "\t confusion matrix :  [[ 1667   410    37    57    79]\n",
            " [   83 40963    45    37    36]\n",
            " [   21   207   755    11    13]\n",
            " [   42   462     4  2164    18]\n",
            " [   95   188    22    21  1649]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.021283085386430662\n",
            "Training metrics:\n",
            "\t accuracy :  0.9933485516372796\n",
            "\t f1 :  [0.9669492  0.99721032 0.9658879  0.98273608 0.97545048]\n",
            "\t average f1 :  0.9776467962523462\n",
            "\t confusion matrix :  [[  9640    212     31     48     81]\n",
            " [   149 168902     66     96     61]\n",
            " [    29    118   4403     12     26]\n",
            " [    39    138      8  10901     21]\n",
            " [    70    105     21     21   8066]]\n",
            "Validating..\n",
            "Validation loss:  0.157812924667572\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9615776392454061\n",
            "\t f1 :  [0.81087307 0.9834174  0.74533582 0.88532819 0.88511749]\n",
            "\t average f1 :  0.8620143931647087\n",
            "\t confusion matrix :  [[ 1760   300    64    56    70]\n",
            " [  137 40653   229    91    54]\n",
            " [   26   145   799    22    15]\n",
            " [   74   283    19  2293    21]\n",
            " [   94   132    26    28  1695]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.018096090302924608\n",
            "Training metrics:\n",
            "\t accuracy :  0.9945391215365239\n",
            "\t f1 :  [0.97340079 0.99771498 0.97032738 0.98572136 0.98018542]\n",
            "\t average f1 :  0.9814699882323863\n",
            "\t confusion matrix :  [[  9716    162     24     42     57]\n",
            " [   121 168977     68     79     38]\n",
            " [    22    108   4431      9     19]\n",
            " [    40    103      5  10942     17]\n",
            " [    63     95     16     22   8088]]\n",
            "Validating..\n",
            "Validation loss:  0.16327668899263395\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9651835553925763\n",
            "\t f1 :  [0.82260669 0.98450789 0.81109925 0.8812475  0.8949633 ]\n",
            "\t average f1 :  0.878884926943015\n",
            "\t confusion matrix :  [[ 1783   301    36    41    89]\n",
            " [  139 40862    55    44    64]\n",
            " [   33   189   760     9    16]\n",
            " [   70   372     5  2204    39]\n",
            " [   60   122    11    14  1768]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_ffnn(\n",
        "    {\n",
        "        \"batch_size\": 512,\n",
        "        \"d_emb\": 64,\n",
        "        \"d_hidden\": 128,\n",
        "        \"window_size\": 2,\n",
        "        \"num_epochs\": 5,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"l2\": 1e-6,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h52XPGEg7JOu"
      },
      "source": [
        "Please re-run with `window_size=1`. How does the final performance change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TvikQAmC2aW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc13c11f-5e68-47b9-bb24-2ad95571d782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'window_size': 1, 'num_epochs': 5, 'learning_rate': 0.01, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "203621 examples loaded.\n",
            "Loading data from eng.val\n",
            "49086 examples loaded.\n",
            "FFNN(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (word_embeddings): Embedding(20103, 64)\n",
            "  (hidden_layer): Linear(in_features=192, out_features=128, bias=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.18385889035853692\n",
            "Training metrics:\n",
            "\t accuracy :  0.9452633028967254\n",
            "\t f1 :  [0.73770847 0.97704746 0.71192011 0.8098598  0.80163026]\n",
            "\t average f1 :  0.8076332199473617\n",
            "\t confusion matrix :  [[  6812   2020    202    433    546]\n",
            " [   547 167378    159    946    240]\n",
            " [   340   1057   2816    213    161]\n",
            " [   298   1737     59   8838    181]\n",
            " [   458   1158     88    283   6294]]\n",
            "Validating..\n",
            "Validation loss:  0.14819860929856077\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9564030477121787\n",
            "\t f1 :  [0.76811594 0.98066834 0.78030704 0.84151878 0.86262149]\n",
            "\t average f1 :  0.8466463182828731\n",
            "\t confusion matrix :  [[ 1590   430    54    51   125]\n",
            " [   95 40938    57    31    43]\n",
            " [   27   215   737    14    14]\n",
            " [   86   549     8  2039     8]\n",
            " [   92   194    26    21  1642]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.039941322013867145\n",
            "Training metrics:\n",
            "\t accuracy :  0.988615790302267\n",
            "\t f1 :  [0.9277072  0.99668886 0.92548237 0.97795375 0.94515429]\n",
            "\t average f1 :  0.9545972931400202\n",
            "\t confusion matrix :  [[  9201    318    114     83    293]\n",
            " [   208 168867     79     77     52]\n",
            " [   122    185   4173     33     73]\n",
            " [    73     96     16  10868     54]\n",
            " [   223    107     50     58   7841]]\n",
            "Validating..\n",
            "Validation loss:  0.16386455599179803\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9548751171413438\n",
            "\t f1 :  [0.78474903 0.98019553 0.81304348 0.80511182 0.84786822]\n",
            "\t average f1 :  0.8461936153701327\n",
            "\t confusion matrix :  [[ 1626   381    35    39   169]\n",
            " [  108 40857    43    36   120]\n",
            " [   32   187   748    15    25]\n",
            " [   71   639     1  1890    89]\n",
            " [   57   137     6    25  1750]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.028222821021841964\n",
            "Training metrics:\n",
            "\t accuracy :  0.9916807698362721\n",
            "\t f1 :  [0.95159101 0.99704205 0.95273687 0.98272986 0.96364074]\n",
            "\t average f1 :  0.9695481046007188\n",
            "\t confusion matrix :  [[  9465    248     63     44    189]\n",
            " [   169 168873     71     87     66]\n",
            " [    67    132   4334     14     40]\n",
            " [    47    137      8  10897     22]\n",
            " [   136     92     35     24   8004]]\n",
            "Validating..\n",
            "Validation loss:  0.1715222543522638\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9602126879354602\n",
            "\t f1 :  [0.8        0.98236679 0.79893617 0.85569225 0.86908517]\n",
            "\t average f1 :  0.861216076261852\n",
            "\t confusion matrix :  [[ 1646   376    47    83    98]\n",
            " [   69 40892    56    94    53]\n",
            " [   20   193   751    29    14]\n",
            " [   48   435     5  2191    11]\n",
            " [   82   192    14    34  1653]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.02665923530604193\n",
            "Training metrics:\n",
            "\t accuracy :  0.991410185768262\n",
            "\t f1 :  [0.95608482 0.99664085 0.95452548 0.97614861 0.96758617]\n",
            "\t average f1 :  0.970197184288692\n",
            "\t confusion matrix :  [[  9514    222     52     55    164]\n",
            " [   170 168819     82    142     65]\n",
            " [    46    153   4345     16     26]\n",
            " [    45    209     17  10825     17]\n",
            " [   120     95     22     28   8015]]\n",
            "Validating..\n",
            "Validation loss:  0.17968273118458455\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9578087438373467\n",
            "\t f1 :  [0.77023752 0.98092976 0.8034188  0.84389744 0.87891919]\n",
            "\t average f1 :  0.8554805431897025\n",
            "\t confusion matrix :  [[ 1589   453    47    56   105]\n",
            " [  105 40893    53    50    63]\n",
            " [   31   201   752     9    14]\n",
            " [   71   514     6  2057    42]\n",
            " [   80   151     7    13  1724]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.022537694608215572\n",
            "Training metrics:\n",
            "\t accuracy :  0.9927237484256927\n",
            "\t f1 :  [0.96098026 0.99725767 0.96175824 0.98350757 0.96751208]\n",
            "\t average f1 :  0.9742031646627879\n",
            "\t confusion matrix :  [[  9568    207     42     48    139]\n",
            " [   149 168917     60     85     66]\n",
            " [    37    121   4376     16     34]\n",
            " [    43    124      9  10913     24]\n",
            " [   112    117     29     17   8011]]\n",
            "Validating..\n",
            "Validation loss:  0.18064601077518697\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9603145499735158\n",
            "\t f1 :  [0.792923   0.98207085 0.80846561 0.85191883 0.88092836]\n",
            "\t average f1 :  0.8632613286097005\n",
            "\t confusion matrix :  [[ 1591   430    41    60   128]\n",
            " [   71 40917    58    63    55]\n",
            " [   17   188   764    20    18]\n",
            " [   31   487    10  2120    42]\n",
            " [   53   142    10    24  1746]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Run again with window size 1\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_ffnn(\n",
        "    {\n",
        "        \"batch_size\": 512,\n",
        "        \"d_emb\": 64,\n",
        "        \"d_hidden\": 128,\n",
        "        \"window_size\": 1,\n",
        "        \"num_epochs\": 5,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"l2\": 1e-6,\n",
        "    }\n",
        ")\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGgA0zExVtg9"
      },
      "source": [
        "### Question **(1 point)**\n",
        "\n",
        "If everything works as expected, you should see the loss decrease and the accuracy increase for both training and validation. The final accuracy can be pretty high; you should probably debug if it's below 92%. However, **is accuracy a good metric for this problem? Why?**. Hint: look at the F1 scores for different tags and the confusion matrix.\n",
        "\n",
        "**TODO: Please fill in your answer here**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No especially when dealing with imbalanced datasets or when certain classes are more important than others, accuracy alone might not provide a complete picture of model performance so we should use other metrics like precision, recall, and F1 score"
      ],
      "metadata": {
        "id": "BkoFYtJR8Zrm"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}