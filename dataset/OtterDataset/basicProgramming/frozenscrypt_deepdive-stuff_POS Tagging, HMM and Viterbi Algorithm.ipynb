{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75ae17e",
   "metadata": {},
   "source": [
    "## Why am I writing this?\n",
    "\n",
    "Well, it all starts with a dream! Watching a youtube video, I saw an advertisement about Virtual Assistants and then had a dream about it. So eventhough much of my work in the past has not been around NLP, NLU and dialogue systems, I started reading about them, through research papers, reading articles, watching videos and reaching out to Professors and people in industry working on the problem. While doing so, I also found the need to brush on some of the basics. To do so, I enrolled into a Coursera course, which I should have checked beforehand that it was beginner level and hopelessly easy! Continiuing, to learn more, I opened my notes from back when I was in my Masters and took a course on Natural Language Processing. Going through it, I found the need to document it into a post about use of HMM and Viterbi Algorithm for Parts of Speech (POS) tagging. \n",
    "\n",
    "So, here I am writing this article on POS tagging. Before diving into the problm , let's first understand what is POS. Essentially, given a sentence we are looking to assign an appropriate tag for each word in the sentence. \n",
    "Example, \"The dog laughs.\" This sentence can be tagged as following,\n",
    "\n",
    "The = Determinant\n",
    "\n",
    "Dog = Noun\n",
    "\n",
    "Laughs = Verb\n",
    "\n",
    "Similary for any word in a given sentence, we want to assign appropriate tags for each word in the sentence.\n",
    "\n",
    "So the problem is not to just find a tag for each word individually because words can have different tags depending on the sentence as they are used. We want to find most appropriate tag for each word in a sentence/sequence.  Essentially, we want a tag sequence of a given word sequence that attains the highest 'score'.\n",
    "\n",
    "\n",
    "<b> $T_{1}$$^{n}$ = argmax$t_{1}$$^{n}$ score($t_{1}$...$t_{n}$) </b>\n",
    "\n",
    "\n",
    "Well, we can pose this problem as a Hidden Markov Model (HMM). Let's define how:\n",
    "\n",
    "Consider, that we have a document with multiple sentences or a collection of documents and we decide to split it intp sequences of words. Each word in a sequence can be assigned a tag. We want to develop a model that will allow us to give a tag to each word in a new given sequence.\n",
    "\n",
    "<b> HMM definition:</b>\n",
    "\n",
    "Q = $q_{1}$,$q_{2}$,...,$q_{n}$ are the sqeuence of N states (essentially the tags which are unknown for a new sequence)\n",
    "\n",
    "A = $a_{11}$,$a_{ij}$,...,$a_{nn}$ are the transition probabilities defining the probability of moving from state i to state j, Where sum($q_{ij}$) = 1\n",
    "\n",
    "O = $o_{1}$,$o_{2}$,...,$o_{n}$ are the observations which are the words in a given sequence (visible)\n",
    "\n",
    "B = $b_i$($o_{t}$) defines the observation likelihoods or emission probabilities, the probability of a word given a tag.\n",
    "\n",
    "$\\pi$ = $\\pi_{1}$$\\pi_{2}$...$\\pi_{n}$ is the initial probability distribution over states\n",
    "\n",
    "\n",
    "#### Assumptions to simplify our HMM Model\n",
    "\n",
    "1. The probability of a word depends only on its own tag and is independant of neighboring words and tags.\n",
    "2. The probability of a tag is only dependant on its previous tag. (First order Markov assumption)\n",
    "\n",
    "\n",
    "#### Components of a HMM Tagger\n",
    "\n",
    "\n",
    "1. The Transition Probability (A)\n",
    "    P($t_{i}$/$t_{i-1}$) = Counts($t_{i}$,$t_{i-1}$)/Counts($t_{i-1}$), where $t_{i}$ is tag at ith position in the sequence and $t_{i-1}$ is tag at i-1 th position in the sequence and Counts is the number of times the entities appear in the collection.\n",
    "\n",
    "2. The Emission Probability (B)\n",
    "    P($w_{i}$/$t_{i}$) = Counts($w_{i}$,$t_{i}$)/Counts($t_{i}$), where $w_{i}$ is the word at ith position in the seqeunce and $t_{i}$ is the tag at ith position in the sequene and Counts is the number of times the entities appear in the collection.\n",
    "    \n",
    "    \n",
    "#### Goal\n",
    "\n",
    "The posterior probability will be given by P($t_{i}$/$w_{i}$)\n",
    "\n",
    "we want to find the tag sequence such that,\n",
    "\n",
    "$T_{1}$$^{n}$ = argmax $t_{1}$$^{n}$  P($t_{1}$...$t_{n}$/$w_{1}$...$w_{n}$)\n",
    "\n",
    " =   argmax $t_{1}$$^{n}$  P($w_{1}$...$w_{n}$/$t_{1}$...$t_{n}$)*P($t_{1}$...$t_{n}$)/P($w_{1}$...$w_{n}$)\n",
    " \n",
    " \n",
    "The denominator remains same across all tag sequences, because it is just probability of words in a given sequence so we can remove that and our goal becomes,\n",
    "\n",
    "$T_{1}$$^{n}$ = argmax $t_{1}$$^{n}$  P($t_{1}$...$t_{n}$/$w_{1}$...$w_{n}$)\n",
    "\n",
    " =   argmax $t_{1}$$^{n}$  P($w_{1}$...$w_{n}$/$t_{1}$...$t_{n}$)*P($t_{1}$...$t_{n}$)\n",
    " \n",
    " \n",
    " \n",
    "<b> Clearly, brute force approach is going to be very inefficient. Why? </b>\n",
    "\n",
    "Let's take an example. Suppose the given sequence is \"The dog laughs\"\n",
    "\n",
    "If we have only 3 types of tags with determinant, noun and verb. and we try to brute-force compute the best targ sequence, the number of tag sequences escalates pretty quickly with $|S|^n$ where |S| is number of tags and n is the length of the sequence.\n",
    "\n",
    "\n",
    "\n",
    "#### Viterbi Algorithm to the rescue\n",
    "\n",
    "The Viterbi Algorithm implements the miniumn eduit distance algorithm of dynamic programming. It sets up a probability matrix or lattice where each column corresponds to an obervation and each row corresponds to each state in the state graph. Each cell $V_{t}(j)$ is the probability that the HMM is in state j after seeing the first t observations and passig through the ost probable sequence $q_{1}$,...$q_{t-1}$ . Given HMM $\\lambda$, the most probable path leading to the cell j is given by\n",
    "\n",
    "\n",
    "$V_{t}(j)$ = $max_{q_{1},...q_{t-1}}$ P($q_{1}$,...$q_{t-1}$, $o_{1}$,...$o_{t}$, $q_{t}$=j/$\\lambda$}\n",
    "\n",
    "=  $max_{1}$$^{n}$ $V_{t}(i)$*P($q_{t}$=j/$q_{t-1}$=i)*P($w_{t}$/$q_{t}$=j)\n",
    "\n",
    "               \n",
    "\n",
    "       \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
