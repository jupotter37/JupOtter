{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definer Experiments\n",
    "Previous notebook was getting quite full, here is a new notebooks for the Definer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all imports here to be efficient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode_156_large...\n",
      "Processing episode_053_large...\n",
      "Processing episode_098_large...\n",
      "Processing episode_245_large...\n",
      "Processing episode_299_large...\n",
      "Processing episode_178_large...\n",
      "Processing episode_113_large...\n",
      "Processing episode_286_large...\n",
      "Processing episode_275_large...\n",
      "Processing episode_239_large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" is a good way to solve the problems of physics, of chemistry, of biology, and perhaps of humanity and so on. Yes, I think understanding physics in terms of information theory might be the best way to really understand what's going on here., From our understanding of a universal Turing machine, from our understanding of a computer, do you think there's something outside of the capabilities of a computer that is present in our universe? You have a disagreement with Roger Penrose about the nature of consciousness., He thinks that consciousness is more than just a computation. Do you think all of it, the whole shebangs, can be a computation? Yeah, I've had many fascinating debates with Sir Roger Penrose, and obviously he's famously, and I read, you know, Emperors of the New Mind and his books, his classical books,, and they were pretty influential in the 90s. And he believes that there's something more, something quantum that is needed to explain consciousness in the brain. I think about what we're doing actually at DeepMind and what my career is being, we're almost like Turing's champion., So we are pushing Turing machines or classical computation to the limits. What are the limits of what classical computing can do? Now, and at the same time, I've also studied neuroscience to see, and that's why I did my PhD in, was to see, also to look at, you know, is there anything quantum in the brain\\n from a neuroscience or biological perspective? And so far, I think most neuroscientists and most mainstream biologists and neuroscientists would say there's no evidence of any quantum systems or effects in the brain. As far as we can see, it can be mostly explained by classical theories.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way to generate a random test input using transcripts from Lex Fridman's podcast\n",
    "# Make sure you have the transcripts downloaded in the folder lex_whisper_transcripts\n",
    "\n",
    "import test_on_lex\n",
    "\n",
    "transcripts = test_on_lex.load_lex_transcripts(random_n=10, transcript_folder=\"./lex_whisper_transcripts/\", chunk_time_seconds=15)\n",
    "\n",
    "import random\n",
    "def generate_test_input():\n",
    "    idx = random.randint(0, 10)\n",
    "    key = list(transcripts.keys())[idx]\n",
    "    transcript = transcripts[key]\n",
    "    trans_idx = random.randint(10, len(transcript)-10)\n",
    "    latest = transcript[trans_idx:trans_idx+7]\n",
    "    prev_transcripts, curr_transcripts = str.join(\",\", list(latest[0:5])), latest[5]\n",
    "    return prev_transcripts + \"\\n\" + curr_transcripts\n",
    "\n",
    "generate_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_data(list_data: list):\n",
    "    return \"\\n\".join([f\"{i+1}. {example}\" for i, example in enumerate(list_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_rare_word_agent_prompt_blueprint = \"\"\"\n",
    "# Objective\n",
    "Your role is to identify and define \"Rare Entities (REs)\" in a transcript. Types of REs include rare words, jargons, adages, concepts, people, places, organizations, events etc that are not well known to the average high schooler, in accordance to current trends. You can also intelligently detect REs that are described in the conversation but not explicitly mentioned.\n",
    "\n",
    "# Criteria for Rare Entities in order of importance\n",
    "1. Rarity: Select entities that are unlikely for an average high schooler to know. Well known entities are like Fortune 500 organizations, worldwide-known events, popular locations, and entities popularized by recent news or events such as \"COVID-19\", \"Bitcoin\", or \"Generative AI\".\n",
    "2. Utility: Definition should help a user understand the conversation better and achieve their goals.\n",
    "3. No Redundancy: Exclude definitions if already defined in the conversation.\n",
    "4. Complexity: Choose phrases with non-obvious meanings, such that their meaning cannot be derived from simple words within the entity name, such as \"Butterfly Effect\" which has a totally different meaning from its base words, but not \"Electric Car\" nor \"Lane Keeping System\" as they're easily derived.\n",
    "5. Definability: Must be clearly and succinctly definable in under 10 words.\n",
    "6. Existance: Don't select entities if you have no knowledge of them\n",
    "\n",
    "# Conversation Transcript:\n",
    "<Transcript start>{conversation_context}<Transcript end>\n",
    "\n",
    "# Output Guidelines:\n",
    "Output an array of the entities using the following template: `[{{ name: string, definition: string, search_keyword: string }}]`\n",
    "- name is the RE name shown to the user, if the name is mistranscribed, autocorrect it into the most well known form with proper spelling, capitalization and punctuation\n",
    "- definition is concise (< 12 words) and uses simple words\n",
    "- search_keyword is the best specific Internet search keywords to search for the RE, you might need to use their complete official RE name, or autocorrect RE name, or add additional context keywords (like the entity type) for better searchability, especially if the entity is ambiguous or has multiple meanings. Additionally, for rare words, add \"definition\" to the search keyword.\n",
    "- it's OK to output an empty array - most of the time, the array will be empty, only include items if the fit all the requirements\n",
    "\n",
    "## Additional Guidelines:\n",
    "- Only define NOUNS.\n",
    "- Select RE that have an entry in an encyclopedia, wikipedia, dictionary, or other reference material.\n",
    "- Do not select a RE you yourself are unfamiliar with, you can infer the implied entity or autocorrect mistransribed names only if you are 99% confident, never select an entity if you will define it as \"Unknown Entity\".\n",
    "- Multiple REs can be defined from one phrase, for example, \"The Lugubrious Game\" is a candidate to define, the rare word \"lugubrious\" is also a candidate.\n",
    "- Limit results to {number_of_definitions} REs.\n",
    "- Examples:\n",
    "    - Completing incomplete name example: Conversation mentions \"Balmer\" and \"Microsoft\", the keyword is \"Steve Balmer + person\", and the name would be \"Steve Balmer\" because it is complete.\n",
    "    - Replacing unofficial name example: Conversation mentions \"Clay Institute\", the keyword is \"Clay Mathematics Institute + organization\", using the official name.\n",
    "    - Add context example: Conversation mentions \"Theory of everything\", the keyword needs context keywords such as \"Theory of everything + concept\", because there is a popular movie with the same name. \n",
    "    - Autocorrect transcript example: Conversation mentions \"Coleman Sachs\" in the context of finance, if you are confident it is supposed to be \"Goldman Sachs\", you autocorrect it and define \"Goldman Sachs\".\n",
    "\n",
    "## Recent Definitions:\n",
    "These REs have already been defined so don't define them again:\n",
    "{definitions_history}\n",
    "\n",
    "## Example Output:\n",
    "entities: [{{ name: \"80/20 Rule\", definition: \"Productivity concept; Majority of results come from few causes\", search_keyword: \"80/20 Rule + concept\" }}]\n",
    "\n",
    "{format_instructions} \n",
    "If there are no relevant entities, output an empty array.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proactive_rare_word_agent_and_definer(\n",
    "    conversation_context: str, definitions_history: list = []\n",
    "):\n",
    "    # run proactive agent to find out which expert agents we should run\n",
    "    proactive_rare_word_agent_response = run_proactive_rare_word_agent(\n",
    "        conversation_context, definitions_history\n",
    "    )\n",
    "\n",
    "    # do nothing else if proactive meta agent didn't specify an agent to run\n",
    "    if proactive_rare_word_agent_response == []:\n",
    "        return []\n",
    "\n",
    "    # pass words to define to definer agent\n",
    "    print(\"proactive_rare_word_agent_response\", proactive_rare_word_agent_response)\n",
    "    \n",
    "    return proactive_rare_word_agent_response\n",
    "\n",
    "class ProactiveRareWordAgentQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Proactive rare word agent that identifies rare entities in a conversation context\n",
    "    \"\"\"\n",
    "\n",
    "    to_define_list: list = Field(\n",
    "        description=\"the rare entities to define\",\n",
    "    )\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"entity name\",\n",
    "    )\n",
    "    definition: str = Field(\n",
    "        description=\"entity definition\",\n",
    "    )\n",
    "    search_keyword: str = Field(\n",
    "        description=\"keyword to search for entity on the Internet\",\n",
    "    )\n",
    "\n",
    "class ConversationEntities(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        description=\"list of entities and their definitions\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "proactive_rare_word_agent_query_parser = PydanticOutputParser(\n",
    "    pydantic_object=ConversationEntities\n",
    ")\n",
    "\n",
    "def run_proactive_rare_word_agent(conversation_context: str, definitions_history: list):\n",
    "    # start up GPT4 connection\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=os.environ.get(\"OPEN_AI_API_KEY\"),\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    extract_proactive_rare_word_agent_query_prompt = PromptTemplate(\n",
    "        template=proactive_rare_word_agent_prompt_blueprint,\n",
    "        input_variables=[\n",
    "            \"conversation_context\",\n",
    "            \"definitions_history\",\n",
    "        ],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": proactive_rare_word_agent_query_parser.get_format_instructions(),\n",
    "            \"number_of_definitions\": 3,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if len(definitions_history) > 0:\n",
    "        definitions_history = format_list_data(definitions_history)\n",
    "    else:\n",
    "        definitions_history = \"None\"\n",
    "\n",
    "    proactive_rare_word_agent_query_prompt_string = (\n",
    "        extract_proactive_rare_word_agent_query_prompt.format_prompt(\n",
    "            conversation_context=conversation_context,\n",
    "            definitions_history=definitions_history,\n",
    "        ).to_string()\n",
    "    )\n",
    "\n",
    "    # print(\"Proactive meta agent query prompt string\", proactive_rare_word_agent_query_prompt_string)\n",
    "\n",
    "    response = llm(\n",
    "        [HumanMessage(content=proactive_rare_word_agent_query_prompt_string)]\n",
    "    )\n",
    "\n",
    "    print(response.content)\n",
    "    try:\n",
    "        res = proactive_rare_word_agent_query_parser.parse(\n",
    "            response.content\n",
    "        )\n",
    "        return res\n",
    "    except OutputParserException as e:\n",
    "        print(\"Error parsing output\" , e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falcon LLM model\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hugging Face\",\n",
      "      \"definition\": \"AI company specializing in natural language processing\",\n",
      "      \"search_keyword\": \"Hugging Face + AI company\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"definition\": \"AI research lab known for advanced AI models\",\n",
      "      \"search_keyword\": \"OpenAI + AI research lab\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Falcon LLM model\",\n",
      "      \"definition\": \"A large language model for AI applications\",\n",
      "      \"search_keyword\": \"Falcon LLM model + AI\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hugging Face', definition='AI company specializing in natural language processing', search_keyword='Hugging Face + AI company'), Entity(name='OpenAI', definition='AI research lab known for advanced AI models', search_keyword='OpenAI + AI research lab'), Entity(name='Falcon LLM model', definition='A large language model for AI applications', search_keyword='Falcon LLM model + AI')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Hugging Face', definition='AI company specializing in natural language processing', search_keyword='Hugging Face + AI company'), Entity(name='OpenAI', definition='AI research lab known for advanced AI models', search_keyword='OpenAI + AI research lab'), Entity(name='Falcon LLM model', definition='A large language model for AI applications', search_keyword='Falcon LLM model + AI')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_transcript = generate_test_input()\n",
    "test_transcript = \"\"\"\n",
    "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falcon LLM model\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search tool\n",
    "EKG is unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Literal\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "k: int = 3\n",
    "gl: str = \"us\"\n",
    "hl: str = \"en\"\n",
    "tbs = None\n",
    "num_sentences = 7\n",
    "serper_api_key=os.environ.get(\"SERPER_API_KEY\")\n",
    "search_type: Literal[\"news\", \"search\", \"places\", \"images\"] = \"images\"\n",
    "result_key_for_type = {\n",
    "        \"news\": \"news\",\n",
    "        \"places\": \"places\",\n",
    "        \"images\": \"images\",\n",
    "        \"search\": \"organic\",\n",
    "    }\n",
    "\n",
    "async def serper_search_async(\n",
    "    search_term: str, search_type: str = \"search\", **kwargs: Any\n",
    ") -> dict:\n",
    "    headers = {\n",
    "        \"X-API-KEY\": serper_api_key or \"\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": search_term,\n",
    "        **{key: value for key, value in kwargs.items() if value is not None},\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(f\"https://google.serper.dev/{search_type}\", headers=headers, json=params) as response:\n",
    "            response.raise_for_status()\n",
    "            search_results = await response.json()\n",
    "            return search_results\n",
    "\n",
    "\n",
    "async def parse_snippets_async(results: dict, scrape_pages: bool = False, summarize_pages: bool = True, num_sentences: int = 3) -> List[str]:\n",
    "    snippets = []\n",
    "    if results.get(\"answerBox\"):\n",
    "        answer_box = results.get(\"answerBox\", {})\n",
    "        if answer_box.get(\"answer\"):\n",
    "            snippets.append(f\"The answer is {answer_box.get('answer')}\")\n",
    "        elif answer_box.get(\"snippet\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippet')}\")\n",
    "        elif answer_box.get(\"snippetHighlighted\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippetHighlighted')}\")\n",
    "\n",
    "    if results.get(\"knowledgeGraph\"):\n",
    "        kg = results.get(\"knowledgeGraph\", {})\n",
    "        title = kg.get(\"title\")\n",
    "        entity_type = kg.get(\"type\")\n",
    "        if entity_type:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {entity_type}.\")\n",
    "        description = kg.get(\"description\")\n",
    "        if description:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {description}.\")\n",
    "        for attribute, value in kg.get(\"attributes\", {}).items():\n",
    "            snippets.append(f\"Knowledge Graph Results: {title} {attribute}: {value}.\")\n",
    "\n",
    "    if scrape_pages:\n",
    "        tasks = []\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            task = asyncio.create_task(scrape_page_async(result[\"link\"], summarize_page=summarize_pages, num_sentences=num_sentences))\n",
    "            tasks.append(task)\n",
    "        summarized_pages = await asyncio.gather(*tasks)\n",
    "        for i, page in enumerate(summarized_pages):\n",
    "            result = results[result_key_for_type[search_type]][i]\n",
    "            if page:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\nSummarized Page: {page}\")\n",
    "            else:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "    else:\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "\n",
    "    if len(snippets) == 0:\n",
    "        return [\"No good Google Search Result was found\"]\n",
    "    return snippets\n",
    "\n",
    "import requests\n",
    "\n",
    "def can_embed_url(url: str):\n",
    "    response = requests.head(url)\n",
    "\n",
    "    # Check the headers for 'X-Frame-Options' or 'Content-Security-Policy'\n",
    "    x_frame_options = response.headers.get('X-Frame-Options')\n",
    "    csp = response.headers.get('Content-Security-Policy')\n",
    "\n",
    "    return not (x_frame_options or ('frame-ancestors' in csp if csp else False))\n",
    "\n",
    "def extract_entity_url_and_image(search_results: dict, image_results: dict):\n",
    "    # Only get the first top url and image_url\n",
    "    res = {}\n",
    "    if search_results.get(\"knowledgeGraph\"):\n",
    "        result = search_results.get(\"knowledgeGraph\", {})\n",
    "        if result.get(\"descriptionSource\") == \"Wikipedia\":\n",
    "            ref_url = result.get(\"descriptionLink\")\n",
    "            res[\"url\"] = ref_url\n",
    "\n",
    "    for result in search_results[result_key_for_type[\"search\"]][:k]:\n",
    "        if \"url\" not in res and result.get(\"link\") and can_embed_url(result.get(\"link\")):\n",
    "            res[\"url\"] = result.get(\"link\")\n",
    "            break\n",
    "\n",
    "    if image_results is None:\n",
    "        return res\n",
    "    \n",
    "    for result in image_results[result_key_for_type[\"images\"]][:k]:\n",
    "        if \"image_url\" not in res and result.get(\"imageUrl\"):\n",
    "            res[\"image_url\"] = result.get(\"imageUrl\")\n",
    "            break\n",
    "\n",
    "    return res\n",
    "\n",
    "async def search_url_for_entity_async(query: str):\n",
    "    async def inner_search(query:str): \n",
    "        search_task = asyncio.create_task(serper_search_async(\n",
    "            search_term=query,\n",
    "            gl=gl,\n",
    "            hl=hl,\n",
    "            num=k,\n",
    "            tbs=tbs,\n",
    "            search_type=\"search\",\n",
    "        ))\n",
    "\n",
    "        image_search_task = None if \"definition\" in query else asyncio.create_task(serper_search_async(\n",
    "            search_term=query,\n",
    "            gl=gl,\n",
    "            hl=hl,\n",
    "            num=k,\n",
    "            tbs=tbs,\n",
    "            search_type=\"images\",\n",
    "        ))\n",
    "\n",
    "        tasks = [search_task]\n",
    "        if image_search_task:\n",
    "            tasks.append(image_search_task)\n",
    "\n",
    "        search_results, image_results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return extract_entity_url_and_image(search_results, image_results)\n",
    "    \n",
    "    res = await inner_search(query)\n",
    "    print(res)\n",
    "    if \"url\" not in res:\n",
    "        res = await inner_search(query + \" wiki\") # fallback search using wiki\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientResponseError",
     "evalue": "400, message='Bad Request', url=URL('https://google.serper.dev/images')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m search_url_for_entity_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow is chatgpt doing medium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 144\u001b[0m, in \u001b[0;36msearch_url_for_entity_async\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    140\u001b[0m     search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n\u001b[0;32m--> 144\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m inner_search(query)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "Cell \u001b[0;32mIn[7], line 140\u001b[0m, in \u001b[0;36msearch_url_for_entity_async.<locals>.inner_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_search_task:\n\u001b[1;32m    138\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(image_search_task)\n\u001b[0;32m--> 140\u001b[0m search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mserper_search_async\u001b[0;34m(search_term, search_type, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://google.serper.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m---> 33\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m search_results\n",
      "File \u001b[0;32m~/mambaforge/envs/tosg/lib/python3.11/site-packages/aiohttp/client_reqrep.py:1062\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1065\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1066\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1067\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 400, message='Bad Request', url=URL('https://google.serper.dev/images')"
     ]
    }
   ],
   "source": [
    "await search_url_for_entity_async(\"how is chatgpt doing medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what has been so revolutionary in the last 10 years, I would 15 years and thinking about the internet, I would say things like, hopefully I'm not saying anything ridiculous, but everything from Wikipedia to Twitter., So like these kind of websites, not so much AI, but like I would expect to see some kind of big productivity increases from just the connectivity between people and the access to more information., Yeah, well, so that's another area I've done quite a bit of research on actually, is these free goods like Wikipedia, Facebook, Twitter, Zoom. We're actually doing this in person, but almost everything else I do these days is online. The interesting thing about all those is most of them have a price of zero., What do you pay for Wikipedia? Maybe like a little bit for the electrons to come to your house. Basically zero, right? Take a small pause and say, I donate to Wikipedia. Often you should too. It's good for you, yeah. So, but what does that do mean for GDP? GDP is based on the price and quantity, of all the goods, things bought and sold. If something has zero price, you know how much it contributes to GDP? To a first approximation, zero. So these digital goods that we're getting more and more of, we're spending more and more hours a day consuming stuff off of screens, little screens, big screens,\n",
      " that doesn't get priced into GDP. It's like they don't exist. That doesn't mean they don't create value. I get a lot of value from watching cat videos and reading Wikipedia articles and listening to podcasts, even if I don't pay for them. So we've got a mismatch there. Now, in fairness, economists,\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"GDP\",\n",
      "      \"definition\": \"Total value of goods, services produced in a country\",\n",
      "      \"search_keyword\": \"Gross Domestic Product + concept\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='GDP', definition='Total value of goods, services produced in a country', search_keyword='Gross Domestic Product + concept')]\n",
      "entities=[Entity(name='GDP', definition='Total value of goods, services produced in a country', search_keyword='Gross Domestic Product + concept')]\n"
     ]
    },
    {
     "ename": "ClientResponseError",
     "evalue": "400, message='Bad Request', url=URL('https://google.serper.dev/images')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entities \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mentities:\n\u001b[0;32m----> 6\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_url_for_entity_async(entities\u001b[38;5;241m.\u001b[39msearch_keyword)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m      8\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_url_for_entity_async(entities\u001b[38;5;241m.\u001b[39msearch_keyword \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m wiki\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 144\u001b[0m, in \u001b[0;36msearch_url_for_entity_async\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    140\u001b[0m     search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n\u001b[0;32m--> 144\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m inner_search(query)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "Cell \u001b[0;32mIn[7], line 140\u001b[0m, in \u001b[0;36msearch_url_for_entity_async.<locals>.inner_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_search_task:\n\u001b[1;32m    138\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(image_search_task)\n\u001b[0;32m--> 140\u001b[0m search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mserper_search_async\u001b[0;34m(search_term, search_type, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://google.serper.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m---> 33\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m search_results\n",
      "File \u001b[0;32m~/mambaforge/envs/tosg/lib/python3.11/site-packages/aiohttp/client_reqrep.py:1062\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1065\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1066\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1067\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 400, message='Bad Request', url=URL('https://google.serper.dev/images')"
     ]
    }
   ],
   "source": [
    "test_transcript = generate_test_input()\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "print(res)\n",
    "for entities in res.entities:\n",
    "    res = await search_url_for_entity_async(entities.search_keyword)\n",
    "    if \"url\" not in res:\n",
    "        res = await search_url_for_entity_async(entities.search_keyword + \" wiki\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just go for the best intuitive way that works the best\n",
    "\n",
    "Pipeline\n",
    "1. Check if page can be embed\n",
    "2. Check if url is accurate for definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if page can be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page can be embedded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the page you want to check\n",
    "url = 'https://en.wikipedia.org/wiki/Borat_Sagdiyev'\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.head(url)\n",
    "\n",
    "# Check the headers for 'X-Frame-Options' or 'Content-Security-Policy'\n",
    "x_frame_options = response.headers.get('X-Frame-Options')\n",
    "csp = response.headers.get('Content-Security-Policy')\n",
    "\n",
    "if x_frame_options or ('frame-ancestors' in csp if csp else False):\n",
    "    print(\"The page cannot be embedded.\")\n",
    "else:\n",
    "    print(\"The page can be embedded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touring machines epitomize the quintessence of computational esotericism, manipulating symbols on a tape according to a tableau of rules. These automata traverse the tape's aleph-null segments, effectuating state transitions within a discrete, preternatural milieu. Ineffably, they delineate the demarcation of decidability and recursively enumerable conundrums.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Turing machines\",\n",
      "      \"definition\": \"Abstract machines that manipulate symbols\",\n",
      "      \"search_keyword\": \"Turing machines + concept\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Aleph-null\",\n",
      "      \"definition\": \"Smallest infinity in set theory\",\n",
      "      \"search_keyword\": \"Aleph-null + set theory\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Recursively enumerable\",\n",
      "      \"definition\": \"Semi-decidable problem class in computability\",\n",
      "      \"search_keyword\": \"Recursively enumerable + computability theory\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Turing machines', definition='Abstract machines that manipulate symbols', search_keyword='Turing machines + concept'), Entity(name='Aleph-null', definition='Smallest infinity in set theory', search_keyword='Aleph-null + set theory'), Entity(name='Recursively enumerable', definition='Semi-decidable problem class in computability', search_keyword='Recursively enumerable + computability theory')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Turing machines', definition='Abstract machines that manipulate symbols', search_keyword='Turing machines + concept'), Entity(name='Aleph-null', definition='Smallest infinity in set theory', search_keyword='Aleph-null + set theory'), Entity(name='Recursively enumerable', definition='Semi-decidable problem class in computability', search_keyword='Recursively enumerable + computability theory')])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test mispelling\n",
    "test_transcript = \"\"\"Touring machines epitomize the quintessence of computational esotericism, manipulating symbols on a tape according to a tableau of rules. These automata traverse the tape's aleph-null segments, effectuating state transitions within a discrete, preternatural milieu. Ineffably, they delineate the demarcation of decidability and recursively enumerable conundrums.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enceladus, an enigmatic spheroid ensconced within Saturn's magniloquent rings, exudes cryptic cryovolcanic plumes. These plumes are a melange of volatile compounds, festooning the E-ring with a diaphanous, icy effulgence. Amidst the celestial ballet, Enceladus pirouettes, a harbinger of astrobiological enigmas and cosmochemical perplexities.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Enceladus\",\n",
      "      \"definition\": \"Moon of Saturn with geysers and possible subsurface ocean\",\n",
      "      \"search_keyword\": \"Enceladus + moon\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cryovolcanism\",\n",
      "      \"definition\": \"Volcanic activity in icy celestial bodies\",\n",
      "      \"search_keyword\": \"Cryovolcanism + definition\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Astrobiology\",\n",
      "      \"definition\": \"Study of life in the universe\",\n",
      "      \"search_keyword\": \"Astrobiology + definition\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Enceladus', definition='Moon of Saturn with geysers and possible subsurface ocean', search_keyword='Enceladus + moon'), Entity(name='Cryovolcanism', definition='Volcanic activity in icy celestial bodies', search_keyword='Cryovolcanism + definition'), Entity(name='Astrobiology', definition='Study of life in the universe', search_keyword='Astrobiology + definition')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Enceladus', definition='Moon of Saturn with geysers and possible subsurface ocean', search_keyword='Enceladus + moon'), Entity(name='Cryovolcanism', definition='Volcanic activity in icy celestial bodies', search_keyword='Cryovolcanism + definition'), Entity(name='Astrobiology', definition='Study of life in the universe', search_keyword='Astrobiology + definition')])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test rare terms\n",
    "test_transcript = \"\"\"Enceladus, an enigmatic spheroid ensconced within Saturn's magniloquent rings, exudes cryptic cryovolcanic plumes. These plumes are a melange of volatile compounds, festooning the E-ring with a diaphanous, icy effulgence. Amidst the celestial ballet, Enceladus pirouettes, a harbinger of astrobiological enigmas and cosmochemical perplexities.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Marble Caves of Patagonia, a rare and ethereal natural wonder, are often overshadowed by more renowned landmarks yet offer an otherworldly beauty to those few who navigate their secluded azure chambers.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Marble Caves of Patagonia\",\n",
      "      \"definition\": \"Stunning natural caves with blue waters\",\n",
      "      \"search_keyword\": \"Marble Caves of Patagonia\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Marble Caves of Patagonia', definition='Stunning natural caves with blue waters', search_keyword='Marble Caves of Patagonia')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Marble Caves of Patagonia', definition='Stunning natural caves with blue waters', search_keyword='Marble Caves of Patagonia')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test unofficial names\n",
    "test_transcript = \"\"\"The Marble Caves of Patagonia, a rare and ethereal natural wonder, are often overshadowed by more renowned landmarks yet offer an otherworldly beauty to those few who navigate their secluded azure chambers.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaking of rare things, I was reading about Oymyakon. Can you imagine living in such cold? Jamie: Hard to even think about! Speaking of unique places, I'd love to visit Timbuktu someday. It's almost like a real-life cacophony of culture and history. Alex: Absolutely. Switching topics, are you going to the Dyngus Day celebration? I hear it's a blast. Jamie: I'd love to! It's always fun to learn about and participate in traditions from around the world. Just like heliolatry practices, they have such deep roots. Alex: Speaking of the world, the Bilderberg Meeting's recent convention had some interesting discussions. It's like a modern-day triumvirate in some ways.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Oymyakon\",\n",
      "      \"definition\": \"Coldest inhabited place on Earth\",\n",
      "      \"search_keyword\": \"Oymyakon + place\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Timbuktu\",\n",
      "      \"definition\": \"Historic city in Mali, cultural center\",\n",
      "      \"search_keyword\": \"Timbuktu + historic city\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Dyngus Day\",\n",
      "      \"definition\": \"Polish Easter Monday celebration\",\n",
      "      \"search_keyword\": \"Dyngus Day + celebration\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Oymyakon', definition='Coldest inhabited place on Earth', search_keyword='Oymyakon + place'), Entity(name='Timbuktu', definition='Historic city in Mali, cultural center', search_keyword='Timbuktu + historic city'), Entity(name='Dyngus Day', definition='Polish Easter Monday celebration', search_keyword='Dyngus Day + celebration')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Oymyakon', definition='Coldest inhabited place on Earth', search_keyword='Oymyakon + place'), Entity(name='Timbuktu', definition='Historic city in Mali, cultural center', search_keyword='Timbuktu + historic city'), Entity(name='Dyngus Day', definition='Polish Easter Monday celebration', search_keyword='Dyngus Day + celebration')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test transcript from chatgpt vtt\n",
    "test_transcript = \"\"\"Speaking of rare things, I was reading about Oymyakon. Can you imagine living in such cold? Jamie: Hard to even think about! Speaking of unique places, I'd love to visit Timbuktu someday. It's almost like a real-life cacophony of culture and history. Alex: Absolutely. Switching topics, are you going to the Dyngus Day celebration? I hear it's a blast. Jamie: I'd love to! It's always fun to learn about and participate in traditions from around the world. Just like heliolatry practices, they have such deep roots. Alex: Speaking of the world, the Bilderberg Meeting's recent convention had some interesting discussions. It's like a modern-day triumvirate in some ways.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaking of rare things, I was reading about Oymiakon. Can you imagine living in such cold? Jamie: Hard to even think about! Speaking of unique places, I'd love to visit Timbucktoo someday. It's almost like a real-life cackophony of culture and history. Alex: Absolutely. Switching topics, are you going to the Dyngus Dey celebration? I hear it's a blast. Jamie: I'd love to! It's always fun to learn about and participate in traditions from around the world. Just like heliolotry practices, they have such deep roots. Alex: Speaking of the world, the Bilderburg Meeting's recent convention had some interesting discussions. It's like a modern-day triunvirate in some ways. Jamie: Right. And speaking of politics, defenistration seems like an extreme form of political maneuvering. Imagine the history behind that!'\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Oymyakon\",\n",
      "      \"definition\": \"Coldest inhabited place on Earth\",\n",
      "      \"search_keyword\": \"Oymyakon + place\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Timbuktu\",\n",
      "      \"definition\": \"Historic city in Mali, West Africa\",\n",
      "      \"search_keyword\": \"Timbuktu + historic city\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Dyngus Day\",\n",
      "      \"definition\": \"Polish Easter Monday celebration\",\n",
      "      \"search_keyword\": \"Dyngus Day + celebration\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Oymyakon', definition='Coldest inhabited place on Earth', search_keyword='Oymyakon + place'), Entity(name='Timbuktu', definition='Historic city in Mali, West Africa', search_keyword='Timbuktu + historic city'), Entity(name='Dyngus Day', definition='Polish Easter Monday celebration', search_keyword='Dyngus Day + celebration')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Oymyakon', definition='Coldest inhabited place on Earth', search_keyword='Oymyakon + place'), Entity(name='Timbuktu', definition='Historic city in Mali, West Africa', search_keyword='Timbuktu + historic city'), Entity(name='Dyngus Day', definition='Polish Easter Monday celebration', search_keyword='Dyngus Day + celebration')])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noised up version, mispelling, etc\n",
    "test_transcript = \"\"\"Speaking of rare things, I was reading about Oymiakon. Can you imagine living in such cold? Jamie: Hard to even think about! Speaking of unique places, I'd love to visit Timbucktoo someday. It's almost like a real-life cackophony of culture and history. Alex: Absolutely. Switching topics, are you going to the Dyngus Dey celebration? I hear it's a blast. Jamie: I'd love to! It's always fun to learn about and participate in traditions from around the world. Just like heliolotry practices, they have such deep roots. Alex: Speaking of the world, the Bilderburg Meeting's recent convention had some interesting discussions. It's like a modern-day triunvirate in some ways. Jamie: Right. And speaking of politics, defenistration seems like an extreme form of political maneuvering. Imagine the history behind that!'\n",
    "\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tosg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
