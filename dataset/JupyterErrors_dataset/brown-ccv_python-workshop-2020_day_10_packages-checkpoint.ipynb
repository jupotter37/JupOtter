{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Please go to https://ccv.jupyter.brown.edu </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> What we learned so far... </center>\n",
    "- Variables: integers, floats, booleans, strings\n",
    "- Container types: lists, dictionaries\n",
    "- Control flow: if-else statements, for loops, comprehensions\n",
    "- How to write simple functions and functions with control flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Packages and the Pandas package </center>\n",
    "## By the end of the day you'll be able to \n",
    "- import packages\n",
    "- describe why the Pandas package is useful\n",
    "- create a dataframe and summarize it\n",
    "- read a CSV file into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## what are packages\n",
    "- libraries of code\n",
    "- specific to tasks/functions\n",
    "- a lot of common functions are already written by computer scientists and are much faster than you can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- import packages\n",
    "- <font color='LIGHTGRAY'> describe why the Pandas package is useful</font>\n",
    "- <font color='LIGHTGRAY'> create a dataframe and summarize it </font>\n",
    "- <font color='LIGHTGRAY'> read a CSV file into a Pandas dataframe</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cdfa6a06497f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnums_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "nums_list = [1,2,3,4,5,10,20,50,200]\n",
    "print(mean(nums_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> Let's google it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.77777777777778\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "print(statistics.mean(nums_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean in module statistics:\n",
      "\n",
      "mean(data)\n",
      "    Return the sample arithmetic mean of data.\n",
      "    \n",
      "    >>> mean([1, 2, 3, 4, 4])\n",
      "    2.8\n",
      "    \n",
      "    >>> from fractions import Fraction as F\n",
      "    >>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\n",
      "    Fraction(13, 21)\n",
      "    \n",
      "    >>> from decimal import Decimal as D\n",
      "    >>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\n",
      "    Decimal('0.5625')\n",
      "    \n",
      "    If ``data`` is empty, StatisticsError will be raised.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(statistics.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.77777777777778\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(mean(nums_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.77777777777778\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(nums_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean in module numpy.core.fromnumeric:\n",
      "\n",
      "mean(a, axis=None, dtype=None, out=None, keepdims=<no value>)\n",
      "    Compute the arithmetic mean along the specified axis.\n",
      "    \n",
      "    Returns the average of the array elements.  The average is taken over\n",
      "    the flattened array by default, otherwise over the specified axis.\n",
      "    `float64` intermediate and return values are used for integer inputs.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Array containing numbers whose mean is desired. If `a` is not an\n",
      "        array, a conversion is attempted.\n",
      "    axis : None or int or tuple of ints, optional\n",
      "        Axis or axes along which the means are computed. The default is to\n",
      "        compute the mean of the flattened array.\n",
      "    \n",
      "        .. versionadded:: 1.7.0\n",
      "    \n",
      "        If this is a tuple of ints, a mean is performed over multiple axes,\n",
      "        instead of a single axis or all the axes as before.\n",
      "    dtype : data-type, optional\n",
      "        Type to use in computing the mean.  For integer inputs, the default\n",
      "        is `float64`; for floating point inputs, it is the same as the\n",
      "        input dtype.\n",
      "    out : ndarray, optional\n",
      "        Alternate output array in which to place the result.  The default\n",
      "        is ``None``; if provided, it must have the same shape as the\n",
      "        expected output, but the type will be cast if necessary.\n",
      "        See `doc.ufuncs` for details.\n",
      "    \n",
      "    keepdims : bool, optional\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "        If the default value is passed, then `keepdims` will not be\n",
      "        passed through to the `mean` method of sub-classes of\n",
      "        `ndarray`, however any non-default value will be.  If the\n",
      "        sub-class' method does not implement `keepdims` any\n",
      "        exceptions will be raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    m : ndarray, see dtype parameter above\n",
      "        If `out=None`, returns a new array containing the mean values,\n",
      "        otherwise a reference to the output array is returned.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    average : Weighted average\n",
      "    std, var, nanmean, nanstd, nanvar\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The arithmetic mean is the sum of the elements along the axis divided\n",
      "    by the number of elements.\n",
      "    \n",
      "    Note that for floating-point input, the mean is computed using the\n",
      "    same precision the input has.  Depending on the input data, this can\n",
      "    cause the results to be inaccurate, especially for `float32` (see\n",
      "    example below).  Specifying a higher-precision accumulator using the\n",
      "    `dtype` keyword can alleviate this issue.\n",
      "    \n",
      "    By default, `float16` results are computed using `float32` intermediates\n",
      "    for extra precision.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([[1, 2], [3, 4]])\n",
      "    >>> np.mean(a)\n",
      "    2.5\n",
      "    >>> np.mean(a, axis=0)\n",
      "    array([ 2.,  3.])\n",
      "    >>> np.mean(a, axis=1)\n",
      "    array([ 1.5,  3.5])\n",
      "    \n",
      "    In single precision, `mean` can be inaccurate:\n",
      "    \n",
      "    >>> a = np.zeros((2, 512*512), dtype=np.float32)\n",
      "    >>> a[0, :] = 1.0\n",
      "    >>> a[1, :] = 0.1\n",
      "    >>> np.mean(a)\n",
      "    0.54999924\n",
      "    \n",
      "    Computing the mean in float64 is more accurate:\n",
      "    \n",
      "    >>> np.mean(a, dtype=np.float64)\n",
      "    0.55000000074505806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: google the standard deviation function in the `statistics` and `numpy` python packages. import the packages and then use the functions on `nums_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.905715436562325\n",
      "64.6002665973171\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "print(np.std(nums_list))\n",
    "print(statistics.stdev(nums_list))\n",
    "help(np.std)\n",
    "help(statistics.stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <font color='LIGHTGRAY'> import packages </font>\n",
    "- describe why the Pandas package and dataframes are useful\n",
    "- <font color='LIGHTGRAY'> create a dataframe and summarize it </font>\n",
    "- <font color='LIGHTGRAY'> read a CSV file into a Pandas dataframe</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why `Pandas`?\n",
    "- Work with tabular data with mixed types, like an excel sheet\n",
    "- Next week, we will work with Pandas to store the data that we scrape from the web\n",
    "- The following week, we will do data cleaning, calculations, and plotting using `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The `DataFrame` container type\n",
    "\n",
    "* Part of Pandas package\n",
    "* Spreadsheet or table-like representation of data\n",
    "* Can store mixed types\n",
    "* Columns and rows are named\n",
    "* Like a nested list, where all the sublists have the same shape (basically a matrix)\n",
    "* Lots of functions for cleaning and massaging data, grouping, aggregations, plotting\n",
    "* Exceptionally popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "|index| name    | age | address    |\n",
    "|-|---------|-----|------------|\n",
    "|0| Ashley  | 30  | Providence |\n",
    "|...| ...     | ... | ...        |\n",
    "|10| Rihanna | 32  | Barbados   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![title](./panda.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <font color='LIGHTGRAY'> import packages </font>\n",
    "- <font color='LIGHTGRAY'> describe why the Pandas package is useful</font>\n",
    "- create a dataframe and summarize it\n",
    "- <font color='LIGHTGRAY'>read a CSV file into a Pandas dataframe </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating a dataframe from a dictionary of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "names_list = ['Ashley', 'Andras', 'Rihanna', 'Emily']\n",
    "ages_list = [30, 36, 28, 33]\n",
    "birthplaces_list = ['USA', 'Hungary', 'Barbados', 'USA']\n",
    "singers_list = [False, False, True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>is_singer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ashley</td>\n",
       "      <td>30</td>\n",
       "      <td>USA</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andras</td>\n",
       "      <td>36</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rihanna</td>\n",
       "      <td>28</td>\n",
       "      <td>Barbados</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily</td>\n",
       "      <td>33</td>\n",
       "      <td>USA</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  age birthplace  is_singer\n",
       "0   Ashley   30        USA      False\n",
       "1   Andras   36    Hungary      False\n",
       "2  Rihanna   28   Barbados       True\n",
       "3    Emily   33        USA      False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df = pd.DataFrame({\n",
    "    \"name\": names_list,\n",
    "    \"age\": ages_list,\n",
    "    \"birthplace\": birthplaces_list,\n",
    "    \"is_singer\": singers_list\n",
    "})\n",
    "people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1395, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'age', 'birthplace', 'is_singer'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name          object\n",
       "age            int64\n",
       "birthplace    object\n",
       "is_singer       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "people_df.to_csv('people.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- <font color='LIGHTGRAY'> import packages </font>\n",
    "- <font color='LIGHTGRAY'> describe why the Pandas package is useful</font>\n",
    "- <font color='LIGHTGRAY'> create a dataframe and summarize it</font>\n",
    "- read a CSV file into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reading in a dataframe from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ID                            Date  \\\n",
      "0     586278450392133633  Thu Apr 09 21:24:09 +0000 2015   \n",
      "1     586260156155043843  Thu Apr 09 20:11:28 +0000 2015   \n",
      "2     586248551811932160  Thu Apr 09 19:25:21 +0000 2015   \n",
      "3     586229697165586432  Thu Apr 09 18:10:26 +0000 2015   \n",
      "4     586215972731822080  Thu Apr 09 17:15:53 +0000 2015   \n",
      "5     586202004583768064  Thu Apr 09 16:20:23 +0000 2015   \n",
      "6     586189609865994242  Thu Apr 09 15:31:08 +0000 2015   \n",
      "7     586176873153110016  Thu Apr 09 14:40:31 +0000 2015   \n",
      "8     586160615019909121  Thu Apr 09 13:35:55 +0000 2015   \n",
      "9     585988063731519488  Thu Apr 09 02:10:16 +0000 2015   \n",
      "10    585966734714241024  Thu Apr 09 00:45:30 +0000 2015   \n",
      "11    585942798525947904  Wed Apr 08 23:10:24 +0000 2015   \n",
      "12    585921278856552449  Wed Apr 08 21:44:53 +0000 2015   \n",
      "13    585897710496182272  Wed Apr 08 20:11:14 +0000 2015   \n",
      "14    585888685796827137  Wed Apr 08 19:35:22 +0000 2015   \n",
      "15    585869747540992001  Wed Apr 08 18:20:07 +0000 2015   \n",
      "16    585851005935476736  Wed Apr 08 17:05:39 +0000 2015   \n",
      "17    585830915496157184  Wed Apr 08 15:45:49 +0000 2015   \n",
      "18    585811991312396289  Wed Apr 08 14:30:37 +0000 2015   \n",
      "19    585801678462570496  Wed Apr 08 13:49:38 +0000 2015   \n",
      "20    585625669805219840  Wed Apr 08 02:10:14 +0000 2015   \n",
      "21    585600675045834752  Wed Apr 08 00:30:55 +0000 2015   \n",
      "22    585574164624494592  Tue Apr 07 22:45:34 +0000 2015   \n",
      "23    585552694858928129  Tue Apr 07 21:20:16 +0000 2015   \n",
      "24    585536473463685123  Tue Apr 07 20:15:48 +0000 2015   \n",
      "25    585518768077611009  Tue Apr 07 19:05:27 +0000 2015   \n",
      "26    585506633377255426  Tue Apr 07 18:17:14 +0000 2015   \n",
      "27    585488500704866304  Tue Apr 07 17:05:11 +0000 2015   \n",
      "28    585477338424614913  Tue Apr 07 16:20:49 +0000 2015   \n",
      "29    585477187266076672  Tue Apr 07 16:20:13 +0000 2015   \n",
      "...                  ...                             ...   \n",
      "1365  551151017673637888  Fri Jan 02 23:00:17 +0000 2015   \n",
      "1366  551135865976926208  Fri Jan 02 22:00:04 +0000 2015   \n",
      "1367  551110823289311233  Fri Jan 02 20:20:34 +0000 2015   \n",
      "1368  551108941363826688  Fri Jan 02 20:13:05 +0000 2015   \n",
      "1369  551075334700171265  Fri Jan 02 17:59:32 +0000 2015   \n",
      "1370  551056050414452736  Fri Jan 02 16:42:55 +0000 2015   \n",
      "1371  551053593068179456  Fri Jan 02 16:33:09 +0000 2015   \n",
      "1372  551022454974730240  Fri Jan 02 14:29:25 +0000 2015   \n",
      "1373  551021660565164033  Fri Jan 02 14:26:16 +0000 2015   \n",
      "1374  551021611718311936  Fri Jan 02 14:26:04 +0000 2015   \n",
      "1375  551021597424099328  Fri Jan 02 14:26:00 +0000 2015   \n",
      "1376  550778986394300416  Thu Jan 01 22:21:57 +0000 2015   \n",
      "1377  550750142144737281  Thu Jan 01 20:27:20 +0000 2015   \n",
      "1378  550722415693529088  Thu Jan 01 18:37:10 +0000 2015   \n",
      "1379  550684907186704385  Thu Jan 01 16:08:07 +0000 2015   \n",
      "1380  550662646559961088  Thu Jan 01 14:39:40 +0000 2015   \n",
      "1381  550651682091446273  Thu Jan 01 13:56:06 +0000 2015   \n",
      "1382  550434118412947456  Wed Dec 31 23:31:35 +0000 2014   \n",
      "1383  550397643831406592  Wed Dec 31 21:06:38 +0000 2014   \n",
      "1384  550304680858124288  Wed Dec 31 14:57:14 +0000 2014   \n",
      "1385  550109517577744384  Wed Dec 31 02:01:44 +0000 2014   \n",
      "1386  550082513532911618  Wed Dec 31 00:14:25 +0000 2014   \n",
      "1387  550068668223459328  Tue Dec 30 23:19:24 +0000 2014   \n",
      "1388  550042035504570368  Tue Dec 30 21:33:35 +0000 2014   \n",
      "1389  550041841396355073  Tue Dec 30 21:32:48 +0000 2014   \n",
      "1390  550009352690868224  Tue Dec 30 19:23:43 +0000 2014   \n",
      "1391  550002825393340417  Tue Dec 30 18:57:46 +0000 2014   \n",
      "1392  549982055854247936  Tue Dec 30 17:35:14 +0000 2014   \n",
      "1393  549975811408003072  Tue Dec 30 17:10:26 +0000 2014   \n",
      "1394  549951219457081344  Tue Dec 30 15:32:42 +0000 2014   \n",
      "\n",
      "                                                  Tweet  \n",
      "0     Planning to hire a personal trainer? Read thes...  \n",
      "1     RT @AnnaMedaris: Any dads out their who strugg...  \n",
      "2     America's problem with diabetes in one map: ht...  \n",
      "3     Think water &amp; fiber will cure your constip...  \n",
      "4     About to lose it? Here, try one of these offic...  \n",
      "5     Should you get your baby's DNA decoded? http:/...  \n",
      "6     3 easy ways to eliminate work #stress: http://...  \n",
      "7     7 steps for choosing &amp; keeping the right p...  \n",
      "8     Getting ready for bikini season? Don't. @krist...  \n",
      "9     There's a reason you're still fat. In fact, 6 ...  \n",
      "10    Are your #allergies just annoying -- or do the...  \n",
      "11    Don't think your digestive system can handle a...  \n",
      "12    How about some mood-boosting veggies for dinne...  \n",
      "13    Teen pregnancy rates are lower than ever, but ...  \n",
      "14    A hip replacement in your 90s? For some, it's ...  \n",
      "15    Eat spinach every day -- and make your brain 1...  \n",
      "16    Do you faint a lot? How to know if it's a sign...  \n",
      "17    Everything you thought you knew about #constip...  \n",
      "18    The problem with opioids for chronic #pain: ht...  \n",
      "19    Boost your mood with food this spring! @WholeG...  \n",
      "20    12 spring #superfoods – from leeks to beets: h...  \n",
      "21    Step into spring with one of these 10 epic tra...  \n",
      "22    Eating out tonight? You *can* do it healthfull...  \n",
      "23    This is what #sugar does to your brain: http:/...  \n",
      "24    Do you run for #beer? How to balance working o...  \n",
      "25    What's it like to #exercise with a disability?...  \n",
      "26    5 #PhysicalTherapy procedures that are more qu...  \n",
      "27    What's the difference between #allergies that ...  \n",
      "28    RT @HealthyLiving: Do you sweat too much? via ...  \n",
      "29      Do alcohol taxes save lives? http://ow.ly/Libq2  \n",
      "...                                                 ...  \n",
      "1365  The psychological problems that come with HIV ...  \n",
      "1366  For people with COPD, nutrition is everything....  \n",
      "1367  RT @usnews: Gaining weight to stay alive: 5 #e...  \n",
      "1368  Don't forget to join us, @MyLively, @PolarGlob...  \n",
      "1369  RT @angelahaupt: They had to gain weight to st...  \n",
      "1370  Want to lose weight in 2015? Don't know which ...  \n",
      "1371  Kimatni dropped 50 pounds, all for his kids. W...  \n",
      "1372  Everything we know about preventing cancer mig...  \n",
      "1373  RT @Steph_Steinberg: Made a resolution to eat ...  \n",
      "1374  RT @usnews: Healthy fido, healthy family: http...  \n",
      "1375  RT @angelahaupt: Forget that New Year's resolu...  \n",
      "1376  The cold days of winter got you down? @Tobyami...  \n",
      "1377  2015 is here – and that means it's time to say...  \n",
      "1378  Is losing weight part of your 2015 resolution?...  \n",
      "1379  Party too hard last night? Here's how to get r...  \n",
      "1380  If your resolution is to quit smoking, your sm...  \n",
      "1381  From everyone at U.S. News, we wish you a happ...  \n",
      "1382  Getting the body you want is important – and s...  \n",
      "1383  Want to lose weight in 2015? Join our Twitter ...  \n",
      "1384  People love to go nuts on New Years Eve – but ...  \n",
      "1385  Poetry is helping people fight dementia. http:...  \n",
      "1386  If your spouse is diagnosed with a critical il...  \n",
      "1387  Mental health can play a large role in weight ...  \n",
      "1388  RT @kimacastro: In just one week, @USNewsHealt...  \n",
      "1389  RT @AnnaMedaris: Strapped for a New Year's #re...  \n",
      "1390  RT @AnnaMedaris: Have you tried a #dance party...  \n",
      "1391  Going gray early? Here's how to stop it. http:...  \n",
      "1392  Sure, we all get nervous sometimes. But how to...  \n",
      "1393  RT @leonardkl: Millions have signed up for hea...  \n",
      "1394  RT @leonardkl: Are you getting #healthinsuranc...  \n",
      "\n",
      "[1395 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ID                            Date  \\\n",
      "0     586278450392133633  Thu Apr 09 21:24:09 +0000 2015   \n",
      "1     586260156155043843  Thu Apr 09 20:11:28 +0000 2015   \n",
      "2     586248551811932160  Thu Apr 09 19:25:21 +0000 2015   \n",
      "3     586229697165586432  Thu Apr 09 18:10:26 +0000 2015   \n",
      "4     586215972731822080  Thu Apr 09 17:15:53 +0000 2015   \n",
      "5     586202004583768064  Thu Apr 09 16:20:23 +0000 2015   \n",
      "6     586189609865994242  Thu Apr 09 15:31:08 +0000 2015   \n",
      "7     586176873153110016  Thu Apr 09 14:40:31 +0000 2015   \n",
      "8     586160615019909121  Thu Apr 09 13:35:55 +0000 2015   \n",
      "9     585988063731519488  Thu Apr 09 02:10:16 +0000 2015   \n",
      "10    585966734714241024  Thu Apr 09 00:45:30 +0000 2015   \n",
      "11    585942798525947904  Wed Apr 08 23:10:24 +0000 2015   \n",
      "12    585921278856552449  Wed Apr 08 21:44:53 +0000 2015   \n",
      "13    585897710496182272  Wed Apr 08 20:11:14 +0000 2015   \n",
      "14    585888685796827137  Wed Apr 08 19:35:22 +0000 2015   \n",
      "15    585869747540992001  Wed Apr 08 18:20:07 +0000 2015   \n",
      "16    585851005935476736  Wed Apr 08 17:05:39 +0000 2015   \n",
      "17    585830915496157184  Wed Apr 08 15:45:49 +0000 2015   \n",
      "18    585811991312396289  Wed Apr 08 14:30:37 +0000 2015   \n",
      "19    585801678462570496  Wed Apr 08 13:49:38 +0000 2015   \n",
      "20    585625669805219840  Wed Apr 08 02:10:14 +0000 2015   \n",
      "21    585600675045834752  Wed Apr 08 00:30:55 +0000 2015   \n",
      "22    585574164624494592  Tue Apr 07 22:45:34 +0000 2015   \n",
      "23    585552694858928129  Tue Apr 07 21:20:16 +0000 2015   \n",
      "24    585536473463685123  Tue Apr 07 20:15:48 +0000 2015   \n",
      "25    585518768077611009  Tue Apr 07 19:05:27 +0000 2015   \n",
      "26    585506633377255426  Tue Apr 07 18:17:14 +0000 2015   \n",
      "27    585488500704866304  Tue Apr 07 17:05:11 +0000 2015   \n",
      "28    585477338424614913  Tue Apr 07 16:20:49 +0000 2015   \n",
      "29    585477187266076672  Tue Apr 07 16:20:13 +0000 2015   \n",
      "...                  ...                             ...   \n",
      "1365  551151017673637888  Fri Jan 02 23:00:17 +0000 2015   \n",
      "1366  551135865976926208  Fri Jan 02 22:00:04 +0000 2015   \n",
      "1367  551110823289311233  Fri Jan 02 20:20:34 +0000 2015   \n",
      "1368  551108941363826688  Fri Jan 02 20:13:05 +0000 2015   \n",
      "1369  551075334700171265  Fri Jan 02 17:59:32 +0000 2015   \n",
      "1370  551056050414452736  Fri Jan 02 16:42:55 +0000 2015   \n",
      "1371  551053593068179456  Fri Jan 02 16:33:09 +0000 2015   \n",
      "1372  551022454974730240  Fri Jan 02 14:29:25 +0000 2015   \n",
      "1373  551021660565164033  Fri Jan 02 14:26:16 +0000 2015   \n",
      "1374  551021611718311936  Fri Jan 02 14:26:04 +0000 2015   \n",
      "1375  551021597424099328  Fri Jan 02 14:26:00 +0000 2015   \n",
      "1376  550778986394300416  Thu Jan 01 22:21:57 +0000 2015   \n",
      "1377  550750142144737281  Thu Jan 01 20:27:20 +0000 2015   \n",
      "1378  550722415693529088  Thu Jan 01 18:37:10 +0000 2015   \n",
      "1379  550684907186704385  Thu Jan 01 16:08:07 +0000 2015   \n",
      "1380  550662646559961088  Thu Jan 01 14:39:40 +0000 2015   \n",
      "1381  550651682091446273  Thu Jan 01 13:56:06 +0000 2015   \n",
      "1382  550434118412947456  Wed Dec 31 23:31:35 +0000 2014   \n",
      "1383  550397643831406592  Wed Dec 31 21:06:38 +0000 2014   \n",
      "1384  550304680858124288  Wed Dec 31 14:57:14 +0000 2014   \n",
      "1385  550109517577744384  Wed Dec 31 02:01:44 +0000 2014   \n",
      "1386  550082513532911618  Wed Dec 31 00:14:25 +0000 2014   \n",
      "1387  550068668223459328  Tue Dec 30 23:19:24 +0000 2014   \n",
      "1388  550042035504570368  Tue Dec 30 21:33:35 +0000 2014   \n",
      "1389  550041841396355073  Tue Dec 30 21:32:48 +0000 2014   \n",
      "1390  550009352690868224  Tue Dec 30 19:23:43 +0000 2014   \n",
      "1391  550002825393340417  Tue Dec 30 18:57:46 +0000 2014   \n",
      "1392  549982055854247936  Tue Dec 30 17:35:14 +0000 2014   \n",
      "1393  549975811408003072  Tue Dec 30 17:10:26 +0000 2014   \n",
      "1394  549951219457081344  Tue Dec 30 15:32:42 +0000 2014   \n",
      "\n",
      "                                                  Tweet  \n",
      "0     Planning to hire a personal trainer? Read thes...  \n",
      "1     RT @AnnaMedaris: Any dads out their who strugg...  \n",
      "2     America's problem with diabetes in one map: ht...  \n",
      "3     Think water &amp; fiber will cure your constip...  \n",
      "4     About to lose it? Here, try one of these offic...  \n",
      "5     Should you get your baby's DNA decoded? http:/...  \n",
      "6     3 easy ways to eliminate work #stress: http://...  \n",
      "7     7 steps for choosing &amp; keeping the right p...  \n",
      "8     Getting ready for bikini season? Don't. @krist...  \n",
      "9     There's a reason you're still fat. In fact, 6 ...  \n",
      "10    Are your #allergies just annoying -- or do the...  \n",
      "11    Don't think your digestive system can handle a...  \n",
      "12    How about some mood-boosting veggies for dinne...  \n",
      "13    Teen pregnancy rates are lower than ever, but ...  \n",
      "14    A hip replacement in your 90s? For some, it's ...  \n",
      "15    Eat spinach every day -- and make your brain 1...  \n",
      "16    Do you faint a lot? How to know if it's a sign...  \n",
      "17    Everything you thought you knew about #constip...  \n",
      "18    The problem with opioids for chronic #pain: ht...  \n",
      "19    Boost your mood with food this spring! @WholeG...  \n",
      "20    12 spring #superfoods – from leeks to beets: h...  \n",
      "21    Step into spring with one of these 10 epic tra...  \n",
      "22    Eating out tonight? You *can* do it healthfull...  \n",
      "23    This is what #sugar does to your brain: http:/...  \n",
      "24    Do you run for #beer? How to balance working o...  \n",
      "25    What's it like to #exercise with a disability?...  \n",
      "26    5 #PhysicalTherapy procedures that are more qu...  \n",
      "27    What's the difference between #allergies that ...  \n",
      "28    RT @HealthyLiving: Do you sweat too much? via ...  \n",
      "29      Do alcohol taxes save lives? http://ow.ly/Libq2  \n",
      "...                                                 ...  \n",
      "1365  The psychological problems that come with HIV ...  \n",
      "1366  For people with COPD, nutrition is everything....  \n",
      "1367  RT @usnews: Gaining weight to stay alive: 5 #e...  \n",
      "1368  Don't forget to join us, @MyLively, @PolarGlob...  \n",
      "1369  RT @angelahaupt: They had to gain weight to st...  \n",
      "1370  Want to lose weight in 2015? Don't know which ...  \n",
      "1371  Kimatni dropped 50 pounds, all for his kids. W...  \n",
      "1372  Everything we know about preventing cancer mig...  \n",
      "1373  RT @Steph_Steinberg: Made a resolution to eat ...  \n",
      "1374  RT @usnews: Healthy fido, healthy family: http...  \n",
      "1375  RT @angelahaupt: Forget that New Year's resolu...  \n",
      "1376  The cold days of winter got you down? @Tobyami...  \n",
      "1377  2015 is here – and that means it's time to say...  \n",
      "1378  Is losing weight part of your 2015 resolution?...  \n",
      "1379  Party too hard last night? Here's how to get r...  \n",
      "1380  If your resolution is to quit smoking, your sm...  \n",
      "1381  From everyone at U.S. News, we wish you a happ...  \n",
      "1382  Getting the body you want is important – and s...  \n",
      "1383  Want to lose weight in 2015? Join our Twitter ...  \n",
      "1384  People love to go nuts on New Years Eve – but ...  \n",
      "1385  Poetry is helping people fight dementia. http:...  \n",
      "1386  If your spouse is diagnosed with a critical il...  \n",
      "1387  Mental health can play a large role in weight ...  \n",
      "1388  RT @kimacastro: In just one week, @USNewsHealt...  \n",
      "1389  RT @AnnaMedaris: Strapped for a New Year's #re...  \n",
      "1390  RT @AnnaMedaris: Have you tried a #dance party...  \n",
      "1391  Going gray early? Here's how to stop it. http:...  \n",
      "1392  Sure, we all get nervous sometimes. But how to...  \n",
      "1393  RT @leonardkl: Millions have signed up for hea...  \n",
      "1394  RT @leonardkl: Are you getting #healthinsuranc...  \n",
      "\n",
      "[1395 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('usnewshealth.txt', sep='|', header=None, names=['ID', 'Date', 'Tweet'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ID                            Date  \\\n",
      "0  586278450392133633  Thu Apr 09 21:24:09 +0000 2015   \n",
      "1  586260156155043843  Thu Apr 09 20:11:28 +0000 2015   \n",
      "2  586248551811932160  Thu Apr 09 19:25:21 +0000 2015   \n",
      "3  586229697165586432  Thu Apr 09 18:10:26 +0000 2015   \n",
      "4  586215972731822080  Thu Apr 09 17:15:53 +0000 2015   \n",
      "5  586202004583768064  Thu Apr 09 16:20:23 +0000 2015   \n",
      "6  586189609865994242  Thu Apr 09 15:31:08 +0000 2015   \n",
      "7  586176873153110016  Thu Apr 09 14:40:31 +0000 2015   \n",
      "8  586160615019909121  Thu Apr 09 13:35:55 +0000 2015   \n",
      "9  585988063731519488  Thu Apr 09 02:10:16 +0000 2015   \n",
      "\n",
      "                                               Tweet  \n",
      "0  Planning to hire a personal trainer? Read thes...  \n",
      "1  RT @AnnaMedaris: Any dads out their who strugg...  \n",
      "2  America's problem with diabetes in one map: ht...  \n",
      "3  Think water &amp; fiber will cure your constip...  \n",
      "4  About to lose it? Here, try one of these offic...  \n",
      "5  Should you get your baby's DNA decoded? http:/...  \n",
      "6  3 easy ways to eliminate work #stress: http://...  \n",
      "7  7 steps for choosing &amp; keeping the right p...  \n",
      "8  Getting ready for bikini season? Don't. @krist...  \n",
      "9  There's a reason you're still fat. In fact, 6 ...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
