{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TNBoW Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Â© 2020 Nokia\n",
    "\n",
    "Licensed under the BSD 3 Clause license\n",
    "\n",
    "SPDX-License-Identifier: BSD-3-Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"snippets_collection\"] = \"so-ds-feb20\"\n",
    "os.environ[\"valid_dataset\"] = \"so-ds-feb20-valid\"\n",
    "os.environ[\"test_dataset\"] = \"so-ds-feb20-test\"\n",
    "os.environ[\"text_input_raw\"] = \"so-python-question-titles-feb20\"\n",
    "\n",
    "output_dir = Path(\"test\")\n",
    "os.environ[\"output_dir\"] = str(output_dir)\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## python SO titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### preprocessing hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  119172 lr:  0.000000 avg.loss:  2.039815 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 300954 bytes to test/python_so_preprocess0.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  27293\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  102490 lr:  0.000000 avg.loss:  2.026490 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 301007 bytes to test/python_so_preprocess1.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 13M words\n",
      "Number of words:  24787\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  123289 lr:  0.000000 avg.loss:  2.056436 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 301122 bytes to test/python_so_preprocess2.html\n"
     ]
    }
   ],
   "source": [
    "text_overrides = [{}, {\"lemmatize\": False}, {\"remove_stop\": False}] \n",
    "text_inputs = [\"SO-python-question-titles-feb20-lemma.tok.txt\", \"SO-python-question-titles-feb20.tok.txt\", \"SO-python-question-titles-feb20-stopwords.tok.txt\"]\n",
    "\n",
    "for i, (text_overrides_, text_input) in enumerate(zip(text_overrides, text_inputs)):\n",
    "    os.environ[\"text_overrides\"] = json.dumps(text_overrides_)\n",
    "    os.environ[\"text_input\"] = text_input\n",
    "    output_base = str(output_dir/f\"python_so_preprocess{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "### fast text hyper-params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### num epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  118018 lr:  0.000000 avg.loss:  1.987444 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 300900 bytes to test/python_so_fasttext_epochs0.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  120927 lr:  0.000000 avg.loss:  1.942516 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 301012 bytes to test/python_so_fasttext_epochs1.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  121045 lr:  0.000000 avg.loss:  1.934516 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 301006 bytes to test/python_so_fasttext_epochs2.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  122140 lr:  0.000000 avg.loss:  1.908306 ETA:   0h 0m 0s54s 121938 lr:  0.018339 avg.loss:  1.951851 ETA:   0h 0m30s 122195 lr:  0.003027 avg.loss:  1.913483 ETA:   0h 0m 4s\n",
      "[NbConvertApp] Writing 300946 bytes to test/python_so_fasttext_epochs3.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  119383 lr:  0.000000 avg.loss:  1.906274 ETA:   0h 0m 0s lr:  0.043385 avg.loss:  2.063299 ETA:   0h 1m31s\n",
      "[NbConvertApp] Writing 300779 bytes to test/python_so_fasttext_epochs4.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  115592 lr:  0.000000 avg.loss:  1.809837 ETA:   0h 0m 0s  1.4% words/sec/thread:  107611 lr:  0.049290 avg.loss:  2.209091 ETA:   0h 3m50s lr:  0.035038 avg.loss:  1.974594 ETA:   0h 2m40s\n",
      "[NbConvertApp] Writing 300924 bytes to test/python_so_fasttext_epochs5.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  118878 lr:  0.000000 avg.loss:  1.240100 ETA:   0h 0m 0s 34.7% words/sec/thread:  117020 lr:  0.032631 avg.loss:  1.905229 ETA:   0h 4m40sm15s\n",
      "[NbConvertApp] Writing 300861 bytes to test/python_so_fasttext_epochs6.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  119195 lr:  0.000000 avg.loss:  0.942936 ETA:   0h 0m 0s 36.7% words/sec/thread:  120112 lr:  0.031644 avg.loss:  1.848149 ETA:   0h 6m36s lr:  0.029219 avg.loss:  1.816703 ETA:   0h 6m 6s 41.9% words/sec/thread:  120094 lr:  0.029033 avg.loss:  1.804039 ETA:   0h 6m 4s 61.5% words/sec/thread:  120488 lr:  0.019252 avg.loss:  1.338571 ETA:   0h 4m 0s 0.016606 avg.loss:  1.261814 ETA:   0h 3m27s avg.loss:  1.122952 ETA:   0h 2m15s lr:  0.005338 avg.loss:  1.025039 ETA:   0h 1m 7s 119091 lr:  0.003160 avg.loss:  0.989878 ETA:   0h 0m39s\n",
      "[NbConvertApp] Writing 301017 bytes to test/python_so_fasttext_epochs7.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  120038 lr:  0.000000 avg.loss:  0.694238 ETA:   0h 0m 0s 1.988358 ETA:   0h17m 4s 120011 lr:  0.043926 avg.loss:  1.940552 ETA:   0h15m19s 17.3% words/sec/thread:  119887 lr:  0.041366 avg.loss:  1.858161 ETA:   0h14m26s 17.6% words/sec/thread:  119898 lr:  0.041179 avg.loss:  1.854445 ETA:   0h14m22s 27.1% words/sec/thread:  120348 lr:  0.036450 avg.loss:  1.698668 ETA:   0h12m40s 28.4% words/sec/thread:  120346 lr:  0.035778 avg.loss:  1.635958 ETA:   0h12m26s 0.017647 avg.loss:  0.931289 ETA:   0h 6m10s words/sec/thread:  119751 lr:  0.015440 avg.loss:  0.891476 ETA:   0h 5m23s\n",
      "[NbConvertApp] Writing 300930 bytes to test/python_so_fasttext_epochs8.html\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"text_overrides\"] = \"{}\"\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20-lemma.tok.txt\"\n",
    "\n",
    "fast_text_overrides = [{\"epoch\": 10}, {\"epoch\": 20}, {\"epoch\": 30}, {\"epoch\": 40}, {\"epoch\": 50}, {\"epoch\": 100}, {\"epoch\": 200}, {\"epoch\": 300}, {\"epoch\": 500}]\n",
    "for i, (fast_text_overrides_) in enumerate(fast_text_overrides):\n",
    "    os.environ[\"fast_text_overrides\"] = json.dumps(fast_text_overrides_)\n",
    "    output_base = str(output_dir/f\"python_so_fasttext_epochs{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  120512 lr:  0.000000 avg.loss:  1.240487 ETA:   0h 0m 0s\n",
      "[NbConvertApp] Writing 300966 bytes to test/python_so_fasttext_lr0.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress:   1.0% words/sec/thread:  117991 lr:  0.197971 avg.loss: 23.545143 ETA:   0h 7m 1s[NbConvertApp] ERROR | Error while converting 'tnbow.ipynb'\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/nbconvertapp.py\", line 410, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/exporters/exporter.py\", line 179, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/exporters/exporter.py\", line 197, in from_file\n",
      "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/exporters/html.py\", line 95, in from_notebook_node\n",
      "    return super(HTMLExporter, self).from_notebook_node(nb, resources, **kw)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/exporters/templateexporter.py\", line 307, in from_notebook_node\n",
      "    nb_copy, resources = super(TemplateExporter, self).from_notebook_node(nb, resources, **kw)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/exporters/exporter.py\", line 139, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/exporters/exporter.py\", line 316, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/preprocessors/base.py\", line 47, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py\", line 405, in preprocess\n",
      "    nb, resources = super(ExecutePreprocessor, self).preprocess(nb, resources)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/preprocessors/base.py\", line 69, in preprocess\n",
      "    nb.cells[index], resources = self.preprocess_cell(cell, resources, index)\n",
      "  File \"/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py\", line 448, in preprocess_cell\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, out)\n",
      "nbconvert.preprocessors.execute.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "from functools import partial\n",
      "\n",
      "from codesearch.encoders import BasicEncoder\n",
      "from codesearch.text_preprocessing import preprocess_text\n",
      "from codesearch.utils import SaveableFunction\n",
      "from codesearch import embedding_pretraining\n",
      "from codesearch.embedding_pretraining import create_input_file_from_text, train_fasttext_model_from_text, load_fasttext_model\n",
      "\n",
      "def dummy(c): return c\n",
      "if fast_text_checkpoint:\n",
      "    model, enc = load_fasttext_model(fast_text_checkpoint)\n",
      "    print(\"Loaded fasttext checkpoint\")\n",
      "    \n",
      "else:\n",
      "    preprocess_text_ = partial(preprocess_text, **text_overrides)\n",
      "    enc = SaveableFunction(preprocess_text_)\n",
      "    \n",
      "    if not text_input.exists():\n",
      "        # Preprocess raw text input\n",
      "        create_input_file_from_text(text_input, text_input_raw, enc)\n",
      "    \n",
      "    model = train_fasttext_model_from_text(text_input, enc, fast_text_overrides, \"./\", save=False)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-7-0660c5921f15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m     20\u001b[0m         \u001b[0mcreate_input_file_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_input_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fasttext_model_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_text_overrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m/nfs/datasets/geert/codebook/codesearch/codesearch/embedding_pretraining.py\u001b[0m in \u001b[0;36mtrain_fasttext_model_from_text\u001b[0;34m(input_file, encoder, hyperparams, model_dir, save)\u001b[0m\n",
      "\u001b[1;32m     31\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training skipgram fasttext model with the following hyper-param overrides {hyperparams}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_unsupervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skipgram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     35\u001b[0m         \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m/nfs/datasets/geert/anaconda3/envs/codebook/lib/python3.7/site-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mtrain_unsupervised\u001b[0;34m(*kargs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    489\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    490\u001b[0m     \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    493\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Encountered NaN.\n",
      "RuntimeError: Encountered NaN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"text_overrides\"] = \"{}\"\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20-lemma.tok.txt\"\n",
    "\n",
    "fast_text_overrides = [ {\"lr\": 0.05, \"epoch\": 200}, {\"lr\": 0.2, \"epoch\": 200}]\n",
    "for i, (fast_text_overrides_) in enumerate(fast_text_overrides):\n",
    "    os.environ[\"fast_text_overrides\"] = json.dumps(fast_text_overrides_)\n",
    "    output_base = str(output_dir/f\"python_so_fasttext_lr{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  121934 lr:  0.000000 avg.loss:  1.253203 ETA:   0h 0m 0s words/sec/thread:  122555 lr:  0.043873 avg.loss:  1.989214 ETA:   0h 5m59s 1.848451 ETA:   0h 3m22s avg.loss:  1.484386 ETA:   0h 1m18s\n",
      "[NbConvertApp] Writing 300840 bytes to test/python_so_fasttext_epochs.20.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  121546 lr:  0.000000 avg.loss:  1.245780 ETA:   0h 0m 0s  0h 6m 6s  0h 3m35s 121600 lr:  0.016975 avg.loss:  1.746396 ETA:   0h 2m20s\n",
      "[NbConvertApp] Writing 300868 bytes to test/python_so_fasttext_epochs.21.html\n",
      "[NbConvertApp] Converting notebook tnbow.ipynb to html\n",
      "[NbConvertApp] Executing notebook with kernel: build_central\n",
      "Read 11M words\n",
      "Number of words:  24739\n",
      "Number of labels: 0\n",
      "Progress:  23.1% words/sec/thread:  121099 lr:  0.038465 avg.loss:  1.954048 ETA:   0h 5m19s  0h 5m43s"
     ]
    }
   ],
   "source": [
    "os.environ[\"text_overrides\"] = \"{}\"\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20-lemma.tok.txt\"\n",
    "\n",
    "fast_text_overrides = [ {\"epoch\": 200}] * 5 + [{\"epoch\": 50}] * 5 + [{\"epoch\": 100}] * 5 + [{\"epoch\": 150}] * 5\n",
    "for i, (fast_text_overrides_) in enumerate(fast_text_overrides):\n",
    "    os.environ[\"fast_text_overrides\"] = json.dumps(fast_text_overrides_)\n",
    "    output_base = str(output_dir/f\"python_so_fasttext_epochs.2{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### minCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ[\"text_overrides\"] = \"{}\"\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20-lemma.tok.txt\"\n",
    "\n",
    "fast_text_overrides = [{\"epoch\": 200, \"minCount\": 1},{\"epoch\": 200, \"minCount\": 3}, {\"epoch\": 200, \"minCount\": 5}, {\"epoch\": 200, \"minCount\": 10} ]\n",
    "\n",
    "for i, (fast_text_overrides_) in enumerate(fast_text_overrides):\n",
    "    os.environ[\"fast_text_overrides\"] = json.dumps(fast_text_overrides_)\n",
    "    output_base = str(output_dir/f\"python_so_fasttext_minCount{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ[\"text_overrides\"] = \"{}\"\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20-lemma.tok.txt\"\n",
    "\n",
    "fast_text_overrides = [{\"epoch\": 150, \"ws\": 10}] * 5 + [{\"epoch\": 150, \"ws\": 20}] * 5 + [{\"epoch\": 200, \"ws\": 10}] * 5\n",
    "\n",
    "for i, (fast_text_overrides_) in enumerate(fast_text_overrides):\n",
    "    os.environ[\"fast_text_overrides\"] = json.dumps(fast_text_overrides_)\n",
    "    output_base = str(output_dir/f\"python_so_fasttext_windowsize{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### recheck non-lemmatized with optimal fasttext hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ[\"text_overrides\"] = json.dumps({'lemmatize': False})\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20.tok.txt\"\n",
    "fast_text_overrides = [{\"epoch\": 300, \"ws\": 10}] \n",
    "\n",
    "for i, (fast_text_overrides_) in enumerate(fast_text_overrides):\n",
    "    os.environ[\"fast_text_overrides\"] = json.dumps(fast_text_overrides_)\n",
    "    output_base = str(output_dir/f\"python_so_fasttext_nolemma{i}\")\n",
    "    !python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Save optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ[\"text_overrides\"] = '{}'\n",
    "os.environ[\"text_input\"] = \"SO-python-question-titles-feb20-lemma.tok.txt\"\n",
    "os.environ[\"fast_text_overrides\"] = json.dumps({\"epoch\": 300})\n",
    "os.environ[\"model_filename\"] = str(output_dir/\"best_tnbow_embedder\")\n",
    "\n",
    "output_base = str(output_dir/f\"best_tnbow\")\n",
    "!python -m nbconvert tnbow.ipynb --execute --NbConvertApp.output_base=$output_base --ExecutePreprocessor.timeout=3600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
