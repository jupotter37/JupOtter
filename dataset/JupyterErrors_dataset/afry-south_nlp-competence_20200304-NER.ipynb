{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitbddbbfcb535140b084434bb7bc7ed6c4",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER\n",
    "What is NER? \n",
    "\n",
    ">NER is a subtask of information extraction that seeks to locate and classify named entity mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. \n",
    "\n",
    "Wikipedia.org\n",
    "\n",
    "One example of such a task could be the following sentence:\n",
    ">    Jim bought 300 shares of Acme Corp. in 2006.\n",
    "\n",
    "Which would be tagged\n",
    "\n",
    ">    Jim_person bought 300_unit shares of [[Acme Corp.]]_org in 2006_year.\n",
    "\n",
    "System easily reach near human F-scoring today which is really awesome.  \n",
    "Remember: SparkNLP. In SparkNLP there was a built in NER-tagger which works really well on English. But it might be a bit \"blocking\" as it might not have the entity-types that you're searching (e.g. a Hospital might want to find all medicines as different Entities to improve data retrieved by reports by doctors).\n",
    "\n",
    "## Approaches\n",
    "1. Build our own NER from ground up\n",
    "2. Use/Train spaCy which includes a quick statistical NER tagger. It's possible to add a EntityMatcher on top in order to have more power off the decisions.\n",
    "3. Use/Train a Neural Network (in our case I think we'll choose a Transformer, BERT namely which is SOTA, and make use of \"Transfer Learning\")\n",
    "\n",
    "Extra:  \n",
    "On the JVM actually a few \"out-of-the-box\" approaches exists, StanfordNLP, CoreNLP and SparkNLP. One could also pick up Deeplearning4j and build a Neural Network for it, but as Python is de-facto it's easier to keep up-to-date with the latest SOTA.\n",
    "\n",
    "## Examples of use-cases\n",
    "* Summarize documents  \n",
    "    * Summarizing documents could be assisted by understanding what entities exist in the document.\n",
    "* Optimizing search engine\n",
    "    * Do a one-time parse of each article and create keywords of the entities found.\n",
    "* Power Recommendation Systems\n",
    "    * Same as above really\n",
    "* Simplifying Customer Support\n",
    "    * By extracting entities we could improve the result of classifying where an component should go and further it could extract information into slots.\n",
    "\n",
    "\n",
    "\n",
    "## Datasets\n",
    "We have a few different datasets to work with.\n",
    "### Swedish\n",
    "- [Manually Annotated](https://github.com/klintan/swedish-ner-corpus/)\n",
    "- [Stockholm Internet Corpus (SIC)](https://www.ling.su.se/english/nlp/corpora-and-resources/sic)\n",
    "- [SUC 3.0](https://spraakbanken.gu.se/en/resources/suc3)\n",
    "\n",
    "### English\n",
    "- [Emergin](https://github.com/leondz/emerging_entities_17)\n",
    "- [A lot of different](https://github.com/juand-r/entity-recognition-datasets)\n",
    "- [Kaggle](https://www.kaggle.com/akshay235/bert-implementation-on-ner-corpus). Dataset [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "\n",
    "### Self-built\n",
    "We can build our own Dataset either by manual tagging or by making use of wikidata.org which could be helpful in finding certain types of entities.\n",
    "\n",
    "\n",
    "NOTES:\n",
    "\n",
    "https://spacy.io/usage/v2-1\n",
    "https://spacy.io/universe/project/neuralcoref\n",
    "https://spacy.io/universe/project/NeuroNER\n",
    "https://spacy.io/universe/project/spacy-transformers\n",
    "https://rasa.com/\n",
    "\n",
    "\n",
    "https://github.com/doccano/doccano\n",
    "https://www.wikidata.org/wiki/Q53747 \n",
    "https://spacy.io/universe/project/sense2vec\n",
    "\n",
    "https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Collecting en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n\u001b[K    100% |████████████████████████████████| 12.0MB 18.0MB/s \n\u001b[?25hCollecting spacy>=2.2.2 (from en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n\u001b[K    100% |████████████████████████████████| 10.4MB 7.2MB/s \n\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/e7/b5/3e1714ebda8fd7c5859f9b216e381adc0a38b962f071568fd00d67e1b1ca/cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\nCollecting srsly<1.1.0,>=0.1.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/0e/9a/70bd934dd4d25545c9aa6c8cd4edbac2a33ba9c915439a9209b69f0ec0ad/srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185kB)\n\u001b[K    100% |████████████████████████████████| 194kB 8.4MB/s \n\u001b[?25hCollecting setuptools (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/3d/72/1c1498c1e908e0562b1e1cd30012580baa7d33b5b0ffdbeb5fde2462cc71/setuptools-45.2.0-py3-none-any.whl (584kB)\n\u001b[K    100% |████████████████████████████████| 593kB 8.9MB/s \n\u001b[?25hCollecting blis<0.5.0,>=0.4.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n\u001b[K    100% |████████████████████████████████| 3.7MB 10.1MB/s \n\u001b[?25hCollecting plac<1.2.0,>=0.9.6 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\nCollecting requests<3.0.0,>=2.13.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n\u001b[K    100% |████████████████████████████████| 61kB 68.6MB/s \n\u001b[?25hCollecting thinc<7.4.0,>=7.3.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n\u001b[K    100% |████████████████████████████████| 2.2MB 10.1MB/s \n\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\nCollecting numpy>=1.15.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n\u001b[K    100% |████████████████████████████████| 20.2MB 16.0MB/s \n\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/21/e1/e4e7b754e6be3a79c400eb766fb34924a6d278c43bb828f94233e0124a21/wasabi-0.6.0-py3-none-any.whl\nCollecting preshed<3.1.0,>=3.0.2 (from spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n\u001b[K    100% |████████████████████████████████| 122kB 28.0MB/s \n\u001b[?25hCollecting idna<3,>=2.5 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl (58kB)\n\u001b[K    100% |████████████████████████████████| 61kB 33.1MB/s \n\u001b[?25hCollecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n\u001b[K    100% |████████████████████████████████| 163kB 28.0MB/s \n\u001b[?25hCollecting chardet<4,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n\u001b[K    100% |████████████████████████████████| 143kB 28.3MB/s \n\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n\u001b[K    100% |████████████████████████████████| 133kB 40.1MB/s \n\u001b[?25hCollecting tqdm<5.0.0,>=4.10.0 (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/47/55/fd9170ba08a1a64a18a7f8a18f088037316f2a41be04d2fe6ece5a653e8f/tqdm-4.43.0-py2.py3-none-any.whl (59kB)\n\u001b[K    100% |████████████████████████████████| 61kB 77.6MB/s \n\u001b[?25hCollecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/8b/03/a00d504808808912751e64ccf414be53c29cad620e3de2421135fcae3025/importlib_metadata-1.5.0-py2.py3-none-any.whl\nCollecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5)\n  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\nInstalling collected packages: cymem, murmurhash, srsly, setuptools, numpy, blis, plac, idna, certifi, chardet, urllib3, requests, wasabi, preshed, tqdm, thinc, zipp, importlib-metadata, catalogue, spacy, en-core-web-sm\n  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed blis-0.4.1 catalogue-1.0.0 certifi-2019.11.28 chardet-3.0.4 cymem-2.0.3 en-core-web-sm-2.2.5 idna-2.9 importlib-metadata-1.5.0 murmurhash-1.0.2 numpy-1.18.1 plac-1.1.3 preshed-3.0.2 requests-2.23.0 setuptools-45.2.0 spacy-2.2.3 srsly-1.0.2 thinc-7.3.1 tqdm-4.43.0 urllib3-1.25.8 wasabi-0.6.0 zipp-3.1.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\nSegmentation fault (core dumped)\n"
    }
   ],
   "source": [
    "%%capture\n",
    "import spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-992e7db01633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Apple is looking at buying U.K. startup for $1 billion\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm') \n",
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "  \n",
    "doc = nlp(sentence) \n",
    "  \n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}