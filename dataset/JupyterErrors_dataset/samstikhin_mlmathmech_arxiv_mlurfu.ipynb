{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особая тонкость: последний случай нельзя проверить регрессией из sklearn, потому что мы минимизируем норму **всех** весов в том числе того, что без признака, а встроенная регерессия пользуется регуляризацией, где не учитывается вес без признака. Для проверки работы можете использовать `LinearRegression(fit_intercept=False)` и сравнить ее со своей, предварительно убрав из нее вес без признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Свертка тензора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам на вход подается целочисленный тензор 3-го измерения. Сначала возьмите перемножение из всех чисел по $0$ и $2$ координатам. А потом среди оставшихся чисел возьмите минимум.\n",
    "### Sample\n",
    "#### Input:\n",
    "```python\n",
    "X = np.array([[[ 1, 2, 3],\n",
    "               [ 4, 5, 6],\n",
    "               [ 7, 8, 9]],\n",
    "              \n",
    "              [[ 1, 2, 3],\n",
    "               [ 4, 5, 6],\n",
    "               [ 7, 8, 9]]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "36\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precondition\n",
    "assert_no_tokens(['print', 'while', 'map', 'for', 'open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def max_result(X: np.ndarray) -> np.float:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_result(X: np.ndarray) -> float:\n",
    "    return X.prod(axis=(0,2)).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "assert_equal(\n",
    "    max_result(np.array([[[ 1, 2, 3],\n",
    "                          [ 4, 5, 6],\n",
    "                          [ 7, 8, 9]],\n",
    "                         [[ 1, 2, 3],\n",
    "                          [ 4, 5, 6],\n",
    "                          [ 7, 8, 9]]])),\n",
    "    36\n",
    ")\n",
    "######################################################\n",
    "assert_equal(\n",
    "    max_result(np.array([[[ 0, 0],\n",
    "                          [ 0, 0],\n",
    "                          [ 0, 0]],\n",
    "                         [[ 0, 0],\n",
    "                          [ 0, 0],\n",
    "                          [ 0, 0]]])),\n",
    "    0\n",
    ")\n",
    "######################################################\n",
    "assert_equal(\n",
    "    max_result(np.array([[[ 1,  1],\n",
    "                          [ 1,  1]],\n",
    "                         [[ 1,  1],\n",
    "                          [ 1,  1]]])),\n",
    "    1\n",
    ")\n",
    "######################################################\n",
    "assert_almost_equal(\n",
    "    max_result(np.array([[[  5.0,  -3.2],\n",
    "                          [42.27, -27.4]],\n",
    "                         [[  4.2,   5.2],\n",
    "                          [  -42,  2.42]]])),\n",
    "    -349.440000,\n",
    "    decimal=6\n",
    ")\n",
    "######################################################\n",
    "assert_equal(\n",
    "    max_result(np.array(\n",
    "     [[[ 3,  3,  0,  2,  6],\n",
    "       [24, 13,  0, 13,  1],\n",
    "       [19,  2,  0, 18,  1],\n",
    "       [18,  8,  0, 22, 12],\n",
    "       [25, 25,  0, 25, 19]],\n",
    "      \n",
    "      [[ 3,  3,  0,  2,  6],\n",
    "       [24, 13,  0, 13,  1],\n",
    "       [19,  2,  0, 18,  1],\n",
    "       [18,  8,  0, 22, 12],\n",
    "       [25, 25,  0, 25, 19]]\n",
    "     ])),\n",
    "    0\n",
    ")\n",
    "######################################################\n",
    "assert_equal(\n",
    "    max_result(np.array(\n",
    "     [[[ 3,  3,  5,  2,  6],\n",
    "       [24, 13,  7, 13,  1],\n",
    "       [19,  2,  4, 18,  1],\n",
    "       [18,  8,  6, 22, 12],\n",
    "       [25, 25,  7, 25, 19]],\n",
    "      \n",
    "      [[ 3,  3,  12,  2,  6],\n",
    "       [24, 13,  12, 13,  1],\n",
    "       [19,  2,  54, 18,  1],\n",
    "       [18,  8,  78, 22, 12],\n",
    "       [25, 25,  42, 25, 19]]\n",
    "     ])),\n",
    "    699840\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Мой GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам необходимо реализовать собственный простой гридсерч - перебор параметров с поиском наилучшего для данного эстиматора.\n",
    "\n",
    "Она принимает на вход: \n",
    "* `estimator`, для которого ищем наилучшие параметры\n",
    "* `param_grid` - словарь параметров и их возможных значений, по которым будем перебирать модели\n",
    "* `cv` ($k$) - объект разбиения `KFold` или количество фолдов, на которые мы бьем данные (по умолчанию 3)\n",
    "* `scoring` - функция метрики, по которой оцениваем полученные результаты (замечание: в реальной `cross_val_score` здесь будет стоять объект `scorer`, у нас же будет стоять функция, например `metrics.accuracy_score`)\n",
    "\n",
    "По сути, вам достаточно реализовать только функцию `fit`, но никто не запрещает добавлять доп. функции и менять что-то в `init` и `predict`\n",
    "\n",
    "При вызове `fit` вам нужно найти 2 атрибута:\n",
    "* `cv_results_` - словарь параметров\n",
    "* `best_params` - словарь наилучших параметров (пример `{max_depth: 4, min_samples_leaf: 5}`)\n",
    "\n",
    "В нашем `cv_results_` будет только 2 колонки:\n",
    "* `params` - словарь с параметрами вида: `{max_depth: 4, min_samples_leaf: 5}`\n",
    "* `mean_test_score` - среднее значение кроссвалидации\n",
    "\n",
    "\n",
    "### Sample\n",
    "#### Input:\n",
    "```python\n",
    "estimator = LinearRegression()\n",
    "cv = 2\n",
    "scoring = sklearn.metrics.r2_score\n",
    "\n",
    "\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([0.75, 0.83673469])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYGridSearch:\n",
    "    def __init__(self, estimator, param_grid, cv, scoring):\n",
    "        self.estimator = estimator\n",
    "        self.grid_params = param_grid\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        \n",
    "        self.cv_results = {'params': []\n",
    "                           'mean_rest_score': []} # [{'params': _, 'mean_test_score': _}]\n",
    "        self.best_params = {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Найдите лучшие значения и обучитесь на них\n",
    "        \"\"\"\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        self.estimator.set_params(self.best_params)\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Авторское решение\n",
    "class GridSearchCV:\n",
    "    def __init__(self, estimator, grid_params, cv=3, scoring=accuracy_score):\n",
    "        self.estimator = estimator\n",
    "        self.grid_params = grid_params\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.cv_results = [] # [{'params': _, 'mean_test_score': _}]\n",
    "        self.best_params = {}\n",
    "        \n",
    "    def get_params_product_list(self):\n",
    "        items = sorted(self.grid_params.items())\n",
    "        keys, values = zip(*items)\n",
    "        for v in product(*values):\n",
    "            params = dict(zip(keys, v))\n",
    "            yield params\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        best_score = 0.0\n",
    "        for params in get_params_product_list():\n",
    "            self.estimator.set_params(params)\n",
    "            cv_score = cross_val_score(self.estimator, X, y, n_splits=self.cv, scoring=self.scoring)\n",
    "            mean_score = np.mean(cv_score)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                self.best_params = params\n",
    "\n",
    "            self.cv_results['params'].append(params)\n",
    "            self.cv_results['mean_test_score'].append(mean_score)\n",
    "            \n",
    "        #refit\n",
    "        self.estimator.set_params(self.best_params)\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) Первая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы взяли [известные данные](https://www.kaggle.com/uciml/pima-indians-diabetes-database) для задачи бинарной классификации и разбили их произвольным образом на тренировачную и тестовую выборки в отношении 4:1. \n",
    "\n",
    "$Y$ в этих данных выступает столбик `Outcome`, в качаестве $X$ - все остальное. \n",
    "\n",
    "Вам на вход подается **тренировачная** выборка. Вы можете используя любой классификатор, как угодно изменяя гиперпараметры, попытаться получить на тестовых данных на сервере долю правильных ответов больше $0.7$. В задаче также необходимо вернуть подходящую обученную модель. Доля правильных ответов, это количество совпаших ответов, к количеству несовпавших ответов. Подробнее: [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
    "\n",
    "Алгоритм решения:\n",
    "* Скачиваете данные по ссылке\n",
    "* Изучаете данные, может как-то изменяете их\n",
    "* Сами разделяете выборку на тренировачную и тестовую с помощью `train_test_split`\n",
    "* Подбираете алгоритм, чтобы он давал необходимый (и даже с запасом) результат при различных разбиениях: изменяйте параметр `random_state` в `train_test_split`\n",
    "* Для нахождения доли правильных ответов воспользуйтесь `sklearn.metrics.accuracy_score`\n",
    "* В самой функции можете как-то изменить данные\n",
    "* Вызываете необходимый алгоритм и обучаете его на входных данных внутри функции\n",
    "* Верните обученную модель в функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def classification(X_train: np.ndarray, y_train: np.ndarray):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(X_train: np.ndarray, y_train: np.ndarray):\n",
    "    return LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'diabetes.csv' does not exist: b'diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2cc3a2540704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Outcome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Outcome'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'diabetes.csv' does not exist: b'diabetes.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "y = data['Outcome']\n",
    "X = data.drop(columns=['Outcome']).values\n",
    "######################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "y_pred = classification(X_train, y_train).predict(X_test)\n",
    "assert accuracy_score(y_pred, y_test) > 0.7\n",
    "######################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "y_pred = classification(X_train, y_train).predict(X_test)\n",
    "assert accuracy_score(y_pred, y_test) > 0.7\n",
    "######################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "y_pred = classification(X_train, y_train).predict(X_test)\n",
    "assert accuracy_score(y_pred, y_test) > 0.7\n",
    "######################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=91)\n",
    "y_pred = classification(X_train, y_train).predict(X_test)\n",
    "assert accuracy_score(y_pred, y_test) > 0.7\n",
    "######################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "y_pred = classification(X_train, y_train).predict(X_test)\n",
    "assert accuracy_score(y_pred, y_test) > 0.7\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Регрессия и kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот мы добрались и до [соревнования](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)! Вам нужно решить задачу регрессии и предсказать цену на покупку дома. \n",
    "\n",
    "Пока мы ограничены в возможностях вам дается урезанная задача: мы оставили только численные признаки `df._get_numeric_data().fillna(0)` и разбили данные произвольным образом на тренировачную и тестовую выборки в отношении 4:1. $y$ в этих данных выступает столбик `SalePrice`, в качаестве $X$ - все остальное. \n",
    "\n",
    "Вам на вход подается только **тренировачная** выборка. Вы можете используя любой регрессор, как угодно изменяя гиперпараметры, попытаться получить на тестовых данных на сервере результат `RMSLE` между вашим ответом и правильным **меньше** $0.3$. В задаче также необходимо вернуть подходящую обученную модель. \n",
    "\n",
    "`RMSLE` - root mean squared log error или \n",
    "$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(log(y^i_{pred}) - log(y^i_{real}))^2}$$\n",
    "\n",
    "Подробнее: [MSLE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html).\n",
    "\n",
    "Алгоритм решения:\n",
    "* Скачиваете данные по ссылке и загружаете в `DataFrame`\n",
    "* Вызываете команду `df._get_numeric_data().fillna(0)` чтобы оставить только численные данные и заполнить Nan-ы 0-ми.\n",
    "* Изучаете данные, может как-то изменяете их\n",
    "* Сами разделяете выборку на тренировачную и тестовую с помощью `train_test_split`\n",
    "* Подбираете алгоритм, чтобы он давал необходимый (и даже с запасом) результат при различных разбиениях: изменяйте параметр `random_state` в `train_test_split`\n",
    "* Для нахождения `RMSLE` воспользуйтесь `np.sqrt(sklearn.metrics.mean_squared_log_error(.,.))`\n",
    "* В самой функции как-то можете изменить данные\n",
    "* Вызываете необходимый алгоритм и обучаете его на входных данных внутри функции\n",
    "* Верните обученную модель в функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def regression(X: np.ndarray, y:np.ndarray):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(X: np.ndarray, y:np.ndarray):\n",
    "    return RandomForestRegressor(max_depth= 8, n_estimators = 150).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "data = pd.read_csv('houseprice/train.csv')\n",
    "y = data['SalePrice']\n",
    "X = data._get_numeric_data().fillna(0).drop(columns=['SalePrice']).values\n",
    "######################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "y_pred = regression(X_train, y_train).predict(X_test)\n",
    "assert np.sqrt(mean_squared_log_error(y_pred, y_test)) < 0.3\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('houseprice/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('houseprice/test.csv')._get_numeric_data().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2._get_numeric_data().fillna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df._get_numeric_data().dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['SalePrice']\n",
    "X = data.drop(columns=['SalePrice']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1121, 37), (1121,))"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([227286.82182012, 196557.96053216, 222772.94061721, ...,\n",
       "       223960.77437461, 131513.95653512, 152162.43258722])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsle: \n",
      "0.16816613729199362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "lm = RandomForestRegressor(max_depth= 10, n_estimators = 150)\n",
    "\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# model evaluation\n",
    "lm_yhat = lm.predict(X_test)\n",
    "\n",
    "print(\"rmsle: \")\n",
    "print(np.sqrt(metrics.mean_squared_error(np.log(y_test), np.log(lm_yhat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsle: \n",
      "36547.443679754775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "lm_yhat = lm.predict(X_test)\n",
    "lm_yhat[lm_yhat < 0] = 0\n",
    "\n",
    "print(\"rmsle: \")\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, lm_yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-18722.45784953])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.predict(X_real_test._get_numeric_data().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, array([[1.000e+00, 6.000e+01, 6.500e+01, ..., 0.000e+00, 2.000e+00,\n        2.008e+03],\n       [2.000e+00, 2.000e+01, 8.000e+01, ..., 0.000e+00, 5.000e+00,\n        2.007e+03],\n       [3.000e+00, 6.000e+01, 6.800e+01, ..., 0.000e+00, 9.000e+00,\n        2.008e+03],\n       ...,\n       [1.458e+03, 7.000e+01, 6.600e+01, ..., 2.500e+03, 5.000e+00,\n        2.010e+03],\n       [1.459e+03, 2.000e+01, 6.800e+01, ..., 0.000e+00, 4.000e+00,\n        2.010e+03],\n       [1.460e+03, 2.000e+01, 7.500e+01, ..., 0.000e+00, 6.000e+00,\n        2.008e+03]]) was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-417-d5885d41bfcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \"\"\"\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# To ensure multimetric format is not supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0;32m--> 270\u001b[0;31m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, array([[1.000e+00, 6.000e+01, 6.500e+01, ..., 0.000e+00, 2.000e+00,\n        2.008e+03],\n       [2.000e+00, 2.000e+01, 8.000e+01, ..., 0.000e+00, 5.000e+00,\n        2.007e+03],\n       [3.000e+00, 6.000e+01, 6.800e+01, ..., 0.000e+00, 9.000e+00,\n        2.008e+03],\n       ...,\n       [1.458e+03, 7.000e+01, 6.600e+01, ..., 2.500e+03, 5.000e+00,\n        2.010e+03],\n       [1.459e+03, 2.000e+01, 6.800e+01, ..., 0.000e+00, 4.000e+00,\n        2.010e+03],\n       [1.460e+03, 2.000e+01, 7.500e+01, ..., 0.000e+00, 6.000e+00,\n        2.008e+03]]) was passed"
     ]
    }
   ],
   "source": [
    "cross_val_score(X, y, LinearRegression(), cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def regression(X: np.ndarray, y:np.ndarray):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'n_estimators': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(regression)\n",
    "assert ' print(' not in lines\n",
    "assert ' open(' not in lines\n",
    "######################################################\n",
    "np.random.seed(228)\n",
    "n = 200\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=3, scale=2, size=(n, 2)) #второй класс\n",
    "X_clf = np.vstack([a, b]) #двумерный количественный признак\n",
    "y_clf = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "model = fit_gs(X_clf, y_clf)\n",
    "\n",
    "df = pd.DataFrame(model.cv_results_)\n",
    "params = df[df['rank_test_score'] == 1].reset_index(drop=True).loc[0]['params']\n",
    "assert params['max_depth'] == 3\n",
    "assert params['n_estimators'] == 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rule(x_train, y_train):\n",
    "    def entropy(y):\n",
    "        n = len(y)\n",
    "        p0 = len(y[y == 0]) / n\n",
    "        p1 = len(y[y == 1]) / n\n",
    "        if p0 == 0 or p1 == 0:  # when there is only one class in the group, entropy is 0\n",
    "            return 0\n",
    "        return -p0 * np.log2(p0) - p1 * np.log2(p1)\n",
    "\n",
    "    def ig(x_train, y_train, threshold):\n",
    "        group0 = y_train[x_train <= threshold]\n",
    "        group1 = y_train[x_train > threshold]\n",
    "        n = len(y_train)\n",
    "        n0 = len(group0)\n",
    "        n1 = len(group1)\n",
    "        return entropy(y_train) - (n0 / n) * entropy(group0) - (n1 / n) * entropy(group1)\n",
    "    \n",
    "    best_t = x_train[-1]\n",
    "    best_score = 0\n",
    "    for t in np.unique(x_train)[:-1]:\n",
    "        cur_score = ig(x_train, y_train, t)\n",
    "        if cur_score > best_score:\n",
    "            best_t = t\n",
    "            best_score = cur_score\n",
    "    return best_t, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) ID3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3(Таблица примеров, Целевой признак, Признаки)\n",
    "\n",
    "    Если все примеры положительны, то возвратить узел с меткой «+».\n",
    "    Если все примеры отрицательны, то возвратить узел с меткой «-».\n",
    "    Если множество признаков пустое, то возвратить узел с меткой, которая больше других встречается в значениях целевого признака в примерах.\n",
    "    Иначе:\n",
    "        A — признак, который лучше всего классифицирует примеры (с максимальной информационной выгодой).\n",
    "        Создать корень дерева решения; признаком в корне будет являться A {\\displaystyle A} A.\n",
    "        Для каждого возможного значения A {\\displaystyle A} A ( v i {\\displaystyle v_{i}} v_{i}):\n",
    "            Добавить новую ветвь дерева ниже корня с узлом со значением A = v i {\\displaystyle A=v_{i}} A=v_{i}\n",
    "            Выделить подмножество E x a m p l e s ( v i ) {\\displaystyle Examples(v_{i})} Examples(v_{i}) примеров, у которых A = v i {\\displaystyle A=v_{i}} A=v_{i}.\n",
    "            Если подмножество примеров пусто, то ниже этой новой ветви добавить узел с меткой, которая больше других встречается в значениях целевого признака в примерах.\n",
    "            Иначе, ниже этой новой ветви добавить поддерево, вызывая рекурсивно ID3( E x a m p l e s ( v i ) {\\displaystyle Examples(v_{i})} Examples(v_{i}), Целевой признак, Признаки)\n",
    "    Возвратить корень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class DecisionTreeClassifier(BaseEstimator):\n",
    "    def __init__(self, max_depth):\n",
    "        self.depth_ = 0\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, x, y, par_node={}, depth=0):\n",
    "    \"\"\"\n",
    "    x: Feature set\n",
    "    y: target variable\n",
    "    par_node: will be the tree generated for this x and y. \n",
    "    depth: the depth of the current layer\n",
    "    \"\"\"\n",
    "    if par_node is None:   # base case 1: tree stops at previous level\n",
    "        return None\n",
    "    elif len(y) == 0:   # base case 2: no data in this group\n",
    "        return None\n",
    "    elif self.all_same(y):   # base case 3: all y is the same in this group\n",
    "        return {'val':y[0]}\n",
    "    elif depth >= self.max_depth:   # base case 4: max depth reached \n",
    "        return None\n",
    "    # Recursively generate trees! \n",
    "        # find one split given an information gain \n",
    "    col, cutoff, entropy = self.find_best_split_of_all(x, y)   \n",
    "    y_left = y[x[:, col] < cutoff]  # left hand side data\n",
    "    y_right = y[x[:, col] >= cutoff]  # right hand side data\n",
    "    par_node = {'col': iris.feature_names[col], 'index_col':col,\n",
    "                'cutoff':cutoff,\n",
    "                'val': np.round(np.mean(y))}  # save the information \n",
    "    # generate tree for the left hand side data\n",
    "    par_node['left'] = self.fit(x[x[:, col] < cutoff], y_left, {}, depth+1)   \n",
    "    # right hand side trees\n",
    "    par_node['right'] = self.fit(x[x[:, col] >= cutoff], y_right, {}, depth+1)  \n",
    "    self.depth += 1   # increase the depth since we call fit once\n",
    "    self.trees = par_node  \n",
    "    return par_node\n",
    "    \n",
    "def all_same(self, items):\n",
    "    return all(x == items[0] for x in items)\n",
    "    \n",
    "    def predict(X_test):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.arange(20)\n",
    "y_train = np.array([0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99277445, 0.9612366 , 0.59167278])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tree_.impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "model = DTC(criterion='entropy', max_depth=1).fit(x_train[:,np.newaxis], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.5, -2. , -2. ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tree_.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree_image(clf):\n",
    "    from sklearn.tree import export_graphviz\n",
    "    export_graphviz(clf, out_file='tree.dot', feature_names = ['x'],\n",
    "                    class_names = np.array(['0','1']),\n",
    "                    rounded = True, proportion = False, precision = 2, filled = True)\n",
    "    from subprocess import call\n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "    # Display in python\n",
    "    plt.figure(figsize = (9, 7))\n",
    "    plt.imshow(plt.imread('tree.png'))\n",
    "    plt.axis('off');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "create_tree_image(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.453877639491069e-14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0\n",
    "1e-15 * np.log(p + (1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi3wyEmJhJ0d"
   },
   "source": [
    "# SLIDE (1) Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DtUNvZYZmQB"
   },
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окрестности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{\\theta}{J(\\theta)}$$\n",
    "$$ \\theta = \\theta - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\eta$ — learning rate\n",
    " - $\\theta$ — вектор параметров (в нашем случае — $w$)\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJL9LX3zZmQD"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHVhEDwEZmQF"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDMomentum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n",
    "        self.gradient = gradient\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            # пересчитайте веса в стохаистическом градиентном спуске\n",
    "            '''\n",
    "            .∧＿∧ \n",
    "            ( ･ω･｡)つ━☆・*。 \n",
    "            ⊂  ノ    ・゜+. \n",
    "            しーＪ   °。+ *´¨) \n",
    "                    .· ´¸.·*´¨) \n",
    "                    (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "            '''\n",
    "            self.w = \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m80r-erdZmQI"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDMomentum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n",
    "        self.gradient = gradient\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            cur_grad = self.gradient(self.w, X[index, :], np.array(y[index]), self.l)\n",
    "            v = self.gamma * v + self.lr * cur_grad\n",
    "            nw = self.w - v\n",
    "            if np.linalg.norm(self.w - nw) < 1e-10:\n",
    "                break\n",
    "            self.w = nw\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VY4fN2FBZmQK"
   },
   "outputs": [],
   "source": [
    "X = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n",
    "         1.63136986e-01, -5.44506820e-01]])\n",
    "y = np.array([-1])\n",
    "\n",
    "r0 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=1)\n",
    "r0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r0.fit(X, y)\n",
    "r1 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=0.1)\n",
    "r1.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r1.fit(X, y)\n",
    "######################################################\n",
    "assert np.allclose(r0.w.reshape(6), np.array([ 0.01753165,  0.0350633,   0.05259494,  0.0350633,   0.01753165, -0.01753165]))\n",
    "assert np.allclose(r1.w.reshape(6), np.array([-0.05887894, -0.11775788, -0.17663682, -0.11775788, -0.05887894,  0.05887894]))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIiy9577hUyT"
   },
   "source": [
    "# SLIDE (2) Adagrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwbogZBshUyX"
   },
   "source": [
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное отличие данного метода от SGD состоит в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженными данными большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $\\theta_i$ на итерации $t$ как $g_{t,i} = \\nabla_{\\theta}J(\\theta_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $\\theta_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $\\theta_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfnLBculhUyZ"
   },
   "source": [
    "# TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ermoemkRhUyb"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDAdagrad(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000, eps=1e-8):\n",
    "        self.gradient = gradient\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps # используйте его для пересчёта весов\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            # пересчитайте веса в стохаистическом градиентном спуске\n",
    "            '''\n",
    "            .∧＿∧ \n",
    "            ( ･ω･｡)つ━☆・*。 \n",
    "            ⊂  ノ    ・゜+. \n",
    "            しーＪ   °。+ *´¨) \n",
    "                    .· ´¸.·*´¨) \n",
    "                    (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "            '''\n",
    "            self.w = \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPiTtpLJhUyg"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDAdagrad(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000, eps=1e-8):\n",
    "        self.gradient = gradient\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps # используйте его для пересчёта весов\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        G = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            cur_grad = self.gradient(self.w, X[index, :], np.array(y[index]), self.l)\n",
    "            G += cur_grad ** 2\n",
    "            nw = self.w - self.lr / np.sqrt(G + self.eps) *  cur_grad\n",
    "            if np.linalg.norm(self.w - nw) < 1e-10:\n",
    "                break\n",
    "            self.w = nw\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXjRCq5yhUyj"
   },
   "outputs": [],
   "source": [
    "X = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n",
    "         1.63136986e-01, -5.44506820e-01]])\n",
    "y = np.array([-1])\n",
    "\n",
    "r0 = SGDAdagrad(5, lambda a, b, c, d: a * c, max_iter=5, l=1, lr=1)\n",
    "r0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r0.fit(X, y)\n",
    "r1 = SGDAdagrad(5, lambda a, b, c, d: a * c, max_iter=5, l=1, lr=0.1)\n",
    "r1.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r1.fit(X, y)\n",
    "######################################################\n",
    "assert np.allclose(r0.w.reshape(6), np.array([4.46637091, 4.53067376, 4.59204786, 4.53067376, 4.46637091, -4.46637091]))\n",
    "assert np.allclose(r1.w.reshape(6), np.array([0.50417066, 0.58148037, 0.66822661, 0.58148037, 0.50417066, -0.50417066]))\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ri5gYSkodkqs"
   },
   "outputs": [],
   "source": [
    "# SLIDE (1) Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCLGO-nXkXI7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
