{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/d/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Let's login to huggingface ig to get results\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the training data into captions, frames, and scene graph from TVQA dataset\n",
    "# Encode the individual parts\n",
    "# Combine them into one embedding space\n",
    "# Train a vanilla Flan-T5 based transformer\n",
    "# Evaluate "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_questions = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/TVQA/data/tvqa_qa_release/tvqa_train.jsonl\"\n",
    "train_data_frames = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/uncompressed_frames/frames_hq\" # All frames\n",
    "train_data_audio = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/uncompressed_audio\" # All audio\n",
    "subtitles = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/TVQA/data/tvqa_preprocessed_subtitles.jsonl\" # All subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_questions = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/TVQA/data/tvqa_qa_release/tvqa_test_public.jsonl\"\n",
    "val_data_questions = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/TVQA/data/tvqa_qa_release/tvqa_val.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_df_train = pd.read_json(train_data_questions, lines=True)\n",
    "# questions_df_test = pd.read_json(test_data_questions, lines=True)\n",
    "subtitles_df = pd.read_json(subtitles, lines=True)\n",
    "subtitles_df = subtitles_df.set_index('vid_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-xxl'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/d/.cache/huggingface/datasets/json/default-448cf64c9f4da10c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6811736c3d44d808dde149d737c8382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading in datasets\n",
    "# Should we read into dataframe or dataset?\n",
    "# questions_df = pd.read_json(train_data_questions, lines=True)\n",
    "# subtitles_df = pd.read_json(train_data_subtitles, lines=True)\n",
    "\n",
    "# \n",
    "dataset = load_dataset('json', data_files = {\"train\": train_data_questions, \"val\": val_data_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6770f685278f4da4949a1ded29b79294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3736145604e744feb452532444a59b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e435386ac44a993b8bc8b7df11598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "For stability purposes, it is recommended to have accelerate installed when using this model in torch.float16, please install it with `pip install accelerate`\n",
      "Some weights of the model checkpoint at google/flan-t5-large were not used when initializing T5EncoderModel: ['decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'lm_head.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer, logging\n",
    "\n",
    "\n",
    "# # Load the CLIP tokenizer\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# # Load the CLIP model\n",
    "# model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5EncoderModel.from_pretrained(\"google/flan-t5-large\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "def get_last_hidden_layer(sentence):\n",
    "    # Tokenize the sentence and convert it to a PyTorch tensor\n",
    "    tokens =tokenizer(sentence, return_tensors=\"pt\", padding=False, truncation=False).to(device)\n",
    "\n",
    "    # Generate the last hidden layer of the CLIP encoder\n",
    "    lhs = model(**tokens).last_hidden_state\n",
    "\n",
    "    return lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_last_hidden_layer(\"3 inches is above average\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtitle_from_video(vid_name: str, ts: str):\n",
    "  if type(ts) != str:\n",
    "    print(type(ts))\n",
    "    print(ts)\n",
    "  start_time, end_time = ts.split('-')\n",
    "  subtitle = ''\n",
    "  sub = subtitles_df.loc[vid_name]\n",
    "  for text in sub['sub']:\n",
    "    if text['start'] >= float(start_time) and text['end'] <= float(end_time):\n",
    "      subtitle += text['text'] + ' '\n",
    "  return subtitle.strip()\n",
    "def get_all_subtitles(vid_name: str):\n",
    "  sub = ' '.join([_dict[\"text\"] for _dict in subtitles_df.loc[vid_name][\"sub\"]])\n",
    "  return sub.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chase : That\\'s all this is? Yeah.  House : Because his white blood cell count was down, he was vulnerable.  House : Because it\\'s really down, it might kill him.  Chase : That\\'s all this is.  House : Is he still pooping his pants? It just started again. Again?  Chase : It stopped when he was getting better. When he was feeling better. He was never getting better.  Foreman : So, all we have to answer is, what causes a 22-year-old kid  Foreman : to become immunocompromised  Foreman : with GI involvement and shocks? Just stop and start for no apparent reason.  Cameron : And we need to find an answer  Cameron : before this infection kills him.  House : That would be the ideal. You. Intravenous broad spectrum antibiotics. You get a cervical, thoracic, and lumbar T2-weighted fast spin-echo MRI. And you, track down all those Richie Rich\\'s who went to Jamaica,  House : see if any of them have got the shocks, the trots or the hots. The hots, it\\'s the fever. You guys gonna join us for dinner Thursday? Gotta do my laundry.  Cameron : You\\'re not curious?  Chase : I\\'m curious about crocs, but I don\\'t stick my head in their mouths.  Foreman : I\\'m out, too. Look, House is a freak!  Foreman : There\\'s no virus that causes that, no DNA mutation. You\\'re gonna have one dinner with two people.  Foreman : Sixty minutes, most of it spent chewing or talking about the weather.  Foreman : Unless they say something like,  Foreman : \"Do you prefer the chardonnay or the merlot?\" and, \"Oh, we kept Greg locked in a closet for 17 years\". You\\'re not gonna learn anything. Okay.  Cameron : We\\'d like to talk to some of your friends,  Cameron : see if they have any similar symptoms.  Ken : The doctors in the ER already asked me that.  Cameron : We understand,  Cameron : but they didn\\'t have all the information that we have now.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_subtitles(\"house_s02e05_seg02_clip_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_text_encoding(question_sample):\n",
    "  '''\n",
    "  param: \n",
    "  dictionary with ['a0',\n",
    " 'a1',\n",
    " 'a2',\n",
    " 'a3',\n",
    " 'a4',\n",
    " 'answer_idx',\n",
    " 'q',\n",
    " 'qid',\n",
    " 'show_name',\n",
    " 'ts',\n",
    " 'vid_name']\n",
    "    \n",
    "    \n",
    "\n",
    "  returns: list of last hidden state of encoding, (prompt, subtitles, answer) for each answer\n",
    "  '''\n",
    "  all_encodings = []\n",
    "  for i in range(5):\n",
    "    subtitles = get_all_subtitles(question_sample[\"vid_name\"])\n",
    "    ans_candidate = question_sample[f\"a{i}\"]\n",
    "    prompt = f\"Context: {subtitles}. Question: {question_sample['q']} Is it '{ans_candidate}'?\"\n",
    "    all_encodings.append(get_last_hidden_layer(prompt))\n",
    "  return all_encodings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 382, 1024])\n",
      "torch.Size([1, 351, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 258, 1024])\n",
      "torch.Size([1, 345, 1024])\n",
      "torch.Size([1, 619, 1024])\n",
      "torch.Size([1, 300, 1024])\n",
      "torch.Size([1, 208, 1024])\n",
      "torch.Size([1, 518, 1024])\n",
      "torch.Size([1, 505, 1024])\n",
      "torch.Size([1, 227, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  sample = dataset['train'][i]\n",
    "  print(output_text_encoding(sample)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a0',\n",
       " 'a1',\n",
       " 'a2',\n",
       " 'a3',\n",
       " 'a4',\n",
       " 'answer_idx',\n",
       " 'q',\n",
       " 'qid',\n",
       " 'show_name',\n",
       " 'ts',\n",
       " 'vid_name']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset['train'].features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/d/.cache/huggingface/datasets/json/default-448cf64c9f4da10c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-77b7e2af7910ad4a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 69\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "input = f\"You are given the following dialogue: . 'Given the question , which of the following options is the most likely answer?\\n\"\n",
    "prompt_size = len(tokenizer(input, truncation=True))\n",
    "\n",
    "# The maximum question length\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"val\"]]).map(lambda x: tokenizer(x[\"q\"], truncation=True), batched=True, remove_columns=list(dataset['train'].features.keys()))\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length + prompt_size}\")\n",
    "\n",
    "# # The maximum answer length\n",
    "# # Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "# tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"val\"]]).map(lambda x: tokenizer(x[f\"a{x[\"answer_idx\"]- 1}\"], truncation=True), batched=True, remove_columns=list(dataset['train'].features.keys()))\n",
    "# max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "# print(f\"Max source length: {max_source_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was lying on the floor.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import randrange\n",
    "sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n",
    "print(sample['a0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subtitles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House : That was the whole idea.  Wilson : You wanted to kill yourself?  House : I wanted to nearly kill myself.  House : Is he better? No.\n"
     ]
    }
   ],
   "source": [
    "# Test cell\n",
    "question =  sample\n",
    "video = question['vid_name']\n",
    "test_sentence = get_subtitle_from_video(video, question['ts'])\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122039, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert subtitles to CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/d/.cache/huggingface/datasets/json/default-448cf64c9f4da10c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-1caa1af1fc903c62.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c0b47b6db9477d927131f6bc08d344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Dataset:   0%|          | 0/15253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # Convert to relevant information\n",
    "    a0, a1, a2, a3, a4, answer_idx, q, ts, vid_name = sample['a0'], sample['a1'], sample['a2'], sample['a3'], sample['a4'], sample['answer_idx'], sample['q'], sample['ts'], sample['vid_name']\n",
    "    # Get the subtitle given the video and time stamp\n",
    "    subtitle = get_subtitle_from_video(vid_name, ts)\n",
    "    # Create an answer list\n",
    "    answer_list = [a0, a1, a2, a3, a4]\n",
    "    # Convert the answer id to relevant answer\n",
    "    answer = answer_list[answer_idx - 1]\n",
    "    # add prefix to the input for t5\n",
    "    input = f\"You are given the following dialogue: '{subtitle}'. 'Given the question '{q}', which of the following options is the most likely answer?\\n{answer_list}\"\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(input)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(answer)\n",
    "    # labels = tokenizer(text_target=answer,  padding=padding, truncation=True)\n",
    "\n",
    "    # # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # # padding in the loss.\n",
    "    # if padding == \"max_length\":\n",
    "    #     labels[\"input_ids\"] = [\n",
    "    #         [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    #     ]\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# preprocess_function(dataset['val'][1])\n",
    "# Should be batching, but I'm not because some of the modifications I make are on an iterative level; should definitely change for scaling purposes.\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched = False, remove_columns=\n",
    "                                ['qid', 'show_name', 'a0', 'a1', 'a2', 'a3', 'a4', 'answer_idx', 'q', 'ts', 'vid_name'], desc='Processing Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9318, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = torch.tensor(tokenized_dataset['train'][0]['input_ids']).unsqueeze(0)\n",
    "_label = torch.tensor(tokenized_dataset['train'][0]['labels']).unsqueeze(0)\n",
    "# print(_input)\n",
    "# print(_label)\n",
    "# print(_input)\n",
    "# outputs = model(input_ids=_input)\n",
    "\n",
    "\n",
    "# input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "# labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n",
    "# print(input_ids)\n",
    "# print(labels)\n",
    "# loss = model(input_ids=input_ids, labels=labels).loss\n",
    "\n",
    "loss = model(input_ids=_input, labels=_label).loss\n",
    "loss\n",
    "# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "# import nltk\n",
    "import numpy as np\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# helper function to postprocess text\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    # prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    # result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.27.1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = \"premo/vpt_tvqa\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    report_to=\"tensorboard\",\n",
    "    push_to_hub=False,\n",
    "    # Wrong version?\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token()\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/d/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/d/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='38140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    3/38140 00:00 < 2:05:57, 5.05 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "_compute() got an unexpected keyword argument 'use_stemmer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/trainer.py:1994\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1994\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1996\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1997\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1998\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2230\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[1;32m   2231\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[1;32m   2232\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2233\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2234\u001b[0m             )\n\u001b[1;32m   2235\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2236\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/trainer_seq2seq.py:78\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     gen_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m gen_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnum_beams\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gen_kwargs \u001b[39m=\u001b[39m gen_kwargs\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mevaluate(eval_dataset, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/trainer.py:2932\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2929\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2931\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2932\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2933\u001b[0m     eval_dataloader,\n\u001b[1;32m   2934\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2935\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2936\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2937\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2938\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2939\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2940\u001b[0m )\n\u001b[1;32m   2942\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2943\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/transformers/trainer.py:3220\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3216\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3217\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3218\u001b[0m         )\n\u001b[1;32m   3219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3220\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3222\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Some simple post-processing\u001b[39;00m\n\u001b[1;32m     27\u001b[0m decoded_preds, decoded_labels \u001b[39m=\u001b[39m postprocess_text(decoded_preds, decoded_labels)\n\u001b[0;32m---> 29\u001b[0m result \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49mdecoded_preds, references\u001b[39m=\u001b[39;49mdecoded_labels, use_stemmer\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# result = {k: round(v * 100, 4) for k, v in result.items()}\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# result[\"gen_len\"] = np.mean(prediction_lens)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/evaluate/module.py:444\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    443\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> 444\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _compute() got an unexpected keyword argument 'use_stemmer'"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our tokenizer and create model card\n",
    "tokenizer.save_pretrained(repository_id)\n",
    "trainer.create_model_card()\n",
    "# Push the results to the hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def qa_to_prompt(qa):\n",
    "  '''\n",
    "  TODO: update prompt to be VPT-compatible. \n",
    "  TODO: add </s> token to end of prompt... during training.\n",
    "  '''\n",
    "\n",
    "  subtitle = get_subtitle_from_video(qa['vid_name'], float(qa['ts'].split('-')[0]), float(qa['ts'].split('-')[-1]))\n",
    "\n",
    "  # image_embed = get_clip_embed_from_vid_name(qa['vid_name'], float(qa['ts'].split('-')[0]), float(qa['ts'].split('-')[-1]))\n",
    "\n",
    "  return [\n",
    "      f\"Context: {subtitle}. Question: {qa['q']} Is it '{ans_candidate}'?\"\n",
    "      for ans_candidate in [qa['a0'], qa['a1'], qa['a2'], qa['a3'], qa['a4']]\n",
    "  ]\n",
    "\n",
    "def run_prompts_get_best_answer(prompts):\n",
    "  all_yes_scores = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for prompt in prompts:\n",
    "      # print(prompt)\n",
    "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # Batch size 1\n",
    "      outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, early_stopping=True, max_new_tokens=2)\n",
    "      # print(outputs)\n",
    "      all_yes_scores.append(outputs.scores[0][0][4273].cpu())\n",
    "      # print('yes_score =', outputs.scores[0][0][4273])\n",
    "      # print('no_score =', outputs.scores[0][0][150])\n",
    "      # print(self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False))\n",
    "\n",
    "def accuracy(actual, predicted):\n",
    "  correct = 0\n",
    "  total = len(actual)\n",
    "  for i in range(total):\n",
    "    if actual[i] == predicted[i]:\n",
    "      correct += 1\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "for i in range(5):\n",
    "  predicted_ans_idx = eval.run_prompts_get_best_answer(eval.qa_to_prompt(eval.train_qa_json[i]))\n",
    "  actual.append(int(eval.train_qa_json[i]['answer_idx']))\n",
    "  predicted.append(predicted_ans_idx)\n",
    "  print(\"Predicted answer:\", predicted_ans_idx)\n",
    "  print(\"Actual answer:   \", eval.train_qa_json[i]['answer_idx'])\n",
    "\n",
    "print(\"Accuracy:\", eval.accuracy(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from clip_encoder import ClipEncoder\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main Evaluation Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_encoder import ClipEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/teton/utils/cache/huggingface'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/mnt/teton/utils/cache/datasets'\n",
    "\n",
    "sys.path.append(\"../../data_preprocessing/parallel_processing\")\n",
    "from clip_encoder import ClipEncoder\n",
    "from text_encoder import FlanT5Encoder\n",
    "\n",
    "\n",
    "class TVQA_eval():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    train_filepath = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/TVQA/data/tvqa_qa_release/tvqa_train.jsonl\"\n",
    "    with open(train_filepath, 'r') as f:\n",
    "      self.train_qa_json = [json.loads(line) for line in f]\n",
    "    # len(train_qa_json)  # 122,039\n",
    "\n",
    "    subtitles_filepath = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/TVQA/data/tvqa_preprocessed_subtitles.jsonl\"\n",
    "    with open(subtitles_filepath, 'r') as f:\n",
    "      # self.subtitles = [json.loads(line) for line in f]\n",
    "      self.subtitles = pd.read_json(subtitles_filepath, lines=True)\n",
    "      self.subtitles = self.subtitles.set_index('vid_name')\n",
    "    # len(subtitles)  # 21,793\n",
    "\n",
    "    # instantiate expert models\n",
    "    self.clip_encoder = ClipEncoder(debug=True)\n",
    "    self.text_encoder = FlanT5Encoder()\n",
    "\n",
    "    self.tvqa_train_to_path = {\n",
    "        \"House M.D.\": 'house_frames',\n",
    "        \"The Big Bang Theory\": 'bbt_frames',\n",
    "        \"Castle\": 'castle_frames',\n",
    "        \"How I Met You Mother\": 'met_frames',\n",
    "        \"Grey's Anatomy\": 'grey_frames',\n",
    "        \"Friends\": 'friends_frames',\n",
    "    }\n",
    "\n",
    "    self.vid_name_prefix_to_path = {\n",
    "        \"house\": 'house_frames',\n",
    "        \"castle\": 'castle_frames',\n",
    "        \"met\": 'met_frames',\n",
    "        \"grey\": 'grey_frames',\n",
    "        \"friends\": 'friends_frames',\n",
    "        \"\": 'bbt_frames',  # no prefix at all used for bbt, it's the \"default\"\n",
    "    }\n",
    "\n",
    "  # Deprecated\n",
    "  def get_subtitle_from_clip(self, vid_name: str, ts: str):\n",
    "    start_time, end_time = ts.split('-')\n",
    "    subtitle = ''\n",
    "    sub = self.subtitles.loc[vid_name]\n",
    "    for text in sub['sub']:\n",
    "      if text['start'] >= float(start_time) and text['end'] <= float(end_time):\n",
    "        subtitle += text['text'] + ' '\n",
    "    return subtitle.strip()\n",
    "\n",
    "  def get_all_subtitles(self, vid_name: str):\n",
    "    '''Returns a string with every subtitle from a given video'''\n",
    "    sub = ' '.join([_dict[\"text\"] for _dict in self.subtitles.loc[vid_name][\"sub\"]])\n",
    "    return sub.strip()\n",
    "\n",
    "  def qa_to_prompt(self, qa: Dict):\n",
    "    '''\n",
    "    TODO: update prompt to be VPT-compatible. \n",
    "    TODO: add </s> token to end of prompt... during training.\n",
    "    '''\n",
    "    # In this version, we only get the subtitles relevant to the time stamp\n",
    "    # subtitle = self.get_subtitle_from_clip(qa['vid_name'], float(qa['ts'].split('-')[0]), float(qa['ts'].split('-')[-1]))\n",
    "    # In this version, we get all subtitles from the video\n",
    "    print(qa)\n",
    "    print(qa['vid_name'])\n",
    "    subtitle = self.get_all_subtitles(qa['vid_name'])\n",
    "\n",
    "    # Should change this to all frames. Either way, this is irrelevant for this function\n",
    "    # image_embed = self.get_clip_embed_from_vid_name(qa['vid_name'], float(qa['ts'].split('-')[0]), float(qa['ts'].split('-')[-1]))\n",
    "\n",
    "    return [\n",
    "        f\"Context: {subtitle}. Question: {qa['q']} Is it '{ans_candidate}'?\"\n",
    "        for ans_candidate in [qa['a0'], qa['a1'], qa['a2'], qa['a3'], qa['a4']]\n",
    "    ]\n",
    "\n",
    "  def combine_modality_encodings(self, text_encoding, image_encoding):\n",
    "    '''Untested btw'''\n",
    "    num_text_embeddings, _ = text_encoding.shape\n",
    "    num_image_embeddings, _ = image_encoding.shape\n",
    "\n",
    "    assert num_image_embeddings + num_text_embeddings <= 1024, f\"the given encoding has more than 1024 embeddings! Text is {num_text_embeddings} and image is {num_image_embeddings}\"\n",
    "\n",
    "    combined_tensor = torch.cat((text_encoding, image_encoding), dim=0)\n",
    "    # Pad the tensor with -100 to make it a [1024, 1024] tensor\n",
    "\n",
    "    # There shouldn't be any padding left\n",
    "    # pad_size = (1024 - combined_tensor.shape[0], 1024)\n",
    "    # padded_tensor = torch.nn.functional.pad(combined_tensor, pad_size, value=-100)\n",
    "\n",
    "    # Reinsert \n",
    "    assert combined_tensor.shape[0] == 1024, f\"the combined encoding does not have exactly 1024 embeddings! Dimension: {combined_tensor.shape}\"\n",
    "\n",
    "    return combined_tensor\n",
    "\n",
    "  def create_context_vectors(self, question):\n",
    "    '''Combine the two vectors to create the context vector'''\n",
    "    try:\n",
    "      all_context_vectors = []\n",
    "      text_encodings = self.get_flant5_embed_from_vid_name(question)\n",
    "      image_encoding = self.get_clip_embed_from_vid_name(question['vid_name'])  # this should be get_clip_embed_from_vid_name\n",
    "      for text_encoding in text_encodings:\n",
    "        all_context_vectors.append(self.combine_modality_encodings(text_encoding, image_encoding))\n",
    "      return all_context_vectors\n",
    "    except FileNotFoundError as e:\n",
    "      print(e)\n",
    "      print(f\"WARNING: Could not find video {question['vid_name']}. Skipping...\")\n",
    "\n",
    "  def pad_or_truncate_tensor(self, tensor, truncate_shape):\n",
    "      target_shape = [truncate_shape, 1024]\n",
    "      tensor_shape = tensor.shape\n",
    "\n",
    "      # If tensor shape is larger than the target shape, truncate the tensor\n",
    "      if tensor_shape[0] > target_shape[0]:\n",
    "          truncated_tensor = tensor[:target_shape[0], :]\n",
    "          return truncated_tensor\n",
    "\n",
    "      # If tensor shape is smaller than the target shape, pad the tensor\n",
    "      elif tensor_shape[0] < target_shape[0]:\n",
    "          padding_shape = (target_shape[0] - tensor_shape[0], target_shape[1])\n",
    "          padded_tensor = torch.nn.functional.pad(tensor, (0, 0, 0, padding_shape[0]), value=-100)\n",
    "          return padded_tensor\n",
    "\n",
    "      # If tensor shape is already the target shape, return the tensor\n",
    "      else:\n",
    "          return tensor\n",
    "      \n",
    "  def vid_name_to_frames_path(self, vid_name):\n",
    "    '''\n",
    "    Example:\n",
    "    vid_name: house_s02e05_seg02_clip_11\n",
    "    \n",
    "    show_name_path = house_frames\n",
    "    clip_name_path = s02e05_seg02_clip_11\n",
    "    \n",
    "    returns: /mnt/teton/vpt/data/benchmark_datasets/TVQA/uncompressed_frames/frames_hq/house_frames/s02e05_seg02_clip_11\n",
    "    '''\n",
    "\n",
    "    show_name, clip_name_path = vid_name.split('_', 1)[0], vid_name.split('_', 1)[1]\n",
    "    if show_name not in self.vid_name_prefix_to_path:\n",
    "      show_name_path = 'bbt_frames'\n",
    "      clip_name_path = vid_name  # no prefix at all used for bbt. vid_name is actually the 'clip_name_path'\n",
    "    else:\n",
    "      show_name_path = self.vid_name_prefix_to_path[show_name]\n",
    "\n",
    "    frames_dir = os.path.join('/mnt/teton/vpt/data/benchmark_datasets/TVQA/uncompressed_frames/frames_hq', show_name_path, clip_name_path)\n",
    "    if not os.path.exists(frames_dir):\n",
    "      raise FileNotFoundError(f\"frames_dir {frames_dir} does not exist\")\n",
    "\n",
    "    return frames_dir\n",
    "\n",
    "  def get_clip_embed_from_vid_name(self, vid_name, max_encodings_to_make=220):\n",
    "    '''\n",
    "    ✅ Working\n",
    "    \n",
    "    PARAMS\n",
    "    vid_name (from the subtitles jsonl file)\n",
    "    max_encodings_to_make: int. It will only encode the FIRST max_encodings_to_make frames.\n",
    "    \n",
    "    RETURNS\n",
    "    A list of clip embeddings for that video segment. There is a VARIABLE number of frames per \"clip\". \n",
    "    \n",
    "    Notes: \n",
    "      * Castle avg frames/clip = 274.79\n",
    "      * BBT avg frames/clip    = 186.38\n",
    "    \n",
    "    220 frames per clip is a good number to use.\n",
    "    1024 (Flan-T5-XXL window size) - 220 = 804 text encodings to use. \n",
    "    '''\n",
    "    # collect all frames from filepath\n",
    "    segment_frames_paths = pathlib.Path(self.vid_name_to_frames_path(vid_name)).glob('*.jpg')\n",
    "    frames_PIL_list = []\n",
    "    for path in segment_frames_paths:\n",
    "      if len(frames_PIL_list) >= max_encodings_to_make:\n",
    "        break\n",
    "      with Image.open(path) as img:\n",
    "        frames_PIL_list.append(np.array(img))\n",
    "\n",
    "    # split frames_PIL_list into batches of 100 (to avoid OOM)\n",
    "    batch_list = list(more_itertools.batched(frames_PIL_list, 110))\n",
    "\n",
    "    clip_embeddings = []\n",
    "    for batch in batch_list:\n",
    "      clip_embeddings.extend(self.clip_encoder.run_clip(batch, only_return_pooled_embeds=True))\n",
    "\n",
    "\n",
    "    # Converting a list of tensors to a tensor\n",
    "    # Get the shape of the first tensor in the list\n",
    "    tensor_shape = clip_embeddings[0].shape\n",
    "    # Create a tensor of zeros with the appropriate shape\n",
    "    tensor = torch.zeros(len(clip_embeddings), *tensor_shape)\n",
    "    # Fill the tensor with the values from the input list\n",
    "    for i, t in enumerate(clip_embeddings):\n",
    "        tensor[i, ...] = t\n",
    "    fixed_tensor = self.pad_or_truncate_tensor(tensor, max_encodings_to_make)\n",
    "    fixed_tensor.cpu()\n",
    "    return fixed_tensor\n",
    "\n",
    "  def get_flant5_embed_from_vid_name(self, question_sample, max_encodings_to_make=804):\n",
    "    '''\n",
    "    param: \n",
    "    question_sample is a dictionary with ['a0',  'a1',  'a2',  'a3', 'a4',    'answer_idx',   'q',   'qid',  'show_name', 'ts', 'vid_name'] as keys\n",
    "\n",
    "    returns: list of last hidden state of encoding, (prompt, subtitles, answer) for each answer\n",
    "    '''\n",
    "    assert type(question_sample) == dict, f\"question_sample must be a dictionary. It is a {type(question_sample)}.\"\n",
    "\n",
    "    all_prompts = self.qa_to_prompt(question_sample)\n",
    "    all_encodings = []\n",
    "    for prompt in all_prompts:\n",
    "      all_encodings.append(self.text_encoder.encode_tvqa(prompt, truncate_shape=max_encodings_to_make))\n",
    "    return all_encodings\n",
    "\n",
    "  def run_prompts_get_best_answer(self, prompts: List[str]):\n",
    "    all_yes_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for prompt in prompts:\n",
    "        # print(prompt)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)  # Batch size 1\n",
    "        outputs = self.model.generate(**inputs, return_dict_in_generate=True, output_scores=True, early_stopping=True, max_new_tokens=2)\n",
    "        # print(outputs)\n",
    "        all_yes_scores.append(outputs.scores[0][0][4273].cpu())\n",
    "        # print('yes_score =', outputs.scores[0][0][4273])\n",
    "        # print('no_score =', outputs.scores[0][0][150])\n",
    "        # print(self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=False))\n",
    "\n",
    "    return np.array(all_yes_scores).argmax()\n",
    "\n",
    "  def accuracy(self, actual, predicted):\n",
    "    correct = 0\n",
    "    total = len(actual)\n",
    "    for i in range(total):\n",
    "      if actual[i] == predicted[i]:\n",
    "        correct += 1\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0...\n",
      "Done setting up CLIP...\n",
      "In FlanT5Encoder cuda:1\n"
     ]
    }
   ],
   "source": [
    "eval = TVQA_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a0': 'Penny means that the man is turning Amy on. ',\n",
       " 'a1': 'Penny means that her shoes are too tight. ',\n",
       " 'a2': 'Penny means that she ate too much. ',\n",
       " 'a3': 'Penny means that she is drunk. ',\n",
       " 'a4': \"Penny isn't sure what she means but it sounds good. \",\n",
       " 'answer_idx': 0,\n",
       " 'q': \"What does Penny mean when she says she knows what's causing Amy's condition?\",\n",
       " 'qid': 3,\n",
       " 'show_name': 'The Big Bang Theory',\n",
       " 'ts': '43.42-46.3',\n",
       " 'vid_name': 's04e10_seg01_clip_03'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset['train'][3]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a0': 'Penny means that the man is turning Amy on. ', 'a1': 'Penny means that her shoes are too tight. ', 'a2': 'Penny means that she ate too much. ', 'a3': 'Penny means that she is drunk. ', 'a4': \"Penny isn't sure what she means but it sounds good. \", 'answer_idx': 0, 'q': \"What does Penny mean when she says she knows what's causing Amy's condition?\", 'qid': 3, 'show_name': 'The Big Bang Theory', 'ts': '43.42-46.3', 'vid_name': 's04e10_seg01_clip_03'}\n",
      "s04e10_seg01_clip_03\n",
      "⏰  Runtime of preprocessing: 0.60 seconds\n",
      "RIGHT before running clip 📸\n",
      "⏰ CLIP Runtime on 110 images: 0.01 seconds\n",
      "⏰  Runtime of preprocessing: 0.30 seconds\n",
      "RIGHT before running clip 📸\n",
      "⏰ CLIP Runtime on 54 images: 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor[1024, 1024] n=1048576 x∈[-100.000, 6.544] μ=-67.114 σ=47.008,\n",
       " tensor[1024, 1024] n=1048576 x∈[-100.000, 6.544] μ=-67.163 σ=46.990,\n",
       " tensor[1024, 1024] n=1048576 x∈[-100.000, 6.544] μ=-67.163 σ=46.990,\n",
       " tensor[1024, 1024] n=1048576 x∈[-100.000, 6.544] μ=-67.261 σ=46.954,\n",
       " tensor[1024, 1024] n=1048576 x∈[-100.000, 6.544] μ=-66.919 σ=47.078]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.create_context_vectors(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_tvqa_whole/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "ename": "DatasetCorruptError",
     "evalue": "The HEAD node of the branch main of this dataset is in a corrupted state and is likely not recoverable. Try using `reset=True` to reset HEAD changes and load the previous commit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/deeplake/util/version_control.py:106\u001b[0m, in \u001b[0;36mintegrity_check\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m n1 \u001b[39m!=\u001b[39m n2:\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensor meta and chunk id encoder have different number of samples (\u001b[39m\u001b[39m{\u001b[39;00mn1\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mn2\u001b[39m}\u001b[39;00m\u001b[39m respectively) for tensor \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    109\u001b[0m num_sequences \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(engine\u001b[39m.\u001b[39msequence_encoder, \u001b[39m\"\u001b[39m\u001b[39mnum_samples\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor meta and chunk id encoder have different number of samples (117730 and 117725 respectively) for tensor _label_id.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetCorruptError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdeeplake\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdl\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ds \u001b[39m=\u001b[39m dl\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_tvqa_whole\u001b[39;49m\u001b[39m\"\u001b[39;49m, read_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      4\u001b[0m ds\u001b[39m.\u001b[39msummary()\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(ds\u001b[39m.\u001b[39msize_approx() \u001b[39m/\u001b[39m \u001b[39m1e9\u001b[39m) \u001b[39m# bytes to GB\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/deeplake/util/spinner.py:138\u001b[0m, in \u001b[0;36mspinner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m spinner \u001b[39m=\u001b[39m Spinner()\n\u001b[1;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m run_spinner(spinner):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/tvqa_finetune/lib/python3.8/site-packages/deeplake/api/dataset.py:596\u001b[0m, in \u001b[0;36mdataset.load\u001b[0;34m(path, read_only, memory_cache_size, local_cache_size, creds, token, org_id, verbose, access_method, reset)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m reset:\n\u001b[1;32m    595\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, DatasetCorruptError):\n\u001b[0;32m--> 596\u001b[0m         \u001b[39mraise\u001b[39;00m DatasetCorruptError(\n\u001b[1;32m    597\u001b[0m             message\u001b[39m=\u001b[39me\u001b[39m.\u001b[39mmessage,\n\u001b[1;32m    598\u001b[0m             action\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTry using `reset=True` to reset HEAD changes and load the previous commit.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    599\u001b[0m             cause\u001b[39m=\u001b[39me\u001b[39m.\u001b[39m__cause__,\n\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetCorruptError(\n\u001b[1;32m    602\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mException occured (see Traceback). The dataset maybe corrupted. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTry using `reset=True` to reset HEAD changes and load the previous commit. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis will delete all uncommitted changes on the branch you are trying to load.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\u001b[39m.\u001b[39m_reset_and_load(\n\u001b[1;32m    607\u001b[0m     cache_chain, access_method, dataset_kwargs, address, e\n\u001b[1;32m    608\u001b[0m )\n",
      "\u001b[0;31mDatasetCorruptError\u001b[0m: The HEAD node of the branch main of this dataset is in a corrupted state and is likely not recoverable. Try using `reset=True` to reset HEAD changes and load the previous commit."
     ]
    }
   ],
   "source": [
    "import deeplake as dl\n",
    "import numpy as np\n",
    "ds = dl.load(\"/mnt/teton/vpt/data/benchmark_datasets/TVQA/_deeplake/mar_28_TVQA_encode_tvqa_whole\", read_only=True)\n",
    "ds.summary()\n",
    "\n",
    "print(ds.size_approx() / 1e9) # bytes to GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = \"/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_items = os.listdir(frames_path)\n",
    "\n",
    "all_dirs = [os.path.join(frames_path, show_path) for show_path in directory_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/met_frames 1512\n",
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/castle_frames 4706\n",
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/friends_frames 5353\n",
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/grey_frames 1435\n",
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/bbt_frames 4199\n",
      "/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/house_frames 4624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21829"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames = 0\n",
    "for i in all_dirs:\n",
    "    length = len(os.listdir(i))\n",
    "    total_frames += length\n",
    "    print(i, length)\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20995"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4199 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'met_s06e15_seg02_clip_05'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_met_clips = os.listdir(all_dirs[0])\n",
    "all_met_clips.sort()\n",
    "all_met_clips[-193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = os.path.join(all_dirs[0], \"met_s06e15_seg02_clip_05\")\n",
    "os.path.exists(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq/met_frames/s06e15_seg02_clip_05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = dataset['train'][0]\n",
    "s2 = dataset['train'][1]\n",
    "s3 = dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a0': 'Penny means that the man is turning Amy on. ',\n",
       " 'a1': 'Penny means that her shoes are too tight. ',\n",
       " 'a2': 'Penny means that she ate too much. ',\n",
       " 'a3': 'Penny means that she is drunk. ',\n",
       " 'a4': \"Penny isn't sure what she means but it sounds good. \",\n",
       " 'answer_idx': 0,\n",
       " 'q': \"What does Penny mean when she says she knows what's causing Amy's condition?\",\n",
       " 'qid': 3,\n",
       " 'show_name': 'The Big Bang Theory',\n",
       " 'ts': '43.42-46.3',\n",
       " 'vid_name': 's04e10_seg01_clip_03'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TVQA_FRAMES_DIR = '/mnt/teton/vpt/data/benchmark_datasets/TVQA/march_28_uncompressed_frames/frames_hq'\n",
    "\n",
    "tvqa_train_to_path = {\n",
    "    \"House M.D.\": 'house_frames',\n",
    "    \"The Big Bang Theory\": 'bbt_frames',\n",
    "    \"Castle\": 'castle_frames',\n",
    "    \"How I Met You Mother\": 'met_frames',\n",
    "    \"Grey's Anatomy\": 'grey_frames',\n",
    "    \"Friends\": 'friends_frames',\n",
    "}\n",
    "\n",
    "vid_name_prefix_to_path = {\n",
    "    \"house\": 'house_frames',\n",
    "    \"castle\": 'castle_frames',\n",
    "    \"met\": 'met_frames',\n",
    "    \"grey\": 'grey_frames',\n",
    "    \"friends\": 'friends_frames',\n",
    "    \"\": 'bbt_frames',  # no prefix at all used for bbt, it's the \"default\"\n",
    "}\n",
    "\n",
    "def vid_name_to_frames_path(vid_name):\n",
    "  '''\n",
    "  Example:\n",
    "  vid_name: house_s02e05_seg02_clip_11\n",
    "  \n",
    "  show_name_path = house_frames\n",
    "  clip_name_path = s02e05_seg02_clip_11\n",
    "  \n",
    "  returns: /mnt/teton/vpt/data/benchmark_datasets/TVQA/uncompressed_frames/frames_hq/house_frames/s02e05_seg02_clip_11\n",
    "  '''\n",
    "\n",
    "  show_name, clip_name_path = vid_name.split('_', 1)[0], vid_name.split('_', 1)[1]\n",
    "  if show_name not in vid_name_prefix_to_path:\n",
    "    show_name_path = 'bbt_frames'\n",
    "    # clip_name_path = vid_name  # no prefix at all used for bbt. vid_name is actually the 'clip_name_path'\n",
    "  else:\n",
    "    show_name_path = vid_name_prefix_to_path[show_name]\n",
    "\n",
    "  frames_dir = os.path.join(TVQA_FRAMES_DIR, show_name_path, vid_name)\n",
    "  if not os.path.exists(frames_dir):\n",
    "    raise FileNotFoundError(f\"frames_dir {frames_dir} does not exist\")\n",
    "\n",
    "  return frames_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(vid_name_to_frames_path(s1['vid_name']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TVQA_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
