{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고문서: https://python.langchain.com/docs/modules/data_connection/document_transformers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분할기\n",
    "\n",
    "문서를 로드한 후에는 애플리케이션에 더 적합하도록 문서를 변형하고 싶을 때가 많습니다. 가장 간단한 예로, 긴 문서를 모델의 컨텍스트 창에 맞도록 작은 덩어리로 분할하고 싶을 수 있습니다.\n",
    "\n",
    "LangChain에는 문서를 쉽게 분할, 결합, 필터링 및 기타 조작할 수 있는 여러 가지 문서 변환기가 내장되어 있습니다.\n",
    "\n",
    "긴 텍스트를 다루고 싶을 때는 해당 텍스트를 여러 조각으로 분할해야 합니다. 간단하게 들리지만 여기에는 많은 잠재적 복잡성이 있습니다. 이상적으로는 의미적으로 관련된 텍스트 조각을 함께 보관하는 것이 좋습니다. \"의미적으로 연관된\"이란 텍스트 유형에 따라 달라질 수 있습니다. 이 노트북에서는 이를 위한 몇 가지 방법을 보여드립니다.\n",
    "\n",
    "큰 틀에서 텍스트 분할기는 다음과 같이 작동합니다.\n",
    "\n",
    "- 텍스트를 의미적으로 의미 있는 작은 덩어리(주로 문장)로 나눕니다.\n",
    "- 특정 크기(특정 함수로 측정한 크기)에 도달할 때까지 이 작은 청크들을 더 큰 청크로 결합하기 시작합니다.\n",
    "- 그 크기에 도달하면 그 청크를 자체 텍스트 조각으로 만든 다음, 청크 사이의 맥락을 유지하기 위해 약간의 겹침이 있는 새로운 텍스트 청크를 만들기 시작합니다.\n",
    "\n",
    "즉, 텍스트 분할기를 사용자 지정할 수 있는 두 개의 다른 축이 있다는 뜻입니다.\n",
    "\n",
    "- 텍스트 분할 방식\n",
    "- 청크 크기를 측정하는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습에 활용한 문서\n",
    "\n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 문서 로드를 위한 다양한 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 일반적으로 많이 활용되는 방법이며, 대부분의 일반적인 PDF 파일을 문제 없이 불러 올 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① loader.load()\n",
    "\n",
    "문서를 페이지 단위로 불러옵니다. `document` 변수에는 페이지 별 `Document` 객체가 리스트 형태로 존재하며, **1개 Document == PDF 문서의 1개 페이지** 를 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일경로\n",
    "filepath = \"data/SPRI_AI_Brief_2023년12월호_F.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 23\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(filepath)\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "document = loader.load()\n",
    "\n",
    "print(f\"문서의 수: {len(document)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로드한 `document` 의 내용을 확인해 보면 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 년 12월호\n",
      "==============================\n",
      "{'source': 'data/SPRI_AI_Brief_2023년12월호_F.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# page_content 에는 본문의 내용이 있음\n",
    "print(document[0].page_content[:200])  # 일부내용 출력\n",
    "print(\"===\" * 10)\n",
    "# metadata 출력\n",
    "print(document[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 23\n"
     ]
    }
   ],
   "source": [
    "# PDF문서의 페이지 수 == len(document)\n",
    "print(f\"문서의 수: {len(document)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② TextSplitter 활용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서를 로드시 페이지 단위로 분리 저장하는 방법이 아닌, chunk_size 단위로 저장할 수 있는데, chunk_size 단위로 분리/저장하기 위해서는 다음과 같이 `TextSplitter` 를 활용할 수 있습니다.\n",
    "\n",
    "- `chunk_size`: 하나의 chunk(단위) 당 보관할 토큰 수\n",
    "- `chunk_overlap`: chunk 간 겹쳐지는 토큰의 개수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.pdf.PyPDFLoader object at 0x11e576890>\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(filepath)\n",
    "print(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharacterTextSplitter\n",
    "\n",
    "- [API 문서](https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.CharacterTextSplitter.html?highlight=charactertext#langchain.text_splitter.CharacterTextSplitter)\n",
    "\n",
    "가장 간단한 방법입니다. 이 방법은 문자(기본값은 \"\")를 기준으로 분할하고 문자 수에 따라 Chunk 의 길이를 측정합니다.\n",
    "\n",
    "- 텍스트 분할 방법: Character\n",
    "- Chunk 크기 측정 방법: Character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=0, separator=\".\")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다.안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은',\n",
       " '랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제 이름은 테디입니다.',\n",
       " '이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제 이름은',\n",
       " '제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요. 반갑습니다. 제',\n",
       " '반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다. 안녕하세요.',\n",
       " '안녕하세요. 반갑습니다. 제 이름은 테디입니다. 랭체인은 정말 좋은 프로젝트입니다.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=10, separator=\" \")\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterTextSplitter \t\t 사용시 문서의 수: 23\n"
     ]
    }
   ],
   "source": [
    "# splitter 정의\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# 문서 로드 및 분할 (load_and_split)\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"CharacterTextSplitter \\t\\t 사용시 문서의 수: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국, 안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 발표 \\nn미국 바이든 대통령이 ‘안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 ’에 서명하고 \\n광범위한 행정 조치를 명시\\nn행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 \\n보호 △노동자 지원 △혁신과 경쟁 촉진 △국제협력을 골자로 함KEY Contents\\n£바이든 대통령 , AI 행정명령 통해 안전하고 신뢰할 수 있는 AI 개발과 활용 추진\\nn미국 바이든 대통령이 2023년 10월 30일 연방정부 차원에서 안전하고 신뢰할 수 있는 AI 개발과 \\n사용을 보장하기 위한 행정명령을 발표\\n∙행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 보호 \\n△노동자 지원 △혁신과 경쟁 촉진 △국제협력에 관한 내용을 포괄\\nn(AI 안전과 보안 기준) 강력한 AI 시스템을 개발하는 기업에게 안전 테스트 결과와 시스템에 관한 \\n주요 정보를 미국 정부와 공유할 것을 요구하고 , AI 시스템의 안전성과 신뢰성 확인을 위한 표준 및 \\nAI 생성 콘텐츠 표시를 위한 표준과 모범사례 확립을 추진\\n∙△1026 플롭스 (FLOPS, Floating Point Operation Per Second) 를 초과하는 컴퓨팅 성능 또는 생물학적 \\n서열 데이터를 주로 사용하고 1023플롭스를 초과하는 컴퓨팅 성능을 사용하는 모델 △단일 데이터센터에서 \\n1,000Gbit/s 이상의 네트워킹으로 연결되며 AI 훈련에서 이론상 최대 1020 플롭스를 처리할 수 있는 \\n컴퓨팅 용량을 갖춘 컴퓨팅 클러스터가 정보공유 요구대상\\nn(형평성과 시민권 향상) 법률, 주택, 보건 분야에서 AI의 무책임한 사용으로 인한 차별과 편견 및 기타 \\n문제를 방지하는 조치를 확대\\n∙형사사법 시스템에서 AI 사용 모범사례를 개발하고 , 주택 임대 시 AI 알고리즘 차별을 막기 위한 명확한 \\n지침을 제공하며 , 보건복지 부문에서 책임 있는 AI 배포와 사용을 위한 전략을 마련 \\nn(소비자 보호와 근로자 지원) 의료 분야에서 책임 있는 AI 사용을 촉진하고 맞춤형 개인교습 등 학교 \\n내 AI 교육 도구 관련 자원을 개발하며 , AI로 인한 근로자 피해를 완화하고 이점을 극대화하는 원칙과 \\n모범사례를 마련\\nn(혁신과 경쟁 촉진) 국가AI연구자원 (National Artificial Intelligence Research Resource, NAIRR)* 을 \\n통해 미국 전역의 AI 연구를 촉진하고 , 중소기업과 개발자에 기술과 인프라를 지원\\n* 국가 차원에서 AI 연구 인프라를 확충해 더 많은 AI 연구자에게 인프라를 지원하는 프로그램 \\n∙비자 기준과 인터뷰 절차의 현대화와 간소화로 AI 관련 주요 분야의 전문 지식을 갖춘 외국인들이 미국에서 \\n공부하고 취업할 수 있도록 지원\\n☞ 출처 : The White House, Executive Order on the Safe, Secure, and Trustworthy Development and Use of \\nArtificial Intelligence (E.O. 14110), 2023.10.30.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterTextSplitter \t\t 사용시 문서의 수: 43\n"
     ]
    }
   ],
   "source": [
    "# splitter 정의\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50, separator=\"\\n\")\n",
    "\n",
    "# 문서 로드 및 분할 (load_and_split)\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"CharacterTextSplitter \\t\\t 사용시 문서의 수: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 정책/법제  2. 기업/산업 3. 기술/연구  4. 인력/교육\\n미국, 안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 발표 \\nn미국 바이든 대통령이 ‘안전하고 신뢰할 수 있는 AI 개발과 사용에 관한 행정명령 ’에 서명하고 \\n광범위한 행정 조치를 명시\\nn행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 \\n보호 △노동자 지원 △혁신과 경쟁 촉진 △국제협력을 골자로 함KEY Contents\\n£바이든 대통령 , AI 행정명령 통해 안전하고 신뢰할 수 있는 AI 개발과 활용 추진\\nn미국 바이든 대통령이 2023년 10월 30일 연방정부 차원에서 안전하고 신뢰할 수 있는 AI 개발과 \\n사용을 보장하기 위한 행정명령을 발표\\n∙행정명령은 △AI의 안전과 보안 기준 마련 △개인정보보호 △형평성과 시민권 향상 △소비자 보호 \\n△노동자 지원 △혁신과 경쟁 촉진 △국제협력에 관한 내용을 포괄\\nn(AI 안전과 보안 기준) 강력한 AI 시스템을 개발하는 기업에게 안전 테스트 결과와 시스템에 관한 \\n주요 정보를 미국 정부와 공유할 것을 요구하고 , AI 시스템의 안전성과 신뢰성 확인을 위한 표준 및 \\nAI 생성 콘텐츠 표시를 위한 표준과 모범사례 확립을 추진\\n∙△1026 플롭스 (FLOPS, Floating Point Operation Per Second) 를 초과하는 컴퓨팅 성능 또는 생물학적 \\n서열 데이터를 주로 사용하고 1023플롭스를 초과하는 컴퓨팅 성능을 사용하는 모델 △단일 데이터센터에서 \\n1,000Gbit/s 이상의 네트워킹으로 연결되며 AI 훈련에서 이론상 최대 1020 플롭스를 처리할 수 있는 \\n컴퓨팅 용량을 갖춘 컴퓨팅 클러스터가 정보공유 요구대상\\nn(형평성과 시민권 향상) 법률, 주택, 보건 분야에서 AI의 무책임한 사용으로 인한 차별과 편견 및 기타 \\n문제를 방지하는 조치를 확대\\n∙형사사법 시스템에서 AI 사용 모범사례를 개발하고 , 주택 임대 시 AI 알고리즘 차별을 막기 위한 명확한'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "\n",
    "- [참고문서](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
    "- [API 문서](https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html?highlight=charactertext)\n",
    "\n",
    "이 텍스트 분할기는 일반 텍스트에 권장되는 텍스트 분할기입니다. 문자 목록으로 매개변수화됩니다. 청크가 충분히 작아질 때까지 순서대로 분할을 시도합니다. 기본 목록은 `[\"\\n\\n\", \"\\n\", \" \", \"\"]` 입니다.\n",
    "\n",
    "이렇게 하면 일반적으로 의미적으로 가장 연관성이 강한 텍스트 조각으로 보이는 모든 단락(그리고 문장, 단어)을 가능한 한 길게 유지하려는 효과가 있습니다.\n",
    "\n",
    "- 텍스트를 분할하는 방법: **list of characters**\n",
    "- Chunk 크기 측정 방법: **number of characters**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveCharacterTextSplitter \t 사용시 문서의 수: 43\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# 문서 로드 및 분할 (load_and_split)\n",
    "split_doc = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"RecursiveCharacterTextSplitter \\t 사용시 문서의 수: {len(split_doc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# splitter 정의\u001b[39;00m\n\u001b[1;32m      4\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m TokenTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241m.\u001b[39mload_and_split(text_splitter\u001b[38;5;241m=\u001b[39mtext_splitter)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenTextSplitter \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m 사용시 문서의 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(split_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# splitter 정의\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "print(f\"TokenTextSplitter \\t\\t 사용시 문서의 수: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_docs[10].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
