{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "698e8919-31fb-4da5-8db3-ed0313944502",
   "metadata": {},
   "source": [
    "# dask-awkward IO Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674364a-c9d2-434f-97c7-b27d703f75bf",
   "metadata": {},
   "source": [
    "_last updated 2024-01-26_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f369935-c703-43d8-8c77-64fe94b391a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">package versions:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "package versions:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">awkward:       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "awkward:       \u001b[1;36m2.5\u001b[0m.\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">dask-awkward:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024.1</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.</span>dev6+gda37bea\n",
       "</pre>\n"
      ],
      "text/plain": [
       "dask-awkward:  \u001b[1;36m2024.1\u001b[0m.\u001b[1;36m3.\u001b[0mdev6+gda37bea\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import awkward\n",
    "import dask_awkward\n",
    "print(\"package versions:\")\n",
    "print(f\"awkward:       {awkward.__version__}\")\n",
    "print(f\"dask-awkward:  {dask_awkward.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f366f-8fad-477e-b872-9e1af0f8012e",
   "metadata": {},
   "source": [
    "## I. The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455b808-73e0-4e3b-ac70-9efbc2c2b555",
   "metadata": {},
   "source": [
    "### Data on disk\n",
    "\n",
    "`dask-awkward` supports a number of file formats out-of-the-box. Those include:\n",
    "\n",
    "- Parquet\n",
    "- JSON\n",
    "- Plain text\n",
    "\n",
    "\n",
    "> _Note_: The [uproot project](https://github.com/scikit-hep/uproot5) provides the `uproot.dask` module for reading the ROOT file format into dask-awkward collections. This tutorial will focus on file formats that have support baked into `dask-awkward`.\n",
    "\n",
    "Since this is Dask, data on disk is _staged_ for reading when using a dask-awkward read function. Data will not be read until a `compute` or (`persist`) call is reached.\n",
    "\n",
    "**Note**: we can also create dask-awkward `Array` collections from other Dask collections (bag, array, dataframe, delayed) or from data that already exists in memory (awkward arrays and Python lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638e10e-8b65-40e7-a487-fdfa32f956e9",
   "metadata": {},
   "source": [
    "Let's jump into a quick example reading a Parquet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6245c24-48c6-4607-a9ba-4f80d29e74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "pq_dir = os.path.join(\"data\", \"parquet\")\n",
    "dataset = dak.from_parquet(pq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf75914-471b-4688-b9d5-0ed2ce3bad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.awkward<from-parquet, npartitions=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa0243-d1bf-4d7f-a317-bea7090ce32a",
   "metadata": {},
   "source": [
    "By default the `dak.from_parquet` function will partition by file. Since the directory has four files, we will have four partitions in our Dask collection.\n",
    "\n",
    "Without computing anything, the `from_parquet` function has extracted some metadata, so we are able to get a peek at the structure of what the eventual awkward array would be if we did compute this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91228e47-838a-42bb-a2e5-be9b47a6683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[...]\n",
       "-------------------------\n",
       "type: ## * {\n",
       "    type: string,\n",
       "    scoring: var * {\n",
       "        player: string,\n",
       "        basket: string,\n",
       "        distance: float64\n",
       "    }\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array-typetracer [...] type='## * {type: string, scoring: var * {player: s...'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e35098-1790-4cf4-9543-d2bc5cc94908",
   "metadata": {},
   "source": [
    "You'll notice this dataset has two top level fields:\n",
    "- `type`\n",
    "- `scoring`\n",
    "\n",
    "Inside of the `scoring` field (or Parquet column) are three subfields (`scoring` is a `Record` in awkward array terminology):\n",
    "\n",
    "- `player`\n",
    "- `basket`\n",
    "- `distance`\n",
    "\n",
    "We can also see that for each element in the top level array, we have exactly one entry for the `type` field, and some variable (showing array raggedness) number of `scoring` entries.\n",
    "\n",
    "The data we have here is some made up data about basketball games/matches. Each game is labeled as either a \"friendly\" match or a \"league\" match. Each game has some number of total scores, each score being made by some player as some type of basket at some distance. The raggedness of the array comes from each match having a different total number of scores.\n",
    "\n",
    "Since this first section of the tutorial is meant to show the basics of the IO functions, we won't worry too much about the details of the dataset, but we will revisit the structure in the next section!\n",
    "\n",
    "Since this tutorial is using a small toy dataset we can easily compute it quickly to see a concrete awkward array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a519e29-2f87-47cd-8490-efa8a19eada5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computed_dataset = dataset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b148260-ae3a-4c7c-b666-bd0263d14ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[{type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " ...,\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;friendly&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]},\n",
       " {type: &#x27;league&#x27;, scoring: [{...}, ..., {...}]}]\n",
       "--------------------------------------------------\n",
       "type: 200 * {\n",
       "    type: string,\n",
       "    scoring: var * {\n",
       "        player: string,\n",
       "        basket: string,\n",
       "        distance: float64\n",
       "    }\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array [{type: 'league', ...}, ..., {...}] type='200 * {type: string, scori...'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581deaea-e157-4e08-9cb7-49de574a009c",
   "metadata": {},
   "source": [
    "With parquet, we can restrict our data reading to only grab a specific set of columns from the files. In this toy dataset we're working with, if we only care about the specific players which did some scoring, we can specific that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cab1ce4-d972-4c25-97dd-fe58fa9f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_parquet(pq_dir, columns=[\"scoring.player\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ca01b0-546c-4569-96b5-bed86a003648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[...]\n",
       "----------------------\n",
       "type: ## * {\n",
       "    scoring: var * {\n",
       "        player: string\n",
       "    }\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array-typetracer [...] type='## * {scoring: var * {player: string}}'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed602dd-b5cc-4535-937e-8d153272bd0d",
   "metadata": {},
   "source": [
    "Notice that when we peek at the metadata now, we see our array is going to contain less information, as expected! If we tied to access one of the fields we didn't request, we'd hit an `AttributeError` (before compute time!). Since we are able to track metadata at graph construction time, we can fail as early as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63742ac7-c18d-4801-9fda-6030a0c4e164",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "distance not in fields.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/software/repos/dask-awkward/src/dask_awkward/lib/core.py:1508\u001b[0m, in \u001b[0;36mArray.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     cls_method \u001b[38;5;241m=\u001b[39m \u001b[43mgetattr_static\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/inspect.py:1853\u001b[0m, in \u001b[0;36mgetattr_static\u001b[0;34m(obj, attr, default)\u001b[0m\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m-> 1853\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: distance",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance\u001b[49m\n",
      "File \u001b[0;32m~/software/repos/dask-awkward/src/dask_awkward/lib/core.py:1510\u001b[0m, in \u001b[0;36mArray.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     cls_method \u001b[38;5;241m=\u001b[39m getattr_static(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta, attr)\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in fields.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cls_method, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dask_get\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: distance not in fields."
     ]
    }
   ],
   "source": [
    "dataset.scoring.distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab1067-fa0f-46df-b055-27463795d271",
   "metadata": {},
   "source": [
    "Let's go back to the original dataset and save it to JSON after repartitioning the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d11eb1-f822-4fc9-ae0b-c10fb6c8ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_parquet(pq_dir)\n",
    "smaller_partition_dataset = dataset.repartition(15)\n",
    "dak.to_json(smaller_partition_dataset, os.path.join(\"data\", \"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17805aaa-26c4-4c2d-8664-bbe842d87c56",
   "metadata": {},
   "source": [
    "`dask-awkward`'s `to_*` functions have a bit of special treatmeant compared to other dask-awkward functions. They are the only parts of dask-awkward that are _eagerly_ computed. The `to_*` functions have a `compute=` argument that defaults to `True`. If you'd like to stage a data writing step without compute, you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad88a084-6d83-4eb7-a4a8-7befe58543d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_it = dak.to_json(smaller_partition_dataset, os.path.join(\"data\", \"json2\"), compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532776bb-0789-45d0-9bd8-d108d5143f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.awkward<to-json, type=Scalar, dtype=float64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b9879-c0d6-43a3-ad91-5f32209617ee",
   "metadata": {},
   "source": [
    "Notice that the `write_it` object is a dask-awkward `Scalar` collection that can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5d00ee-2ec1-455e-b0e8-4c64f6e8d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_it.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8e243-9b83-4165-8073-9021e835ba09",
   "metadata": {},
   "source": [
    "Now we can reload our data with `dak.from_json`. Realistically, taking data stored in parquet to then save it as JSON to be read later is likely a bad idea! But we're just doing this to show example usage of the dask-awkward API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a59ea8ad-8ca6-444c-86cd-a4a4d9fc853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_json(os.path.join(\"data\", \"json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60ef65b6-793e-40df-b2fa-f8c74b2ee8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.awkward<from-json-files, npartitions=15>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eddfcb-ab19-44dd-b8e5-f72b73716aad",
   "metadata": {},
   "source": [
    "## II. Column (buffer) optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb43949-40bd-4fdf-bd80-99c0a7b9376e",
   "metadata": {},
   "source": [
    "Dask workflows can be separated into two stages: first is task graph construction, and second is task graph execution. During task graph construction we are able to track metadata about our awkward array collections; with that metadata knowledge we are able, just before execution time, to know which parts of the Array are necessary to complete a computation. This is possible by running the task graph on a metadata only version of the arrays. When we run the metadata task graph, components of the data-less array are \"touched\" by the execution of the graph, and when that happens we know that's a part of the data on disk that needs to be read. \n",
    "\n",
    "Let's look at a quick example with Parquet. Recall the dataset from the previou section. We have these columns:\n",
    "\n",
    "- `type`\n",
    "- `scoring.player`\n",
    "- `scoring.basket`\n",
    "- `scoring.distance`\n",
    "\n",
    "If we want to calculate the average distance of each scored basket during each game, ignoreing all freethrows, we can calculate that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62fcb593-63d5-4444-9d26-d0e23258f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_parquet(pq_dir)\n",
    "free_throws = dak.str.match_substring(dataset.scoring.basket, \"freethrow\")\n",
    "distances = dataset.scoring.distance[free_throws == False]\n",
    "result = dak.mean(distances, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a0003-f538-4eac-a4d9-15006e6fdc7f",
   "metadata": {},
   "source": [
    "The `result` will be the average distance of each non-free-throw shot. Notice we only used two of the four columns: `scoring.basket` and `scoring.distance`, If we wanted to be explicit about it, we could use the `columns=` argument in the `dak.from_parquet` call. But we can also just rely on dask-awkward to do this for us! The columns/buffer optimization will detect that the graph is only going to need those columns, rewriting the internal `ak.from_parquet` call at the node in the task graph that actually reads the data from disk. We can actually see this logic without running the compute with the `dak.necessary_columns` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840b3ebe-1454-4dca-bee0-50a31f9c0df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from-parquet-b7916bd949c3744cf0ec38dea00d0bd6': frozenset({'scoring.basket',\n",
       "            'scoring.distance'})}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dak.necessary_columns(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4e54c-33c1-42a4-9b0f-81bf6348b6d1",
   "metadata": {},
   "source": [
    "We see the name of the input layer, and the names of the columns that are going to be read by that input layer.\n",
    "\n",
    "This will also work with JSON. Awkward-Array's `from_json` has a feature that allows users to pass in a JSONSchema that instructs the reader which parts of the JSON dataset should be read. The reader still has to process all of the bytes in the text based file format but with a schema declared, the reader can intelligently skip over different keys in the JSON, saving memory and and time during array building.\n",
    "\n",
    "Here's the same computation but starting with a JSON dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13914bf9-1f45-4860-8dc7-ec8eeb746bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dak.from_json(os.path.join(\"data\", \"json\"))\n",
    "free_throws = dak.str.match_substring(dataset.scoring.basket, \"freethrow\")\n",
    "distances = dataset.scoring.distance[free_throws == False]\n",
    "result = dak.mean(distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bc6e94b-ee5e-42d1-b789-6f80859b1d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from-json-files-6eebaf87f3a09a08c1234137dd381b61': frozenset({'scoring.basket',\n",
       "            'scoring.distance'})}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dak.necessary_columns(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9380f6-c4d4-4a2b-bb99-5e15e1da9039",
   "metadata": {},
   "source": [
    "We see the exact same necessary columns.\n",
    "\n",
    "A final little detail. The way that we generate the JSON schema which is then passed to the reading node is with `dak.layout_to_jsonschema`. Once the column/buffer optimization has determined which are the fields will be necessary, we can select those fields from the awkward array form that we start with after the `dak.from_json` call. We then generate an awkward array layout from the sub-form generated by selecting a subset of the columns. Finally, we create a JSONSchema from that layout:\n",
    "\n",
    "In our small example case here, we know the columns are `scoring.basket` and `scoring.distance`. We can show this step manually here (starting with the first array collection created with the `dak.from_json call):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e444fa35-03ee-4292-8730-490dacd145fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the subform based on the columns we need:\n",
    "subform = dataset.form.select_columns([\"scoring.basket\", \"scoring.distance\"])\n",
    "# create an awkward array layout:\n",
    "sublayout = subform.length_zero_array(highlevel=False)\n",
    "# and convert that to JSONSchema:\n",
    "necessary_schema = dak.layout_to_jsonschema(sublayout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "146a84b4-26ce-45c5-ad16-9c8967b60214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'untitled',\n",
       " 'description': 'Auto generated by dask-awkward',\n",
       " 'type': 'object',\n",
       " 'properties': {'scoring': {'type': 'array',\n",
       "   'items': {'type': 'object',\n",
       "    'properties': {'basket': {'type': 'string'},\n",
       "     'distance': {'type': 'number'}}}}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "necessary_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0952470-5557-4dca-919b-84970788cfdd",
   "metadata": {},
   "source": [
    "This feature can be turned off when running dask-awkward graphs with the config parameter \"awkward.optimization.enabled\". By default this setting is `True`. We can run the same compute with the feature turned off via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2d75df4-c8a7-4abd-942c-f1e94c124ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "with dask.config.set({\"awkward.optimization.enabled\": False}):\n",
    "    result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b953ff-8edf-46a6-b069-c2aa6f802485",
   "metadata": {},
   "source": [
    "This could be useful for debugging. If the compute fails with the optimization enabled, but succeeds with the optimization disabled, then there is likely a bug in dask-awkward or awkward-array that should be raised!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
