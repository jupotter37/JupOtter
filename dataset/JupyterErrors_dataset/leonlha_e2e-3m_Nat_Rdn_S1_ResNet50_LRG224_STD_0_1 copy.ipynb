{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12877209380996894173\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10977044071\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8230961568352886679\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10977044071\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 3072917814300508055\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10977044071\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 5277878442475201630\n",
      "physical_device_desc: \"device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10975051776\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 11795311730991270435\n",
      "physical_device_desc: \"device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"paulnh\" # username from the json file \n",
    "os.environ['KAGGLE_KEY'] = \"9e06528f4d63aefff0897e8d0b289fe7\" # key from the json file\n",
    "!kaggle competitions download -c inaturalist-2019-fgvc6 # api copied from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxf train_val2019.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxf test2019.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir test_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv test2019 test_2019/0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm train_val2019.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd test2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l|head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####=======================\n",
    "# #### Move back images\n",
    "# #####\n",
    "\n",
    "# import csv\n",
    "# import glob\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# #move color files  \n",
    "# #divide 2-2 randomly\n",
    "\n",
    "# # # get parts of image's path\n",
    "# def get_image_parts(image_path):\n",
    "#     \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "#     parts = image_path.split(os.path.sep)\n",
    "#     #print(parts)\n",
    "#     filename = parts[2]\n",
    "#     filename_no_ext = filename.split('.')[0]\n",
    "#     classname = parts[1]\n",
    "# #     sub_folder = parts[1]\n",
    "#     train_or_test = parts[0]\n",
    "\n",
    "#     return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "# move_folders = ['valid_edited']\n",
    "# dest_folder = 'train_edited'\n",
    "# data_file = []\n",
    "\n",
    "# # look for all images in sub-folders\n",
    "# for folder in move_folders:\n",
    "#     class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "#     print('folder %s' %class_folders)\n",
    "    \n",
    "# #     for sub_folder in class_folders:\n",
    "# #         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "# #         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "#     for iid_class in class_folders:\n",
    "#         class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "# #             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "#         #Determize Set# (No Suffle)\n",
    "#         set = len(class_files)# all images\n",
    "\n",
    "#         inner = range(0*set, 1*set)\n",
    "\n",
    "#         print('moving %d files' %(len(inner)))\n",
    "\n",
    "# #             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "# #             for idx in range(len(random_list)):\n",
    "\n",
    "#         for idx in range(len(inner)):\n",
    "#             src = class_files[inner[idx]]\n",
    "\n",
    "#             #record paths\n",
    "# #                 data_file.append(src)\n",
    "\n",
    "#             train_or_test, classname, _, filename = get_image_parts(src)\n",
    "#             dst = os.path.join(dest_folder, classname, filename) #\n",
    "\n",
    "#             # image directory\n",
    "#             img_directory = os.path.join(dest_folder, classname)\n",
    "\n",
    "#             # create folder if not existed\n",
    "#             if not os.path.exists(img_directory):\n",
    "#                 os.makedirs(img_directory)\n",
    "\n",
    "#             #moving file\n",
    "#             shutil.move(src, dst)\n",
    "# #                 shutil.copy(src, dst)\n",
    "\n",
    "# # #save recorded paths\n",
    "# # with open('data_file.csv', 'w') as fout:\n",
    "# #     writer = csv.writer(fout)\n",
    "# #     writer.writerows(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####=======================\n",
    "# #### Move back images\n",
    "# #####\n",
    "\n",
    "# import csv\n",
    "# import glob\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# #move color files  \n",
    "# #divide 2-2 randomly\n",
    "\n",
    "# # # get parts of image's path\n",
    "# def get_image_parts(image_path):\n",
    "#     \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "#     parts = image_path.split(os.path.sep)\n",
    "#     #print(parts)\n",
    "#     filename = parts[2]\n",
    "#     filename_no_ext = filename.split('.')[0]\n",
    "#     classname = parts[1]\n",
    "# #     sub_folder = parts[1]\n",
    "#     train_or_test = parts[0]\n",
    "\n",
    "#     return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "# move_folders = ['train_rnd']\n",
    "# dest_folder = 'train_edited'\n",
    "# data_file = []\n",
    "\n",
    "# # look for all images in sub-folders\n",
    "# for folder in move_folders:\n",
    "#     class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "#     print('folder %s' %class_folders)\n",
    "    \n",
    "# #     for sub_folder in class_folders:\n",
    "# #         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "# #         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "#     for iid_class in class_folders:\n",
    "#         class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "# #             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "#         #Determize Set# (No Suffle)\n",
    "#         set = len(class_files)# all images\n",
    "\n",
    "#         inner = range(0*set, 1*set)\n",
    "\n",
    "#         print('moving %d files' %(len(inner)))\n",
    "\n",
    "# #             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "# #             for idx in range(len(random_list)):\n",
    "\n",
    "#         for idx in range(len(inner)):\n",
    "#             src = class_files[inner[idx]]\n",
    "\n",
    "#             #record paths\n",
    "# #                 data_file.append(src)\n",
    "\n",
    "#             train_or_test, classname, _, filename = get_image_parts(src)\n",
    "#             dst = os.path.join(dest_folder, classname, filename) #\n",
    "\n",
    "#             # image directory\n",
    "#             img_directory = os.path.join(dest_folder, classname)\n",
    "\n",
    "#             # create folder if not existed\n",
    "#             if not os.path.exists(img_directory):\n",
    "#                 os.makedirs(img_directory)\n",
    "\n",
    "#             #moving file\n",
    "#             shutil.move(src, dst)\n",
    "# #                 shutil.copy(src, dst)\n",
    "\n",
    "# # #save recorded paths\n",
    "# # with open('data_file.csv', 'w') as fout:\n",
    "# #     writer = csv.writer(fout)\n",
    "# #     writer.writerows(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####=======================\n",
    "# import glob\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# #move color files  \n",
    "# #divide 2-2 randomly\n",
    "\n",
    "# # # get parts of image's path\n",
    "# def get_image_parts(image_path):\n",
    "#     \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "#     parts = image_path.split(os.path.sep)\n",
    "#     #print(parts)\n",
    "#     filename = parts[3]\n",
    "#     filename_no_ext = filename.split('.')[0]\n",
    "#     classname = parts[2]\n",
    "#     sub_folder = parts[1]\n",
    "#     train_or_test = parts[0]\n",
    "\n",
    "#     return train_or_test, sub_folder, classname, filename_no_ext, filename\n",
    "\n",
    "# move_folders = ['train_val2019']\n",
    "\n",
    "# # look for all images in sub-folders\n",
    "# for folder in move_folders:\n",
    "#     class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "#     print('folder %s' %class_folders)\n",
    "    \n",
    "#     for sub_folder in class_folders:\n",
    "#         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "#         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "#         for iid_class in sub_class_folders:\n",
    "#             class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "#             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "# #             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "#             random_list = range(0,len(class_files)) #All dataset\n",
    "    \n",
    "#             for idx in range(len(random_list)):\n",
    "#                 src = class_files[random_list[idx]]\n",
    "\n",
    "#                 train_or_test, sub_folder, classname, _, filename = get_image_parts(src)\n",
    "#                 dst = os.path.join('train_edited', sub_folder+'_'+classname, train_or_test+'_'+ filename)\n",
    "                \n",
    "#                 # image directory\n",
    "#                 img_directory = os.path.join('train_edited', sub_folder+'_'+classname)\n",
    "\n",
    "#                 # create folder if not existed\n",
    "#                 if not os.path.exists(img_directory):\n",
    "#                     os.makedirs(img_directory)\n",
    "\n",
    "#                 #moving file\n",
    "#                 shutil.move(src, dst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####=======================\n",
    "# import glob\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# #move color files  \n",
    "# #divide 2-2 randomly\n",
    "\n",
    "# # # get parts of image's path\n",
    "# def get_image_parts(image_path):\n",
    "#     \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "#     parts = image_path.split(os.path.sep)\n",
    "#     #print(parts)\n",
    "#     filename = parts[2]\n",
    "#     filename_no_ext = filename.split('.')[0]\n",
    "#     classname = parts[1]\n",
    "# #     sub_folder = parts[1]\n",
    "#     train_or_test = parts[0]\n",
    "\n",
    "#     return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "# move_folders = ['train_edited']\n",
    "# dest_folder = 'valid_edited'\n",
    "# data_file = []\n",
    "\n",
    "# # look for all images in sub-folders\n",
    "# for folder in move_folders:\n",
    "#     class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "#     print('folder %s' %class_folders)\n",
    "    \n",
    "# #     for sub_folder in class_folders:\n",
    "# #         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "# #         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "#     for iid_class in class_folders:\n",
    "#         class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "# #             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "#         #Determize Set# (No Suffle)\n",
    "#         set = len(class_files)//5\n",
    "\n",
    "#         inner = range(2*set, 3*set) #Set 3\n",
    "\n",
    "#         print('moving %d files' %(len(inner)))\n",
    "\n",
    "# #             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "# #             for idx in range(len(random_list)):\n",
    "\n",
    "#         for idx in range(len(inner)):\n",
    "#             src = class_files[inner[idx]]\n",
    "\n",
    "#             #record paths\n",
    "# #                 data_file.append(src)\n",
    "\n",
    "#             train_or_test, classname, _, filename = get_image_parts(src)\n",
    "#             dst = os.path.join(dest_folder, classname, train_or_test+'_'+ filename)\n",
    "\n",
    "#             # image directory\n",
    "#             img_directory = os.path.join(dest_folder, classname)\n",
    "\n",
    "#             # create folder if not existed\n",
    "#             if not os.path.exists(img_directory):\n",
    "#                 os.makedirs(img_directory)\n",
    "\n",
    "#             #moving file\n",
    "#             shutil.move(src, dst)\n",
    "# #                 shutil.copy(src, dst)\n",
    "\n",
    "# # #save recorded paths\n",
    "# # with open('data_file.csv', 'w') as fout:\n",
    "# #     writer = csv.writer(fout)\n",
    "# #     writer.writerows(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####=======================\n",
    "# #### Move files back\n",
    "\n",
    "# #####======================\n",
    "# import glob\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# #move color files  \n",
    "# #divide 2-2 randomly\n",
    "\n",
    "# # # get parts of image's path\n",
    "# def get_image_parts(image_path):\n",
    "#     \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "#     parts = image_path.split(os.path.sep)\n",
    "#     #print(parts)\n",
    "#     filename = parts[2]\n",
    "#     filename_no_ext = filename.split('.')[0]\n",
    "#     classname = parts[1]\n",
    "# #     sub_folder = parts[1]\n",
    "#     train_or_test = parts[0]\n",
    "\n",
    "#     return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "# move_folders = ['valid_edited']\n",
    "# dest_folders = 'train_edited'\n",
    "\n",
    "# data_file = []\n",
    "\n",
    "# # look for all images in sub-folders\n",
    "# for folder in move_folders:\n",
    "#     class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "#     print('folder %s' %class_folders)\n",
    "    \n",
    "# #     for sub_folder in class_folders:\n",
    "# #         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "# #         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "#     for iid_class in class_folders:\n",
    "#         class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "# #             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "#         #Move all files back\n",
    "#         set = len(class_files)\n",
    "\n",
    "#         inner = range(0, set) \n",
    "\n",
    "#         print('moving %d files' %(len(inner)))\n",
    "\n",
    "# #             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "# #             for idx in range(len(random_list)):\n",
    "\n",
    "#         for idx in range(len(inner)):\n",
    "#             src = class_files[inner[idx]]\n",
    "\n",
    "#             #record paths\n",
    "# #                 data_file.append(src)\n",
    "\n",
    "#             train_or_test, classname, _, filename = get_image_parts(src)\n",
    "#             dst = os.path.join(dest_folders, classname, train_or_test+'_'+ filename)\n",
    "\n",
    "#             # image directory\n",
    "#             img_directory = os.path.join(dest_folders, classname)\n",
    "\n",
    "#             # create folder if not existed\n",
    "#             if not os.path.exists(img_directory):\n",
    "#                 os.makedirs(img_directory)\n",
    "\n",
    "#             #moving file\n",
    "#             shutil.move(src, dst)\n",
    "# #                 shutil.copy(src, dst)\n",
    "\n",
    "# # #save recorded paths\n",
    "# # with open('data_file.csv', 'w') as fout:\n",
    "# #     writer = csv.writer(fout)\n",
    "# #     writer.writerows(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir train_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####=======================\n",
    "\n",
    "\n",
    "######\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "#move color files  \n",
    "\n",
    "# # get parts of image's path\n",
    "def get_image_parts(image_path):\n",
    "    \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "    parts = image_path.split(os.path.sep)\n",
    "    #print(parts)\n",
    "    filename = parts[2]\n",
    "    filename_no_ext = filename.split('.')[0]\n",
    "    classname = parts[1]\n",
    "#     sub_folder = parts[1]\n",
    "    train_or_test = parts[0]\n",
    "\n",
    "    return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "move_folders = ['train_edited']\n",
    "dest_folder = 'train_rnd'\n",
    "\n",
    "data_file =[]\n",
    "\n",
    "# look for all images in sub-folders\n",
    "for folder in move_folders:\n",
    "    class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "#     print('folder %s' %class_folders)\n",
    "    \n",
    "#     for sub_folder in class_folders:\n",
    "#         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "#         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "    for iid_class in class_folders:\n",
    "        class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "        print('moving %d files' %(len(class_files)))\n",
    "\n",
    "#             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "        random_list = range(0,len(class_files)) #All dataset\n",
    "\n",
    "        #suffle list\n",
    "        suffle_list = random.sample(range(len(class_files)), int(len(class_files)))\n",
    "\n",
    "#         print(len(random_list))\n",
    "        for idx in range(len(random_list)):\n",
    "            src = class_files[random_list[idx]]\n",
    "\n",
    "            train_or_test, classname, _, filename = get_image_parts(src)\n",
    "            dst = os.path.join(dest_folder, classname, '{:07d}'.format(suffle_list[idx])+'_'+filename)\n",
    "\n",
    "            # image directory\n",
    "            img_directory = os.path.join(dest_folder, classname)\n",
    "\n",
    "            # create folder if not existed\n",
    "            if not os.path.exists(img_directory):\n",
    "                os.makedirs(img_directory)\n",
    "\n",
    "            # moving file\n",
    "            shutil.move(src, dst) \n",
    "\n",
    "            #recording\n",
    "            data_file.append([src, dst])\n",
    "\n",
    "with open('nat_data_file.csv', 'w') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerows(data_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir valid_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####=======================\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "#move color files  \n",
    "#divide 2-2 randomly\n",
    "\n",
    "# # get parts of image's path\n",
    "def get_image_parts(image_path):\n",
    "    \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "    parts = image_path.split(os.path.sep)\n",
    "    #print(parts)\n",
    "    filename = parts[2]\n",
    "    filename_no_ext = filename.split('.')[0]\n",
    "    classname = parts[1]\n",
    "#     sub_folder = parts[1]\n",
    "    train_or_test = parts[0]\n",
    "\n",
    "    return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "move_folders = ['train_rnd']\n",
    "dest_folder = 'valid_rnd'\n",
    "data_file = []\n",
    "\n",
    "# look for all images in sub-folders\n",
    "for folder in move_folders:\n",
    "    class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "    print('folder %s' %class_folders)\n",
    "    \n",
    "#     for sub_folder in class_folders:\n",
    "#         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "#         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "    for iid_class in class_folders:\n",
    "        class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "#             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "        #Determize Set# (No Suffle)\n",
    "        set = len(class_files)//5\n",
    "\n",
    "        inner = range(2*set, 3*set) #Set 3\n",
    "\n",
    "        print('moving %d files' %(len(inner)))\n",
    "\n",
    "#             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "#             for idx in range(len(random_list)):\n",
    "\n",
    "        for idx in range(len(inner)):\n",
    "            src = class_files[inner[idx]]\n",
    "\n",
    "            #record paths\n",
    "#                 data_file.append(src)\n",
    "\n",
    "            train_or_test, classname, _, filename = get_image_parts(src)\n",
    "            dst = os.path.join(dest_folder, classname, train_or_test+'_'+ filename)\n",
    "\n",
    "            # image directory\n",
    "            img_directory = os.path.join(dest_folder, classname)\n",
    "\n",
    "            # create folder if not existed\n",
    "            if not os.path.exists(img_directory):\n",
    "                os.makedirs(img_directory)\n",
    "\n",
    "            #moving file\n",
    "            shutil.move(src, dst)\n",
    "#                 shutil.copy(src, dst)\n",
    "\n",
    "# #save recorded paths\n",
    "# with open('data_file.csv', 'w') as fout:\n",
    "#     writer = csv.writer(fout)\n",
    "#     writer.writerows(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####=======================\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "#move color files  \n",
    "#divide 2-2 randomly\n",
    "\n",
    "# # get parts of image's path\n",
    "def get_image_parts(image_path):\n",
    "    \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "    parts = image_path.split(os.path.sep)\n",
    "    #print(parts)\n",
    "    filename = parts[2]\n",
    "    filename_no_ext = filename.split('.')[0]\n",
    "    classname = parts[1]\n",
    "#     sub_folder = parts[1]\n",
    "    train_or_test = parts[0]\n",
    "\n",
    "    return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "move_folders = ['valid_rnd']\n",
    "dest_folder = 'train_rnd'\n",
    "data_file = []\n",
    "\n",
    "# look for all images in sub-folders\n",
    "for folder in move_folders:\n",
    "    class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "    print('folder %s' %class_folders)\n",
    "    \n",
    "#     for sub_folder in class_folders:\n",
    "#         sub_class_folders = glob.glob(os.path.join(sub_folder, '*'))    \n",
    "#         print('sub folder %s' %sub_class_folders)\n",
    "        \n",
    "    for iid_class in class_folders:\n",
    "        class_files = glob.glob(os.path.join(iid_class, '*.jpg'))\n",
    "\n",
    "#             print('moving %d files' %(len(class_files)))\n",
    "\n",
    "        #Determize Set# (No Suffle)\n",
    "        set = len(class_files)//2 #move back 50% (using kfold of 20)\n",
    "\n",
    "        inner = range(0*set, 1*set) #Set 1\n",
    "\n",
    "        print('moving %d files' %(len(inner)))\n",
    "\n",
    "#             random_list = random.sample(range(len(class_files)), int(len(class_files))) #All dataset\n",
    "#             for idx in range(len(random_list)):\n",
    "\n",
    "        for idx in range(len(inner)):\n",
    "            src = class_files[inner[idx]]\n",
    "\n",
    "            #record paths\n",
    "#                 data_file.append(src)\n",
    "\n",
    "            train_or_test, classname, _, filename = get_image_parts(src)\n",
    "            dst = os.path.join(dest_folder, classname, filename)\n",
    "\n",
    "            # image directory\n",
    "            img_directory = os.path.join(dest_folder, classname)\n",
    "\n",
    "            # create folder if not existed\n",
    "            if not os.path.exists(img_directory):\n",
    "                os.makedirs(img_directory)\n",
    "\n",
    "            #moving file\n",
    "            shutil.move(src, dst)\n",
    "#                 shutil.copy(src, dst)\n",
    "\n",
    "# #save recorded paths\n",
    "# with open('data_file.csv', 'w') as fout:\n",
    "#     writer = csv.writer(fout)\n",
    "#     writer.writerows(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd train_edited/Amphibians_153/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l|head -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,##\n",
    "# #     brightness_range=[0.5, 1.5],##\n",
    "#     channel_shift_range=10,##\n",
    "#     fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('train_edited',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "valid_set = test_datagen.flow_from_directory('valid_edited',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.image import imread\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# get image parts\n",
    "def get_image_parts(image_path):\n",
    "    \"\"\"Given a full path to an image, return its parts.\"\"\"\n",
    "    parts = image_path.split(os.path.sep)\n",
    "    #print(parts)\n",
    "    filename = parts[2]\n",
    "    filename_no_ext = filename.split('.')[0]\n",
    "    classname = parts[1]\n",
    "    train_or_test = parts[0]\n",
    "    \n",
    "    return train_or_test, classname, filename_no_ext, filename\n",
    "    \n",
    "    \n",
    "sample_images = list(glob.glob(os.path.join('train_rnd/', '*/*'), recursive=True))\n",
    "np.random.seed(7)\n",
    "rand_imgs = np.random.choice(sample_images, size=6*7)\n",
    "fig, axarr = plt.subplots(6, 7, figsize=(20, 25))\n",
    "\n",
    "for i, rand_img in enumerate(rand_imgs):\n",
    "    train_or_test, classname, filename_no_ext, filename = get_image_parts(rand_img)\n",
    "    \n",
    "    j = i // 7\n",
    "    k = i % 7\n",
    "    axarr[j][k].imshow(imread(rand_img))\n",
    "    axarr[j][k].title.set_text(classname)\n",
    "    axarr[j][k].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras.applications.resnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bceeef7e9212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from keras.applications.inception_resnet_v2 import InceptionResNetV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from keras.applications import MobileNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras.applications.resnet'"
     ]
    }
   ],
   "source": [
    "#MUL 1 - Inception - ST\n",
    "\n",
    "# from keras.applications import InceptionV3\n",
    "# from keras.applications import Xception\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# from keras.applications import MobileNet\n",
    "from keras.applications.resnet import ResNet50\n",
    "\n",
    "from keras.models import Model\n",
    "# from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, Reshape, Concatenate,Bidirectional\n",
    "from keras.applications.resnet import preprocess_input\n",
    "\n",
    "# from keras.layers import GaussianNoise\n",
    "\n",
    "# f1_base = Xception(weights='imagenet', include_top=False, input_shape=(450,450,3))\n",
    "# f1_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
    "f1_base = ResNet50(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
    "f1_x = f1_base.output\n",
    "f1_x = GlobalAveragePooling2D()(f1_x)\n",
    "\n",
    "# f1_x = Reshape([1,2048])(f1_x)  \n",
    "\n",
    "# f1_x = Bidirectional(LSTM(1024, \n",
    "#                                  return_sequences=False, \n",
    "# #                                  dropout=0.8\n",
    "#                                 ),\n",
    "#                             input_shape=[1,2048],\n",
    "#                             merge_mode='concat')(f1_x)\n",
    "\n",
    "\n",
    "#Regularization with noise\n",
    "# f1_x = GaussianNoise(0.1)(f1_x)\n",
    "\n",
    "f1_x = Dense(4096, activation='relu')(f1_x)\n",
    "f1_x = Dense(1010, activation='softmax')(f1_x)\n",
    "model_1 = Model(inputs=[f1_base.input],outputs=[f1_x])\n",
    "\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, concatenate\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def multi_gpu_model(model, gpus):\n",
    "    if isinstance(gpus, (list, tuple)):\n",
    "        num_gpus = len(gpus)\n",
    "        target_gpu_ids = gpus\n",
    "    else:\n",
    "        num_gpus = gpus\n",
    "        target_gpu_ids = range(num_gpus)\n",
    "\n",
    "    def get_slice(data, i, parts):\n",
    "        shape = tf.shape(data)\n",
    "        batch_size = shape[:1]\n",
    "        input_shape = shape[1:]\n",
    "        step = batch_size // parts\n",
    "        if i == num_gpus - 1:\n",
    "            size = batch_size - step * i\n",
    "        else:\n",
    "            size = step\n",
    "        size = tf.concat([size, input_shape], axis=0)\n",
    "        stride = tf.concat([step, input_shape * 0], axis=0)\n",
    "        start = stride * i\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    all_outputs = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU,\n",
    "    # each getting a slice of the inputs.\n",
    "    for i, gpu_id in enumerate(target_gpu_ids):\n",
    "        with tf.device('/gpu:%d' % gpu_id):\n",
    "            with tf.name_scope('replica_%d' % gpu_id):\n",
    "                inputs = []\n",
    "                # Retrieve a slice of the input.\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_i = Lambda(get_slice,\n",
    "                                   output_shape=input_shape,\n",
    "                                   arguments={'i': i,\n",
    "                                              'parts': num_gpus})(x)\n",
    "                    inputs.append(slice_i)\n",
    "\n",
    "                # Apply model on slice\n",
    "                # (creating a model replica on the target device).\n",
    "                outputs = model(inputs)\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "\n",
    "                # Save the outputs for merging back together later.\n",
    "                for o in range(len(outputs)):\n",
    "                    all_outputs[o].append(outputs[o])\n",
    "\n",
    "    # Merge outputs on CPU.\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for name, outputs in zip(model.output_names, all_outputs):\n",
    "            merged.append(concatenate(outputs,\n",
    "                                    axis=0, name=name))\n",
    "        return Model(model.inputs, merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "# from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import multiprocessing\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,##\n",
    "#     brightness_range=[0.5, 1.5],##\n",
    "    channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "#     rescale = 1./255\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "NUM_GPU = 4\n",
    "batch_size = NUM_GPU * 96\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('train_rnd',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "valid_set = test_datagen.flow_from_directory('valid_rnd',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "# testing_set = test_datagen.flow_from_directory('test_edited',\n",
    "#                                                  target_size = (224, 224),\n",
    "#                                                  batch_size = batch_size,\n",
    "#                                                  class_mode = 'categorical',\n",
    "#                                                  shuffle=False,\n",
    "#                                                  seed=7,\n",
    "# #                                                  subset=\"validation\"\n",
    "#                                              )\n",
    "\n",
    "model_txt = 'bi-lstm'\n",
    "# Helper: Save the model.\n",
    "savedfilename = os.path.join('Nat', 'Nat_Rdn_S1_MobileNetV1_LRG224_STD_0_1.hdf5')\n",
    "\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_acc', verbose=1, \n",
    "                          save_best_only=True, mode='max',save_weights_only=True)\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('nat_output', 'logs', model_txt))\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('nat_output', 'logs', model_txt + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "class EarlyStoppingByAccVal(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.79, verbose=1)\n",
    "\n",
    "# print('Loading pretrained weights')\n",
    "# model.load_weights(savedfilename)\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    mul_model = multi_gpu_model(model_1, gpus=NUM_GPU)\n",
    "\n",
    "epochs = 40##!!!\n",
    "lr = 1e-5\n",
    "decay = lr/epochs\n",
    "optimizer = Adam(lr=lr, decay=decay)\n",
    "if NUM_GPU != 1:\n",
    "    mul_model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "else:\n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "step_size_train=ceil(training_set.n/training_set.batch_size)\n",
    "step_size_valid=ceil(valid_set.n/valid_set.batch_size)\n",
    "# step_size_test=ceil(testing_set.n//testing_set.batch_size)\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    result = mul_model.fit_generator(training_set,\n",
    "                              steps_per_epoch = step_size_train,\n",
    "                              epochs = epochs,\n",
    "                              validation_data = valid_set,\n",
    "                              validation_steps = step_size_valid,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[tb, csv_logger, checkpointer,earlystopping],\n",
    "                              max_queue_size=20,\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=10,\n",
    "                             )\n",
    "else:    \n",
    "    result = model.fit_generator(training_set,\n",
    "                              steps_per_epoch = step_size_train,\n",
    "                              epochs = epochs,\n",
    "                              validation_data = valid_set,\n",
    "                              validation_steps = step_size_valid,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[tb, csv_logger, checkpointer,earlystopping],\n",
    "                              max_queue_size=20,\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=10,\n",
    "                             )\n",
    "\n",
    "# loss, acc = model.evaluate_generator(testing_set, steps=step_size_test, verbose=2)\n",
    "# print('test loss = %f, accuracy: %f' %(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_acc', verbose=1, \n",
    "                          save_best_only=True, mode='max',save_weights_only=True)\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.75, verbose=1)\n",
    "\n",
    "# print('Loading pretrained weights')\n",
    "# model.load_weights(savedfilename)\n",
    "\n",
    "epochs = 8##!!!\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    result = mul_model.fit_generator(training_set,\n",
    "                              steps_per_epoch = step_size_train,\n",
    "                              epochs = epochs,\n",
    "                              validation_data = valid_set,\n",
    "                              validation_steps = step_size_valid,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[tb, csv_logger, checkpointer, earlystopping],\n",
    "                              max_queue_size=8,\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=4,\n",
    "                             )\n",
    "else:\n",
    "    result = model.fit_generator(training_set,\n",
    "                              steps_per_epoch = step_size_train,\n",
    "                              epochs = epochs,\n",
    "                              validation_data = valid_set,\n",
    "                              validation_steps = step_size_valid,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[tb, csv_logger, checkpointer,earlystopping],\n",
    "                              max_queue_size=8,\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=4,\n",
    "                             )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, concatenate\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def multi_gpu_model(model, gpus):\n",
    "    if isinstance(gpus, (list, tuple)):\n",
    "        num_gpus = len(gpus)\n",
    "        target_gpu_ids = gpus\n",
    "    else:\n",
    "        num_gpus = gpus\n",
    "        target_gpu_ids = range(num_gpus)\n",
    "\n",
    "    def get_slice(data, i, parts):\n",
    "        shape = tf.shape(data)\n",
    "        batch_size = shape[:1]\n",
    "        input_shape = shape[1:]\n",
    "        step = batch_size // parts\n",
    "        if i == num_gpus - 1:\n",
    "            size = batch_size - step * i\n",
    "        else:\n",
    "            size = step\n",
    "        size = tf.concat([size, input_shape], axis=0)\n",
    "        stride = tf.concat([step, input_shape * 0], axis=0)\n",
    "        start = stride * i\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    all_outputs = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU,\n",
    "    # each getting a slice of the inputs.\n",
    "    for i, gpu_id in enumerate(target_gpu_ids):\n",
    "        with tf.device('/gpu:%d' % gpu_id):\n",
    "            with tf.name_scope('replica_%d' % gpu_id):\n",
    "                inputs = []\n",
    "                # Retrieve a slice of the input.\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_i = Lambda(get_slice,\n",
    "                                   output_shape=input_shape,\n",
    "                                   arguments={'i': i,\n",
    "                                              'parts': num_gpus})(x)\n",
    "                    inputs.append(slice_i)\n",
    "\n",
    "                # Apply model on slice\n",
    "                # (creating a model replica on the target device).\n",
    "                outputs = model(inputs)\n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "\n",
    "                # Save the outputs for merging back together later.\n",
    "                for o in range(len(outputs)):\n",
    "                    all_outputs[o].append(outputs[o])\n",
    "\n",
    "    # Merge outputs on CPU.\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for name, outputs in zip(model.output_names, all_outputs):\n",
    "            merged.append(concatenate(outputs,\n",
    "                                    axis=0, name=name))\n",
    "        return Model(model.inputs, merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT ON OFFICIAL TEST\n",
    "test_datagen1 = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_set1 = test_datagen1.flow_from_directory('test_2019',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "if NUM_GPU != 1:\n",
    "    predict1=mul_model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "else:\n",
    "    predict1=model.predict_generator(test_set1, steps = ceil(test_set1.n/test_set1.batch_size),verbose=1)\n",
    "    \n",
    "predicted_class_indices=np.argmax(predict1,axis=1)\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions1 = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"file_name\":filenames,\n",
    "                      \"predicted1\":predictions1,\n",
    "                      })\n",
    "results.to_csv('Nat_Rdn_S1_MobileNetV1_LRG224_STD_0_1_0511_v1.csv')\n",
    "results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionResNetV2(weights='imagenet',include_top=False,input_shape=(421,421,3))\n",
    "# x = base_model.output\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Crop-Official Test\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "test_datagen_crop = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "testing_set_crop = test_datagen_crop.flow_from_directory('test_2019',\n",
    "                                                 target_size = (501, 501),\n",
    "                                                 batch_size = 1,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "#customized generator\n",
    "test_crops = crop_generator(testing_set_crop, 401)\n",
    "\n",
    "step_size_test_crop = ceil(testing_set_crop.n/testing_set_crop.batch_size)\n",
    "\n",
    "tta_steps = 15\n",
    "predictions = []\n",
    "\n",
    "# import tensorflow as tf\n",
    "# with tf.device('/gpu:0'):\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    testing_set_crop.reset()\n",
    "    if NUM_GPU != 1:\n",
    "        preds=mul_model.predict_generator(test_crops, \n",
    "                                           steps = step_size_test_crop,\n",
    "                                           max_queue_size=16,\n",
    "#                                                use_multiprocessing=True,\n",
    "                                           workers=1,\n",
    "                                           verbose=1)    \n",
    "    else:\n",
    "        preds=model.predict_generator(test_crops, \n",
    "                                           steps = step_size_test_crop,\n",
    "                                           max_queue_size=16,\n",
    "#                                                use_multiprocessing=True,\n",
    "                                           workers=1,\n",
    "                                           verbose=1)           \n",
    "    predictions.append(preds)\n",
    "\n",
    "mean_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred,axis=1)\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Nat_Rdn_S1_Icp_LRG401_STD_0_1_0710_v1.csv')\n",
    "results.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############Crop-Official Test\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "test_datagen_crop = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "testing_set_crop = test_datagen_crop.flow_from_directory('test_2019',\n",
    "                                                 target_size = (501, 501),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "#customized generator\n",
    "test_crops = crop_generator(testing_set_crop, 401)\n",
    "\n",
    "step_size_test_crop = ceil(testing_set_crop.n/testing_set_crop.batch_size)\n",
    "\n",
    "tta_steps = 2\n",
    "# predictions = []\n",
    "\n",
    "# import tensorflow as tf\n",
    "# with tf.device('/gpu:0'):\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    testing_set_crop.reset()\n",
    "    if NUM_GPU != 1:\n",
    "        preds=mul_model.predict_generator(test_crops, \n",
    "                                           steps = step_size_test_crop,\n",
    "                                           max_queue_size=16,\n",
    "#                                                use_multiprocessing=True,\n",
    "                                           workers=1,\n",
    "                                           verbose=1)    \n",
    "    else:\n",
    "        preds=model.predict_generator(test_crops, \n",
    "                                           steps = step_size_test_crop,\n",
    "                                           max_queue_size=16,\n",
    "#                                                use_multiprocessing=True,\n",
    "                                           workers=1,\n",
    "                                           verbose=1)           \n",
    "    predictions.append(preds)\n",
    "\n",
    "mean_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred,axis=1)\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Nat_Rdn_S1_Icp_LRG401_STD_0_1_0710_v2.csv')\n",
    "results.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Adding 15 more ##########\n",
    "#Crop-Official Test\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "test_datagen_crop = ImageDataGenerator(\n",
    "#     rescale = 1./255,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "testing_set_crop = test_datagen_crop.flow_from_directory('test_2019',\n",
    "                                                 target_size = (501, 501),\n",
    "                                                 batch_size = 1,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "#                                                  subset=\"training\"\n",
    "                                              )\n",
    "#customized generator\n",
    "test_crops = crop_generator(testing_set_crop, 401)\n",
    "\n",
    "step_size_test_crop = ceil(testing_set_crop.n/testing_set_crop.batch_size)\n",
    "\n",
    "tta_steps = 15\n",
    "# predictions = []\n",
    "\n",
    "# import tensorflow as tf\n",
    "# with tf.device('/gpu:0'):\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    testing_set_crop.reset()\n",
    "    preds=model.predict_generator(test_crops, \n",
    "                                           steps = step_size_test_crop,\n",
    "                                           max_queue_size=16,\n",
    "#                                                use_multiprocessing=True,\n",
    "                                           workers=1,\n",
    "                                           verbose=1)    \n",
    "    predictions.append(preds)\n",
    "\n",
    "mean_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "predicted_class_indices_mean=np.argmax(mean_pred,axis=1)\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Nat_Rdn_S1_Icp_LRG401_STD_0_1_v4.csv')\n",
    "results.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_modeljson = os.path.join('Nat', 'Nat_Rdn_S1_Icp_LRG421_Adv_0_1_L1_Bkp.json')\n",
    "saved_modelweights = os.path.join('Nat', 'Nat_Rdn_S1_Icp_LRG421_Adv_0_1_L1_Bkp.h5')\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = mul_model.to_json()\n",
    "with open(saved_modeljson, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_json.save_weights(saved_modelweights)\n",
    "print(\"Saved model to disk\")\n",
    "  \n",
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "# from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "\n",
    "NUM_GPU = 1\n",
    "batch_size = NUM_GPU * 16\n",
    "\n",
    "savedfilename = os.path.join('Nat', 'Nat_Rdn_S1_Icp_LRG421_Adv_0_1_L1.hdf5')\n",
    "\n",
    "print('Loading pretrained weights')\n",
    "model.load_weights(savedfilename)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(predictions[0])\n",
    "pred_mean = [0]*num_samples\n",
    "\n",
    "####\n",
    "# find max prediction/index\n",
    "# select top-17\n",
    "# mean of 17\n",
    "\n",
    "for tx2 in range(num_samples):\n",
    "    ars_max = []\n",
    "#     print('=======&%d&=======' %tx2)\n",
    "    for idx in range(len(predictions)):   \n",
    "        ars_max.append(numpy.amax(predictions[idx][tx2]))\n",
    "        ars_max_a = numpy.array(ars_max)\n",
    "        ars_idx = ars_max_a.argsort()[-17:][::-1]\n",
    "        \n",
    "    pred_mean[tx2] = 0                   \n",
    "    for ind in range(len(ars_idx)):\n",
    "        pred_mean[tx2] = pred_mean[tx2] + predictions[ars_idx[ind]][tx2]\n",
    "        \n",
    "    pred_mean[tx2] = pred_mean[tx2]/len(ars_idx) \n",
    "\n",
    "#top-17out of 30\n",
    "predicted_class_indices_mean=np.argmax(pred_mean,axis=1)\n",
    "labels = (training_set.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "finalpre = [labels[k] for k in predicted_class_indices_mean]\n",
    "\n",
    "import pandas as pd\n",
    "filenames=test_set1.filenames\n",
    "results=pd.DataFrame({\"id\":filenames,\n",
    "                      \"predicted\":finalpre,\n",
    "                      })\n",
    "results.to_csv('Nat_Rdn_S1_Icp_LRG401_STD_0_1_v5.csv')\n",
    "results.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading pretrained weights')\n",
    "model.load_weights(savedfilename)\n",
    "\n",
    "loss, acc = model.evaluate_generator(valid_set, steps=step_size_valid)\n",
    "print('test loss = %f, accuracy: %f' %(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join('Nat', '2_Set1_MobileNet_224x224_Global_dense4096x2_E2ETrainingV2.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Loading pretrained weights')\n",
    "# model.load_weights(savedfilename)\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.71, verbose=1)\n",
    "\n",
    "epochs = 10\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    result = model.fit_generator(training_set,\n",
    "                              steps_per_epoch = step_size_train,\n",
    "                              epochs = epochs,\n",
    "                              validation_data = valid_set,\n",
    "                              validation_steps = step_size_valid,\n",
    "                              shuffle=True,\n",
    "                              max_queue_size=16,\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=8,                             \n",
    "                              verbose=1,\n",
    "                              callbacks=[tb, csv_logger, checkpointer,earlystopping],\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Loading pretrained weights')\n",
    "# model.load_weights(savedfilename)\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_acc', value=0.65, verbose=1)\n",
    "\n",
    "epochs = 40\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    result = model.fit_generator(training_set,\n",
    "                              steps_per_epoch = step_size_train,\n",
    "                              epochs = epochs,\n",
    "                              validation_data = valid_set,\n",
    "                              validation_steps = step_size_valid,\n",
    "                              shuffle=True,\n",
    "                              max_queue_size=16,\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=8,                             \n",
    "                              verbose=2,\n",
    "                              callbacks=[tb, csv_logger, checkpointer,earlystopping],\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# list all data in history\n",
    "# history = result.history\n",
    "#print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(result.history['acc'])\n",
    "plt.plot(result.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(result.history['loss'])\n",
    "plt.plot(result.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# list all data in history\n",
    "# history = result.history\n",
    "#print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(result.history['acc'])\n",
    "plt.plot(result.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(result.history['loss'])\n",
    "plt.plot(result.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir Nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm Nat/2_1_Set1_MobileNet_224x224_Global_dense4096x2_E2ETrainingV2.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%rm -r train_edited\n",
    "%rm -r valid_edited\n",
    "%rm -r train_val2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r train_edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r valid_edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r train_val2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
