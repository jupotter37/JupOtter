{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df0c563d98e5ac0144c5feb8829020e59982b325"
   },
   "source": [
    "- V1 : subsector_id  \n",
    "- V2 : merchant_category_id\n",
    "- V3 : city_id\n",
    "- V4 : merchant_category_id + TRICK\n",
    "- V6 : v2 + TRICK\n",
    "- V7 : TRICK 제거 + -1의 갯수 \n",
    "- V8 : -1인 정보 모두 제거?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#settings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(2018)\n",
    "version = 7\n",
    "#logger\n",
    "def get_logger():\n",
    "    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\n",
    "    logging.basicConfig(format=FORMAT)\n",
    "    logger = logging.getLogger('main')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b75705f2ec6e0eaa7565a20168b478cd8381f5fb"
   },
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6f04d2cab5024304f77a3ae5c1d4e6d43c944b62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:19:18,867:main:Start read data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 4.04 MB\n",
      "Decreased by 56.2%\n",
      "Memory usage after optimization is: 2.24 MB\n",
      "Decreased by 52.5%\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger()\n",
    "#load data\n",
    "logger.info('Start read data')\n",
    "train_df = reduce_mem_usage(pd.read_csv('input/train.csv'))\n",
    "test_df = reduce_mem_usage(pd.read_csv('input/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "42abdd22516cf8b2a476b69f95d4c2290659fbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1749.11 MB\n",
      "Decreased by 43.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:20:41,360:main:Start processing NAs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 114.20 MB\n",
      "Decreased by 45.5%\n"
     ]
    }
   ],
   "source": [
    "historical_trans_df = reduce_mem_usage(pd.read_csv('input/historical_transactions.csv'))\n",
    "new_merchant_trans_df = reduce_mem_usage(pd.read_csv('input/new_merchant_transactions.csv'))\n",
    "\n",
    "#process NAs\n",
    "logger.info('Start processing NAs')\n",
    "#process NAs for merchant\n",
    "\n",
    "#process NA2 for transactions\n",
    "for df in [historical_trans_df, new_merchant_trans_df]:\n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    \n",
    "#define function for aggregation\n",
    "def create_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "62c5f994a65e3a312b079d393ff9afa0201d4468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1의 갯수 \n",
    "historical_trans_df['none_cnt'] = 0 \n",
    "historical_trans_df.loc[(historical_trans_df['city_id']==-1) | (historical_trans_df['installments']==-1) | (historical_trans_df['merchant_category_id']==-1)\n",
    "                       | (historical_trans_df['state_id']==-1) | (historical_trans_df['subsector_id']==-1), 'none_cnt'] = 1\n",
    "\n",
    "df = historical_trans_df[['card_id','none_cnt']]\n",
    "df = df.groupby('card_id')['none_cnt'].agg({'mean','var'}).reset_index()\n",
    "df.columns = ['card_id','hist_none_cnt_var','hist_none_cnt_mean']\n",
    "train_df = pd.merge(train_df,df,how='left',on='card_id')\n",
    "test_df = pd.merge(test_df,df,how='left',on='card_id')\n",
    "\n",
    "del df\n",
    "del historical_trans_df['none_cnt']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2cbc7fff2a6ae42d5a8891b0f74d6334ef2ac851"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = 'merchant_category_id'\n",
    "uniquecardidcity = historical_trans_df.groupby(['card_id'])[feature].unique().reset_index()\n",
    "uniquecardidcity['histset_{}'.format(feature)] = uniquecardidcity[feature].apply(set)\n",
    "newhistuniquecardidcity = new_merchant_trans_df.groupby(['card_id'])[feature].unique().reset_index() \n",
    "newhistuniquecardidcity['newhistset_{}'.format(feature)] = newhistuniquecardidcity[feature].apply(set)\n",
    "uniquecardidcity = uniquecardidcity.merge(newhistuniquecardidcity[['card_id','newhistset_{}'.format(feature)]], on='card_id',how='left')\n",
    "uniquecardidcity_na = uniquecardidcity[uniquecardidcity['newhistset_{}'.format(feature)].isnull()]\n",
    "uniquecardidcity = uniquecardidcity.dropna(axis=0)\n",
    "\n",
    "uniquecardidcity['union'] = uniquecardidcity.apply(lambda x: len(x['histset_{}'.format(feature)].union(x['newhistset_{}'.format(feature)])), axis=1)\n",
    "uniquecardidcity['hist_new_difference_{}'.format(feature)] = uniquecardidcity.apply(lambda x: len(x['histset_{}'.format(feature)].difference(x['newhistset_{}'.format(feature)])), axis=1)\n",
    "uniquecardidcity['new_hist_difference_{}'.format(feature)] = uniquecardidcity.apply(lambda x: len(x['newhistset_{}'.format(feature)].difference(x['histset_{}'.format(feature)])), axis=1)\n",
    "uniquecardidcity['intersection_{}'.format(feature)] = uniquecardidcity.apply(lambda x: len(x['histset_{}'.format(feature)].intersection(x['newhistset_{}'.format(feature)])), axis=1)\n",
    "\n",
    "uniquecardidcity['hist_new_difference_{}'.format(feature)] = uniquecardidcity['hist_new_difference_{}'.format(feature)]/uniquecardidcity['union']\n",
    "uniquecardidcity['new_hist_difference_{}'.format(feature)] = uniquecardidcity['new_hist_difference_{}'.format(feature)]/uniquecardidcity['union']\n",
    "uniquecardidcity['intersection_{}'.format(feature)] = uniquecardidcity['intersection_{}'.format(feature)]/uniquecardidcity['union']\n",
    "\n",
    "uniquecardidcity = uniquecardidcity[['card_id','hist_new_difference_{}'.format(feature),'new_hist_difference_{}'.format(feature),'intersection_{}'.format(feature)]]\n",
    "\n",
    "uniquecardidcity_na['union'] = uniquecardidcity_na['histset_{}'.format(feature)].apply(lambda x : len(x))\n",
    "uniquecardidcity_na['hist_new_difference_{}'.format(feature)] = 1\n",
    "uniquecardidcity_na['new_hist_difference_{}'.format(feature)] = np.nan\n",
    "uniquecardidcity_na['intersection_{}'.format(feature)] = np.nan\n",
    "uniquecardidcity_na = uniquecardidcity_na[['card_id','hist_new_difference_{}'.format(feature),'new_hist_difference_{}'.format(feature),'intersection_{}'.format(feature)]]\n",
    "\n",
    "unique = pd.concat([uniquecardidcity,uniquecardidcity_na])\n",
    "train_df = pd.merge(train_df,unique,how='left',on='card_id')\n",
    "test_df = pd.merge(test_df,unique,how='left',on='card_id')\n",
    "del unique,uniquecardidcity,uniquecardidcity_na\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "39b29222c3845041cefdecb87098a0500db5eaae"
   },
   "outputs": [],
   "source": [
    "len_hist = historical_trans_df.shape[0]\n",
    "hist_new_df_all = pd.concat([historical_trans_df,new_merchant_trans_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "7cf51d48f21a5d796d48235ee255e3a1dbd2ba88"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorized_flag</th>\n",
       "      <th>card_id</th>\n",
       "      <th>city_id</th>\n",
       "      <th>category_1</th>\n",
       "      <th>installments</th>\n",
       "      <th>category_3</th>\n",
       "      <th>merchant_category_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>month_lag</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>category_2</th>\n",
       "      <th>state_id</th>\n",
       "      <th>subsector_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>C_ID_4e6213e9bc</td>\n",
       "      <td>88</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>80</td>\n",
       "      <td>M_ID_e020e9b302</td>\n",
       "      <td>-8</td>\n",
       "      <td>-0.703331</td>\n",
       "      <td>2017-06-25 15:33:07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>C_ID_4e6213e9bc</td>\n",
       "      <td>88</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>367</td>\n",
       "      <td>M_ID_86ec983688</td>\n",
       "      <td>-7</td>\n",
       "      <td>-0.733128</td>\n",
       "      <td>2017-07-15 12:10:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>C_ID_4e6213e9bc</td>\n",
       "      <td>88</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>80</td>\n",
       "      <td>M_ID_979ed661fc</td>\n",
       "      <td>-6</td>\n",
       "      <td>-0.720386</td>\n",
       "      <td>2017-08-09 22:04:29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>C_ID_4e6213e9bc</td>\n",
       "      <td>88</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>560</td>\n",
       "      <td>M_ID_e6d5ae8ea6</td>\n",
       "      <td>-5</td>\n",
       "      <td>-0.735352</td>\n",
       "      <td>2017-09-02 10:06:26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>C_ID_4e6213e9bc</td>\n",
       "      <td>88</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>80</td>\n",
       "      <td>M_ID_e020e9b302</td>\n",
       "      <td>-11</td>\n",
       "      <td>-0.722865</td>\n",
       "      <td>2017-03-10 01:14:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  authorized_flag          card_id  city_id category_1  installments  \\\n",
       "0               Y  C_ID_4e6213e9bc       88          N             0   \n",
       "1               Y  C_ID_4e6213e9bc       88          N             0   \n",
       "2               Y  C_ID_4e6213e9bc       88          N             0   \n",
       "3               Y  C_ID_4e6213e9bc       88          N             0   \n",
       "4               Y  C_ID_4e6213e9bc       88          N             0   \n",
       "\n",
       "  category_3  merchant_category_id      merchant_id  month_lag  \\\n",
       "0          A                    80  M_ID_e020e9b302         -8   \n",
       "1          A                   367  M_ID_86ec983688         -7   \n",
       "2          A                    80  M_ID_979ed661fc         -6   \n",
       "3          A                   560  M_ID_e6d5ae8ea6         -5   \n",
       "4          A                    80  M_ID_e020e9b302        -11   \n",
       "\n",
       "   purchase_amount        purchase_date  category_2  state_id  subsector_id  \n",
       "0        -0.703331  2017-06-25 15:33:07         1.0        16            37  \n",
       "1        -0.733128  2017-07-15 12:10:45         1.0        16            16  \n",
       "2        -0.720386  2017-08-09 22:04:29         1.0        16            37  \n",
       "3        -0.735352  2017-09-02 10:06:26         1.0        16            34  \n",
       "4        -0.722865  2017-03-10 01:14:19         1.0        16            37  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_new_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "b131a1e33deeb2b7e87a88886ca2024451c6f277"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:54<00:00, 34.91s/it]\n"
     ]
    }
   ],
   "source": [
    "def frequency_encoding(frame, col):\n",
    "    freq_encoding = frame.groupby([col]).size()/frame.shape[0] \n",
    "    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n",
    "    return frame.merge(freq_encoding, on=col, how='left')\n",
    "\n",
    "cat_cols = ['city_id','merchant_category_id','merchant_id','state_id','subsector_id']\n",
    "\n",
    "freq_cat_cols = ['{}_Frequency'.format(col) for col in cat_cols]\n",
    "\n",
    "for col in tqdm(cat_cols):\n",
    "    hist_new_df_all = frequency_encoding(hist_new_df_all, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "b92d34ff1b1847489a21ad14e95ecc202fc5f26d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_trans_df = hist_new_df_all[:len_hist]\n",
    "new_merchant_trans_df = hist_new_df_all[len_hist:]\n",
    "del hist_new_df_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "455342ffb0d7948c52e282b1e3ba6241cbd22095"
   },
   "source": [
    "feature = 'merchant_category_id'\n",
    "uniquecardidcity = historical_trans_df.groupby(['card_id'])[feature].unique().reset_index()\n",
    "uniquecardidcity['histcityidset'] = uniquecardidcity[feature].apply(set)\n",
    "newhistuniquecardidcity = new_merchant_trans_df.groupby(['card_id'])[feature].unique().reset_index() \n",
    "newhistuniquecardidcity['newhistcityidset'] = newhistuniquecardidcity[feature].apply(set)\n",
    "uniquecardidcity = uniquecardidcity.merge(newhistuniquecardidcity[['card_id','newhistcityidset']], on='card_id',how='left') \n",
    "uniquecardidcity = uniquecardidcity.dropna() # newhist에 없는 cardid drop uniquecardidcity['union'] = uniquecardidcity.apply(lambda row: row['histcityidset'].union(row['newhistcityidset']), axis=1)\n",
    "uniquecardidcity['union'] = uniquecardidcity.apply(lambda row: len(row['histcityidset'].union(row['newhistcityid_set'])), axis=1)\n",
    "\n",
    "uniquecardidcity['intersection'] = uniquecardidcity.apply(lambda row: len(row['histcityidset'].intersection(row['newhistcityid_set'])), axis=1)\n",
    "uniquecardidcity['diff_hist_new_{}'.format(feature)] = uniquecardidcity.apply(lambda row: row['histcityidset'].difference(row['newhistcityid_set']), axis=1)\n",
    "uniquecardidcity['diff_new_hist_{}'.format(feature)] = uniquecardidcity.apply(lambda row: row['newhistcityid_set'].difference(row['histcityidset']), axis=1)\n",
    "\n",
    "uniquecardidcity['intersection'] = uniquecardidcity['intersection']/uniquecardidcity['union']\n",
    "uniquecardidcity['diff_hist_new_{}'.format(feature)] = uniquecardidcity['diff_hist_new_{}'.format(feature)]/uniquecardidcity['union']\n",
    "uniquecardidcity['diff_new_hist_{}'.format(feature)] = uniquecardidcity['diff_new_hist_{}'.format(feature)]/uniquecardidcity['union']\n",
    "del uniquecardidcity['union']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ebac157046b7890eda92d8c006f78d5e3b9f3ea1"
   },
   "source": [
    "uniquecardidcity['intersection'] = uniquecardidcity.apply(lambda row: len(row['histcityidset'].intersection(row['newhistcityid_set'])), axis=1)\n",
    "uniquecardidcity['diff_hist_new_{}'.format(feature)] = uniquecardidcity.apply(lambda row: row['histcityidset'].difference(row['newhistcityid_set']), axis=1)\n",
    "uniquecardidcity['diff_new_hist_{}'.format(feature)] = uniquecardidcity.apply(lambda row: row['newhistcityid_set'].difference(row['histcityidset']), axis=1)\n",
    "\n",
    "uniquecardidcity['intersection'] = uniquecardidcity['intersection']/uniquecardidcity['union']\n",
    "uniquecardidcity['diff_hist_new_{}'.format(feature)] = uniquecardidcity['diff_hist_new_{}'.format(feature)]/uniquecardidcity['union']\n",
    "uniquecardidcity['diff_new_hist_{}'.format(feature)] = uniquecardidcity['diff_new_hist_{}'.format(feature)]/uniquecardidcity['union']\n",
    "del uniquecardidcity['union']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "efb9cafc4383b48fa4d8baa4893824b2ad6aaf24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:26:01,430:main:process historical and new merchant datasets\n"
     ]
    }
   ],
   "source": [
    "#data processing historical and new merchant data\n",
    "logger.info('process historical and new merchant datasets')\n",
    "for df in [historical_trans_df, new_merchant_trans_df]:\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2}) \n",
    "    df['month_diff'] = ((pd.datetime(2012,4,1) - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    # Reference_date는 여기서 201903과 같은 형식으로 계산됩니다. \n",
    "    df['reference_date'] = (df['year']+(df['month'] - df['month_lag'])//12)*100 + (((df['month'] - df['month_lag'])%12) + 1)*1\n",
    "\n",
    "    #3.691\n",
    "    #df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    #df['installments'].replace(999, np.nan,inplace=True)\n",
    "    \n",
    "    #-1은 NAN을 상징하고 999는 12보다 큰 것을 의미하지 않을까???\n",
    "    #df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    #df['installments'].replace(999, 13, inplace=True)\n",
    "\n",
    "    # trim\n",
    "    # 3.691단일 커널의 코드를 가져왔고, amount의 max도 중요하다고 생각해서 전처리는 하지 않았습니다. \n",
    "    #df['purchase_amount'] = df['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "    df['duration'] = df['purchase_amount']*df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount']/df['month_diff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc4f8ad919c58d9f91cd3a5cc8f06d057e5b4a97"
   },
   "source": [
    "점수 잘 안나옴. \n",
    "https://www.kaggle.com/prashanththangavel/c-ustomer-l-ifetime-v-alue\n",
    "hist = historical_trans_df[['card_id','purchase_date','purchase_amount']]\n",
    "hist = hist.sort_values(by=['card_id', 'purchase_date'], ascending=[True, True])\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "z = hist.groupby('card_id')['purchase_date'].max().reset_index()\n",
    "q = hist.groupby('card_id')['purchase_date'].min().reset_index()\n",
    "\n",
    "z.columns = ['card_id', 'Max']\n",
    "q.columns = ['card_id', 'Min']\n",
    "\n",
    "## Extracting current timestamp\n",
    "curr_date = pd.datetime(2012,4,1)\n",
    "\n",
    "rec = pd.merge(z,q,how = 'left',on = 'card_id')\n",
    "rec['Min'] = pd.to_datetime(rec['Min'])\n",
    "rec['Max'] = pd.to_datetime(rec['Max'])\n",
    "\n",
    "## Time value \n",
    "rec['Recency'] = (curr_date - rec['Max']).astype('timedelta64[D]') ## current date - most recent date\n",
    "\n",
    "## Recency value\n",
    "rec['Time'] = (rec['Max'] - rec['Min']).astype('timedelta64[D]') ## Age of customer, MAX - MIN\n",
    "\n",
    "rec = rec[['card_id','Time','Recency']]\n",
    "\n",
    "## Frequency\n",
    "freq = hist.groupby('card_id').size().reset_index()\n",
    "freq.columns = ['card_id', 'Frequency']\n",
    "freq.head()\n",
    "\n",
    "## Monitary\n",
    "mon = hist.groupby('card_id')['purchase_amount'].sum().reset_index()\n",
    "mon.columns = ['card_id', 'Monitary']\n",
    "mon.head()\n",
    "\n",
    "final = pd.merge(freq,mon,how = 'left', on = 'card_id')\n",
    "final = pd.merge(final,rec,how = 'left', on = 'card_id')\n",
    "\n",
    "final['historic_CLV'] = final['Frequency'] * final['Monitary'] \n",
    "final['AOV'] = final['Monitary']/final['Frequency'] ## AOV - Average order value (i.e) total_purchase_amt/total_trans\n",
    "final['Predictive_CLV'] = final['Time']*final['AOV']*final['Monitary']*final['Recency'] \n",
    "historical_trans_df = pd.merge(historical_trans_df,final,on='card_id',how='left')\n",
    "\n",
    "del historical_trans_df['Frequency']\n",
    "del final\n",
    "del mon\n",
    "del freq\n",
    "del rec\n",
    "del z\n",
    "del q\n",
    "del curr_date\n",
    "del hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a9903ec29b9929ebe8d2e92075df0f6263923a7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:26:47,787:main:process frequency of date cusotmer comeback by historical\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 부분은 카드가 사용되어지고 다음 카드가 사용되어지기 까지의 시간을 계산한 것 입니다. (diff)\n",
    "# 메모리 문제 때문에 card_id만 가지고 했는데 다른 상점 id를 활용하면 더 좋을 것 같습니다. \n",
    "# 중간에 1440으로 나눠주는데 이는 분으로 계산한 것을 day로 바꿔주기 위함입니다. \n",
    "logger.info('process frequency of date cusotmer comeback by historical')\n",
    "\n",
    "df = historical_trans_df[['card_id', 'purchase_date']]\n",
    "df.sort_values(['card_id','purchase_date'], inplace=True) \n",
    "df['purchase'] = df.groupby(['card_id'])['purchase_date'].agg(['diff']).dropna(axis=0).astype('timedelta64[m]')\n",
    "df['purchase'] = df['purchase'] //1440 #몇 일마다 사람이 방문하는지를 반영. \n",
    "del df['purchase_date']\n",
    "#del df['subsector_id']\n",
    "\n",
    "aggs = {}\n",
    "aggs['purchase'] = ['min','max','mean','std','median']\n",
    "\n",
    "df = df.groupby('card_id')['purchase'].agg(aggs).reset_index()\n",
    "\n",
    "new_columns = ['card_id']\n",
    "new_columns1 = create_new_columns('hist_freq',aggs)\n",
    "\n",
    "for i in new_columns1:\n",
    "    new_columns.append(i)\n",
    "\n",
    "df.columns = new_columns\n",
    "\n",
    "train_df = train_df.merge(df, on='card_id', how='left')\n",
    "test_df = test_df.merge(df, on='card_id', how='left')\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c0b7b9753195d7aa62d53ae302819e2db0aad7aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:34:41,288:main:process frequency of date cusotmer comeback by new\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info('process frequency of date cusotmer comeback by new')\n",
    "\n",
    "df = new_merchant_trans_df[['card_id', 'purchase_date']]\n",
    "df.sort_values(['card_id','purchase_date'], inplace=True) \n",
    "df['purchase'] = df.groupby(['card_id'])['purchase_date'].agg(['diff']).dropna(axis=0).astype('timedelta64[m]')\n",
    "df['purchase'] = df['purchase'] //1440 #몇 일마다 사람이 방문하는지를 반영. \n",
    "del df['purchase_date']\n",
    "#del df['subsector_id']\n",
    "\n",
    "aggs = {}\n",
    "aggs['purchase'] = ['min','max','mean','std','median']\n",
    "\n",
    "df = df.groupby('card_id')['purchase'].agg(aggs).reset_index()\n",
    "\n",
    "new_columns = ['card_id']\n",
    "new_columns1 = create_new_columns('new_hist_freq',aggs)\n",
    "\n",
    "for i in new_columns1:\n",
    "    new_columns.append(i)\n",
    "\n",
    "df.columns = new_columns\n",
    "\n",
    "train_df = train_df.merge(df, on='card_id', how='left')\n",
    "test_df = test_df.merge(df, on='card_id', how='left')\n",
    "del df,new_columns1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "15e0094c780e1b039b7a90df3aeb0bc3dd117d0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:36:13,279:main:process reference_date by hist\n"
     ]
    }
   ],
   "source": [
    "# 기존에는 card_id를 기준으로 평균을 내었는데 이번에는 거꾸로 Reference를 기준으로 aggregation을 해봤습니다. \n",
    "logger.info('process reference_date by hist')\n",
    "\n",
    "historical_trans_df_re = historical_trans_df[['reference_date','purchase_amount','authorized_flag','month_lag']]\n",
    "\n",
    "aggs = {}\n",
    "aggs['purchase_amount'] = ['min','max','mean','sum','std','median']\n",
    "aggs['authorized_flag'] = ['min','max','mean','sum','std','median']\n",
    "\n",
    "historical_trans_df_re = historical_trans_df_re.groupby(['reference_date'])[['purchase_amount','authorized_flag']].agg(aggs).reset_index()\n",
    "\n",
    "new_columns = ['hist_reference_date_median']\n",
    "new_columns1 = create_new_columns('hist_reference',aggs)\n",
    "\n",
    "for i in new_columns1:\n",
    "    new_columns.append(i)\n",
    "\n",
    "historical_trans_df_re.columns = new_columns\n",
    "del new_columns1\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "a63740166efe53960380fc21c56533a29fb8b936"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:36:22,042:main:process reference_date by new\n"
     ]
    }
   ],
   "source": [
    "logger.info('process reference_date by new')\n",
    "\n",
    "new_merchant_trans_df_re = new_merchant_trans_df[['reference_date','purchase_amount']]\n",
    "\n",
    "aggs = {}\n",
    "aggs['purchase_amount'] = ['max','mean','std','median']\n",
    "\n",
    "\n",
    "new_merchant_trans_df_re = new_merchant_trans_df_re.groupby(['reference_date'])['purchase_amount'].agg(aggs).reset_index()\n",
    "\n",
    "new_columns = ['hist_reference_date_median']\n",
    "new_columns1 = create_new_columns('new_hist_reference',aggs)\n",
    "\n",
    "for i in new_columns1:\n",
    "    new_columns.append(i)\n",
    "\n",
    "new_merchant_trans_df_re.columns = new_columns\n",
    "del new_columns1\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "fbc52b96c0f785d9159824ef86387f8aa0a8cb5a"
   },
   "outputs": [],
   "source": [
    "# month_lag를 활용하여 purchase_amount에 가중치를 준 것입니다. \n",
    "# 이 부분을 더 개선시킬 수 있을 것 같은데 잘 안되는 중 입니다 ㅠㅠ...\n",
    "historical_trans_df1 = historical_trans_df[['card_id','month_lag','purchase_amount']]\n",
    "historical_trans_df3 = historical_trans_df1.groupby(['card_id','month_lag'])['purchase_amount'].agg({'count','mean'}).reset_index()\n",
    "historical_trans_df3.columns = ['card_id','month_lag','month_lag_cnt','month_lag_amount_mean']\n",
    "historical_trans_df3['month_lag_cnt'] = historical_trans_df3['month_lag_cnt']/(1-historical_trans_df3['month_lag']) \n",
    "historical_trans_df3['month_lag_amount_mean'] = historical_trans_df3['month_lag_amount_mean']/(1-historical_trans_df3['month_lag'])\n",
    "del historical_trans_df3['month_lag']\n",
    "\n",
    "aggs = {}\n",
    "aggs['month_lag_cnt'] = ['min','max','mean','sum','std']\n",
    "aggs['month_lag_amount_mean'] = ['min','max','mean','sum','std']\n",
    "\n",
    "\n",
    "historical_trans_df3 = historical_trans_df3.groupby(['card_id']).agg(aggs).reset_index()\n",
    "\n",
    "new_columns = ['card_id']\n",
    "new_columns1 = create_new_columns('hist_weight',aggs)\n",
    "\n",
    "for i in new_columns1:\n",
    "    new_columns.append(i)\n",
    "    \n",
    "historical_trans_df3.columns = new_columns\n",
    "\n",
    "del historical_trans_df1\n",
    "\n",
    "#merge with train, test\n",
    "train_df = train_df.merge(historical_trans_df3,on='card_id',how='left')\n",
    "test_df = test_df.merge(historical_trans_df3,on='card_id',how='left')\n",
    "del historical_trans_df3,new_columns1,new_columns\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "f7507440d959e4baf42ca66189f626f40bb9ef19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:36:30,904:main:Aggregate historical trans\n"
     ]
    }
   ],
   "source": [
    "#define aggregations with historical_trans_df\n",
    "logger.info('Aggregate historical trans')\n",
    "aggs = {}\n",
    "\n",
    "for col in ['subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "for col in ['month', 'hour', 'weekofyear', 'dayofweek', 'year']:\n",
    "    aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean', 'min', 'max', 'var']\n",
    "aggs['authorized_flag'] = ['sum', 'mean', 'min', 'max']\n",
    "aggs['weekend'] = ['sum', 'mean', 'min', 'max']\n",
    "aggs['category_1'] = ['sum', 'mean', 'min', 'max']\n",
    "#aggs['category_2'] = ['sum', 'mean', 'min', 'max']\n",
    "#aggs['category_3'] = ['sum', 'mean', 'min', 'max']\n",
    "aggs['card_id'] = ['size', 'count']\n",
    "aggs['reference_date'] = ['median']\n",
    "\n",
    "## 아래 부분이 3.691커널에서 가져온 코드입니다. \n",
    "\n",
    "aggs['duration']=['mean','min','max','var','skew']\n",
    "aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "aggs['price'] = ['sum','mean','max','min','var']\n",
    "\n",
    "## Version3 Encoding\n",
    "aggs['city_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['merchant_category_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['merchant_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['state_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['subsector_id_Frequency'] = ['mean','sum','var','median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2942b29904edcbc79a847f8f15fa46c3a36f260e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = create_new_columns('hist',aggs)\n",
    "historical_trans_group_df = historical_trans_df.groupby('card_id').agg(aggs)\n",
    "historical_trans_group_df.columns = new_columns\n",
    "historical_trans_group_df.reset_index(drop=False,inplace=True)\n",
    "historical_trans_group_df['hist_purchase_date_diff'] = (historical_trans_group_df['hist_purchase_date_max'] - historical_trans_group_df['hist_purchase_date_min']).dt.days\n",
    "historical_trans_group_df['hist_purchase_date_average'] = historical_trans_group_df['hist_purchase_date_diff']/historical_trans_group_df['hist_card_id_size']\n",
    "historical_trans_group_df['hist_purchase_date_uptonow'] = (pd.datetime(2012,4,1) - historical_trans_group_df['hist_purchase_date_max']).dt.days\n",
    "historical_trans_group_df['hist_purchase_date_uptomin'] = (pd.datetime(2012,4,1) - historical_trans_group_df['hist_purchase_date_min']).dt.days\n",
    "#merge with train, test\n",
    "train_df = train_df.merge(historical_trans_group_df,on='card_id',how='left')\n",
    "test_df = test_df.merge(historical_trans_group_df,on='card_id',how='left')\n",
    "\n",
    "#cleanup memory\n",
    "del historical_trans_group_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "de9ba8be971f34a25410984fc45e58dfd734283a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:41:11,145:main:Aggregate new merchant trans\n"
     ]
    }
   ],
   "source": [
    "#define aggregations with new_merchant_trans_df \n",
    "logger.info('Aggregate new merchant trans')\n",
    "aggs = {}\n",
    "for col in ['subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "for col in ['month', 'hour', 'weekofyear', 'dayofweek', 'year']:\n",
    "    aggs[col] = ['nunique', 'mean', 'min', 'max']\n",
    "\n",
    "    \n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean', 'max', 'min', 'var']\n",
    "aggs['weekend'] = ['sum', 'mean', 'min', 'max']\n",
    "aggs['category_1'] = ['sum', 'mean', 'min', 'max']\n",
    "aggs['authorized_flag'] = ['sum']\n",
    "#aggs['category_2'] = ['sum', 'mean', 'min', 'max']\n",
    "#aggs['category_3'] = ['sum', 'mean', 'min', 'max']\n",
    "aggs['card_id'] = ['size']\n",
    "aggs['reference_date'] = ['median']\n",
    "##3.691\n",
    "aggs['duration']=['mean','min','max','var','skew']\n",
    "aggs['amount_month_ratio']=['mean','min','max','var','skew']\n",
    "aggs['price'] = ['sum','mean','max','min','var']\n",
    "\n",
    "## Version3 Encoding\n",
    "aggs['city_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['merchant_category_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['merchant_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['state_id_Frequency'] = ['mean','sum','var','median']\n",
    "aggs['subsector_id_Frequency'] = ['mean','sum','var','median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "abd6b80cdc53cac92faadecf7c215e007b619e90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns = create_new_columns('new_hist',aggs)\n",
    "new_merchant_trans_group_df = new_merchant_trans_df.groupby('card_id').agg(aggs)\n",
    "new_merchant_trans_group_df.columns = new_columns\n",
    "new_merchant_trans_group_df.reset_index(drop=False,inplace=True)\n",
    "new_merchant_trans_group_df['new_hist_purchase_date_diff'] = (new_merchant_trans_group_df['new_hist_purchase_date_max'] - new_merchant_trans_group_df['new_hist_purchase_date_min']).dt.days\n",
    "new_merchant_trans_group_df['new_hist_purchase_date_average'] = new_merchant_trans_group_df['new_hist_purchase_date_diff']/new_merchant_trans_group_df['new_hist_card_id_size']\n",
    "new_merchant_trans_group_df['new_hist_purchase_date_uptonow'] = (pd.datetime(2012,4,1) - new_merchant_trans_group_df['new_hist_purchase_date_max']).dt.days\n",
    "new_merchant_trans_group_df['new_hist_purchase_date_uptomin'] = (pd.datetime(2012,4,1) - new_merchant_trans_group_df['new_hist_purchase_date_min']).dt.days\n",
    "#merge with train, test\n",
    "train_df = train_df.merge(new_merchant_trans_group_df,on='card_id',how='left')\n",
    "test_df = test_df.merge(new_merchant_trans_group_df,on='card_id',how='left')\n",
    "\n",
    "#clean-up memory\n",
    "del new_merchant_trans_group_df; gc.collect()\n",
    "del historical_trans_df; gc.collect()\n",
    "del new_merchant_trans_df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "ce109bbb0e13594b7c6f7288bf4c0d936316a5a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge with train, test\n",
    "train_df = train_df.merge(historical_trans_df_re,on='hist_reference_date_median',how='left')\n",
    "test_df = test_df.merge(historical_trans_df_re,on='hist_reference_date_median',how='left')\n",
    "\n",
    "train_df = train_df.merge(new_merchant_trans_df_re,on='hist_reference_date_median',how='left')\n",
    "test_df = test_df.merge(new_merchant_trans_df_re,on='hist_reference_date_median',how='left')\n",
    "del historical_trans_df_re\n",
    "del new_merchant_trans_df_re\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "733e2f46485c94fbb93335d3b93cc93d063e02e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]2019-02-06 10:43:12,475:main:Process train\n",
      "[INFO]2019-02-06 10:43:12,585:main:Process train and test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist_year_min\n",
      "hist_authorized_flag_max\n",
      "hist_reference_authorized_flag_min\n",
      "hist_reference_authorized_flag_max\n",
      "hist_reference_authorized_flag_median\n",
      "is_month_start\n"
     ]
    }
   ],
   "source": [
    "#process train\n",
    "logger.info('Process train')\n",
    "train_df['outliers'] = 0\n",
    "train_df.loc[train_df['target'] < -30, 'outliers'] = 1\n",
    "train_df['outliers'].value_counts()\n",
    "\n",
    "logger.info('Process train and test')\n",
    "## process both train and test\n",
    "for df in [train_df, test_df]:\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n",
    "    df['quarter'] = df['first_active_month'].dt.quarter\n",
    "    df['is_month_start'] = df['first_active_month'].dt.is_month_start\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['year'] = df['first_active_month'].dt.year\n",
    "    df['first_active_month1'] = 100*df['year']+df['month']\n",
    "    \n",
    "    df['elapsed_time'] = (pd.datetime(2012,4,1) - df['first_active_month']).dt.days\n",
    "    \n",
    "    #hist_reference_date_median은 201901과 같은 형식인데 이를 2019-01로 바꿔서 pd.to_datetime이 동작핟록 형식을 바꿔주었습니다. \n",
    "    df['hist_reference_date_median'] = df['hist_reference_date_median'].astype(str)\n",
    "    df['hist_reference_date_median'] = df['hist_reference_date_median'].apply(lambda x: x[0:4]+'-'+x[4:6])\n",
    "    df['hist_reference_date_median'] = pd.to_datetime(df['hist_reference_date_median'])\n",
    "    \n",
    "    \n",
    "    df['ref_year'] =df['hist_reference_date_median'].dt.year\n",
    "    df['ref_month'] =df['hist_reference_date_median'].dt.month\n",
    "    df['reference_month1'] = 100*df['ref_year']+df['ref_month']\n",
    "    \n",
    "#    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "#    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "#    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "#    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "#    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "#    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']    \n",
    "\n",
    "    ## 3.691에서 가져온 코드입니다. \n",
    "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    df['purchase_amount_mean'] = df['new_hist_purchase_amount_mean']+df['hist_purchase_amount_mean']\n",
    "    df['purchase_amount_max'] = df['new_hist_purchase_amount_max']+df['hist_purchase_amount_max']\n",
    "    df['purchase_amount_min'] = df['new_hist_purchase_amount_min']+df['hist_purchase_amount_min']\n",
    "    df['purchase_amount_sum_ratio'] = df['new_hist_purchase_amount_sum']/df['hist_purchase_amount_sum']\n",
    "    \n",
    "    #VERSION24에서 RATIO추가\n",
    "    #VERSION25, 26차이. \n",
    "    df['nh_purchase_amount_mean_ratio'] = df['new_hist_purchase_amount_mean']/df['hist_purchase_amount_mean']\n",
    "\n",
    "    ## 이 부분은 거래의 기간을 계산한 값들입니다. ratio로도 활용하면 의미가 있을 것 같지만 시도는 안해봤습니다. \n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days \n",
    "    df['hist_last_buy'] = (df['hist_purchase_date_max'] - df['first_active_month']).dt.days \n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days \n",
    "    df['new_hist_last_buy'] = (df['new_hist_purchase_date_max'] - df['first_active_month']).dt.days \n",
    "    \n",
    "    ## 마찬가지로 거래의 기간을 계산한 값들입니다. 위에는 first_active_month가 기준이고 아래는 reference_date가 기준입니다. \n",
    "    df['year_month'] = df['year']*100 + df['month']\n",
    "    df['hist_diff_reference_date_first'] = (df['hist_reference_date_median'] - df['first_active_month']).dt.days \n",
    "    df['hist_diff_reference_date_min'] = (df['hist_reference_date_median'] - df['hist_purchase_date_min']).dt.days \n",
    "    df['hist_diff_reference_date_max'] = (df['hist_reference_date_median'] - df['hist_purchase_date_max']).dt.days \n",
    "    df['new_hist_diff_reference_date_min'] = (df['hist_reference_date_median'] - df['new_hist_purchase_date_min']).dt.days \n",
    "    df['new_hist_diff_reference_date_max'] = (df['hist_reference_date_median'] - df['new_hist_purchase_date_max']).dt.days \n",
    "    \n",
    "    ## 거래의 기간을 계산한 값입니다. \n",
    "    df['hist_diff_first_last'] = df['hist_last_buy'] - df['hist_first_buy'] \n",
    "    df['new_hist_diff_first_last'] = df['new_hist_last_buy'] - df['new_hist_first_buy'] \n",
    "    \n",
    "    #version11\n",
    "    ## 거래기간동안 얼마나 거래가 이루어진지 평균을 내본 값입니다. \n",
    "    df['hist_diff_first_last_purchase'] = df['hist_purchase_amount_sum'] / df['hist_diff_first_last'] \n",
    "    df['new_hist_diff_first_last_purchase'] = df['new_hist_purchase_amount_sum'] / df['new_hist_diff_first_last'] \n",
    "    \n",
    "    #VERSION24에서 RATIO추가\n",
    "    #VERSION30에서 추가. \n",
    "    df['nh_purchase_mean_average_ratio'] = df['new_hist_diff_first_last_purchase']/df['hist_diff_first_last_purchase'] #중요도 낮음.\n",
    "    \n",
    "    #VERSION25, 27차이. \n",
    "    df['nh_merchant_id_nunique_ratio'] = df['new_hist_merchant_id_nunique']/df['hist_merchant_id_nunique']\n",
    "    \n",
    "    #VERSION4 ID 갯수 비율 추가\n",
    "    #df['nh_city_id_nunique_ratio'] = df['new_hist_city_id_nunique']/df['hist_city_id_nunique']\n",
    "    #df['nh_state_id_nunique_ratio'] = df['new_hist_state_id_nunique']/df['hist_state_id_nunique']\n",
    "    #CV점수 안좋아져서 제거했음. LB는 모름. \n",
    "    #del df['new_hist_city_id_nunique'], df['hist_city_id_nunique']\n",
    "    #del df['new_hist_state_id_nunique'], df['hist_state_id_nunique']\n",
    "    #del df['nh_city_id_nunique_ratio'], df['nh_state_id_nunique_ratio']\n",
    "    \n",
    "    ## 위랑 동일\n",
    "    df['hist_card_id_size_average'] = df['new_hist_card_id_size'] / df['hist_diff_first_last']\n",
    "    df['new_hist_card_id_size_average'] = df['new_hist_card_id_size'] / df['new_hist_diff_first_last']\n",
    "    \n",
    "    # VERSION24에서 RATIO추가\n",
    "    # VERSION31에서 테스트중..\n",
    "    df['nh_card_id_size_average_ratio'] = df['new_hist_card_id_size_average']/df['hist_card_id_size_average'] #중요도 낮음. \n",
    "    \n",
    "    #VERSION25, 28차이. \n",
    "    df['nh_freq_purchase_mean_ratio'] = df['new_hist_freq_purchase_mean']/df['hist_freq_purchase_mean']\n",
    "    \n",
    "    # VERSION32에서 테스트중.. \n",
    "    df['nh_category_1_sum_ratio'] = df['new_hist_category_1_sum']/df['hist_category_1_sum'] #중요도 낮음. \n",
    "    #df['nh_category_1_mean_ratio'] = df['new_hist_category_1_mean']/df['hist_category_1_mean'] #중요도 낮음. \n",
    "\n",
    "    ## 마찬가지로 거래의 기간을 계산 hist와 new와의 관계\n",
    "    df['diff_new_hist_date_min_max'] = (df['new_hist_purchase_date_min'] - df['hist_purchase_date_max']).dt.days \n",
    "    df['diff_new_hist_date_max_max'] = (df['new_hist_purchase_date_max'] - df['hist_purchase_date_max']).dt.days    \n",
    "    df['diff_new_hist_date_max_min'] = (df['new_hist_purchase_date_max'] - df['hist_purchase_date_min']).dt.days \n",
    "\n",
    "    #Version14 : 중요한한 변수를 나눠서 상호작용하도록 만듬.\n",
    "    df['diff_new_hist_date_max_amount_max'] = df['new_hist_purchase_amount_max']/df['diff_new_hist_date_max_max']\n",
    "    \n",
    "    \n",
    "    df['hist_flag_ratio'] = df['hist_authorized_flag_sum'] / df['hist_card_id_size']\n",
    "\n",
    "    ### LB 3.691 커널에서 추가한 부분. \n",
    "    df['installments_total'] = df['new_hist_installments_sum']+df['hist_installments_sum']\n",
    "    df['installments_mean'] = df['new_hist_installments_mean']+df['hist_installments_mean']\n",
    "    df['installments_max'] = df['new_hist_installments_max']+df['hist_installments_max']\n",
    "    df['installments_ratio'] = df['new_hist_installments_sum']/df['hist_installments_sum']\n",
    "    \n",
    "    df['price_total'] = df['purchase_amount_total'] / df['installments_total']\n",
    "    df['price_mean'] = df['purchase_amount_mean'] / df['installments_mean']\n",
    "    df['price_max'] = df['purchase_amount_max'] / df['installments_max']\n",
    "    df['duration_mean'] = df['new_hist_duration_mean']+df['hist_duration_mean']\n",
    "    # VERSION24에서 RATIO추가\n",
    "    #VERSION25, 29차이. \n",
    "    #df['duration_ratio'] = df['new_hist_duration_mean']/df['hist_duration_mean']\n",
    "    \n",
    "    df['duration_min'] = df['new_hist_duration_min']+df['hist_duration_min']\n",
    "    df['duration_max'] = df['new_hist_duration_max']+df['hist_duration_max']\n",
    "    df['amount_month_ratio_mean']=df['new_hist_amount_month_ratio_mean']+df['hist_amount_month_ratio_mean']\n",
    "    df['amount_month_ratio_min']=df['new_hist_amount_month_ratio_min']+df['hist_amount_month_ratio_min']\n",
    "    df['amount_month_ratio_max']=df['new_hist_amount_month_ratio_max']+df['hist_amount_month_ratio_max']\n",
    "    df['new_CLV'] = df['new_hist_card_id_size'] * df['new_hist_purchase_amount_sum'] / df['new_hist_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_size'] * df['hist_purchase_amount_sum'] / df['hist_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']\n",
    "    \n",
    "   \n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "        \n",
    "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
    "    \n",
    "    del df['year']\n",
    "    del df['year_month']\n",
    "    del df['new_hist_reference_date_median']\n",
    "\n",
    "    \n",
    "for f in ['feature_1','feature_2','feature_3']: \n",
    "    order_label = train_df.groupby([f])['outliers'].mean()\n",
    "    train_df[f] = train_df[f].map(order_label)\n",
    "    test_df[f] = test_df[f].map(order_label)\n",
    "    \n",
    "#for df in [train_df, test_df]:    \n",
    "#    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "#    df['feature_mean'] = df['feature_sum']/3\n",
    "#    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "#    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "#    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "\n",
    "## 유니크 값이 1이면 제거하는 코드입니다. \n",
    "for col in train_df.columns:\n",
    "    if train_df[col].nunique() == 1:\n",
    "        print(col)\n",
    "        del train_df[col]\n",
    "        del test_df[col]\n",
    "##\n",
    "\n",
    "train_columns = [c for c in train_df.columns if c not in ['card_id', 'first_active_month','target','outliers','hist_reference_date_median']]\n",
    "\n",
    "target = train_df['target']\n",
    "#del train_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac2501cfd86cc0de8af47f9d763fcc112daf9c84"
   },
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from tqdm import tqdm\n",
    "list_p_value =[]\n",
    "\n",
    "for i in tqdm(train_columns):\n",
    "    list_p_value.append(ks_2samp(test_df[i] , train_df[i])[1])\n",
    "\n",
    "Se = pd.Series(list_p_value, index = train_columns).sort_values() \n",
    "list_discarded = list(Se[Se < .1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2b39f4dcf0c12f5472e4f7a7b8897206146f46b"
   },
   "source": [
    "for i in list_discarded:\n",
    "    train_columns.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.copy()\n",
    "train = train.loc[train['target']>-30]\n",
    "target = train['target']\n",
    "del train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "8b612d4d6d434c052bbbe8b8ab185034970cbfd8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58527\tvalid_1's rmse: 1.57841\n",
      "[200]\ttraining's rmse: 1.55738\tvalid_1's rmse: 1.55564\n",
      "[300]\ttraining's rmse: 1.5434\tvalid_1's rmse: 1.54756\n",
      "[400]\ttraining's rmse: 1.53329\tvalid_1's rmse: 1.54375\n",
      "[500]\ttraining's rmse: 1.52479\tvalid_1's rmse: 1.54166\n",
      "[600]\ttraining's rmse: 1.51721\tvalid_1's rmse: 1.54062\n",
      "[700]\ttraining's rmse: 1.51032\tvalid_1's rmse: 1.53997\n",
      "[800]\ttraining's rmse: 1.50383\tvalid_1's rmse: 1.53939\n",
      "[900]\ttraining's rmse: 1.49772\tvalid_1's rmse: 1.53926\n",
      "[1000]\ttraining's rmse: 1.49179\tvalid_1's rmse: 1.53877\n",
      "[1100]\ttraining's rmse: 1.48597\tvalid_1's rmse: 1.53846\n",
      "[1200]\ttraining's rmse: 1.48039\tvalid_1's rmse: 1.53824\n",
      "[1300]\ttraining's rmse: 1.47479\tvalid_1's rmse: 1.53817\n",
      "[1400]\ttraining's rmse: 1.46938\tvalid_1's rmse: 1.53816\n",
      "[1500]\ttraining's rmse: 1.46396\tvalid_1's rmse: 1.53816\n",
      "Early stopping, best iteration is:\n",
      "[1414]\ttraining's rmse: 1.4686\tvalid_1's rmse: 1.53807\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58299\tvalid_1's rmse: 1.59541\n",
      "[200]\ttraining's rmse: 1.55468\tvalid_1's rmse: 1.575\n",
      "[300]\ttraining's rmse: 1.54063\tvalid_1's rmse: 1.56821\n",
      "[400]\ttraining's rmse: 1.53042\tvalid_1's rmse: 1.56522\n",
      "[500]\ttraining's rmse: 1.52204\tvalid_1's rmse: 1.56358\n",
      "[600]\ttraining's rmse: 1.5147\tvalid_1's rmse: 1.56298\n",
      "[700]\ttraining's rmse: 1.50778\tvalid_1's rmse: 1.56229\n",
      "[800]\ttraining's rmse: 1.50147\tvalid_1's rmse: 1.56189\n",
      "[900]\ttraining's rmse: 1.49538\tvalid_1's rmse: 1.56172\n",
      "[1000]\ttraining's rmse: 1.48941\tvalid_1's rmse: 1.56156\n",
      "[1100]\ttraining's rmse: 1.48354\tvalid_1's rmse: 1.56152\n",
      "[1200]\ttraining's rmse: 1.47784\tvalid_1's rmse: 1.5611\n",
      "[1300]\ttraining's rmse: 1.47231\tvalid_1's rmse: 1.56124\n",
      "Early stopping, best iteration is:\n",
      "[1207]\ttraining's rmse: 1.47745\tvalid_1's rmse: 1.56108\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58312\tvalid_1's rmse: 1.59147\n",
      "[200]\ttraining's rmse: 1.55472\tvalid_1's rmse: 1.57364\n",
      "[300]\ttraining's rmse: 1.54052\tvalid_1's rmse: 1.56771\n",
      "[400]\ttraining's rmse: 1.53034\tvalid_1's rmse: 1.56513\n",
      "[500]\ttraining's rmse: 1.52177\tvalid_1's rmse: 1.564\n",
      "[600]\ttraining's rmse: 1.51436\tvalid_1's rmse: 1.56315\n",
      "[700]\ttraining's rmse: 1.50748\tvalid_1's rmse: 1.56281\n",
      "[800]\ttraining's rmse: 1.50099\tvalid_1's rmse: 1.56214\n",
      "[900]\ttraining's rmse: 1.49483\tvalid_1's rmse: 1.56179\n",
      "[1000]\ttraining's rmse: 1.48897\tvalid_1's rmse: 1.5615\n",
      "[1100]\ttraining's rmse: 1.4831\tvalid_1's rmse: 1.56119\n",
      "[1200]\ttraining's rmse: 1.47738\tvalid_1's rmse: 1.56109\n",
      "[1300]\ttraining's rmse: 1.47184\tvalid_1's rmse: 1.56092\n",
      "Early stopping, best iteration is:\n",
      "[1271]\ttraining's rmse: 1.47337\tvalid_1's rmse: 1.56085\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.5842\tvalid_1's rmse: 1.58834\n",
      "[200]\ttraining's rmse: 1.55621\tvalid_1's rmse: 1.56574\n",
      "[300]\ttraining's rmse: 1.54219\tvalid_1's rmse: 1.55771\n",
      "[400]\ttraining's rmse: 1.53213\tvalid_1's rmse: 1.55412\n",
      "[500]\ttraining's rmse: 1.52387\tvalid_1's rmse: 1.55223\n",
      "[600]\ttraining's rmse: 1.51654\tvalid_1's rmse: 1.55102\n",
      "[700]\ttraining's rmse: 1.50965\tvalid_1's rmse: 1.55031\n",
      "[800]\ttraining's rmse: 1.50341\tvalid_1's rmse: 1.54968\n",
      "[900]\ttraining's rmse: 1.49728\tvalid_1's rmse: 1.54929\n",
      "[1000]\ttraining's rmse: 1.49131\tvalid_1's rmse: 1.54876\n",
      "[1100]\ttraining's rmse: 1.48564\tvalid_1's rmse: 1.54849\n",
      "[1200]\ttraining's rmse: 1.47994\tvalid_1's rmse: 1.54824\n",
      "[1300]\ttraining's rmse: 1.47438\tvalid_1's rmse: 1.54784\n",
      "[1400]\ttraining's rmse: 1.46891\tvalid_1's rmse: 1.54749\n",
      "[1500]\ttraining's rmse: 1.46354\tvalid_1's rmse: 1.54728\n",
      "[1600]\ttraining's rmse: 1.45822\tvalid_1's rmse: 1.54713\n",
      "[1700]\ttraining's rmse: 1.45297\tvalid_1's rmse: 1.54694\n",
      "[1800]\ttraining's rmse: 1.44789\tvalid_1's rmse: 1.54685\n",
      "Early stopping, best iteration is:\n",
      "[1754]\ttraining's rmse: 1.45021\tvalid_1's rmse: 1.54681\n",
      "fold 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58557\tvalid_1's rmse: 1.57619\n",
      "[200]\ttraining's rmse: 1.55716\tvalid_1's rmse: 1.55597\n",
      "[300]\ttraining's rmse: 1.54312\tvalid_1's rmse: 1.54956\n",
      "[400]\ttraining's rmse: 1.53292\tvalid_1's rmse: 1.54642\n",
      "[500]\ttraining's rmse: 1.52441\tvalid_1's rmse: 1.54479\n",
      "[600]\ttraining's rmse: 1.51688\tvalid_1's rmse: 1.54389\n",
      "[700]\ttraining's rmse: 1.51008\tvalid_1's rmse: 1.54354\n",
      "[800]\ttraining's rmse: 1.50369\tvalid_1's rmse: 1.54325\n",
      "[900]\ttraining's rmse: 1.4973\tvalid_1's rmse: 1.543\n",
      "[1000]\ttraining's rmse: 1.49122\tvalid_1's rmse: 1.54266\n",
      "[1100]\ttraining's rmse: 1.48547\tvalid_1's rmse: 1.54233\n",
      "[1200]\ttraining's rmse: 1.47986\tvalid_1's rmse: 1.54229\n",
      "[1300]\ttraining's rmse: 1.47421\tvalid_1's rmse: 1.54226\n",
      "Early stopping, best iteration is:\n",
      "[1278]\ttraining's rmse: 1.47551\tvalid_1's rmse: 1.54221\n",
      "fold 6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58122\tvalid_1's rmse: 1.61197\n",
      "[200]\ttraining's rmse: 1.55321\tvalid_1's rmse: 1.58887\n",
      "[300]\ttraining's rmse: 1.53911\tvalid_1's rmse: 1.58131\n",
      "[400]\ttraining's rmse: 1.52898\tvalid_1's rmse: 1.57753\n",
      "[500]\ttraining's rmse: 1.52057\tvalid_1's rmse: 1.57589\n",
      "[600]\ttraining's rmse: 1.51299\tvalid_1's rmse: 1.57489\n",
      "[700]\ttraining's rmse: 1.50614\tvalid_1's rmse: 1.57433\n",
      "[800]\ttraining's rmse: 1.49966\tvalid_1's rmse: 1.57387\n",
      "[900]\ttraining's rmse: 1.49337\tvalid_1's rmse: 1.57342\n",
      "[1000]\ttraining's rmse: 1.48721\tvalid_1's rmse: 1.57308\n",
      "[1100]\ttraining's rmse: 1.48146\tvalid_1's rmse: 1.57298\n",
      "[1200]\ttraining's rmse: 1.4758\tvalid_1's rmse: 1.57282\n",
      "[1300]\ttraining's rmse: 1.4703\tvalid_1's rmse: 1.57251\n",
      "[1400]\ttraining's rmse: 1.46482\tvalid_1's rmse: 1.57251\n",
      "Early stopping, best iteration is:\n",
      "[1326]\ttraining's rmse: 1.46885\tvalid_1's rmse: 1.57241\n",
      "fold 7\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58501\tvalid_1's rmse: 1.58159\n",
      "[200]\ttraining's rmse: 1.55672\tvalid_1's rmse: 1.55937\n",
      "[300]\ttraining's rmse: 1.54262\tvalid_1's rmse: 1.55151\n",
      "[400]\ttraining's rmse: 1.53234\tvalid_1's rmse: 1.54804\n",
      "[500]\ttraining's rmse: 1.52402\tvalid_1's rmse: 1.54615\n",
      "[600]\ttraining's rmse: 1.5165\tvalid_1's rmse: 1.54507\n",
      "[700]\ttraining's rmse: 1.50956\tvalid_1's rmse: 1.5444\n",
      "[800]\ttraining's rmse: 1.50303\tvalid_1's rmse: 1.54406\n",
      "[900]\ttraining's rmse: 1.49686\tvalid_1's rmse: 1.54356\n",
      "[1000]\ttraining's rmse: 1.49099\tvalid_1's rmse: 1.54353\n",
      "[1100]\ttraining's rmse: 1.48514\tvalid_1's rmse: 1.54333\n",
      "[1200]\ttraining's rmse: 1.47955\tvalid_1's rmse: 1.5432\n",
      "[1300]\ttraining's rmse: 1.47398\tvalid_1's rmse: 1.54296\n",
      "Early stopping, best iteration is:\n",
      "[1286]\ttraining's rmse: 1.47475\tvalid_1's rmse: 1.54293\n",
      "fold 8\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58393\tvalid_1's rmse: 1.59158\n",
      "[200]\ttraining's rmse: 1.55603\tvalid_1's rmse: 1.56867\n",
      "[300]\ttraining's rmse: 1.54196\tvalid_1's rmse: 1.56077\n",
      "[400]\ttraining's rmse: 1.53173\tvalid_1's rmse: 1.55689\n",
      "[500]\ttraining's rmse: 1.52332\tvalid_1's rmse: 1.55466\n",
      "[600]\ttraining's rmse: 1.51588\tvalid_1's rmse: 1.55384\n",
      "[700]\ttraining's rmse: 1.509\tvalid_1's rmse: 1.55297\n",
      "[800]\ttraining's rmse: 1.50249\tvalid_1's rmse: 1.5526\n",
      "[900]\ttraining's rmse: 1.49631\tvalid_1's rmse: 1.55229\n",
      "[1000]\ttraining's rmse: 1.49022\tvalid_1's rmse: 1.55186\n",
      "[1100]\ttraining's rmse: 1.48437\tvalid_1's rmse: 1.55154\n",
      "[1200]\ttraining's rmse: 1.4787\tvalid_1's rmse: 1.55149\n",
      "[1300]\ttraining's rmse: 1.47328\tvalid_1's rmse: 1.55141\n",
      "[1400]\ttraining's rmse: 1.46792\tvalid_1's rmse: 1.55133\n",
      "[1500]\ttraining's rmse: 1.46261\tvalid_1's rmse: 1.55108\n",
      "[1600]\ttraining's rmse: 1.4574\tvalid_1's rmse: 1.55089\n",
      "[1700]\ttraining's rmse: 1.45236\tvalid_1's rmse: 1.55083\n",
      "[1800]\ttraining's rmse: 1.44723\tvalid_1's rmse: 1.55073\n",
      "[1900]\ttraining's rmse: 1.44216\tvalid_1's rmse: 1.55061\n",
      "[2000]\ttraining's rmse: 1.43711\tvalid_1's rmse: 1.55057\n",
      "[2100]\ttraining's rmse: 1.43229\tvalid_1's rmse: 1.55053\n",
      "Early stopping, best iteration is:\n",
      "[2033]\ttraining's rmse: 1.43549\tvalid_1's rmse: 1.55038\n",
      "fold 9\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 1.58261\tvalid_1's rmse: 1.60019\n",
      "[200]\ttraining's rmse: 1.55422\tvalid_1's rmse: 1.57922\n",
      "[300]\ttraining's rmse: 1.54028\tvalid_1's rmse: 1.57184\n",
      "[400]\ttraining's rmse: 1.53015\tvalid_1's rmse: 1.56827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 1.5217\tvalid_1's rmse: 1.5663\n",
      "[600]\ttraining's rmse: 1.51427\tvalid_1's rmse: 1.56529\n",
      "[700]\ttraining's rmse: 1.5074\tvalid_1's rmse: 1.56461\n",
      "[800]\ttraining's rmse: 1.50092\tvalid_1's rmse: 1.5642\n",
      "[900]\ttraining's rmse: 1.49481\tvalid_1's rmse: 1.56414\n",
      "Early stopping, best iteration is:\n",
      "[863]\ttraining's rmse: 1.4971\tvalid_1's rmse: 1.56399\n",
      "1.553231146572967\n"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.015,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 24,\n",
    "         \"seed\": 6}\n",
    "\n",
    "#prepare fit model with cross-validation\n",
    "np.random.seed(2019)\n",
    "\n",
    "folds = KFold(n_splits=9, shuffle=True, random_state=4950)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train)):\n",
    "    strLog = \"fold {}\".format(fold_+1)\n",
    "    print(strLog)\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    #feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    #predictions\n",
    "    predictions += clf.predict(test_df[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "    \n",
    "cv_score = np.sqrt(mean_squared_error(oof, target))\n",
    "print(cv_score)\n",
    "withoutoutlier_predictions = predictions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\n",
    "model_without_outliers[\"target\"] = withoutoutlier_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_without_outliers.to_csv('hyeonwoo_without_outlier.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.copy()\n",
    "target = train['outliers']\n",
    "del train['target']\n",
    "del train['outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0442295\tvalid_1's binary_logloss: 0.0473734\n",
      "[200]\ttraining's binary_logloss: 0.040707\tvalid_1's binary_logloss: 0.0452727\n",
      "[300]\ttraining's binary_logloss: 0.0387534\tvalid_1's binary_logloss: 0.0446588\n",
      "[400]\ttraining's binary_logloss: 0.0374018\tvalid_1's binary_logloss: 0.0445221\n",
      "[500]\ttraining's binary_logloss: 0.0359787\tvalid_1's binary_logloss: 0.0445202\n",
      "[600]\ttraining's binary_logloss: 0.0346823\tvalid_1's binary_logloss: 0.0445798\n",
      "Early stopping, best iteration is:\n",
      "[441]\ttraining's binary_logloss: 0.0368117\tvalid_1's binary_logloss: 0.0444846\n",
      "fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0442358\tvalid_1's binary_logloss: 0.0465009\n",
      "[200]\ttraining's binary_logloss: 0.0407708\tvalid_1's binary_logloss: 0.0447546\n",
      "[300]\ttraining's binary_logloss: 0.0388141\tvalid_1's binary_logloss: 0.0443011\n",
      "[400]\ttraining's binary_logloss: 0.0374916\tvalid_1's binary_logloss: 0.0441638\n",
      "[500]\ttraining's binary_logloss: 0.0361934\tvalid_1's binary_logloss: 0.0441377\n",
      "[600]\ttraining's binary_logloss: 0.0349322\tvalid_1's binary_logloss: 0.0441734\n",
      "[700]\ttraining's binary_logloss: 0.0337826\tvalid_1's binary_logloss: 0.0441806\n",
      "Early stopping, best iteration is:\n",
      "[524]\ttraining's binary_logloss: 0.0358801\tvalid_1's binary_logloss: 0.0441281\n",
      "fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0439676\tvalid_1's binary_logloss: 0.0494357\n",
      "[200]\ttraining's binary_logloss: 0.0404925\tvalid_1's binary_logloss: 0.0471649\n",
      "[300]\ttraining's binary_logloss: 0.0385161\tvalid_1's binary_logloss: 0.0465502\n",
      "[400]\ttraining's binary_logloss: 0.0372008\tvalid_1's binary_logloss: 0.0463185\n",
      "[500]\ttraining's binary_logloss: 0.0358993\tvalid_1's binary_logloss: 0.046212\n",
      "[600]\ttraining's binary_logloss: 0.0346514\tvalid_1's binary_logloss: 0.0461542\n",
      "[700]\ttraining's binary_logloss: 0.0335265\tvalid_1's binary_logloss: 0.0461535\n",
      "[800]\ttraining's binary_logloss: 0.0324144\tvalid_1's binary_logloss: 0.046172\n",
      "Early stopping, best iteration is:\n",
      "[659]\ttraining's binary_logloss: 0.0339513\tvalid_1's binary_logloss: 0.0461255\n",
      "fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0441477\tvalid_1's binary_logloss: 0.0481333\n",
      "[200]\ttraining's binary_logloss: 0.0406855\tvalid_1's binary_logloss: 0.0460168\n",
      "[300]\ttraining's binary_logloss: 0.0387651\tvalid_1's binary_logloss: 0.0453595\n",
      "[400]\ttraining's binary_logloss: 0.0373393\tvalid_1's binary_logloss: 0.0451052\n",
      "[500]\ttraining's binary_logloss: 0.0359428\tvalid_1's binary_logloss: 0.0449329\n",
      "[600]\ttraining's binary_logloss: 0.0347\tvalid_1's binary_logloss: 0.0449139\n",
      "[700]\ttraining's binary_logloss: 0.0334657\tvalid_1's binary_logloss: 0.0449212\n",
      "[800]\ttraining's binary_logloss: 0.0323552\tvalid_1's binary_logloss: 0.0449328\n",
      "Early stopping, best iteration is:\n",
      "[624]\ttraining's binary_logloss: 0.0343975\tvalid_1's binary_logloss: 0.044898\n",
      "fold 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0447077\tvalid_1's binary_logloss: 0.0433106\n",
      "[200]\ttraining's binary_logloss: 0.0412562\tvalid_1's binary_logloss: 0.0412512\n",
      "[300]\ttraining's binary_logloss: 0.0393108\tvalid_1's binary_logloss: 0.0406204\n",
      "[400]\ttraining's binary_logloss: 0.0379329\tvalid_1's binary_logloss: 0.0403771\n",
      "[500]\ttraining's binary_logloss: 0.0365214\tvalid_1's binary_logloss: 0.0402976\n",
      "[600]\ttraining's binary_logloss: 0.0352095\tvalid_1's binary_logloss: 0.0402982\n",
      "[700]\ttraining's binary_logloss: 0.034045\tvalid_1's binary_logloss: 0.0402898\n",
      "[800]\ttraining's binary_logloss: 0.032983\tvalid_1's binary_logloss: 0.0402936\n",
      "Early stopping, best iteration is:\n",
      "[663]\ttraining's binary_logloss: 0.0344837\tvalid_1's binary_logloss: 0.0402586\n",
      "fold 6\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0444368\tvalid_1's binary_logloss: 0.0454981\n",
      "[200]\ttraining's binary_logloss: 0.0409545\tvalid_1's binary_logloss: 0.0436971\n",
      "[300]\ttraining's binary_logloss: 0.039023\tvalid_1's binary_logloss: 0.0431495\n",
      "[400]\ttraining's binary_logloss: 0.0376598\tvalid_1's binary_logloss: 0.0429117\n",
      "[500]\ttraining's binary_logloss: 0.0362652\tvalid_1's binary_logloss: 0.0428391\n",
      "[600]\ttraining's binary_logloss: 0.0349613\tvalid_1's binary_logloss: 0.0427869\n",
      "[700]\ttraining's binary_logloss: 0.0337804\tvalid_1's binary_logloss: 0.0427491\n",
      "[800]\ttraining's binary_logloss: 0.0327159\tvalid_1's binary_logloss: 0.0427687\n",
      "[900]\ttraining's binary_logloss: 0.0316074\tvalid_1's binary_logloss: 0.0428056\n",
      "Early stopping, best iteration is:\n",
      "[774]\ttraining's binary_logloss: 0.0330021\tvalid_1's binary_logloss: 0.0427397\n",
      "fold 7\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.044248\tvalid_1's binary_logloss: 0.0470627\n",
      "[200]\ttraining's binary_logloss: 0.0407925\tvalid_1's binary_logloss: 0.0452016\n",
      "[300]\ttraining's binary_logloss: 0.0388573\tvalid_1's binary_logloss: 0.0446737\n",
      "[400]\ttraining's binary_logloss: 0.0374414\tvalid_1's binary_logloss: 0.0445048\n",
      "[500]\ttraining's binary_logloss: 0.0360458\tvalid_1's binary_logloss: 0.0444246\n",
      "[600]\ttraining's binary_logloss: 0.034788\tvalid_1's binary_logloss: 0.0443936\n",
      "[700]\ttraining's binary_logloss: 0.0335802\tvalid_1's binary_logloss: 0.044353\n",
      "[800]\ttraining's binary_logloss: 0.0325\tvalid_1's binary_logloss: 0.0443804\n",
      "[900]\ttraining's binary_logloss: 0.0315109\tvalid_1's binary_logloss: 0.0444014\n",
      "Early stopping, best iteration is:\n",
      "[703]\ttraining's binary_logloss: 0.0335479\tvalid_1's binary_logloss: 0.0443475\n",
      "fold 8\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.044442\tvalid_1's binary_logloss: 0.0460995\n",
      "[200]\ttraining's binary_logloss: 0.0409348\tvalid_1's binary_logloss: 0.0444552\n",
      "[300]\ttraining's binary_logloss: 0.0389727\tvalid_1's binary_logloss: 0.0438877\n",
      "[400]\ttraining's binary_logloss: 0.0376126\tvalid_1's binary_logloss: 0.0436767\n",
      "[500]\ttraining's binary_logloss: 0.0363127\tvalid_1's binary_logloss: 0.0435938\n",
      "[600]\ttraining's binary_logloss: 0.035027\tvalid_1's binary_logloss: 0.0435441\n",
      "[700]\ttraining's binary_logloss: 0.0338535\tvalid_1's binary_logloss: 0.0435406\n",
      "[800]\ttraining's binary_logloss: 0.0327351\tvalid_1's binary_logloss: 0.0435638\n",
      "Early stopping, best iteration is:\n",
      "[647]\ttraining's binary_logloss: 0.0344812\tvalid_1's binary_logloss: 0.0435145\n",
      "fold 9\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0446418\tvalid_1's binary_logloss: 0.0445946\n",
      "[200]\ttraining's binary_logloss: 0.0411272\tvalid_1's binary_logloss: 0.0424862\n",
      "[300]\ttraining's binary_logloss: 0.0391976\tvalid_1's binary_logloss: 0.0418472\n",
      "[400]\ttraining's binary_logloss: 0.0377892\tvalid_1's binary_logloss: 0.0415803\n",
      "[500]\ttraining's binary_logloss: 0.0364979\tvalid_1's binary_logloss: 0.0414578\n",
      "[600]\ttraining's binary_logloss: 0.035224\tvalid_1's binary_logloss: 0.0413816\n",
      "[700]\ttraining's binary_logloss: 0.0339971\tvalid_1's binary_logloss: 0.0413974\n",
      "[800]\ttraining's binary_logloss: 0.0328523\tvalid_1's binary_logloss: 0.0414102\n",
      "Early stopping, best iteration is:\n",
      "[627]\ttraining's binary_logloss: 0.0348928\tvalid_1's binary_logloss: 0.04136\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-06bd8e000b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CV score: {:<8.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'log_loss' is not defined"
     ]
    }
   ],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'binary',\n",
    "         'max_depth': 5,\n",
    "         'learning_rate': 0.01,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.6,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.7 ,\n",
    "         \"metric\": 'binary_logloss',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 24,\n",
    "         \"random_state\": 6}\n",
    "\n",
    "folds = KFold(n_splits=9, shuffle=True, random_state=4950)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test_df))\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold {}\".format(fold_+1))\n",
    "\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    predictions += clf.predict(test_df[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.04354 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_prob = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\n",
    "df_outlier_prob[\"target\"] = predictions\n",
    "df_outlier_prob.sort_values('target',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
