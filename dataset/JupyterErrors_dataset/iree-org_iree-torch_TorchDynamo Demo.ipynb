{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab81e48e",
   "metadata": {},
   "source": [
    "# TorchDynamo Demo\n",
    "\n",
    "This notebook shows examples of how to use TorchDynamo with Torch-MLIR+IREE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cbcd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdynamo\n",
    "import torch\n",
    "from utils import make_torch_mlir_compiler\n",
    "import torch_mlir\n",
    "import iree_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fb8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, logging\n",
    "warnings.simplefilter(\"ignore\")\n",
    "torchdynamo.config.log_level = logging.ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b58c9d",
   "metadata": {},
   "source": [
    "# Current Steps for Compiling in Torch-MLIR\n",
    "\n",
    "In order to see some of the benefits one gets with TorchDynamo, we will first take a look at the current compilation process that users of Torch-MLIR have to go through to compile PyTorch code. We will then look at some limitations of this approach that TorchDynamo allows us to overcome.\n",
    "\n",
    "To compile PyTorch code using Torch-MLIR+IREE, you must:\n",
    "\n",
    "- Create a `torch.nn.Module`\n",
    "- Ensure that module is scriptable or traceable (could require code changes)\n",
    "- Compile module using `torch_mlir.compile` + `iree_torch.compile_to_vmfb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f73eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_load(module: torch.nn.Module, example_inputs):\n",
    "    linalg_module = torch_mlir.compile(module, example_inputs, \n",
    "                                       output_type=\"linalg-on-tensors\")\n",
    "    compiled_module = iree_torch.compile_to_vmfb(linalg_module)\n",
    "    return iree_torch.load_vmfb(compiled_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ce9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, t):\n",
    "        return 2 * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea683b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModule = MyModule()\n",
    "compiled_module = compile_and_load(myModule, torch.ones((2, 3)))\n",
    "compiled_module.forward(torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65a17a",
   "metadata": {},
   "source": [
    "## Limitations of Current Approach\n",
    "\n",
    "- Input to `torch_mlir.compile` must be a `torch.nn.Module`\n",
    "- Module must be scriptable or traceable\n",
    "- Torch-MLIR is expected to support all of TorchScript (loops, control flow, etc)\n",
    "\n",
    "Note: Both TorchScript and Torch-MLIR support single function workloads, but it requires a [different path in Torch-MLIR](https://github.com/llvm/torch-mlir/blob/4d47f1671a6020ed43af6e71631e932ac56b1f46/lib/Dialect/Torch/Transforms/Passes.cpp#L30) and the Python API currently does not support it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6028265",
   "metadata": {},
   "source": [
    "# Steps for Compiling with TorchDynamo\n",
    "\n",
    "- Add `torchdynamo.optimize` decorator (not limited to `torch.nn.Module`s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8c8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchdynamo.reset()\n",
    "@torchdynamo.optimize(make_torch_mlir_compiler(use_tracing=False, device=\"cpu\", verbose=True))\n",
    "def foo(t):\n",
    "    return 2 * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2882b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling graph...\n",
      "torch.fx graph:\n",
      "graph():\n",
      "    %arg0_1 : [#users=1] = placeholder[target=arg0_1]\n",
      "    %mul : [#users=1] = call_function[target=torch.ops.aten.mul](args = (%arg0_1, 2), kwargs = {})\n",
      "    return mul\n",
      "\n",
      "\n",
      "torch-mlir backend contract graph:\n",
      "module attributes {torch.debug_module_name = \"_lambda\"} {\n",
      "  func.func @forward(%arg0: !torch.vtensor<[2,3],f32>) -> !torch.vtensor<[2,3],f32> {\n",
      "    %int2 = torch.constant.int 2\n",
      "    %0 = torch.aten.mul.Scalar %arg0, %int2 : !torch.vtensor<[2,3],f32>, !torch.int -> !torch.vtensor<[2,3],f32>\n",
      "    return %0 : !torch.vtensor<[2,3],f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f3aa9",
   "metadata": {},
   "source": [
    "The verbose output above shows what is happening inside the Torch-MLIR compiler passed to TorchDynamo. TorchDynamo feeds to the Torch-MLIR compiler the [`torch.fx.GraphModule`](https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule) representing the computation performed in `foo`. This graph is then imported into Torch-MLIR, where several simplification passes are performed to reach the [backend contract](https://github.com/llvm/torch-mlir/blob/main/docs/architecture.md#the-backend-contract), producing the second graph. The MLIR graph is then compiled further.\n",
    "\n",
    "Because TorchDynamo remembers when a computation has been compiled, running `foo` again will no longer result in the verbose compilation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd50cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba2cee",
   "metadata": {},
   "source": [
    "## Graph Breaks\n",
    "\n",
    "TorchDynamo will automatically insert graph breaks to separate code that is expected to run on the backend from code that is expected to run at the Python level. This means Torch-MLIR does not have to worry about things like print statements or data dependent control flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5d4a5",
   "metadata": {},
   "source": [
    "### Graph Break Example: Print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776c2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchdynamo.reset()\n",
    "@torchdynamo.optimize(make_torch_mlir_compiler(use_tracing=False, device=\"cpu\", verbose=True))\n",
    "def foo(a, b):\n",
    "    print(\"Hello!\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964ddfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "Compiling graph...\n",
      "torch.fx graph:\n",
      "graph():\n",
      "    %arg0_1 : [#users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [#users=1] = placeholder[target=arg1_1]\n",
      "    %add : [#users=1] = call_function[target=torch.ops.aten.add](args = (%arg0_1, %arg1_1), kwargs = {})\n",
      "    return add\n",
      "\n",
      "\n",
      "torch-mlir backend contract graph:\n",
      "module attributes {torch.debug_module_name = \"_lambda\"} {\n",
      "  func.func @forward(%arg0: !torch.vtensor<[2,3],f32>, %arg1: !torch.vtensor<[2,3],f32>) -> !torch.vtensor<[2,3],f32> {\n",
      "    %int1 = torch.constant.int 1\n",
      "    %0 = torch.aten.add.Tensor %arg0, %arg1, %int1 : !torch.vtensor<[2,3],f32>, !torch.vtensor<[2,3],f32>, !torch.int -> !torch.vtensor<[2,3],f32>\n",
      "    return %0 : !torch.vtensor<[2,3],f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(torch.ones((2, 3)), torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e855a",
   "metadata": {},
   "source": [
    "Note the lack of a print statement in the compiled graph thanks to the graph break. This computation does not work in the current compilation flow from Torch-MLIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1cb6e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TorchMlirCompilerError",
     "evalue": "Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:\nerror: failed to legalize operation 'torch.constant.str'\nnote: see current operation: %0 = \"torch.constant.str\"() {value = \"Hello!\"} : () -> !torch.str\nerror: Module does not conform to the linalg-on-tensors backend contract. See dialect conversion legality information above.\n\n\nError can be reproduced with:\n$ torch-mlir-opt -pass-pipeline='torch-backend-to-linalg-on-tensors-backend-pipeline' /tmp/PrintStatementModule.mlir\nAdd '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTorchMlirCompilerError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m a \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m      9\u001b[0m printStatementModule \u001b[38;5;241m=\u001b[39m PrintStatementModule()\n\u001b[0;32m---> 10\u001b[0m compiled_module \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprintStatementModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m compiled_module\u001b[38;5;241m.\u001b[39mforward(torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)), torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mcompile_and_load\u001b[0;34m(module, example_inputs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_and_load\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, example_inputs):\n\u001b[0;32m----> 2\u001b[0m     linalg_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_mlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinalg-on-tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     compiled_module \u001b[38;5;241m=\u001b[39m iree_torch\u001b[38;5;241m.\u001b[39mcompile_to_vmfb(linalg_module)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iree_torch\u001b[38;5;241m.\u001b[39mload_vmfb(compiled_module)\n",
      "File \u001b[0;32m~/torch-mlir/build/tools/torch-mlir/python_packages/torch_mlir/torch_mlir/__init__.py:273\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, example_args, output_type, use_tracing, ignore_traced_shapes, verbose)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mb\u001b[38;5;241m.\u001b[39mmodule\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_type \u001b[38;5;241m==\u001b[39m OutputType\u001b[38;5;241m.\u001b[39mLINALG_ON_TENSORS:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mrun_pipeline_with_repro_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch-backend-to-linalg-on-tensors-backend-pipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLowering Torch Backend IR -> Linalg-on-Tensors Backend IR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m====================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/torch-mlir/build/tools/torch-mlir/python_packages/torch_mlir/torch_mlir/compiler_utils.py:73\u001b[0m, in \u001b[0;36mrun_pipeline_with_repro_report\u001b[0;34m(module, pipeline, description)\u001b[0m\n\u001b[1;32m     64\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with the following diagnostics:\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mgetvalue()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m        Add \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdebug_options\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the IR dump for debugging purpose.\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     trimmed_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mlstrip() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchMlirCompilerError(trimmed_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m original_stderr\n",
      "\u001b[0;31mTorchMlirCompilerError\u001b[0m: Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:\nerror: failed to legalize operation 'torch.constant.str'\nnote: see current operation: %0 = \"torch.constant.str\"() {value = \"Hello!\"} : () -> !torch.str\nerror: Module does not conform to the linalg-on-tensors backend contract. See dialect conversion legality information above.\n\n\nError can be reproduced with:\n$ torch-mlir-opt -pass-pipeline='torch-backend-to-linalg-on-tensors-backend-pipeline' /tmp/PrintStatementModule.mlir\nAdd '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.\n"
     ]
    }
   ],
   "source": [
    "class PrintStatementModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        print(\"Hello!\")\n",
    "        return a + b\n",
    "    \n",
    "printStatementModule = PrintStatementModule()\n",
    "compiled_module = compile_and_load(printStatementModule, [torch.ones((2, 3)), torch.ones((2, 3))])\n",
    "compiled_module.forward(torch.ones((2, 3)), torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97dc641",
   "metadata": {},
   "source": [
    "### Graph Break Example: Control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6d410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchdynamo.reset()\n",
    "@torchdynamo.optimize(make_torch_mlir_compiler(use_tracing=False, device=\"cpu\", verbose=True))\n",
    "def foo(a, b):\n",
    "    x = a / 2\n",
    "    if b.max() < 0:\n",
    "        b = b * -1\n",
    "    return x * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a1eef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling graph...\n",
      "torch.fx graph:\n",
      "graph():\n",
      "    %arg0_1 : [#users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [#users=1] = placeholder[target=arg1_1]\n",
      "    %div : [#users=1] = call_function[target=torch.ops.aten.div](args = (%arg0_1, 2), kwargs = {})\n",
      "    %max_1 : [#users=1] = call_function[target=torch.ops.aten.max](args = (%arg1_1,), kwargs = {})\n",
      "    %lt : [#users=1] = call_function[target=torch.ops.aten.lt](args = (%max_1, 0), kwargs = {})\n",
      "    return (div, lt)\n",
      "\n",
      "\n",
      "torch-mlir backend contract graph:\n",
      "module attributes {torch.debug_module_name = \"_lambda\"} {\n",
      "  func.func @forward(%arg0: !torch.vtensor<[2,3],f32>, %arg1: !torch.vtensor<[2,3],f32>) -> (!torch.vtensor<[2,3],f32>, !torch.vtensor<[],i1>) {\n",
      "    %int0 = torch.constant.int 0\n",
      "    %int2 = torch.constant.int 2\n",
      "    %0 = torch.aten.div.Scalar %arg0, %int2 : !torch.vtensor<[2,3],f32>, !torch.int -> !torch.vtensor<[2,3],f32>\n",
      "    %1 = torch.aten.max %arg1 : !torch.vtensor<[2,3],f32> -> !torch.vtensor<[],f32>\n",
      "    %2 = torch.aten.lt.Scalar %1, %int0 : !torch.vtensor<[],f32>, !torch.int -> !torch.vtensor<[],i1>\n",
      "    return %0, %2 : !torch.vtensor<[2,3],f32>, !torch.vtensor<[],i1>\n",
      "  }\n",
      "}\n",
      "\n",
      "Compiling graph...\n",
      "torch.fx graph:\n",
      "graph():\n",
      "    %arg0_1 : [#users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [#users=1] = placeholder[target=arg1_1]\n",
      "    %mul : [#users=1] = call_function[target=torch.ops.aten.mul](args = (%arg1_1, %arg0_1), kwargs = {})\n",
      "    return mul\n",
      "\n",
      "\n",
      "torch-mlir backend contract graph:\n",
      "module attributes {torch.debug_module_name = \"_lambda\"} {\n",
      "  func.func @forward(%arg0: !torch.vtensor<[2,3],f32>, %arg1: !torch.vtensor<[2,3],f32>) -> !torch.vtensor<[2,3],f32> {\n",
      "    %0 = torch.aten.mul.Tensor %arg1, %arg0 : !torch.vtensor<[2,3],f32>, !torch.vtensor<[2,3],f32> -> !torch.vtensor<[2,3],f32>\n",
      "    return %0 : !torch.vtensor<[2,3],f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(torch.ones((2, 3)), torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c7526",
   "metadata": {},
   "source": [
    "Notice that because the body of the `if` statement is not used, it is not compiled. If we change the inputs so that the body of the `if` statement is needed, then TorchDynamo will reuse the compiled graphs for the other parts of the function and only compile the body of the `if` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3280d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling graph...\n",
      "torch.fx graph:\n",
      "graph():\n",
      "    %arg0_1 : [#users=1] = placeholder[target=arg0_1]\n",
      "    %arg1_1 : [#users=1] = placeholder[target=arg1_1]\n",
      "    %mul : [#users=1] = call_function[target=torch.ops.aten.mul](args = (%arg0_1, -1), kwargs = {})\n",
      "    %mul_1 : [#users=1] = call_function[target=torch.ops.aten.mul](args = (%arg1_1, %mul), kwargs = {})\n",
      "    return mul_1\n",
      "\n",
      "\n",
      "torch-mlir backend contract graph:\n",
      "module attributes {torch.debug_module_name = \"_lambda\"} {\n",
      "  func.func @forward(%arg0: !torch.vtensor<[2,3],f32>, %arg1: !torch.vtensor<[2,3],f32>) -> !torch.vtensor<[2,3],f32> {\n",
      "    %int-1 = torch.constant.int -1\n",
      "    %0 = torch.aten.mul.Scalar %arg0, %int-1 : !torch.vtensor<[2,3],f32>, !torch.int -> !torch.vtensor<[2,3],f32>\n",
      "    %1 = torch.aten.mul.Tensor %arg1, %0 : !torch.vtensor<[2,3],f32>, !torch.vtensor<[2,3],f32> -> !torch.vtensor<[2,3],f32>\n",
      "    return %1 : !torch.vtensor<[2,3],f32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo(torch.ones((2, 3)), -torch.ones((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f45cf",
   "metadata": {},
   "source": [
    "This type of control flow would not work in Torch-MLIR using the current compilation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb80efa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TorchMlirCompilerError",
     "evalue": "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\nerror: 'torch.copy.to_vtensor' op failed to verify that operand is corresponding !torch.tensor\nnote: see current operation: %22 = \"torch.copy.to_vtensor\"(%21) : (!torch.tensor<*,f32>) -> !torch.vtensor\n\n\nError can be reproduced with:\n$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline{backend-legal-ops=torch.aten.flatten.using_ints}' /tmp/ControlFlowModule.mlir\nAdd '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTorchMlirCompilerError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m b\n\u001b[1;32m     11\u001b[0m controlFlowModule \u001b[38;5;241m=\u001b[39m ControlFlowModule()\n\u001b[0;32m---> 12\u001b[0m compiled_module \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrolFlowModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m compiled_module\u001b[38;5;241m.\u001b[39mforward(torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)), torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)))\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mcompile_and_load\u001b[0;34m(module, example_inputs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_and_load\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, example_inputs):\n\u001b[0;32m----> 2\u001b[0m     linalg_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_mlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinalg-on-tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     compiled_module \u001b[38;5;241m=\u001b[39m iree_torch\u001b[38;5;241m.\u001b[39mcompile_to_vmfb(linalg_module)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iree_torch\u001b[38;5;241m.\u001b[39mload_vmfb(compiled_module)\n",
      "File \u001b[0;32m~/torch-mlir/build/tools/torch-mlir/python_packages/torch_mlir/torch_mlir/__init__.py:247\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(model, example_args, output_type, use_tracing, ignore_traced_shapes, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m backend_legal_ops \u001b[38;5;241m=\u001b[39m BACKEND_LEGAL_OPS\u001b[38;5;241m.\u001b[39mget(output_type, [])\n\u001b[1;32m    246\u001b[0m option_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mbackend-legal-ops=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(backend_legal_ops) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mrun_pipeline_with_repro_report\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchscript-module-to-torch-backend-pipeline\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moption_string\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLowering TorchScript IR -> Torch Backend IR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m====================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/torch-mlir/build/tools/torch-mlir/python_packages/torch_mlir/torch_mlir/compiler_utils.py:73\u001b[0m, in \u001b[0;36mrun_pipeline_with_repro_report\u001b[0;34m(module, pipeline, description)\u001b[0m\n\u001b[1;32m     64\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with the following diagnostics:\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mgetvalue()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m        Add \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdebug_options\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the IR dump for debugging purpose.\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     trimmed_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([m\u001b[38;5;241m.\u001b[39mlstrip() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchMlirCompilerError(trimmed_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m original_stderr\n",
      "\u001b[0;31mTorchMlirCompilerError\u001b[0m: Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\nerror: 'torch.copy.to_vtensor' op failed to verify that operand is corresponding !torch.tensor\nnote: see current operation: %22 = \"torch.copy.to_vtensor\"(%21) : (!torch.tensor<*,f32>) -> !torch.vtensor\n\n\nError can be reproduced with:\n$ torch-mlir-opt -pass-pipeline='torchscript-module-to-torch-backend-pipeline{backend-legal-ops=torch.aten.flatten.using_ints}' /tmp/ControlFlowModule.mlir\nAdd '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.\n"
     ]
    }
   ],
   "source": [
    "class ControlFlowModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        x = a / 2\n",
    "        if b.max() < 0:\n",
    "            b = b * -1\n",
    "        return x * b\n",
    "    \n",
    "controlFlowModule = ControlFlowModule()\n",
    "compiled_module = compile_and_load(controlFlowModule, [torch.ones((2, 3)), torch.ones((2, 3))])\n",
    "compiled_module.forward(torch.ones((2, 3)), torch.ones((2, 3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
