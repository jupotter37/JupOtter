{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ds4se Tutorial - Traceability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Science for Software Engieering (ds4se) is an academic initiative to perform exploratory analysis on software engieering artifact and metadata. Data Management, Analysis, and Benchmarking for DL and Traceability.\n",
    "\n",
    "In this tutorial, we will use ds4se library to analyze the Libest dataset and find tracebilitity values between various source and target artifacts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ds4se library requries several other libraries to be present and/or up to date. In the following cells, we install/update those libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from gensim) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: boto in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.15.5)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.19.0,>=1.18.5 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.5->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/danielquiroga/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbdev in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (1.1.4)\n",
      "Requirement already satisfied: nbconvert<6 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (5.6.1)\n",
      "Requirement already satisfied: packaging in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (20.1)\n",
      "Requirement already satisfied: pyyaml in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (5.3)\n",
      "Requirement already satisfied: jupyter-client in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (5.3.4)\n",
      "Requirement already satisfied: pip in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (20.2.3)\n",
      "Requirement already satisfied: fastcore>=1.2.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (1.2.0)\n",
      "Requirement already satisfied: nbformat>=4.4.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (5.0.4)\n",
      "Requirement already satisfied: ipykernel in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbdev) (5.1.4)\n",
      "Requirement already satisfied: testpath in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (0.4.4)\n",
      "Requirement already satisfied: pygments in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (2.5.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (1.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (4.3.3)\n",
      "Requirement already satisfied: jupyter-core in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (4.6.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (0.3)\n",
      "Requirement already satisfied: defusedxml in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (0.6.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (2.11.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (0.8.4)\n",
      "Requirement already satisfied: bleach in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbconvert<6->nbdev) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from packaging->nbdev) (2.4.6)\n",
      "Requirement already satisfied: six in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from packaging->nbdev) (1.14.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jupyter-client->nbdev) (18.1.1)\n",
      "Requirement already satisfied: tornado>=4.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jupyter-client->nbdev) (6.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jupyter-client->nbdev) (2.8.1)\n",
      "Requirement already satisfied: ipython-genutils in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbformat>=4.4.0->nbdev) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from nbformat>=4.4.0->nbdev) (3.2.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipykernel->nbdev) (7.18.1)\n",
      "Requirement already satisfied: appnope; platform_system == \"Darwin\" in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipykernel->nbdev) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from traitlets>=4.2->nbconvert<6->nbdev) (4.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jinja2>=2.4->nbconvert<6->nbdev) (1.1.1)\n",
      "Requirement already satisfied: webencodings in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from bleach->nbconvert<6->nbdev) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (1.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (50.3.0)\n",
      "Requirement already satisfied: pickleshare in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (0.14.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (3.0.3)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (0.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (2.2.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->nbdev) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->nbdev) (0.1.8)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel->nbdev) (0.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/danielquiroga/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (0.1.91)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/danielquiroga/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dit in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (1.18.1)\n",
      "Requirement already satisfied: prettytable in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (1.0.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (2.4)\n",
      "Requirement already satisfied: contextlib2 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (0.6.0.post1)\n",
      "Requirement already satisfied: boltons in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (20.2.1)\n",
      "Requirement already satisfied: scipy>=0.15.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (1.4.1)\n",
      "Requirement already satisfied: debtcollector in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from dit) (2.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from prettytable->dit) (50.3.0)\n",
      "Requirement already satisfied: wcwidth in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from prettytable->dit) (0.1.8)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from networkx->dit) (4.4.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from debtcollector->dit) (5.5.0)\n",
      "Requirement already satisfied: wrapt>=1.7.0 in /Users/danielquiroga/anaconda3/lib/python3.7/site-packages (from debtcollector->dit) (1.11.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/danielquiroga/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the ds4se library in your machine, simply run the following command to install it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ds4se\n",
      "  Downloading ds4se-0.1.6-py3-none-any.whl (8.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.7 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: ds4se\n",
      "Successfully installed ds4se-0.1.6\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/danielquiroga/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade ds4se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to being the usage of ds4se, first call the facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#this facade provides an interface for users to use the functionalityies ds4se provides. For the complete list that facade contains, see the project pypi page. \n",
    "import ds4se.facade as facade   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File [libest-pre-req].csv does not exist: '[libest-pre-req].csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6a278e6a193d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msource_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[libest-pre-req].csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[libest-pre-tc].csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File [libest-pre-req].csv does not exist: '[libest-pre-req].csv'"
     ]
    }
   ],
   "source": [
    "source_file = pd.read_csv(\"[libest-pre-req].csv\",names=['ids', 'text'], header=None, sep=' ')\n",
    "target_file = pd.read_csv(\"[libest-pre-tc].csv\",names=['ids', 'text'], header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a preview of the source artifact class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir http uri control est server must suppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir server side key generat respons request...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir http base client authent est server may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir csr attribut request est client request...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir server side key generat est client may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir client author decis issu certif client ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir csr attribut polici may allow inclus cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir simpl enrol client https post simpleenr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir csr attribut follow exampl valid csratt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir http layer http use transfer est messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir client certif request function est clie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir secur consider support basic authent sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir server author client must check est ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir csr attribut respons local configur pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir server key generat est client request s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir certif respons success server respons m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir link ident pop inform server polici det...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir http header control http status valu us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir obtain certif est client request copi c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir client use explicit databas est client ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir full cmc respons enrol success server r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir full pki request messag full pki reques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir proof possess defin section cmc rfc pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir bootstrap distribut certif possibl clie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir http base client authent est server opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir applic layer est client must capabl gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir request asymmetr encrypt privat key spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir obtain certif follow exampl valid cacer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir full cmc est client request certif est ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir simpl enrol client est client renew rek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir tls layer tls provid authent turn enabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir intial enrol authent est server verifi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir client certif reissuanc est client rene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir enrol enrol follow exampl valid simplee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir tls base server authent tls server auth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir tls client authent recommend method ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir terminolog key word must must requir sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir protocol design layer figur provid expa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir inform refer idev ieee standard associ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir certif less tls authent est client est ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir iana consider section defin oid regist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir client use implicit databas est client ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir certif request est client request est d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir certif less tls mutual authent certif l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir full cmc request http post fullcmc vali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir distribut certif est client request cop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir server key generat follow exampl valid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir messag type document use exist media ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir document profil certif enrol client use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir simpl enrol enrol respons enrol success...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir refer rfc freed borenstein multipurpos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>requir certif tls authent est client previous ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ids                                               text\n",
       "0   test_data/LibEST_semeru_format/requirements/RQ...  requir http uri control est server must suppor...\n",
       "1   test_data/LibEST_semeru_format/requirements/RQ...  requir server side key generat respons request...\n",
       "2   test_data/LibEST_semeru_format/requirements/RQ...  requir http base client authent est server may...\n",
       "3   test_data/LibEST_semeru_format/requirements/RQ...  requir csr attribut request est client request...\n",
       "4   test_data/LibEST_semeru_format/requirements/RQ...  requir server side key generat est client may ...\n",
       "5   test_data/LibEST_semeru_format/requirements/RQ...  requir client author decis issu certif client ...\n",
       "6   test_data/LibEST_semeru_format/requirements/RQ...  requir csr attribut polici may allow inclus cl...\n",
       "7   test_data/LibEST_semeru_format/requirements/RQ...  requir simpl enrol client https post simpleenr...\n",
       "8   test_data/LibEST_semeru_format/requirements/RQ...  requir csr attribut follow exampl valid csratt...\n",
       "9   test_data/LibEST_semeru_format/requirements/RQ...  requir http layer http use transfer est messag...\n",
       "10  test_data/LibEST_semeru_format/requirements/RQ...  requir client certif request function est clie...\n",
       "11  test_data/LibEST_semeru_format/requirements/RQ...  requir secur consider support basic authent sp...\n",
       "12  test_data/LibEST_semeru_format/requirements/RQ...  requir server author client must check est ser...\n",
       "13  test_data/LibEST_semeru_format/requirements/RQ...  requir csr attribut respons local configur pol...\n",
       "14  test_data/LibEST_semeru_format/requirements/RQ...  requir server key generat est client request s...\n",
       "15  test_data/LibEST_semeru_format/requirements/RQ...  requir certif respons success server respons m...\n",
       "16  test_data/LibEST_semeru_format/requirements/RQ...  requir link ident pop inform server polici det...\n",
       "17  test_data/LibEST_semeru_format/requirements/RQ...  requir http header control http status valu us...\n",
       "18  test_data/LibEST_semeru_format/requirements/RQ...  requir obtain certif est client request copi c...\n",
       "19  test_data/LibEST_semeru_format/requirements/RQ...  requir client use explicit databas est client ...\n",
       "20  test_data/LibEST_semeru_format/requirements/RQ...  requir full cmc respons enrol success server r...\n",
       "21  test_data/LibEST_semeru_format/requirements/RQ...  requir full pki request messag full pki reques...\n",
       "22  test_data/LibEST_semeru_format/requirements/RQ...  requir proof possess defin section cmc rfc pro...\n",
       "23  test_data/LibEST_semeru_format/requirements/RQ...  requir bootstrap distribut certif possibl clie...\n",
       "24  test_data/LibEST_semeru_format/requirements/RQ...  requir http base client authent est server opt...\n",
       "25  test_data/LibEST_semeru_format/requirements/RQ...  requir applic layer est client must capabl gen...\n",
       "26  test_data/LibEST_semeru_format/requirements/RQ...  requir request asymmetr encrypt privat key spe...\n",
       "27  test_data/LibEST_semeru_format/requirements/RQ...  requir obtain certif follow exampl valid cacer...\n",
       "28  test_data/LibEST_semeru_format/requirements/RQ...  requir full cmc est client request certif est ...\n",
       "29  test_data/LibEST_semeru_format/requirements/RQ...  requir simpl enrol client est client renew rek...\n",
       "30  test_data/LibEST_semeru_format/requirements/RQ...  requir tls layer tls provid authent turn enabl...\n",
       "31  test_data/LibEST_semeru_format/requirements/RQ...  requir intial enrol authent est server verifi ...\n",
       "32  test_data/LibEST_semeru_format/requirements/RQ...  requir client certif reissuanc est client rene...\n",
       "33  test_data/LibEST_semeru_format/requirements/RQ...  requir enrol enrol follow exampl valid simplee...\n",
       "34  test_data/LibEST_semeru_format/requirements/RQ...  requir tls base server authent tls server auth...\n",
       "35  test_data/LibEST_semeru_format/requirements/RQ...  requir tls client authent recommend method ide...\n",
       "36  test_data/LibEST_semeru_format/requirements/RQ...  requir terminolog key word must must requir sh...\n",
       "37  test_data/LibEST_semeru_format/requirements/RQ...  requir protocol design layer figur provid expa...\n",
       "38  test_data/LibEST_semeru_format/requirements/RQ...  requir inform refer idev ieee standard associ ...\n",
       "39  test_data/LibEST_semeru_format/requirements/RQ...  requir certif less tls authent est client est ...\n",
       "40  test_data/LibEST_semeru_format/requirements/RQ...  requir iana consider section defin oid regist ...\n",
       "41  test_data/LibEST_semeru_format/requirements/RQ...  requir client use implicit databas est client ...\n",
       "42  test_data/LibEST_semeru_format/requirements/RQ...  requir certif request est client request est d...\n",
       "43  test_data/LibEST_semeru_format/requirements/RQ...  requir certif less tls mutual authent certif l...\n",
       "44  test_data/LibEST_semeru_format/requirements/RQ...  requir full cmc request http post fullcmc vali...\n",
       "45  test_data/LibEST_semeru_format/requirements/RQ...  requir distribut certif est client request cop...\n",
       "46  test_data/LibEST_semeru_format/requirements/RQ...  requir server key generat follow exampl valid ...\n",
       "47  test_data/LibEST_semeru_format/requirements/RQ...  requir messag type document use exist media ty...\n",
       "48  test_data/LibEST_semeru_format/requirements/RQ...  requir document profil certif enrol client use...\n",
       "49  test_data/LibEST_semeru_format/requirements/RQ...  requir simpl enrol enrol respons enrol success...\n",
       "50  test_data/LibEST_semeru_format/requirements/RQ...  requir refer rfc freed borenstein multipurpos ...\n",
       "51  test_data/LibEST_semeru_format/requirements/RQ...  requir certif tls authent est client previous ..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a preview of target artifact class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us903.c</td>\n",
       "      <td>unit test user stori server simpl enrol august...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3496.c</td>\n",
       "      <td>unit test uri path segment extens support marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us899.c</td>\n",
       "      <td>unit test user stori client simpl enrol septem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us4020.c</td>\n",
       "      <td>unit test user stori unit test client proxi mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us897.c</td>\n",
       "      <td>unit test user stori client cacert june copyri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1060.c</td>\n",
       "      <td>unit test user stori tls srp support server pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>unit test user stori server csr attribut novem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us896.c</td>\n",
       "      <td>unit test user stori client csr attribut novem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us894.c</td>\n",
       "      <td>unit test user stori proxi cacert novemb copyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1005.c</td>\n",
       "      <td>unit test user stori client easi provis novemb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us898.c</td>\n",
       "      <td>unit test user stori client enrol octob copyri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3512.c</td>\n",
       "      <td>unit test uri path segment support server apri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1883.c</td>\n",
       "      <td>unit test user stori enabl token auth mode est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us748.c</td>\n",
       "      <td>unit test user stori proxi simpl enrol august ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3612.c</td>\n",
       "      <td>unit test user stori encrypt privat key suppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us901.c</td>\n",
       "      <td>unit test user stori server cacert june copyri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1864.c</td>\n",
       "      <td>unit test user stori enabl token auth mode ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1159.c</td>\n",
       "      <td>unit test user stori csr attribut enforc octob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us2174.c</td>\n",
       "      <td>unit test user stori proxi simpl enrol august ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us893.c</td>\n",
       "      <td>unit test user stori proxi reenrol octob copyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test_data/LibEST_semeru_format/test/us895.c</td>\n",
       "      <td>unit test user stori proxi csr attribut novemb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ids                                               text\n",
       "0    test_data/LibEST_semeru_format/test/us903.c  unit test user stori server simpl enrol august...\n",
       "1   test_data/LibEST_semeru_format/test/us3496.c  unit test uri path segment extens support marc...\n",
       "2    test_data/LibEST_semeru_format/test/us899.c  unit test user stori client simpl enrol septem...\n",
       "3   test_data/LibEST_semeru_format/test/us4020.c  unit test user stori unit test client proxi mo...\n",
       "4    test_data/LibEST_semeru_format/test/us897.c  unit test user stori client cacert june copyri...\n",
       "5   test_data/LibEST_semeru_format/test/us1060.c  unit test user stori tls srp support server pr...\n",
       "6    test_data/LibEST_semeru_format/test/us900.c  unit test user stori server csr attribut novem...\n",
       "7    test_data/LibEST_semeru_format/test/us896.c  unit test user stori client csr attribut novem...\n",
       "8    test_data/LibEST_semeru_format/test/us894.c  unit test user stori proxi cacert novemb copyr...\n",
       "9   test_data/LibEST_semeru_format/test/us1005.c  unit test user stori client easi provis novemb...\n",
       "10   test_data/LibEST_semeru_format/test/us898.c  unit test user stori client enrol octob copyri...\n",
       "11  test_data/LibEST_semeru_format/test/us3512.c  unit test uri path segment support server apri...\n",
       "12  test_data/LibEST_semeru_format/test/us1883.c  unit test user stori enabl token auth mode est...\n",
       "13   test_data/LibEST_semeru_format/test/us748.c  unit test user stori proxi simpl enrol august ...\n",
       "14  test_data/LibEST_semeru_format/test/us3612.c  unit test user stori encrypt privat key suppor...\n",
       "15   test_data/LibEST_semeru_format/test/us901.c  unit test user stori server cacert june copyri...\n",
       "16  test_data/LibEST_semeru_format/test/us1864.c  unit test user stori enabl token auth mode ser...\n",
       "17  test_data/LibEST_semeru_format/test/us1159.c  unit test user stori csr attribut enforc octob...\n",
       "18  test_data/LibEST_semeru_format/test/us2174.c  unit test user stori proxi simpl enrol august ...\n",
       "19   test_data/LibEST_semeru_format/test/us893.c  unit test user stori proxi reenrol octob copyr...\n",
       "20   test_data/LibEST_semeru_format/test/us895.c  unit test user stori proxi csr attribut novemb..."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to calculate tracebility value in ds4se is called TraceLinkValue. The function can only process one pair of string at a time. Here is example to calculate the traceability between the first source file and the first target file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TraceLinkValue only strings of source and target content. \n",
    "source = source_file['text'][0]  \n",
    "target = target_file['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 01:39:04,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 01:39:04,839 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 01:39:04,841 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 01:39:04,903 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 01:39:04,904 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 01:39:04,905 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 01:39:04,907 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 01:39:04,908 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 01:39:04,908 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 01:39:04,928 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 01:39:04,931 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f61b4352978>\n",
      "2020-11-02 01:39:04,935 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 01:39:04,943 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 01:39:05,147 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 01:39:05,248 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 01:39:05,262 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 01:39:05,266 : INFO : built Dictionary(808 unique tokens: ['\"/.', '://', 'absolut', 'addit', 'append']...) from 2 documents (total 2178 corpus positions)\n",
      "2020-11-02 01:39:14,199 : INFO : Computed distances or similarities ('source', 'target')[[0.31915631110336734, 0.7580602780602862]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance is 0.31915631110336734 , the traceability value is 0.7580602780602862\n"
     ]
    }
   ],
   "source": [
    "result = facade.TraceLinkValue(source, target, \"word2vec\")   #for whole list of supported technique of calculating traceability, see the documentation page\n",
    "distance = result[0]\n",
    "traceability = result[1]\n",
    "print(\"distance is {} , the traceability value is {}\".format(distance, traceability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above the TraceLinkValue function return a tuple of numbers. The first one is *distance* and the second one is the *similarity*, which is what we called \"Traceability\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Traceability using word2vec with WMD metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will calculate Traceability using word2vec technique with WMD metric. Since WMD is the default metric, we don't need to specify it in the function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 02:05:25,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:25,119 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:25,120 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:25,255 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:25,256 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:25,258 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:25,258 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:25,259 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:25,260 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:25,278 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:25,281 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c9dbb518>\n",
      "2020-11-02 02:05:25,283 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:25,287 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:25,491 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:25,595 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:25,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:25,612 : INFO : built Dictionary(808 unique tokens: ['\"/.', '://', 'absolut', 'addit', 'append']...) from 2 documents (total 2178 corpus positions)\n",
      "2020-11-02 02:05:34,775 : INFO : Computed distances or similarities ('source', 'target')[[0.31915631110336734, 0.7580602780602862]]\n",
      "2020-11-02 02:05:34,797 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:34,806 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:34,807 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:34,871 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:34,872 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:34,873 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:34,877 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:34,879 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:34,881 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:34,895 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:34,899 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c9a58e10>\n",
      "2020-11-02 02:05:34,900 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:34,903 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:35,114 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:35,215 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:35,227 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:35,231 : INFO : built Dictionary(502 unique tokens: ['accompani', 'addit', 'agre', 'agreement', 'algorithm']...) from 2 documents (total 2134 corpus positions)\n",
      "2020-11-02 02:05:37,575 : INFO : Computed distances or similarities ('source', 'target')[[0.4005029238604272, 0.7140292126228072]]\n",
      "2020-11-02 02:05:37,597 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:37,608 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:37,609 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:37,670 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:37,671 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:37,674 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:37,676 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:37,679 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:37,681 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:37,695 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:37,699 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6f2e2b0>\n",
      "2020-11-02 02:05:37,702 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:37,707 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:37,908 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:38,020 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:38,034 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:38,039 : INFO : built Dictionary(744 unique tokens: ['addit', 'anon', 'appropri', 'aris', 'associ']...) from 2 documents (total 3734 corpus positions)\n",
      "2020-11-02 02:05:45,544 : INFO : Computed distances or similarities ('source', 'target')[[0.4033192749019417, 0.7125962123408275]]\n",
      "2020-11-02 02:05:45,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:45,577 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:45,579 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:45,638 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:45,639 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:45,640 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:45,641 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:45,642 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:45,643 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:45,666 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:45,669 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6d55c88>\n",
      "2020-11-02 02:05:45,670 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:45,674 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:45,876 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:45,974 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:45,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:45,995 : INFO : built Dictionary(320 unique tokens: ['attribut', 'client', 'csr', 'csrattr', 'desir']...) from 2 documents (total 2119 corpus positions)\n",
      "2020-11-02 02:05:46,286 : INFO : Computed distances or similarities ('source', 'target')[[0.4607951037672738, 0.6845587019158813]]\n",
      "2020-11-02 02:05:46,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:46,317 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:46,321 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:46,381 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:46,382 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:46,384 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:46,386 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:46,388 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:46,389 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:46,411 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:46,413 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6a9f5f8>\n",
      "2020-11-02 02:05:46,414 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:46,418 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:46,627 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:46,723 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:46,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:46,740 : INFO : built Dictionary(335 unique tokens: ['addit', 'algorithm', 'appli', 'archiv', 'associ']...) from 2 documents (total 2763 corpus positions)\n",
      "2020-11-02 02:05:47,476 : INFO : Computed distances or similarities ('source', 'target')[[0.4803363918627721, 0.6755221350342243]]\n",
      "2020-11-02 02:05:47,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:47,510 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:47,513 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:47,576 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:47,578 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:47,580 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:47,582 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:47,583 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:47,584 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:47,602 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:47,609 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c683e0b8>\n",
      "2020-11-02 02:05:47,610 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:47,614 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:47,820 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:47,920 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:47,932 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:47,936 : INFO : built Dictionary(481 unique tokens: ['accept', 'access', 'act', 'addit', 'alway']...) from 2 documents (total 1687 corpus positions)\n",
      "2020-11-02 02:05:50,007 : INFO : Computed distances or similarities ('source', 'target')[[0.33878109591201955, 0.7469481030569591]]\n",
      "2020-11-02 02:05:50,030 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:50,041 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:50,042 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:50,197 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:50,198 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:50,199 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:50,199 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:50,203 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:50,204 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:50,220 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:50,226 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c670df28>\n",
      "2020-11-02 02:05:50,227 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:50,230 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:50,440 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:50,548 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:50,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:50,567 : INFO : built Dictionary(455 unique tokens: ['accord', 'addit', 'advis', 'allow', 'attribut']...) from 2 documents (total 2737 corpus positions)\n",
      "2020-11-02 02:05:51,314 : INFO : Computed distances or similarities ('source', 'target')[[0.5759617608808629, 0.6345331624296919]]\n",
      "2020-11-02 02:05:51,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:51,348 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:51,349 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:51,412 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:51,413 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:51,414 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:51,415 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:51,417 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:51,418 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:51,440 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:51,444 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6a9f860>\n",
      "2020-11-02 02:05:51,445 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:51,450 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:51,659 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:51,757 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:51,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:51,771 : INFO : built Dictionary(363 unique tokens: ['access', 'addit', 'advanc', 'alt', 'applic']...) from 2 documents (total 1024 corpus positions)\n",
      "2020-11-02 02:05:53,205 : INFO : Computed distances or similarities ('source', 'target')[[0.4592863652855143, 0.6852664588587084]]\n",
      "2020-11-02 02:05:53,229 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:53,241 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:53,242 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:53,303 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:53,304 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:53,305 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:53,306 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:53,307 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:53,308 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:53,330 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:53,333 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c665fb70>\n",
      "2020-11-02 02:05:53,334 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:53,338 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:53,554 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:53,665 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:53,677 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:53,681 : INFO : built Dictionary(569 unique tokens: ['accept', 'agent', 'also', 'applic', 'appropri']...) from 2 documents (total 2934 corpus positions)\n",
      "2020-11-02 02:05:55,849 : INFO : Computed distances or similarities ('source', 'target')[[0.344334359333342, 0.7438625614656623]]\n",
      "2020-11-02 02:05:55,872 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:55,881 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:55,883 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:55,939 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:55,941 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:55,941 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:55,942 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:55,943 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:55,944 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:55,963 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:55,966 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6910a58>\n",
      "2020-11-02 02:05:55,967 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:55,969 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:56,170 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:56,286 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:56,298 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:56,301 : INFO : built Dictionary(328 unique tokens: ['addit', 'also', 'associ', 'authent', 'avail']...) from 2 documents (total 1543 corpus positions)\n",
      "2020-11-02 02:05:56,881 : INFO : Computed distances or similarities ('source', 'target')[[0.3907830650915825, 0.7190193964104319]]\n"
     ]
    }
   ],
   "source": [
    "d = {'source': [], 'target': [], 'distance':[],'similarity/traceability':[]}\n",
    "df = pd.DataFrame(data=d)\n",
    "for num in range(0, 10):\n",
    "  source_id = source_file[\"ids\"][num].split('/')[-1]\n",
    "  target_id = target_file[\"ids\"][num].split('/')[-1]\n",
    "  source_string = source_file[\"text\"][num]\n",
    "  target_string = target_file[\"text\"][num]\n",
    "  tvm = facade.TraceLinkValue(source_string, target_string, \"word2vec\")\n",
    "  distance = tvm[0]\n",
    "  traceability = tvm[1]\n",
    "  d2 = {'source': source_id, 'target': target_id, 'distance':distance,'similarity/traceability':traceability}\n",
    "  df = df.append(d2,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>distance</th>\n",
       "      <th>similarity/traceability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RQ17-pre.txt</td>\n",
       "      <td>us903.c</td>\n",
       "      <td>0.319156</td>\n",
       "      <td>0.758060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RQ46-pre.txt</td>\n",
       "      <td>us3496.c</td>\n",
       "      <td>0.400503</td>\n",
       "      <td>0.714029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RQ18-pre.txt</td>\n",
       "      <td>us899.c</td>\n",
       "      <td>0.403319</td>\n",
       "      <td>0.712596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RQ48-pre.txt</td>\n",
       "      <td>us4020.c</td>\n",
       "      <td>0.460795</td>\n",
       "      <td>0.684559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RQ42-pre.txt</td>\n",
       "      <td>us897.c</td>\n",
       "      <td>0.480336</td>\n",
       "      <td>0.675522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RQ29-pre.txt</td>\n",
       "      <td>us1060.c</td>\n",
       "      <td>0.338781</td>\n",
       "      <td>0.746948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RQ47-pre.txt</td>\n",
       "      <td>us900.c</td>\n",
       "      <td>0.575962</td>\n",
       "      <td>0.634533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RQ36-pre.txt</td>\n",
       "      <td>us896.c</td>\n",
       "      <td>0.459286</td>\n",
       "      <td>0.685266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RQ56-pre.txt</td>\n",
       "      <td>us894.c</td>\n",
       "      <td>0.344334</td>\n",
       "      <td>0.743863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RQ15-pre.txt</td>\n",
       "      <td>us1005.c</td>\n",
       "      <td>0.390783</td>\n",
       "      <td>0.719019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source    target  distance  similarity/traceability\n",
       "0  RQ17-pre.txt   us903.c  0.319156                 0.758060\n",
       "1  RQ46-pre.txt  us3496.c  0.400503                 0.714029\n",
       "2  RQ18-pre.txt   us899.c  0.403319                 0.712596\n",
       "3  RQ48-pre.txt  us4020.c  0.460795                 0.684559\n",
       "4  RQ42-pre.txt   us897.c  0.480336                 0.675522\n",
       "5  RQ29-pre.txt  us1060.c  0.338781                 0.746948\n",
       "6  RQ47-pre.txt   us900.c  0.575962                 0.634533\n",
       "7  RQ36-pre.txt   us896.c  0.459286                 0.685266\n",
       "8  RQ56-pre.txt   us894.c  0.344334                 0.743863\n",
       "9  RQ15-pre.txt  us1005.c  0.390783                 0.719019"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Traceability using word2vec with SCM metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the traceability using word2vec but with a difference metric: SCM. We need to specify choice of SCM in the function call as word2vec_metric = \"SCM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 02:05:56,946 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:56,955 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:56,955 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:57,018 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:57,020 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:57,023 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:57,023 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:57,027 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:57,028 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:57,048 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:57,051 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c67a47b8>\n",
      "2020-11-02 02:05:57,052 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:57,056 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:57,322 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:57,431 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:57,445 : INFO : Computed distances or similarities ('source', 'target')[[0.1923789381980896, 0.80762106]]\n",
      "2020-11-02 02:05:57,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:57,486 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:57,487 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:57,561 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:57,562 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:57,563 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:57,567 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:57,568 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:57,570 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:57,589 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:57,592 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c709a940>\n",
      "2020-11-02 02:05:57,593 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:57,597 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:57,817 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:57,918 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:57,931 : INFO : Computed distances or similarities ('source', 'target')[[0.27194344997406006, 0.72805655]]\n",
      "2020-11-02 02:05:57,958 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:57,972 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:57,974 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:58,046 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:58,047 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:58,048 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:58,052 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:58,055 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:58,058 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:58,074 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:58,076 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c9c99b38>\n",
      "2020-11-02 02:05:58,077 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:58,082 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:58,306 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:58,416 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:58,431 : INFO : Computed distances or similarities ('source', 'target')[[0.2489951252937317, 0.7510049]]\n",
      "2020-11-02 02:05:58,456 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:58,470 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:58,472 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:58,633 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:58,634 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:58,635 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:58,639 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:58,641 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:58,642 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:58,665 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:58,668 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c62aa630>\n",
      "2020-11-02 02:05:58,669 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:58,673 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:58,870 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:58,969 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:58,984 : INFO : Computed distances or similarities ('source', 'target')[[0.4454513192176819, 0.5545487]]\n",
      "2020-11-02 02:05:59,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:59,030 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:59,036 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:59,106 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:59,108 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:59,108 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:59,115 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:59,116 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:59,118 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:59,131 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:59,134 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6ebd438>\n",
      "2020-11-02 02:05:59,135 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:59,137 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:59,357 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:59,454 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:59,467 : INFO : Computed distances or similarities ('source', 'target')[[0.2724677324295044, 0.72753227]]\n",
      "2020-11-02 02:05:59,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:59,504 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:59,506 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:59,579 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:05:59,580 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:05:59,581 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:05:59,585 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:05:59,587 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:05:59,588 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:05:59,604 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:05:59,607 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6ebd518>\n",
      "2020-11-02 02:05:59,608 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:05:59,616 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:05:59,812 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:05:59,913 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:05:59,925 : INFO : Computed distances or similarities ('source', 'target')[[0.5596067607402802, 0.44039324]]\n",
      "2020-11-02 02:05:59,949 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:05:59,963 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:05:59,965 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:00,036 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:00,037 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:06:00,038 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:00,043 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:00,045 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:06:00,046 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:00,062 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:06:00,064 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c7134668>\n",
      "2020-11-02 02:06:00,066 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:06:00,068 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:06:00,288 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:06:00,390 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:06:00,403 : INFO : Computed distances or similarities ('source', 'target')[[0.6951860189437866, 0.30481398]]\n",
      "2020-11-02 02:06:00,429 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:00,442 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:00,444 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:00,515 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:00,516 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:06:00,517 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:00,521 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:00,523 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:06:00,526 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:00,543 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:06:00,547 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6b9ff28>\n",
      "2020-11-02 02:06:00,548 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:06:00,555 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:06:00,762 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:06:00,861 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:06:00,873 : INFO : Computed distances or similarities ('source', 'target')[[0.3978919982910156, 0.602108]]\n",
      "2020-11-02 02:06:00,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:00,912 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:00,914 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:00,987 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:00,989 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:06:00,991 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:00,992 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:00,993 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:06:00,995 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:01,010 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:06:01,014 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c6115a90>\n",
      "2020-11-02 02:06:01,015 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:06:01,018 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:06:01,221 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:06:01,337 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:06:01,350 : INFO : Computed distances or similarities ('source', 'target')[[0.3673877716064453, 0.6326122]]\n",
      "2020-11-02 02:06:01,374 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:01,388 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:01,389 : INFO : loading Word2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:01,460 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:01,461 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-11-02 02:06:01,462 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:01,464 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:01,466 : INFO : setting ignored attribute cum_table to None\n",
      "2020-11-02 02:06:01,469 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/word2vec_libest.model\n",
      "2020-11-02 02:06:01,488 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-11-02 02:06:01,491 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fe8c65bcdd8>\n",
      "2020-11-02 02:06:01,492 : INFO : iterating over columns in dictionary order\n",
      "2020-11-02 02:06:01,500 : INFO : PROGRESS: at 0.06% columns (1 / 1815, 0.055096% density, 0.055096% projected density)\n",
      "2020-11-02 02:06:01,720 : INFO : PROGRESS: at 55.15% columns (1001 / 1815, 0.140033% density, 0.209102% projected density)\n",
      "2020-11-02 02:06:01,820 : INFO : constructed a sparse term similarity matrix with 0.173668% density\n",
      "2020-11-02 02:06:01,834 : INFO : Computed distances or similarities ('source', 'target')[[0.2640634775161743, 0.7359365]]\n"
     ]
    }
   ],
   "source": [
    "m = len(source_file[\"ids\"])\n",
    "n = len(target_file[\"ids\"])\n",
    "d = {'source': [], 'target': [], 'distance':[],'similarity/traceability':[]}\n",
    "df2 = pd.DataFrame(data=d)\n",
    "for num in range(0, 10):\n",
    "  source_id = source_file[\"ids\"][num].split('/')[-1]\n",
    "  target_id = target_file[\"ids\"][num].split('/')[-1]\n",
    "  source_string = source_file[\"text\"][num]\n",
    "  target_string = target_file[\"text\"][num]\n",
    "  tvm = facade.TraceLinkValue(source_string, target_string, \"word2vec\", word2vec_metric=\"SCM\")\n",
    "  distance = tvm[0]\n",
    "  traceability = tvm[1]\n",
    "  d2 = {'source': source_id, 'target': target_id, 'distance':distance,'similarity/traceability':traceability}\n",
    "  df2 = df2.append(d2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>distance</th>\n",
       "      <th>similarity/traceability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RQ17-pre.txt</td>\n",
       "      <td>us903.c</td>\n",
       "      <td>0.192379</td>\n",
       "      <td>0.807621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RQ46-pre.txt</td>\n",
       "      <td>us3496.c</td>\n",
       "      <td>0.271943</td>\n",
       "      <td>0.728057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RQ18-pre.txt</td>\n",
       "      <td>us899.c</td>\n",
       "      <td>0.248995</td>\n",
       "      <td>0.751005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RQ48-pre.txt</td>\n",
       "      <td>us4020.c</td>\n",
       "      <td>0.445451</td>\n",
       "      <td>0.554549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RQ42-pre.txt</td>\n",
       "      <td>us897.c</td>\n",
       "      <td>0.272468</td>\n",
       "      <td>0.727532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RQ29-pre.txt</td>\n",
       "      <td>us1060.c</td>\n",
       "      <td>0.559607</td>\n",
       "      <td>0.440393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RQ47-pre.txt</td>\n",
       "      <td>us900.c</td>\n",
       "      <td>0.695186</td>\n",
       "      <td>0.304814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RQ36-pre.txt</td>\n",
       "      <td>us896.c</td>\n",
       "      <td>0.397892</td>\n",
       "      <td>0.602108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RQ56-pre.txt</td>\n",
       "      <td>us894.c</td>\n",
       "      <td>0.367388</td>\n",
       "      <td>0.632612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RQ15-pre.txt</td>\n",
       "      <td>us1005.c</td>\n",
       "      <td>0.264063</td>\n",
       "      <td>0.735937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source    target  distance  similarity/traceability\n",
       "0  RQ17-pre.txt   us903.c  0.192379                 0.807621\n",
       "1  RQ46-pre.txt  us3496.c  0.271943                 0.728057\n",
       "2  RQ18-pre.txt   us899.c  0.248995                 0.751005\n",
       "3  RQ48-pre.txt  us4020.c  0.445451                 0.554549\n",
       "4  RQ42-pre.txt   us897.c  0.272468                 0.727532\n",
       "5  RQ29-pre.txt  us1060.c  0.559607                 0.440393\n",
       "6  RQ47-pre.txt   us900.c  0.695186                 0.304814\n",
       "7  RQ36-pre.txt   us896.c  0.397892                 0.602108\n",
       "8  RQ56-pre.txt   us894.c  0.367388                 0.632612\n",
       "9  RQ15-pre.txt  us1005.c  0.264063                 0.735937"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can tell that since we are using a different metric, the result is not the same as the previous calculated using WMD metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Traceability using doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-02 02:06:52,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:52,096 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:52,097 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:52,127 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:52,129 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:52,130 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:52,131 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:52,133 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:52,144 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:52,736 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:52,740 : INFO : Computed distances or similarities ('source', 'target')[[38.68996047973633, 0.025195288378040817]]\n",
      "2020-11-02 02:06:52,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:52,772 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:52,773 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:52,801 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:52,805 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:52,807 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:52,809 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:52,810 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:52,823 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:53,368 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:53,372 : INFO : Computed distances or similarities ('source', 'target')[[38.183570861816406, 0.025520900163146686]]\n",
      "2020-11-02 02:06:53,395 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:53,404 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:53,407 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:53,531 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:53,532 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:53,533 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:53,535 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:53,536 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:53,551 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:54,282 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:54,286 : INFO : Computed distances or similarities ('source', 'target')[[22.618032455444336, 0.04234052950373874]]\n",
      "2020-11-02 02:06:54,313 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:54,326 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:54,327 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:54,357 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:54,359 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:54,359 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:54,364 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:54,366 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:54,377 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:54,865 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:54,869 : INFO : Computed distances or similarities ('source', 'target')[[26.529417037963867, 0.036324779366775944]]\n",
      "2020-11-02 02:06:54,898 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:54,911 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:54,912 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:54,941 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:54,942 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:54,943 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:54,947 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:54,950 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:54,962 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:55,640 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:55,643 : INFO : Computed distances or similarities ('source', 'target')[[23.053821563720703, 0.04157343552877498]]\n",
      "2020-11-02 02:06:55,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:55,679 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:55,680 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:55,711 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:55,712 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:55,717 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:55,718 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:55,723 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:55,736 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:56,058 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:56,062 : INFO : Computed distances or similarities ('source', 'target')[[23.035171508789062, 0.041605694373111714]]\n",
      "2020-11-02 02:06:56,087 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:56,097 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:56,098 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:56,125 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:56,126 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:56,127 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:56,128 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:56,130 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:56,148 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:56,859 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:56,863 : INFO : Computed distances or similarities ('source', 'target')[[33.54679870605469, 0.02894624212531564]]\n",
      "2020-11-02 02:06:56,887 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:56,896 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:56,898 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:56,924 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:56,925 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:56,926 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:56,928 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:56,929 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:56,942 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:57,204 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:57,208 : INFO : Computed distances or similarities ('source', 'target')[[23.154272079467773, 0.041400543833819164]]\n",
      "2020-11-02 02:06:57,229 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:57,238 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:57,239 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:57,265 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:57,266 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:57,267 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:57,268 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:57,269 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:57,281 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:58,044 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:58,047 : INFO : Computed distances or similarities ('source', 'target')[[34.70668029785156, 0.028005963916510297]]\n",
      "2020-11-02 02:06:58,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-11-02 02:06:58,082 : INFO : built Dictionary(1815 unique tokens: ['@return', 'Converts', 'The', 'a', 'and']...) from 153 documents (total 5769 corpus positions)\n",
      "2020-11-02 02:06:58,084 : INFO : loading Doc2Vec object from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:58,109 : INFO : loading vocabulary recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.vocabulary.* with mmap=None\n",
      "2020-11-02 02:06:58,110 : INFO : loading trainables recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.trainables.* with mmap=None\n",
      "2020-11-02 02:06:58,112 : INFO : loading wv recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.wv.* with mmap=None\n",
      "2020-11-02 02:06:58,113 : INFO : loading docvecs recursively from /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model.docvecs.* with mmap=None\n",
      "2020-11-02 02:06:58,114 : INFO : loaded /usr/local/lib/python3.6/dist-packages/ds4se/model/doc2vec_libest.model\n",
      "2020-11-02 02:06:58,126 : INFO : precomputing L2-norms of doc weight vectors\n",
      "2020-11-02 02:06:58,538 : INFO : Infer Doc2Vec on Source and Target Complete\n",
      "2020-11-02 02:06:58,544 : INFO : Computed distances or similarities ('source', 'target')[[32.3048095703125, 0.030025693372869117]]\n"
     ]
    }
   ],
   "source": [
    "m = len(source_file[\"ids\"])\n",
    "n = len(target_file[\"ids\"])\n",
    "d = {'source': [], 'target': [], 'distance':[],'similarity/traceability':[]}\n",
    "df3 = pd.DataFrame(data=d)\n",
    "for num in range(0, 10):\n",
    "  source_id = source_file[\"ids\"][num].split('/')[-1]\n",
    "  target_id = target_file[\"ids\"][num].split('/')[-1]\n",
    "  source_string = source_file[\"text\"][num]\n",
    "  target_string = target_file[\"text\"][num]\n",
    "  tvm = facade.TraceLinkValue(source_string, target_string, \"doc2vec\")\n",
    "  distance = tvm[0]\n",
    "  traceability = tvm[1]\n",
    "  d2 = {'source': source_id, 'target': target_id, 'distance':distance,'similarity/traceability':traceability}\n",
    "  df3 = df3.append(d2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>distance</th>\n",
       "      <th>similarity/traceability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RQ17-pre.txt</td>\n",
       "      <td>us903.c</td>\n",
       "      <td>38.689960</td>\n",
       "      <td>0.025195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RQ46-pre.txt</td>\n",
       "      <td>us3496.c</td>\n",
       "      <td>38.183571</td>\n",
       "      <td>0.025521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RQ18-pre.txt</td>\n",
       "      <td>us899.c</td>\n",
       "      <td>22.618032</td>\n",
       "      <td>0.042341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RQ48-pre.txt</td>\n",
       "      <td>us4020.c</td>\n",
       "      <td>26.529417</td>\n",
       "      <td>0.036325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RQ42-pre.txt</td>\n",
       "      <td>us897.c</td>\n",
       "      <td>23.053822</td>\n",
       "      <td>0.041573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RQ29-pre.txt</td>\n",
       "      <td>us1060.c</td>\n",
       "      <td>23.035172</td>\n",
       "      <td>0.041606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RQ47-pre.txt</td>\n",
       "      <td>us900.c</td>\n",
       "      <td>33.546799</td>\n",
       "      <td>0.028946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RQ36-pre.txt</td>\n",
       "      <td>us896.c</td>\n",
       "      <td>23.154272</td>\n",
       "      <td>0.041401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RQ56-pre.txt</td>\n",
       "      <td>us894.c</td>\n",
       "      <td>34.706680</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RQ15-pre.txt</td>\n",
       "      <td>us1005.c</td>\n",
       "      <td>32.304810</td>\n",
       "      <td>0.030026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source    target   distance  similarity/traceability\n",
       "0  RQ17-pre.txt   us903.c  38.689960                 0.025195\n",
       "1  RQ46-pre.txt  us3496.c  38.183571                 0.025521\n",
       "2  RQ18-pre.txt   us899.c  22.618032                 0.042341\n",
       "3  RQ48-pre.txt  us4020.c  26.529417                 0.036325\n",
       "4  RQ42-pre.txt   us897.c  23.053822                 0.041573\n",
       "5  RQ29-pre.txt  us1060.c  23.035172                 0.041606\n",
       "6  RQ47-pre.txt   us900.c  33.546799                 0.028946\n",
       "7  RQ36-pre.txt   us896.c  23.154272                 0.041401\n",
       "8  RQ56-pre.txt   us894.c  34.706680                 0.028006\n",
       "9  RQ15-pre.txt  us1005.c  32.304810                 0.030026"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using doc2vec, we can see the library gives another different result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we only checked the first 10 pairs of source and target artifacts, but you can easily extend it to include more. This tutorial should gives you a sense of how to use the library for calculating traceability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
