{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp inference.question_answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "> The Question Answering module within AdaptNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "from torch import tensor\n",
    "from typing import Tuple, List, Union, Dict\n",
    "from collections import OrderedDict, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    XLNetForQuestionAnswering,\n",
    "    XLMForQuestionAnswering,\n",
    "    CamembertForQuestionAnswering,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    RobertaForQuestionAnswering,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    SquadExample,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "from transformers.data.processors.squad import SquadResult\n",
    "\n",
    "from adaptnlp.model import AdaptiveModel, DataLoader\n",
    "from adaptnlp.model_hub import HFModelResult\n",
    "from adaptnlp.inference.utils import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    ")\n",
    "\n",
    "from fastcore.basics import risinstance, nested_attr, Self, patch, listify\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.torch_core import apply, to_detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class QACallback(Callback):\n",
    "    \"Basic Question Answering Data Callback\"\n",
    "    order = -2\n",
    "    _qa_models = [XLMForQuestionAnswering, RobertaForQuestionAnswering, DistilBertForQuestionAnswering]\n",
    "\n",
    "    def __init__(self, xmodel_instances, features):\n",
    "        self.xmodel_instances = xmodel_instances\n",
    "        self.features = features\n",
    "\n",
    "    def before_batch(self):\n",
    "        \"Adjusts `token_type_ids` if model is in `_qa_models`\"\n",
    "        if risinstance(self._qa_models, self.learn.model): del self.learn.inputs[\"token_type_ids\"]\n",
    "        if len(self.xb) > 3: self.example_indices = self.xb[3]\n",
    "\n",
    "        if isinstance(self.learn.model, self.xmodel_instances):\n",
    "            self.learn.inputs.update({'cls_index': self.xb[4], 'p_mask': self.xb[5]})\n",
    "            # for lang_id-sensitive xlm models\n",
    "            if nested_attr(self.learn.model, 'config.lang2id', False):\n",
    "                # Set language id as 0 for now\n",
    "                self.learn.inputs.update(\n",
    "                    {\n",
    "                        'langs': (\n",
    "                        torch.ones(self.xb[0].shape, dtype=torch.int64) * 0\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "    def after_pred(self):\n",
    "        \"Generate SquadResults\"\n",
    "        for i, example_index in enumerate(self.example_indices):\n",
    "            eval_feature = self.features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            output = [self.pred[output][i] for output in self.pred]\n",
    "            output = apply(Self.numpy(), to_detach(output))\n",
    "\n",
    "            if isinstance(self.learn.model, self.xmodel_instances):\n",
    "                # Some models like the ones in `self.xmodel_instances` use 5 arguments for their predictions\n",
    "                start_logits = output[0]\n",
    "                start_top_index = output[1]\n",
    "                end_logits = output[2]\n",
    "                end_top_index = output[3]\n",
    "                cls_logits = output[4]\n",
    "\n",
    "                self.learn.pred = SquadResult(\n",
    "                    unique_id,\n",
    "                    start_logits,\n",
    "                    end_logits,\n",
    "                    start_top_index=start_top_index,\n",
    "                    end_top_index=end_top_index,\n",
    "                    cls_logits=cls_logits\n",
    "                )\n",
    "            else:\n",
    "                start_logits, end_logits = output\n",
    "                self.learn.pred = SquadResult(unique_id, start_logits, end_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from adaptnlp.result import DetailLevel\n",
    "\n",
    "class QAResult:\n",
    "    \"\"\"\n",
    "    A result class designed for Question Answering models\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        examples:List[SquadExample], \n",
    "        top_predictions:Union[str, OrderedDict], \n",
    "        all_nbest_json:List[OrderedDict],\n",
    "        n_best_size:int\n",
    "    ):\n",
    "        self._examples = examples\n",
    "        self._top_predictions = top_predictions\n",
    "        self._all_nbest_json = all_nbest_json\n",
    "        self.n_best_size = n_best_size\n",
    "        \n",
    "    @property\n",
    "    def queries(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        The original queries\n",
    "        \"\"\"\n",
    "        return [e.question_text for e in self._examples]\n",
    "        \n",
    "    @property\n",
    "    def contexts(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        The original context or contexts\n",
    "        \"\"\"\n",
    "        if len(self._examples) < 2:\n",
    "            return self._examples[0].context_text\n",
    "        else:\n",
    "            return [e.context_text for e in self._examples]\n",
    "        \n",
    "    @property\n",
    "    def probs(self) -> List[List[tensor]]:\n",
    "        \"\"\"\n",
    "        The probabilities returned for each question\n",
    "        \"\"\"\n",
    "        return torch.stack([tensor([o['probability'] for o in self._all_nbest_json[i]]) for i in self._all_nbest_json], dim=0)\n",
    "    \n",
    "    @property\n",
    "    def best_answers(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        The best answer from each question\n",
    "        \"\"\"\n",
    "        a = []\n",
    "        for i in range(len(self._all_nbest_json)):\n",
    "            o = OrderedDict()\n",
    "            if self.n_best_size > len(self._all_nbest_json[str(i)]):\n",
    "                print(f\"Warning! `n_best_size` {self.n_best_size} is greater than the actual number of answers {len(self._all_nbest_json[str(i)])}, only returning {len(self._all_nbest_json[str(i)])} answers\")\n",
    "                rng = len(self._all_nbest_json[str(i)])\n",
    "            else:\n",
    "                rng = self.n_best_size\n",
    "            for j in range(rng):\n",
    "                o.update({j:self._all_nbest_json[str(i)][j]['text']})\n",
    "            a.append(o)\n",
    "        return a\n",
    "    \n",
    "    @property\n",
    "    def all_answers(self) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Every answer ordered by probability from each question\n",
    "        \"\"\"\n",
    "        return tuple([tuple([o['text'] for o in self._all_nbest_json[i]]) for i in self._all_nbest_json])\n",
    "    \n",
    "    def to_dict(self, detail_level:DetailLevel=DetailLevel.Low):\n",
    "        \"\"\"\n",
    "        Return details about `self` at various detail levels\n",
    "        \"\"\"\n",
    "        o = {\n",
    "                'queries':self.queries,\n",
    "                'best_answers':self.best_answers,\n",
    "            }\n",
    "        if detail_level == 'medium' or detail_level == 'high':\n",
    "            # Add a dictionary of query and answer and probabilities, also returns contexts\n",
    "            o['pairings'] = OrderedDict({\n",
    "                q:(a,p) for (q,a,p) in zip(self.queries, self.all_answers, self.probs)\n",
    "            })\n",
    "            o['context'] = self.contexts\n",
    "        if detail_level == 'high':\n",
    "            # Add SquadExamples, and n_best_json\n",
    "            o['squad_example'] = self._examples\n",
    "            o['n_best_json'] = self._all_nbest_json\n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformersQuestionAnswering(AdaptiveModel):\n",
    "    \"\"\"Adaptive Model for Transformers Question Answering Model\n",
    "\n",
    "    **Parameters**\n",
    "\n",
    "    * **tokenizer** - A tokenizer object from Huggingface's transformers (TODO)and tokenizers *\n",
    "    * **model** - A transformer Question Answering model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, model: PreTrainedModel):\n",
    "        # Load up model and tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        super().__init__()\n",
    "\n",
    "        # Sets internal model\n",
    "        self.set_model(model)\n",
    "        self.xmodel_instances = (XLNetForQuestionAnswering, XLMForQuestionAnswering)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_name_or_path: str) -> AdaptiveModel:\n",
    "        \"\"\"Class method for loading and constructing this model\n",
    "\n",
    "        * **model_name_or_path** - A key string of one of Transformer's pre-trained Question Answering (SQUAD) models\n",
    "        \"\"\"\n",
    "        # QA tokenizers not compatible with fast tokenizers yet\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
    "        qa_model = cls(tokenizer, model)\n",
    "        return qa_model\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        query: Union[List[str], str],\n",
    "        context: Union[List[str], str],\n",
    "        n_best_size: int = 5,\n",
    "        mini_batch_size: int = 32,\n",
    "        max_answer_length: int = 10,\n",
    "        do_lower_case: bool = False,\n",
    "        version_2_with_negative: bool = False,\n",
    "        verbose_logging: bool = False,\n",
    "        null_score_diff_threshold: float = 0.0,\n",
    "        max_seq_length: int = 512,\n",
    "        doc_stride: int = 128,\n",
    "        max_query_length: int = 64,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Tuple[str, List[OrderedDict]], Tuple[OrderedDict, OrderedDict]]:\n",
    "        \"\"\"Predict method for running inference using the pre-trained question answering model\n",
    "\n",
    "        * **query** - String or list of strings that specify the ordered questions corresponding to `context`\n",
    "        * **context** - String or list of strings that specify the ordered contexts corresponding to `query`\n",
    "        * **n_best_size** - Number of top n results you want\n",
    "        * **mini_batch_size** - Mini batch size\n",
    "        * **max_answer_length** - Maximum token length for answers that are returned\n",
    "        * **do_lower_case** - Set as `True` if using uncased QA models\n",
    "        * **version_2_with_negative** - Set as True if using QA model with SQUAD2.0\n",
    "        * **verbose_logging** - Set True if you want prediction verbose loggings\n",
    "        * **null_score_diff_threshold** - Threshold for predicting null(no answer) in Squad 2.0 Model.  Default is 0.0.  Raise this if you want fewer null answers\n",
    "        * **max_seq_length** - Maximum context token length. Check model configs to see max sequence length the model was trained with\n",
    "        * **doc_stride** - Number of token strides to take when splitting up conext into chunks of size `max_seq_length`\n",
    "        * **max_query_length** - Maximum token length for queries\n",
    "        * **&ast;&ast;kwargs**(Optional) - Optional arguments for the Transformers model (mostly for saving evaluations)\n",
    "        \"\"\"\n",
    "        # Make string input consistent as list\n",
    "        if isinstance(query, str):\n",
    "            query = [query]\n",
    "            context = [context]\n",
    "        assert len(query) == len(context)\n",
    "        examples = self._mini_squad_processor(query=query, context=context)\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples,\n",
    "            self.tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset='pt',\n",
    "            threads=1,\n",
    "        )\n",
    "        all_results = []\n",
    "\n",
    "        dl = DataLoader(dataset, batch_size=mini_batch_size)\n",
    "\n",
    "        cb = QACallback(self.xmodel_instances, features)\n",
    "\n",
    "        all_results, _ = super().get_preds(dl=dl, cbs=[cb])\n",
    "\n",
    "        if isinstance(self.model, self.xmodel_instances):\n",
    "            start_n_top = (\n",
    "                self.model.config.start_n_top\n",
    "                if hasattr(self.model, 'config')\n",
    "                else self.model.module.config.start_n_top\n",
    "            )\n",
    "            end_n_top = (\n",
    "                self.model.config.end_n_top\n",
    "                if hasattr(self.model, 'config')\n",
    "                else self.model.module.config.end_n_top\n",
    "            )\n",
    "\n",
    "            answers, n_best = compute_predictions_log_probs(\n",
    "                examples,\n",
    "                features,\n",
    "                all_results,\n",
    "                n_best_size,\n",
    "                max_answer_length,\n",
    "                start_n_top,\n",
    "                end_n_top,\n",
    "                version_2_with_negative,\n",
    "                self.tokenizer,\n",
    "                verbose_logging,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            answers, n_best = compute_predictions_logits(\n",
    "                examples,\n",
    "                features,\n",
    "                all_results,\n",
    "                n_best_size,\n",
    "                max_answer_length,\n",
    "                do_lower_case,\n",
    "                verbose_logging,\n",
    "                version_2_with_negative,\n",
    "                null_score_diff_threshold,\n",
    "                self.tokenizer,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        return examples, answers, n_best\n",
    "\n",
    "    def _mini_squad_processor(\n",
    "        self, query: List[str], context: List[str]\n",
    "    ) -> List[SquadExample]:\n",
    "        \"\"\"Squad data processor to create `SquadExamples`\n",
    "\n",
    "        * **query** - List of query strings, must be same length as `context`\n",
    "        * **context** - List of context strings, must be same length as `query`\n",
    "\n",
    "        \"\"\"\n",
    "        assert len(query) == len(context)\n",
    "        examples = []\n",
    "        title = 'qa'\n",
    "        is_impossible = False\n",
    "        answer_text = None\n",
    "        start_position_character = None\n",
    "        answers = ['answer']\n",
    "        for idx, (q, c) in enumerate(zip(query, context)):\n",
    "            example = SquadExample(\n",
    "                qas_id=str(idx),\n",
    "                question_text=q,\n",
    "                context_text=c,\n",
    "                answer_text=answer_text,\n",
    "                start_position_character=start_position_character,\n",
    "                title=title,\n",
    "                is_impossible=is_impossible,\n",
    "                answers=answers,\n",
    "            )\n",
    "            examples.append(example)\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EasyQuestionAnswering:\n",
    "    \"\"\"Question Answering Module\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    ```python\n",
    "    >>> qa = adaptnlp.EasyQuestionAnswering()\n",
    "    >>> qa.predict_qa(query='What is life?', context='Life is NLP.', n_best_size=5, mini_batch_size=1)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models: Dict[AdaptiveModel] = defaultdict(bool)\n",
    "\n",
    "    def predict_qa(\n",
    "        self,\n",
    "        query: Union[List[str], str],\n",
    "        context: Union[List[str], str],\n",
    "        n_best_size: int = 5,\n",
    "        mini_batch_size: int = 32,\n",
    "        model_name_or_path: Union[str, HFModelResult] = 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
    "        detail_level = DetailLevel.Low,\n",
    "        **kwargs,\n",
    "    ) -> Union[QAResult, dict]:\n",
    "        \"\"\"Predicts top_n answer spans of query in regards to context\n",
    "\n",
    "        * **query** - String or list of strings that specify the ordered questions corresponding to `context`\n",
    "        * **context** - String or list of strings that specify the ordered contexts corresponding to `query`\n",
    "        * **n_best_size** - The top n answers returned\n",
    "        * **mini_batch_size** - Mini batch size for inference\n",
    "        * **model_name_or_path** - Path to QA model or name of QA model at huggingface.co/models\n",
    "        * **detail_level** - String or DetailLevel of what amount of information should be returned. If `None` will return `QAResult`\n",
    "        * **kwargs**(Optional) - Keyword arguments for `AdaptiveModel`s like `TransformersQuestionAnswering`\n",
    "\n",
    "        **return** - Either a dictionary of results or a QAResult\n",
    "        \"\"\"\n",
    "        name = getattr(model_name_or_path, 'name', model_name_or_path)\n",
    "        try:\n",
    "            if not self.models[name]:\n",
    "                self.models[name] = TransformersQuestionAnswering.load(\n",
    "                    name\n",
    "                )\n",
    "        except OSError:\n",
    "            logger.info(\n",
    "                f'{name} not a valid Transformers pre-trained QA model...check path or huggingface.co/models'\n",
    "            )\n",
    "            raise ValueError(\n",
    "                f'{name} is not a valid path or model name from huggingface.co/models'\n",
    "            )\n",
    "            return OrderedDict(), [OrderedDict()]\n",
    "\n",
    "        model = self.models[name]\n",
    "        \n",
    "        examples, top_answer, top_n_answers = model.predict(\n",
    "            query=query,\n",
    "            context=context,\n",
    "            n_best_size=n_best_size,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        result = QAResult(examples, top_answer, top_n_answers, n_best_size)\n",
    "        \n",
    "        return result.to_dict(detail_level) if detail_level is not None else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 40.17it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 6875.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "qa_model = EasyQuestionAnswering()\n",
    "context = \"\"\"Amazon.com, Inc.[6] (/ˈæməzɒn/), is an American multinational technology company based in Seattle, \n",
    "Washington that focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. \n",
    "It is considered one of the Big Four technology companies along with Google, Apple, and Facebook.[7][8][9] \n",
    "Amazon is known for its disruption of well-established industries through technological innovation and mass \n",
    "scale.[10][11][12] It is the world's largest e-commerce marketplace, AI assistant provider, and cloud computing \n",
    "platform[13] as measured by revenue and market capitalization.[14] Amazon is the largest Internet company by \n",
    "revenue in the world.[15] It is the second largest private employer in the United States[16] and one of the world's \n",
    "most valuable companies. Amazon is the second largest technology company by revenue. Amazon was founded by Jeff Bezos \n",
    "on July 5, 1994, in Bellevue, Washington. The company initially started as an online marketplace for books but later \n",
    "expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon \n",
    "surpassed Walmart as the most valuable retailer in the United States by market capitalization.[17] In 2017, Amazon \n",
    "acquired Whole Foods Market for $13.4 billion, which vastly increased Amazon's presence as a brick-and-mortar \n",
    "retailer.[18] In 2018, Bezos announced that its two-day delivery service, Amazon Prime, had surpassed 100 million \n",
    "subscribers worldwide\n",
    "\"\"\"\n",
    "res = qa_model.predict_qa(\n",
    "    query=[\"What does Amazon do?\"], \n",
    "    context=[context], \n",
    "    n_best_size=10, \n",
    "    mini_batch_size=1, \n",
    "    model_name_or_path=\"distilbert-base-uncased-distilled-squad\",\n",
    ")\n",
    "test_eq(res['best_answers'][0][0], 'disruption of well-established industries')\n",
    "test_eq(len(res['best_answers'][0]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 3/3 [00:00<00:00, 63.24it/s]\n",
      "add example index and unique id: 100%|██████████| 3/3 [00:00<00:00, 19269.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! `n_best_size` 5 is greater than the actual number of answers 4, only returning 4 answers\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "questions = [\"What does Amazon do?\",\n",
    "             \"What happened July 5, 1994?\",\n",
    "             \"How much did Amazon acquire Whole Foods for?\"]\n",
    "\n",
    "results = qa_model.predict_qa(\n",
    "    query=questions, \n",
    "    context=[context]*3,\n",
    "    mini_batch_size=1, \n",
    "    model_name_or_path=\"distilbert-base-uncased-distilled-squad\"\n",
    ")\n",
    "\n",
    "test_eq(len(results['best_answers']), 3)\n",
    "test_eq(len(results['best_answers'][0]), 5)\n",
    "test_eq(len(results['best_answers'][1]), 4)\n",
    "test_eq(len(results['best_answers'][2]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1678/2871540299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_model_by_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'question-answering'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"What does Amazon do?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_best_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disruption of well-established industries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from adaptnlp.model_hub import HFModelHub\n",
    "hub = HFModelHub()\n",
    "models = hub.search_model_by_task('question-answering')\n",
    "model = models[-1]\n",
    "res = qa_model.predict_qa(query=\"What does Amazon do?\", context=context, n_best_size=10, mini_batch_size=1, model_name_or_path=model)\n",
    "test_eq(res['best_answers'][0][0], 'disruption of well-established industries')\n",
    "test_eq(len(res['best_answers'][0]), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
