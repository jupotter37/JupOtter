{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53946cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca157d7d",
   "metadata": {},
   "source": [
    "# Tutorial&#58; Fine-Tuning Sequence Classification on HuggingFace `Datasets` with MRPC\n",
    "> Tuning a Sequence Classification model on the Microsoft MRPC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d8af2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will be showing an end-to-end example of fine-tuning a Transformer for sequence classification on a custom dataset in HuggingFace `Dataset` format.\n",
    "\n",
    "By the end of this you should be able to:\n",
    "\n",
    "1. Build a dataset with the `TaskDatasets` class, and their DataLoaders\n",
    "2. Build a `SequenceClassificationTuner` quickly, find a good learning rate, and train with the One-Cycle Policy\n",
    "3. Save that model away, to be used with deployment or other HuggingFace libraries\n",
    "4. Apply inference using both the `Tuner` available function as well as with the `EasySequenceClassifier` class within AdaptNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44baa5",
   "metadata": {},
   "source": [
    "## Installing the Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce03b17",
   "metadata": {},
   "source": [
    "This tutorial utilizies the latest AdaptNLP version, as well as parts of the `fastai` library. Please run the below code to install them:\n",
    "\n",
    "```python\n",
    "!pip install adaptnlp -U\n",
    "```\n",
    "(or `pip3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VOwOHNTgyJzY",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbverbose.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de58d7",
   "metadata": {},
   "source": [
    "## Getting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a039f4f",
   "metadata": {},
   "source": [
    "First we need a dataset. We will use `dataset`'s `load_dataset` function to quickly generate a raw dataset straight from HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8faf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb182fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef7942",
   "metadata": {},
   "source": [
    "We now have a raw `datasets` dataset, which we can index into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb649bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01caee30",
   "metadata": {},
   "source": [
    "Now that we have the data downloaded, let's decide on a model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320caa02",
   "metadata": {},
   "source": [
    "## Picking a Model with the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b200d",
   "metadata": {},
   "source": [
    "AdaptNLP has a `HFModelHub` class that allows you to communicate with the HuggingFace Hub and pick a model from it, as well as a namespace `HF_TASKS` class with a list of valid tasks we can search by.\n",
    "\n",
    "Let's try and find one suitable for sequence classification.\n",
    "\n",
    "First we need to import the class and generate an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import HFModelHub, HF_TASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = HFModelHub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5e54e",
   "metadata": {},
   "source": [
    "Next we can search for a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = hub.search_model_by_task(HF_TASKS.TEXT_CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec0167",
   "metadata": {},
   "source": [
    "Let's look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17717851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model Name: distilbert-base-uncased-finetuned-sst-2-english, Tasks: [text-classification],\n",
       " Model Name: roberta-base-openai-detector, Tasks: [text-classification],\n",
       " Model Name: roberta-large-mnli, Tasks: [text-classification],\n",
       " Model Name: roberta-large-openai-detector, Tasks: [text-classification]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc23ba",
   "metadata": {},
   "source": [
    "These are models specifically tagged with the `text-classification` tag, so you may not see a few models you would expect such as `bert_base_cased`.\n",
    "\n",
    "Let's search for that one for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7941ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = hub.search_model_by_name('bert-base-uncased', user_uploaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e340d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model Name: bert-base-uncased, Tasks: [fill-mask],\n",
       " Model Name: distilbert-base-uncased-distilled-squad, Tasks: [question-answering],\n",
       " Model Name: distilbert-base-uncased-finetuned-sst-2-english, Tasks: [text-classification],\n",
       " Model Name: distilbert-base-uncased, Tasks: [fill-mask],\n",
       " Model Name: 123abhiALFLKFO/distilbert-base-uncased-finetuned-cola, Tasks: [text-classification]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56640897",
   "metadata": {},
   "source": [
    "We want the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567732b",
   "metadata": {},
   "source": [
    "Now that we have picked a model, let's use the data API to prepare our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c80c1",
   "metadata": {},
   "source": [
    "> Note: It should be mentioned that this is optional, you can always just pass in the string name of a model such as \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676253e",
   "metadata": {},
   "source": [
    "## Building `TaskDatasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa7c29",
   "metadata": {},
   "source": [
    "All of the task-specific high-level data API's (such as `SequenceClassificationDatasets`) all wrap around the `TaskDatasets` class, which is a small wrapper around `datasets` highly efficient `Dataset` class.\n",
    "\n",
    "This integration was valuable because it provides a fast and memory-efficient way to use large datasets with minimal effort. \n",
    "\n",
    "First let's import the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83994a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import TaskDatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965c387",
   "metadata": {},
   "source": [
    "The `TaskDatasets` class has no class constructors outside the normal one. The reason for this is it takes in raw `Datasets` and other tokenizer arguments to build from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cfe3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"TaskDatasets\" class=\"doc_header\"><code>class</code> <code>TaskDatasets</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L168\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>TaskDatasets</code>(**`train_dset`**, **`valid_dset`**, **`tokenizer_name`**:`str`=*`None`*, **`tokenize`**:`bool`=*`True`*, **`tokenize_func`**:`callable`=*`None`*, **`tokenize_kwargs`**:`dict`=*`{}`*, **`auto_kwargs`**:`dict`=*`{}`*, **`remove_cols`**:`Union`\\[`str`, `List`\\[`str`\\]\\]=*`None`*)\n",
       "\n",
       "A set of datasets for a particular task, with a simple API.\n",
       "\n",
       "Note: This is the base API, `items` should be a set of regular text and model-ready labels,\n",
       "      including label or one-hot encoding being applied.\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`train_dset`** : *`<class 'inspect._empty'>`*\t<p>A train `Dataset` object</p>\n",
       "\n",
       "\n",
       " - **`valid_dset`** : *`<class 'inspect._empty'>`*\t<p>A validation `Dataset` object</p>\n",
       "\n",
       "\n",
       " - **`tokenizer_name`** : *`<class 'str'>`*, *optional*\t<p>The string name of a `HuggingFace` tokenizer or model. If `None`, will not tokenize the dataset.</p>\n",
       "\n",
       "\n",
       " - **`tokenize`** : *`<class 'bool'>`*, *optional*\t<p>Whether to tokenize the dataset immediatly</p>\n",
       "\n",
       "\n",
       " - **`tokenize_func`** : *`<built-in function callable>`*, *optional*\t<p>A function to tokenize an item with</p>\n",
       "\n",
       "\n",
       " - **`tokenize_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>Some kwargs for when we call the tokenizer</p>\n",
       "\n",
       "\n",
       " - **`auto_kwargs`** : *`<class 'dict'>`*, *optional*\t<p>Some kwargs when calling `AutoTokenizer.from_pretrained`</p>\n",
       "\n",
       "\n",
       " - **`remove_cols`** : *`typing.Union[str, typing.List[str]]`*, *optional*\t<p>What columns to remove</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from adaptnlp import TaskDatasets\n",
    "show_doc(TaskDatasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f48f0",
   "metadata": {},
   "source": [
    "Anything you would normally pass to the tokenizer call (such as `max_length`, `padding`) should go in `tokenize_kwargs`, and anything going to the `AutoTokenizer.from_pretrained` constructor should be passed to the `auto_kwargs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8b0ef",
   "metadata": {},
   "source": [
    "## Custom Tokenization Function and Finishing our `TaskDatasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a80d0",
   "metadata": {},
   "source": [
    "You may notice there is an extra step here: We need to pass in a `tokenize_func`. In the other tutorials we used a very basic tokenizing function, and this has a default for that as well.\n",
    "\n",
    "However given our dataset, we need to implement our own tokenization function.\n",
    "\n",
    "To do so, your function must take in an `item`, a `tokenizer`, and `tokenize_kwargs`. It should be noted that **you do not have to declare any of these**. All of them are attributes that the `TaskDatasets` has access to, and will be passed to this function implicitly.\n",
    "\n",
    "What you need to declare is *how* you want the tokenizer applied.\n",
    "\n",
    "In our case we have two separate sentences that need to be tokenized at once. These texts live in that dictionary we saw earlier at the keys `sentence1` and `sentence2`.\n",
    "\n",
    "Let's write that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a1e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(\n",
    "    item, # A single item in the dataset\n",
    "    tokenizer, # The implicit tokenizer that `TaskDatasets` has access to\n",
    "    tokenize_kwargs, # Key word arguments passed into the constructor of `TaskDatasets`\n",
    "):\n",
    "    \"A basic tokenization function for two items\"\n",
    "    return tokenizer(\n",
    "        item['sentence1'],\n",
    "        item['sentence2'],\n",
    "        **tokenize_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683bdf5",
   "metadata": {},
   "source": [
    "Along with building our own tokenize function, we need to tell `Datasets` what columns to drop when we pull an item from our dataset. \n",
    "\n",
    "These are synonymous with `Datasets` `remove_cols`.\n",
    "\n",
    "In our problem this includes the `sentence1`, `sentence2`, and `idx` keys, as our tokenized input gets put into a `text` key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = ['sentence1', 'sentence2', 'idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19b76d",
   "metadata": {},
   "source": [
    "Finally we'll declare some arguments for our tokenize function, specifically ensuring our max length is reasonable and that we should pad our samples to that length:\n",
    "\n",
    "> Note: These vary problem to problem. You should look at your specific model and dataset to judge what a proper max length and padding should be. AdaptNLP does its best to use a decent default, but it may not work for every problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_kwargs = {'max_length':64, 'padding':True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224521a",
   "metadata": {},
   "source": [
    "Let's build our `TaskDatasets` now, passing in everything we built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafa7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1ecca674724c0a8a738cb605f20d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8859a7d03af942d4891a91c6a32f3b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dsets = TaskDatasets(\n",
    "    train_dset = raw_datasets['train'], # Our training `Dataset`\n",
    "    valid_dset = raw_datasets['validation'], # Our validation `Dataset`\n",
    "    tokenizer_name = model.name, # The name of our model\n",
    "    tokenize_kwargs = tokenize_kwargs, # The tokenizer kwargs\n",
    "    tokenize_func = tok_func, # The tokenization function\n",
    "    remove_cols = remove_cols # The columns to remove after tokenizing our input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c3911",
   "metadata": {},
   "source": [
    "You may be wondering why we use the `TaskDatasets` class, this is a convience wrapper around much of the functions and tasks you need to call when using `datasets`'s `Dataset` class, and there are a few special behaviors to quickly build working `AdaptiveDataLoaders` as well.\n",
    "\n",
    "Let's build these `AdaptiveDataLoaders`, which are just fastai's `DataLoaders` class, but it overrides a few functions to have it work nicely with HuggingFace's `Dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc36db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TaskDatasets.dataloaders\" class=\"doc_header\"><code>TaskDatasets.dataloaders</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L247\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TaskDatasets.dataloaders</code>(**`batch_size`**:`int`=*`8`*, **`shuffle_train`**:`bool`=*`True`*, **`collate_fn`**:`callable`=*`None`*, **`path`**=*`'.'`*, **`device`**=*`None`*)\n",
       "\n",
       "Creates `DataLoaders` from the dataset\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`batch_size`** : *`<class 'int'>`*, *optional*\t<p>A batch size</p>\n",
       "\n",
       "\n",
       " - **`shuffle_train`** : *`<class 'bool'>`*, *optional*\t<p>Whether to shuffle the training dataset</p>\n",
       "\n",
       "\n",
       " - **`collate_fn`** : *`<built-in function callable>`*, *optional*\t<p>A custom collation function</p>\n",
       "\n",
       "\n",
       " - **`path`** : *`<class 'str'>`*, *optional*\n",
       "\n",
       " - **`device`** : *`<class 'NoneType'>`*, *optional*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TaskDatasets.dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9de40",
   "metadata": {},
   "source": [
    "To build our `DataLoaders`, can call `.dataloaders`, specifying our batch size and a collate function to use. In our case we will collate with the `DataCollatorWithPadding` class out of `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dsets.dataloaders(\n",
    "    batch_size=8,\n",
    "    collate_fn=DataCollatorWithPadding(tokenizer=dsets.tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df0c08",
   "metadata": {},
   "source": [
    "Finally, let's view a batch of data with the `show_batch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ad300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the government said firstenergy nuclear determined that a contractor had established an unprotected high - speed computer connection to its corporate network that allowed the \" slammer \" infection to spread internally. it said firstenergy determined that a contractor had established an unprotected computer connection to its corporate network that allowed the so - called ` ` slammer'' worm to spread internally.</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that failure to act contributed to september the 11th and the failure to act today continues ( to put ) americans in a vulnerable circumstance, \" graham said. \" that failure to act contributed to september 11 and the failure to act today continues [ to put ] americans in a vulnerable circumstance, \" said graham.</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the companies said \" it was not our intention to target or offend any group or persons or to incite hatred or violence. \" \" in creating the game, it was not our intention to target or offend any group or persons or to incite hatred or violence against such groups persons. \"</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the 4th u. s. circuit court of appeals has unsealed a heavily edited transcript of the june 3 court session where classified evidence was discussed out of public earshot. the 4th u. s. circuit court of appeals in richmond, va., released the edited transcript of a closed hearing june 3, which followed a public proceeding.</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23348be",
   "metadata": {},
   "source": [
    "Since this isn't a pre-built `*TaskDatasets` object, the `show_batch` looks a little plain, but it gets across exactly what you would need to see. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5343c100",
   "metadata": {},
   "source": [
    "Next let's build a `Tuner` and train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586172ad",
   "metadata": {},
   "source": [
    "## Building a `Tuner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2d9bf",
   "metadata": {},
   "source": [
    "Next we need to build a compatible `Tuner` for our problem. These tuners contain good defaults for our problem space, including loss functions and metrics.\n",
    "\n",
    "First let's import the `SequenceClassificationTuner` and view it's documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import SequenceClassificationTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WRxJWveszSmY",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SequenceClassificationTuner\" class=\"doc_header\"><code>class</code> <code>SequenceClassificationTuner</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/sequence_classification.py#L222\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SequenceClassificationTuner</code>(**`dls`**:`DataLoaders`, **`model_name`**:`str`, **`tokenizer`**=*`None`*, **`loss_func`**=*`CrossEntropyLoss()`*, **`metrics`**=*`[<function accuracy at 0x7f66ea072280>, <fastai.metrics.AccumMetric object at 0x7f66e9fcb7f0>]`*, **`opt_func`**=*`Adam`*, **`additional_cbs`**=*`None`*, **`expose_fastai_api`**=*`False`*, **`num_classes`**:`int`=*`None`*, **\\*\\*`kwargs`**) :: [`AdaptiveTuner`](/adaptnlp/training.core.html#AdaptiveTuner)\n",
       "\n",
       "An [`AdaptiveTuner`](/adaptnlp/training.core.html#AdaptiveTuner) with good defaults for Sequence Classification tasks\n",
       "\n",
       "**Valid kwargs and defaults:**\n",
       "  - `lr`:float = 0.001\n",
       "  - `splitter`:function = `trainable_params`\n",
       "  - `cbs`:list = None\n",
       "  - `path`:Path = None\n",
       "  - `model_dir`:Path = 'models'\n",
       "  - `wd`:float = None\n",
       "  - `wd_bn_bias`:bool = False\n",
       "  - `train_bn`:bool = True\n",
       "  - `moms`: tuple(float) = (0.95, 0.85, 0.95)\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`dls`** : *`<class 'fastai.data.core.DataLoaders'>`*\t<p>A set of DataLoaders</p>\n",
       "\n",
       "\n",
       " - **`model_name`** : *`<class 'str'>`*\t<p>A HuggingFace model</p>\n",
       "\n",
       "\n",
       " - **`tokenizer`** : *`<class 'NoneType'>`*, *optional*\t<p>A HuggingFace tokenizer</p>\n",
       "\n",
       "\n",
       " - **`loss_func`** : *`<class 'fastai.losses.CrossEntropyLossFlat'>`*, *optional*\t<p>A loss function</p>\n",
       "\n",
       "\n",
       " - **`metrics`** : *`<class 'list'>`*, *optional*\t<p>Metrics to monitor the training with</p>\n",
       "\n",
       "\n",
       " - **`opt_func`** : *`<class 'function'>`*, *optional*\t<p>A fastai or torch Optimizer</p>\n",
       "\n",
       "\n",
       " - **`additional_cbs`** : *`<class 'NoneType'>`*, *optional*\t<p>Additional Callbacks to have always tied to the Tuner,</p>\n",
       "\n",
       "\n",
       " - **`expose_fastai_api`** : *`<class 'bool'>`*, *optional*\t<p>Whether to expose the fastai API</p>\n",
       "\n",
       "\n",
       " - **`num_classes`** : *`<class 'int'>`*, *optional*\t<p>The number of classes</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from adaptnlp import SequenceClassificationTuner\n",
    "show_doc(SequenceClassificationTuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UXvHCw5HzVil",
   "metadata": {},
   "source": [
    "Next we'll pass in our `DataLoaders`, the name of our model, and since we are using raw `Datasets`, the number of classes we have. In our case this is two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efac2b5",
   "metadata": {},
   "source": [
    "> Note: If you are not using the data API (`TaskDatasets`, `SequenceClassificationDatasets`, etc), you need to pass in the tokenizer to the constructor as well with `tokenizer=tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k-gczguJzSjM",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tuner = SequenceClassificationTuner(dls, model.name, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9rAi8TF5zfPz",
   "metadata": {},
   "source": [
    "By default we can see that it used `CrossEntropyLoss` as our loss function, and both `accuracy` and `F1Score` as our metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QTyxXImizjxC",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of CrossEntropyLoss()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x_PEmOXAzlAE",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "f1_score\n"
     ]
    }
   ],
   "source": [
    "_ = [print(m.name) for m in tuner.metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57462869",
   "metadata": {},
   "source": [
    "It is also possible to define your own metrics, these stem from [fastai](https://docs.fast.ai/metrics).\n",
    "\n",
    "To do so, write a function that takes an input and an output, and performs an operation. For example, we will write our own `accuracy` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2802910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourAccuracy(inp, out):\n",
    "    \"A simplified accuracy metric that doesn't flatten\"\n",
    "    return (inp == targ).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed3556",
   "metadata": {},
   "source": [
    "And then we pass it into the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6be0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tuner = SequenceClassificationTuner(dls, model.name, num_classes=2, metrics=[ourAccuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e433d22",
   "metadata": {},
   "source": [
    "If we look at the metrics, you can see that now it is just `ourAccuracy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17457d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ourAccuracy'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.metrics[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77117847",
   "metadata": {},
   "source": [
    "For this tutorial, we will revert it back to the defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee74c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tuner = SequenceClassificationTuner(dls, model.name, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dl-tqB5uzq1k",
   "metadata": {},
   "source": [
    "Finally we just need to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd944d",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ec17d",
   "metadata": {},
   "source": [
    "To fine-tune, AdaptNLP's tuner class provides only a few functions to work with. The important ones are the `tune` and `lr_find` class.\n",
    "\n",
    "As the `Tuner` uses `fastai` under the hood, `lr_find` calls fastai's Learning Rate Finder to help us pick a learning rate. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTaeO5Vm0P-h",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.lr_find\" class=\"doc_header\"><code>AdaptiveTuner.lr_find</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L389\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.lr_find</code>(**`start_lr`**=*`1e-07`*, **`end_lr`**=*`10`*, **`num_it`**=*`100`*, **`stop_div`**=*`True`*, **`show_plot`**=*`True`*, **`suggest_funcs`**=*`valley`*)\n",
       "\n",
       "Runs fastai's `LR Finder`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`start_lr`** : *`<class 'float'>`*, *optional*\n",
       "\n",
       " - **`end_lr`** : *`<class 'int'>`*, *optional*\n",
       "\n",
       " - **`num_it`** : *`<class 'int'>`*, *optional*\n",
       "\n",
       " - **`stop_div`** : *`<class 'bool'>`*, *optional*\n",
       "\n",
       " - **`show_plot`** : *`<class 'bool'>`*, *optional*\n",
       "\n",
       " - **`suggest_funcs`** : *`<class 'function'>`*, *optional*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SequenceClassificationTuner.lr_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2oWmrMic0H0z",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.8/site-packages/fastai/callback/schedule.py:270: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"ro\" (-> color='r'). The keyword argument will take precedence.\n",
      "  ax.plot(val, idx, 'ro', label=nm, c=color)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=7.585775892948732e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jxviy-Nj0Mly",
   "metadata": {},
   "source": [
    "It recommends a learning rate of around 2e-4, however a steeper slope can be found around 5e-5 so we will use that.\n",
    "\n",
    "> Note: Reading the LR Finder is somewhat of an art. The `valley` method is one of the most reliable ones, but also try and figure out an intuition towards finding a learning rate as you go as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpl6Krkl0SLX",
   "metadata": {},
   "source": [
    "Let's look at the documentation for `tune`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CtnRVQOX0T0-",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.tune\" class=\"doc_header\"><code>AdaptiveTuner.tune</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L375\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.tune</code>(**`epochs`**:`int`, **`lr`**:`float`=*`None`*, **`strategy`**:`Strategy`=*`'fit_one_cycle'`*, **`callbacks`**:`list`=*`[]`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Fine tune `self.model` for `epochs` with an `lr` and `strategy`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`epochs`** : *`<class 'int'>`*\t<p>Number of iterations to train for</p>\n",
       "\n",
       "\n",
       " - **`lr`** : *`<class 'float'>`*, *optional*\t<p>If None, finds a new learning rate and uses suggestion_method</p>\n",
       "\n",
       "\n",
       " - **`strategy`** : *`<class 'fastcore.basics.Strategy'>`*, *optional*\t<p>A fitting method</p>\n",
       "\n",
       "\n",
       " - **`callbacks`** : *`<class 'list'>`*, *optional*\t<p>Extra fastai Callbacks</p>\n",
       "\n",
       "\n",
       " - **`kwargs`** : *`<class 'inspect._empty'>`*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SequenceClassificationTuner.tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06gNa3Rk0U-_",
   "metadata": {},
   "source": [
    "We can pass in a number of epochs, a learning rate, a strategy, and additional fastai callbacks to call.\n",
    "\n",
    "Valid strategies live in the `Strategy` namespace class, and consist of:\n",
    "- OneCycle (Also called the [One-Cycle Policy](https://docs.fast.ai/callback.schedule.html#Learner.fit_one_cycle))\n",
    "- [CosineAnnealing](https://docs.fast.ai/callback.schedule.html#Learner.fit_flat_cos)\n",
    "- [SGDR](https://docs.fast.ai/callback.schedule.html#Learner.fit_sgdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5DK9Q-rd0gHN",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xgf9oa4H0nDI",
   "metadata": {},
   "source": [
    "In this tutorial we will train with the One-Cycle policy, as currently it is one of the best schedulers to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4214963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.505338</td>\n",
       "      <td>0.372632</td>\n",
       "      <td>0.835784</td>\n",
       "      <td>0.876611</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286884</td>\n",
       "      <td>0.375253</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.900840</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058951</td>\n",
       "      <td>0.439049</td>\n",
       "      <td>0.860294</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>01:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.tune(3, lr, strategy=Strategy.OneCycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc3547",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aebf7b2",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's save those weights away.\n",
    "\n",
    "Calling `tuner.save` will save both the model and the tokenizer in the same format as how HuggingFace does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b6b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"AdaptiveTuner.save\" class=\"doc_header\"><code>AdaptiveTuner.save</code><a href=\"https://github.com/novetta/adaptnlp/tree/master/adaptnlp/training/core.py#L397\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>AdaptiveTuner.save</code>(**`save_directory`**)\n",
       "\n",
       "Save a pretrained model to a `save_directory`\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`save_directory`** : *`<class 'inspect._empty'>`*\t<p>A folder to save our model to</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SequenceClassificationTuner.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WHCmaQxp0_88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank_distrib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11201/1955740056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'good_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jovyan/adaptnlp/adaptnlp/training/core.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, save_directory)\u001b[0m\n\u001b[1;32m    400\u001b[0m     ):\n\u001b[1;32m    401\u001b[0m         \u001b[0;34m\"Save a pretrained model to a `save_directory`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mrank_distrib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;31m# Don't save if child proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rank_distrib' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.save('good_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8dec2",
   "metadata": {},
   "source": [
    "## Performing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce058be",
   "metadata": {},
   "source": [
    "There are two ways to get predictions, the first is with the `.predict` method in our `tuner`. This is great for if you just finished training and want to see how your model performs on some new data!\n",
    "The other method is with AdaptNLP's inference API, which we will show afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S_OxB8QN1PD2",
   "metadata": {},
   "source": [
    "### In Tuner\n",
    "\n",
    "First let's write a sentence to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d011be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence . Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jHP8lv2P1kKf",
   "metadata": {},
   "source": [
    "And then predict with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KRdOk9bt1lSy",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SequenceClassificationTuner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wPhkn9fp1mi6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac20c59",
   "metadata": {},
   "source": [
    "You'll notice it says `LABEL_1`. We did not build with the `Datasets` wrapper API's, so currently they do not have a vocabulary to work off of. \n",
    "\n",
    "Let's pass in a vocabulary of `not_equivalent` and `equivalent` to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['not_equivalent', 'equivalent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d3bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.predict(sentence, class_names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebc3ae",
   "metadata": {},
   "source": [
    "You can see it gave us much more readable results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaihrDr1taH",
   "metadata": {},
   "source": [
    "### With the Inference API\n",
    "\n",
    "Next we will use the `EasySequenceClassifier` class, which AdaptNLP offers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S2TyPxzh10Tx",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import EasySequenceClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2_5JihBk12kA",
   "metadata": {},
   "source": [
    "We simply construct the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wa2mnVPY10QR",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EasySequenceClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vf_HrdWy1_-Q",
   "metadata": {},
   "source": [
    "And call the `tag_text` method, passing in the sentence, the location of our saved model, and some names for our classes.\n",
    "\n",
    "Similarly here, we can pass in our own vocabulary to use. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UX_sdJkL10JA",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.tag_text(\n",
    "    sentence,\n",
    "    model_name_or_path='good_model',\n",
    "    class_names=names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qXsMgmQC2Qmt",
   "metadata": {},
   "source": [
    "And we got the exact same output and probabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wUyr63752UQo",
   "metadata": {},
   "source": [
    "There are also different levels of predictions we can return (which is also the same with our earlier `predict` call).\n",
    "\n",
    "These live in a namespace `DetailLevel` class, with a few examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jWYd1N4110EK",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptnlp import DetailLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xuFd4z7c2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetailLevel.Low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YcnqBH6b2c8D",
   "metadata": {},
   "source": [
    "While some Easy modules will not return different items at each level, most will return only a few specific outputs at the Low level, and everything possible at the High level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6SxPpTT2nhp",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.tag_text(\n",
    "    sentence,\n",
    "    model_name_or_path = 'good_model',\n",
    "    detail_level=DetailLevel.Low,\n",
    "    class_names=names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IBWnrnWH2pQ4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.tag_text(\n",
    "    sentence,\n",
    "    model_name_or_path = 'good_model',\n",
    "    detail_level=DetailLevel.Medium,\n",
    "    class_names=names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5-Z8qhH2zbZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.tag_text(\n",
    "    sentence,\n",
    "    model_name_or_path = 'good_model',\n",
    "    detail_level=DetailLevel.High,\n",
    "    class_names=names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
