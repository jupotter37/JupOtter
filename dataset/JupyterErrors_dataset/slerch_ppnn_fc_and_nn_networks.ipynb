{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fully-connected-linear-network\" data-toc-modified-id=\"Fully-connected-linear-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Fully connected linear network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-temperature-data\" data-toc-modified-id=\"Get-temperature-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Get temperature data</a></span></li><li><span><a href=\"#Build-fully-connected-model\" data-toc-modified-id=\"Build-fully-connected-model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Build fully connected model</a></span></li><li><span><a href=\"#Predict-for-one-day\" data-toc-modified-id=\"Predict-for-one-day-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Predict for one day</a></span></li><li><span><a href=\"#Post-processing-with-rolling-window-for-2016\" data-toc-modified-id=\"Post-processing-with-rolling-window-for-2016-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Post processing with rolling window for 2016</a></span></li><li><span><a href=\"#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li></ul></li><li><span><a href=\"#Neural-network-with-one-hidden-layer\" data-toc-modified-id=\"Neural-network-with-one-hidden-layer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Neural network with one hidden layer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-network\" data-toc-modified-id=\"Build-network-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build network</a></span></li><li><span><a href=\"#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li><li><span><a href=\"#Making-the-hidden-model-more-complex\" data-toc-modified-id=\"Making-the-hidden-model-more-complex-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Making the hidden model more complex</a></span></li></ul></li><li><span><a href=\"#Add-station-embeddings\" data-toc-modified-id=\"Add-station-embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Add station embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-linear-embedding-model\" data-toc-modified-id=\"Build-linear-embedding-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Build linear embedding model</a></span></li><li><span><a href=\"#Train-2015,-predict-2016\" data-toc-modified-id=\"Train-2015,-predict-2016-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Train 2015, predict 2016</a></span></li><li><span><a href=\"#Embedding-size-hyper-parameter-tuning\" data-toc-modified-id=\"Embedding-size-hyper-parameter-tuning-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Embedding size hyper-parameter tuning</a></span></li></ul></li><li><span><a href=\"#Adding-auxiliary-variables\" data-toc-modified-id=\"Adding-auxiliary-variables-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Adding auxiliary variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-extended-dataset\" data-toc-modified-id=\"Load-extended-dataset-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Load extended dataset</a></span></li><li><span><a href=\"#Linear-model\" data-toc-modified-id=\"Linear-model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Linear model</a></span></li><li><span><a href=\"#Hidden-model\" data-toc-modified-id=\"Hidden-model-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Hidden model</a></span></li><li><span><a href=\"#Even-more-variables\" data-toc-modified-id=\"Even-more-variables-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Even more variables</a></span></li><li><span><a href=\"#Save-pickled-datasets\" data-toc-modified-id=\"Save-pickled-datasets-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Save pickled datasets</a></span></li><li><span><a href=\"#Train-model\" data-toc-modified-id=\"Train-model-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Train model</a></span></li></ul></li><li><span><a href=\"#Additional-variables-with-the-embedding-model\" data-toc-modified-id=\"Additional-variables-with-the-embedding-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Additional variables with the embedding model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-model-with-embeddings\" data-toc-modified-id=\"Linear-model-with-embeddings-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Linear model with embeddings</a></span></li><li><span><a href=\"#Hidden-model-with-embeddings\" data-toc-modified-id=\"Hidden-model-with-embeddings-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Hidden model with embeddings</a></span></li><li><span><a href=\"#A-longer-training-period\" data-toc-modified-id=\"A-longer-training-period-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>A longer training period</a></span></li><li><span><a href=\"#A-longer-training-period-with-more-data\" data-toc-modified-id=\"A-longer-training-period-with-more-data-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>A longer training period with more data</a></span></li></ul></li><li><span><a href=\"#Data-augmentation\" data-toc-modified-id=\"Data-augmentation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Data augmentation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected and neural networks\n",
    "\n",
    "Now that we have established that we can get equivalent EMOS results using a network architecture with SGD, we can now extend this approach to fully connected networks. \n",
    "\n",
    "Here we will try several approaches:\n",
    "- Simple linear fully connected networks\n",
    "- Neural networks with hidden layers\n",
    "- Adding station embeddings\n",
    "- Adding additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda environment: py36_keras\n",
      "Darwin 17.3.0\n",
      "Anaconda environment: py36_keras\n",
      "Darwin 17.3.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../')   # This is where all the python files are!\n",
    "from importlib import reload\n",
    "import emos_network_theano; reload(emos_network_theano)\n",
    "from  emos_network_theano import EMOS_Network\n",
    "from losses import crps_cost_function\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "import keras_models; reload(keras_models)\n",
    "from keras_models import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "DATA_DIR = '/Volumes/STICK/data/ppnn_data/'  # Mac\n",
    "# DATA_DIR = '/project/meteo/w2w/C7/ppnn_data/'   # LMU\n",
    "results_dir = '../results/'\n",
    "window_size = 25   # Days in rolling window\n",
    "fclt = 48   # Forecast lead time in hours\n",
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected linear network\n",
    "\n",
    "As a first step, we can build a linear model which also connects the means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Get temperature data\n",
    "\n",
    "This follows the steps in the EMOS Network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 25 days\n",
      "test set contains 1 days\n"
     ]
    }
   ],
   "source": [
    "date_str = '2011-02-14'\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, predict_date=date_str,\n",
    "                                          fclt=fclt, window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Build fully connected model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model(2, 2, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we have 6 parameters instead of 4 with the standard EMOS Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Predict for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "early_stopping_delta = 1e-4   # How much the CRPS must improve before stopping\n",
    "steps_max = 1000   # How many steps to fit at max\n",
    "batch_size = train_set.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.fit(train_set.features, train_set.targets, epochs=steps_max, \n",
    "             batch_size=batch_size,\n",
    "             validation_data=[test_set.features, test_set.targets], \n",
    "             verbose=0,\n",
    "             callbacks=[EarlyStopping(monitor='loss', \n",
    "                                      min_delta=early_stopping_delta,\n",
    "                                      patience=2)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1270655771548999, 0.76711642082605846)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get train and test CRPS\n",
    "(fc_model.evaluate(train_set.features, train_set.targets, batch_size, verbose=0), \n",
    " fc_model.evaluate(test_set.features, test_set.targets, batch_size, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For this particular day we get a score that is slightly better than the standard EMOS network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Post processing with rolling window for 2016\n",
    "\n",
    "As with the EMOS models let's do a rolling window global post-processing for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "date_str_start = '2016-01-01'\n",
    "date_str_stop = '2017-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model(2, 2, compile=True, optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/366 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 366/366 [08:33<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use the loop function in utils\n",
    "train_crps_list, valid_crps_list, results_df = loop_over_days(\n",
    "    DATA_DIR,\n",
    "    fc_model,\n",
    "    date_str_start, date_str_stop, \n",
    "    window_size=window_size,\n",
    "    fclt=fclt,     \n",
    "    epochs_max=steps_max, \n",
    "    early_stopping_delta=early_stopping_delta, \n",
    "    lr=0.1,   \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98830469069443005, 1.0057136710506112)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_crps_list), np.mean(valid_crps_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we get a slightly better training score and a slightly worse test score. This is a sign of overfitting. But the differences are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(results_dir + 'fc_network_rolling_window.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train 2015, predict 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180849, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model(2, 2, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"118pt\" viewBox=\"0.00 0.00 136.36 118.00\" width=\"136pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 114)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-114 132.362,-114 132.362,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4693707520 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4693707520</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 128.362,-109.5 128.362,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.1812\" y=\"-87.3\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4788921008 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4788921008</title>\n",
       "<polygon fill=\"none\" points=\"12.0552,-0.5 12.0552,-36.5 116.307,-36.5 116.307,-0.5 12.0552,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64.1812\" y=\"-14.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 4693707520&#45;&gt;4788921008 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4693707520-&gt;4788921008</title>\n",
       "<path d=\"M64.1812,-73.3129C64.1812,-65.2895 64.1812,-55.5475 64.1812,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"67.6813,-46.5288 64.1812,-36.5288 60.6813,-46.5289 67.6813,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(fc_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model.compile(keras.optimizers.Adam(0.001), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 1s - loss: 2.9808 - val_loss: 1.9917\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.3618 - val_loss: 1.0581\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0732 - val_loss: 1.0128\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0693 - val_loss: 1.0120\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0692 - val_loss: 1.0125\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0692 - val_loss: 1.0135\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 1s - loss: 1.0693 - val_loss: 1.0121\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 1s - loss: 1.0692 - val_loss: 1.0124\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 1s - loss: 1.0693 - val_loss: 1.0123\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 1s - loss: 1.0693 - val_loss: 1.0133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136249cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: I am running this cell several times (40)\n",
    "fc_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182218/182218 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0133071342532698"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_model.evaluate(test_set.features, test_set.targets, 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Very similar to the standard EMOS Network. This indicates that there is not much additional information in the two extra connections we added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = fc_model.predict(test_set.features)\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'fc_network_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network with one hidden layer\n",
    "\n",
    "Now we will build the first neural network with a hidden layer and a non-linear activation function. We will restrict ourselves to testing the 2015 training, 2016 prediction case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden_model = build_hidden_model(2, 2, hidden_nodes=10, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 52\n",
      "Trainable params: 52\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train 2015, predict 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden_model.compile(keras.optimizers.Adam(0.0001), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 4.1429 - val_loss: 1.4222\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.1146 - val_loss: 1.0142\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0724 - val_loss: 1.0159\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0716 - val_loss: 1.0158\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0718 - val_loss: 1.0135\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0715 - val_loss: 1.0147\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0714 - val_loss: 1.0136\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0710 - val_loss: 1.0144\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0710 - val_loss: 1.0139\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 1.0712 - val_loss: 1.0161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1157c4470>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the same data from above!\n",
    "# Note I am running this cell several times\n",
    "hidden_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "                 validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, the results are pretty similar. This indicates that for the given data, the added nonlinearity is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = hidden_model.predict(test_set.features)\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'hidden_nn_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Making the hidden model more complex\n",
    "\n",
    "Let's see what happens if we make the model more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden_model = build_hidden_model(2, 2, hidden_nodes=[100, 100, 100], compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 20,702\n",
      "Trainable params: 20,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden_model.compile(keras.optimizers.Adam(0.0001), loss=crps_cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0217\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0224\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0221\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0571 - val_loss: 1.0223\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0221\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0221\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0223\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0570 - val_loss: 1.0223\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0569 - val_loss: 1.0219\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 2s - loss: 1.0569 - val_loss: 1.0221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118c75860>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=4096,\n",
    "                 validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we can see that even for a model with 20,000 parameters then training score only goes down a few percent. For a simple bias and spread correction, a linear model seems fully sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Add station embeddings\n",
    "\n",
    "Next we will add a station embedding. Here we are giving every station additional parameters which the model can learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build linear embedding model\n",
    "\n",
    "Let's build a linear embedding model. I tried out hidden layers, but they seem to make the validation score worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 3\n",
    "max_id = int(np.max([train_set.cont_ids.max(), test_set.cont_ids.max()]))\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb_model = build_emb_model(2, 2, [], emb_size, max_id, compile=True,\n",
    "                            lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_21 (InputLayer)            (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 1, 3)          1611        input_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 3)             0           embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 5)             0           input_20[0][0]                   \n",
      "                                                                   flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 2)             12          concatenate_6[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 1,623\n",
      "Trainable params: 1,623\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train 2015, predict 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9690 - val_loss: 0.9132\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9691 - val_loss: 0.9129\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9690 - val_loss: 0.9129\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9690 - val_loss: 0.9136\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9691 - val_loss: 0.9132\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9689 - val_loss: 0.9139\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9689 - val_loss: 0.9131\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9690 - val_loss: 0.9138\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9689 - val_loss: 0.9128\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9690 - val_loss: 0.9134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b9b5c50>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ran this for 40 epochs\n",
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, \n",
    "              epochs=10, batch_size=1024, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = emb_model.predict([test_set.features, test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_fc_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Embedding size hyper-parameter tuning\n",
    "\n",
    "Since embeddings appear to work very well, we will test the impact of the embedding size before building more complex models. Of course, a larger embedding size might be useful when adding more variables, but this should give us some feeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_and_run_emb_model(emb_size):\n",
    "    emb_model = build_emb_model(2, 2, [], emb_size, max_id, compile=True)\n",
    "    emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, \n",
    "                  epochs=40,batch_size=1024, verbose=0,\n",
    "                  validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])\n",
    "    print(emb_model.evaluate([train_set.features, train_set.cont_ids], train_set.targets, verbose=0),\n",
    "          emb_model.evaluate([test_set.features, test_set.cont_ids], test_set.targets, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.975157575805 0.918841918992\n",
      "2\n",
      "0.96694133538 0.913661904937\n",
      "3\n",
      "0.967165116694 0.912928815584\n",
      "5\n",
      "0.967104299998 0.914134442874\n",
      "10\n",
      "0.968233569106 0.915652121524\n",
      "20\n",
      "0.969342381379 0.912989628939\n"
     ]
    }
   ],
   "source": [
    "for emb_size in [1, 2, 3, 5, 10, 20]:\n",
    "    print(emb_size)\n",
    "    build_and_run_emb_model(emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that there is some variability. In our first experiment above with an embedding size of 5 we got a better score than here. For this very simple network an embedding size of three seems sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Adding auxiliary variables\n",
    "\n",
    "Now we can try adding additional variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load extended dataset\n",
    "\n",
    "Using the function defined in utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The prepare_data function takes an ordered dict as an input\n",
    "aux_dict = OrderedDict()\n",
    "aux_dict['data_aux_geo_interpolated.nc'] = ['orog', \n",
    "                                            'station_alt', \n",
    "                                            'station_lat', \n",
    "                                            'station_lon']\n",
    "aux_dict['data_aux_pl500_interpolated_00UTC.nc'] = ['u_pl500_fc',\n",
    "                                                    'v_pl500_fc',\n",
    "                                                    'gh_pl500_fc']\n",
    "aux_dict['data_aux_pl850_interpolated_00UTC.nc'] = ['u_pl850_fc',\n",
    "                                                    'v_pl850_fc',\n",
    "                                                    'q_pl850_fc']\n",
    "aux_dict['data_aux_surface_interpolated_00UTC.nc'] = ['cape_fc',\n",
    "                                                      'sp_fc',\n",
    "                                                      'tcc_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates,\n",
    "                                         aux_dict=aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180849, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model(train_set.features.shape[1], 2, compile=True, \n",
    "                          lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9701 - val_loss: 0.9389\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9699 - val_loss: 0.9395\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9699 - val_loss: 0.9405\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9700 - val_loss: 0.9385\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9702 - val_loss: 0.9380\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9699 - val_loss: 0.9385\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9699 - val_loss: 0.9384\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9699 - val_loss: 0.9400\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9698 - val_loss: 0.9388\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9699 - val_loss: 0.9382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c21d5c0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I am running this cell multiple times\n",
    "fc_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = fc_model.predict([test_set.features, test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_fc_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Hidden model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden_model = build_hidden_model(train_set.features.shape[1], 2, \n",
    "                                  hidden_nodes=[50], compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 50)                1250      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 1,352\n",
      "Trainable params: 1,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9388 - val_loss: 0.9402\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9348 - val_loss: 0.9332\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9346 - val_loss: 0.9367\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9295 - val_loss: 0.9418\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9292 - val_loss: 0.9356\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9275 - val_loss: 0.9392\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9253 - val_loss: 0.9365\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9251 - val_loss: 0.9378\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9239 - val_loss: 0.9441\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9232 - val_loss: 0.9310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1264243c8>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I am running this cell multiple times\n",
    "hidden_model.fit(train_set.features, train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[test_set.features, test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So we see a definite improvement using auxiliary variables. Again, the hidden layer does not seem to improve things a lot compared to the simple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Even more variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "more_aux_dict = aux_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "more_aux_dict['data_aux_surface_more_interpolated_part1_00UTC.nc']  = [\n",
    "    'sshf_fc', 'slhf_fc', 'u10_fc','v10_fc'\n",
    "]\n",
    "more_aux_dict['data_aux_surface_more_interpolated_part2_00UTC.nc']  = [\n",
    "    'ssr_fc', 'str_fc', 'd2m_fc','sm_fc'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "more_train_set, more_test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates,\n",
    "                                         aux_dict=more_aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180849, 40)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_train_set.features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Save pickled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43mauxiliary\u001b[m\u001b[m/                  \u001b[31mdata_interpolated_00UTC.nc\u001b[m\u001b[m*\r\n",
      "\u001b[31mdata_interpolated.nc\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(DATA_DIR + 'aux_15_16.pkl', 'wb') as f:\n",
    "    pickle.dump((more_train_set, more_test_set), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_pickle(fn, train_dates=['2015-01-01', '2016-01-01'], add_current_error=False,\n",
    "                current_error_len=1):\n",
    "    sets = get_train_test_sets(\n",
    "        DATA_DIR, train_dates, test_dates, aux_dict=more_aux_dict,\n",
    "        add_current_error=add_current_error, current_error_len=current_error_len\n",
    "    )\n",
    "    with open(DATA_DIR + fn, 'wb') as f:\n",
    "        pickle.dump(sets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "save_pickle('aux_15_16_current30.pkl', ['2015-01-01', '2016-01-01'], \n",
    "            add_current_error=True, current_error_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 730 days\n",
      "test set contains 366 days\n",
      "train set contains 1095 days\n",
      "test set contains 366 days\n",
      "train set contains 2191 days\n",
      "test set contains 366 days\n",
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "save_pickle('aux_14-15_16.pkl', ['2014-01-01', '2016-01-01'])\n",
    "save_pickle('aux_13-15_16.pkl', ['2013-01-01', '2016-01-01'])\n",
    "save_pickle('aux_10-15_16.pkl', ['2010-01-01', '2016-01-01'])\n",
    "save_pickle('aux_08-15_16.pkl', ['2008-01-01', '2016-01-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 1461 days\n",
      "test set contains 366 days\n",
      "train set contains 1826 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "save_pickle('aux_12-15_16.pkl', ['2012-01-01', '2016-01-01'])\n",
    "save_pickle('aux_11-15_16.pkl', ['2011-01-01', '2016-01-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fc_model = build_fc_model(more_train_set.features.shape[1], 2, compile=True, \n",
    "                          lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9540 - val_loss: 0.9163\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9542 - val_loss: 0.9231\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9546 - val_loss: 0.9159\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9546 - val_loss: 0.9242\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9544 - val_loss: 0.9152\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9549 - val_loss: 0.9251\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9545 - val_loss: 0.9161\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9542 - val_loss: 0.9193\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9545 - val_loss: 0.9177\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9539 - val_loss: 0.9221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2aed4cd4e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I am running this cell multiple times\n",
    "fc_model.fit(more_train_set.features, more_train_set.targets, epochs=10, batch_size=1024,\n",
    "             validation_data=[more_test_set.features, more_test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adding these extra variables gets us another percent or so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional variables with the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = build_emb_model(train_set.features.shape[1], 2, [], 3, max_id, \n",
    "                            compile=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 1, 3)          1611        input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 24)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 3)             0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 27)            0           input_1[0][0]                    \n",
      "                                                                   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             56          concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 1,667\n",
      "Trainable params: 1,667\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9234 - val_loss: 0.9007\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9232 - val_loss: 0.8995\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9233 - val_loss: 0.8985\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9231 - val_loss: 0.9004\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9231 - val_loss: 0.8981\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9230 - val_loss: 0.8987\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9232 - val_loss: 0.8977\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9231 - val_loss: 0.8979\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9229 - val_loss: 0.8969\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.9231 - val_loss: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96c40f5dd8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again I am running this multiple times\n",
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, epochs=10, \n",
    "              batch_size=1024, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden model with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ff5627243658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m emb_model = build_emb_model(train_set.features.shape[1], 2, [50], 3, max_id, \n\u001b[0m\u001b[1;32m      2\u001b[0m                             compile=True, lr=0.01)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "emb_model = build_emb_model(train_set.features.shape[1], 2, [50], 3, max_id, \n",
    "                            compile=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8592 - val_loss: 0.8559\n",
      "Epoch 2/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8536 - val_loss: 0.8559\n",
      "Epoch 3/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8524 - val_loss: 0.8590\n",
      "Epoch 4/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8498 - val_loss: 0.8570\n",
      "Epoch 5/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8500 - val_loss: 0.8580\n",
      "Epoch 6/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8478 - val_loss: 0.8885\n",
      "Epoch 7/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8465 - val_loss: 0.8572\n",
      "Epoch 8/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8449 - val_loss: 0.8564\n",
      "Epoch 9/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8438 - val_loss: 0.8583\n",
      "Epoch 10/10\n",
      "180849/180849 [==============================] - 0s - loss: 0.8440 - val_loss: 0.8613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95e113fcc0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again I am running this multiple times\n",
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, epochs=10, \n",
    "              batch_size=4096, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best score so far. Here the non-linearity seems to make a difference compared to the simple linear model. But we do get some overfitting. Let's try out some techniques. Fewer or more hidden nodes does not seem to change all that much.\n",
    "\n",
    "No we need to be very careful here. I am currently stopping when the validation score does not decrease further. THIS IS CHEATING!\n",
    "\n",
    "Let's try doing the train valid split just with the training set, and see if we can get a good early stopping point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_id = int(np.max([train_set.cont_ids.max(), test_set.cont_ids.max()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = build_emb_model(train_set.features.shape[1], 2, [50], 3, max_id, \n",
    "                            compile=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144679 samples, validate on 36170 samples\n",
      "Epoch 1/50\n",
      "144679/144679 [==============================] - 0s - loss: 4.6977 - val_loss: 2.3896\n",
      "Epoch 2/50\n",
      "144679/144679 [==============================] - 0s - loss: 2.5777 - val_loss: 1.8083\n",
      "Epoch 3/50\n",
      "144679/144679 [==============================] - 0s - loss: 1.3762 - val_loss: 1.1476\n",
      "Epoch 4/50\n",
      "144679/144679 [==============================] - 0s - loss: 1.0260 - val_loss: 1.0796\n",
      "Epoch 5/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9704 - val_loss: 1.0552\n",
      "Epoch 6/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9508 - val_loss: 1.0323\n",
      "Epoch 7/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9369 - val_loss: 1.0165\n",
      "Epoch 8/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9260 - val_loss: 1.0029\n",
      "Epoch 9/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9199 - val_loss: 0.9962\n",
      "Epoch 10/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9133 - val_loss: 0.9905\n",
      "Epoch 11/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9092 - val_loss: 0.9868\n",
      "Epoch 12/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9064 - val_loss: 0.9832\n",
      "Epoch 13/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9035 - val_loss: 0.9789\n",
      "Epoch 14/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.9005 - val_loss: 0.9757\n",
      "Epoch 15/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8962 - val_loss: 0.9704\n",
      "Epoch 16/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8904 - val_loss: 0.9731\n",
      "Epoch 17/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8828 - val_loss: 0.9568\n",
      "Epoch 18/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8703 - val_loss: 0.9546\n",
      "Epoch 19/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8659 - val_loss: 0.9551\n",
      "Epoch 20/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8602 - val_loss: 0.9513\n",
      "Epoch 21/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8587 - val_loss: 0.9509\n",
      "Epoch 22/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8563 - val_loss: 0.9473\n",
      "Epoch 23/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8528 - val_loss: 0.9582\n",
      "Epoch 24/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8512 - val_loss: 0.9488\n",
      "Epoch 25/50\n",
      "144679/144679 [==============================] - 0s - loss: 0.8479 - val_loss: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2a3c190208>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again I am running this multiple times\n",
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, epochs=50, \n",
    "              batch_size=4096, validation_split=0.2,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                       min_delta=0,\n",
    "                                       patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170000/182218 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8661767004406562"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.evaluate([test_set.features, test_set.cont_ids], test_set.targets, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = emb_model.predict([test_set.features, test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_nn_aux_train_2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A longer training period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "long_train_dates = ['2008-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "long_train_set, test_set = get_train_test_sets(DATA_DIR, long_train_dates, test_dates,\n",
    "                                               aux_dict=aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-900b4033026f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m emb_model = build_emb_model(long_train_set.features.shape[1], 2, [50], 3, max_id, \n\u001b[0m\u001b[1;32m      2\u001b[0m                             compile=True, lr=0.01)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'long_train_set' is not defined"
     ]
    }
   ],
   "source": [
    "emb_model = build_emb_model(long_train_set.features.shape[1], 2, [50], 3, max_id, \n",
    "                            compile=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1165581 samples, validate on 291396 samples\n",
      "Epoch 1/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 1.4502 - val_loss: 0.9145\n",
      "Epoch 2/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.9202 - val_loss: 0.8564\n",
      "Epoch 3/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8807 - val_loss: 0.8377\n",
      "Epoch 4/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8701 - val_loss: 0.8407\n",
      "Epoch 5/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8637 - val_loss: 0.8339\n",
      "Epoch 6/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8568 - val_loss: 0.8292\n",
      "Epoch 7/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8490 - val_loss: 0.8261\n",
      "Epoch 8/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8440 - val_loss: 0.8225\n",
      "Epoch 9/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8399 - val_loss: 0.8217\n",
      "Epoch 10/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8383 - val_loss: 0.8156\n",
      "Epoch 11/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8364 - val_loss: 0.8159\n",
      "Epoch 12/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8346 - val_loss: 0.8159\n",
      "Epoch 13/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8327 - val_loss: 0.8124\n",
      "Epoch 14/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8311 - val_loss: 0.8180\n",
      "Epoch 15/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8304 - val_loss: 0.8205\n",
      "Epoch 16/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8298 - val_loss: 0.8160\n",
      "Epoch 17/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8260 - val_loss: 0.8331\n",
      "Epoch 18/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8263 - val_loss: 0.8065\n",
      "Epoch 19/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8244 - val_loss: 0.8076\n",
      "Epoch 20/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8231 - val_loss: 0.8086\n",
      "Epoch 21/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8222 - val_loss: 0.8100\n",
      "Epoch 22/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8205 - val_loss: 0.8088\n",
      "Epoch 23/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8212 - val_loss: 0.8046\n",
      "Epoch 24/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8183 - val_loss: 0.8043\n",
      "Epoch 25/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8190 - val_loss: 0.8064\n",
      "Epoch 26/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8188 - val_loss: 0.8047\n",
      "Epoch 27/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8181 - val_loss: 0.8219\n",
      "Epoch 28/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8192 - val_loss: 0.8082\n",
      "Epoch 29/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8155 - val_loss: 0.8045\n",
      "Epoch 30/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8163 - val_loss: 0.8183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2aec0a6e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again I am running this multiple times\n",
    "emb_model.fit([long_train_set.features, long_train_set.cont_ids], long_train_set.targets, \n",
    "              epochs=50, \n",
    "              batch_size=4096, validation_split=0.2,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                       min_delta=0,\n",
    "                                       patience=5)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170000/182218 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7918927852063512"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.evaluate([test_set.features, test_set.cont_ids], test_set.targets, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = emb_model.predict([test_set.features, test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_nn_aux_train_2008-2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A longer training period with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 2922 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "long_more_train_set, more_test_set = get_train_test_sets(DATA_DIR, long_train_dates, test_dates,\n",
    "                                                         aux_dict=more_aux_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = build_emb_model(long_more_train_set.features.shape[1], 2, [100], 3, max_id, \n",
    "                            compile=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1165581 samples, validate on 291396 samples\n",
      "Epoch 1/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 1.4866 - val_loss: 0.8848\n",
      "Epoch 2/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8875 - val_loss: 0.8467\n",
      "Epoch 3/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8628 - val_loss: 0.8289\n",
      "Epoch 4/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8504 - val_loss: 0.8384\n",
      "Epoch 5/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8424 - val_loss: 0.8217\n",
      "Epoch 6/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8376 - val_loss: 0.8203\n",
      "Epoch 7/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8350 - val_loss: 0.8143\n",
      "Epoch 8/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8291 - val_loss: 0.8176\n",
      "Epoch 9/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8281 - val_loss: 0.8141\n",
      "Epoch 10/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8230 - val_loss: 0.8210\n",
      "Epoch 11/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8231 - val_loss: 0.8084\n",
      "Epoch 12/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8197 - val_loss: 0.8085\n",
      "Epoch 13/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8180 - val_loss: 0.8060\n",
      "Epoch 14/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8159 - val_loss: 0.8058\n",
      "Epoch 15/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8142 - val_loss: 0.8040\n",
      "Epoch 16/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8126 - val_loss: 0.8046\n",
      "Epoch 17/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8129 - val_loss: 0.8077\n",
      "Epoch 18/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8111 - val_loss: 0.8039\n",
      "Epoch 19/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8098 - val_loss: 0.8117\n",
      "Epoch 20/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8095 - val_loss: 0.8014\n",
      "Epoch 21/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8072 - val_loss: 0.8012\n",
      "Epoch 22/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8065 - val_loss: 0.8067\n",
      "Epoch 23/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8069 - val_loss: 0.8171\n",
      "Epoch 24/50\n",
      "1165581/1165581 [==============================] - 2s - loss: 0.8058 - val_loss: 0.8054\n",
      "Epoch 25/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8057 - val_loss: 0.8020\n",
      "Epoch 26/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8039 - val_loss: 0.8081\n",
      "Epoch 27/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8043 - val_loss: 0.8003\n",
      "Epoch 28/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8036 - val_loss: 0.8063\n",
      "Epoch 29/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8041 - val_loss: 0.8029\n",
      "Epoch 30/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8022 - val_loss: 0.8013\n",
      "Epoch 31/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8023 - val_loss: 0.8076\n",
      "Epoch 32/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8029 - val_loss: 0.8074\n",
      "Epoch 33/50\n",
      "1165581/1165581 [==============================] - 1s - loss: 0.8003 - val_loss: 0.8085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2aeb8e6f98>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again I am running this multiple times\n",
    "emb_model.fit([long_more_train_set.features, long_more_train_set.cont_ids], \n",
    "              long_more_train_set.targets, \n",
    "              epochs=50, batch_size=4096, validation_split=0.2,\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                       min_delta=0,\n",
    "                                       patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170000/182218 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79416574978415233"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.evaluate([more_test_set.features, more_test_set.cont_ids], more_test_set.targets, \n",
    "                   batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = emb_model.predict([more_test_set.features, more_test_set.cont_ids])\n",
    "results_df = create_results_df(test_set.date_strs, test_set.station_ids,\n",
    "                               preds[:, 0], preds[:, 1])\n",
    "results_df.to_csv(results_dir + 'embedding_nn_more_aux_train_2008-2015_pred_2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "Let's pick a setup where overfitting is a problem: emb_train_2015_aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(DATA_DIR + 'aux_15_16.pkl', 'rb') as f:\n",
    "    train_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180849, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.25074708,   0.09660812,   0.1843026 ,   0.09279162,\n",
       "         0.03468666,   0.13954796,   0.22625588,   0.07106367,\n",
       "         0.30288959,   0.08776672,   0.02569887,   0.09980575,\n",
       "         0.23046067,   0.06643367,   0.20091382,   0.08406494,\n",
       "         0.19337422,   0.11743076,   0.05346011,   0.06231587,\n",
       "         0.02710444,   0.0817729 ,   0.31899199,   0.23367222,\n",
       "         0.30816635,   0.09623598,   0.91393036,   0.1034622 ,\n",
       "         0.13670425,   0.05991281,   0.13355759,   0.05574263,\n",
       "         0.23230664,   0.14413203,  12.44298172,   0.10215613,\n",
       "         0.02040521,   0.09879841,   0.15911838,   0.10272128], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23806411,  0.09815978,  0.6862545 ,  0.17456083,  0.1688834 ,\n",
       "        0.22585876,  0.17900802,  0.21563105,  0.19999588,  0.01879581,\n",
       "        0.20672929,  0.44667143,  0.15995905,  0.14729995,  0.10025162,\n",
       "        0.09625338,  0.20209451,  0.26052597,  0.21735674,  0.11991174], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.mean(axis=0)[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.25074708,   0.1843026 ,   0.03468666,   0.22625588,\n",
       "         0.30288959,   0.02569887,   0.23046067,   0.20091382,\n",
       "         0.19337422,   0.05346011,   0.02710444,   0.31899199,\n",
       "         0.30816635,   0.91393036,   0.13670425,   0.13355759,\n",
       "         0.23230664,  12.44298172,   0.02040521,   0.15911838], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.std(axis=0)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scales = np.zeros(train_set.features.shape[1])\n",
    "scales[1::2] = train_set.features.mean(axis=0)[1::2]\n",
    "scales[::2] = train_set.features.std(axis=0)[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_aug = train_set.features + np.random.normal(size=train_set.features.shape) * scales * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets_aug = train_set.targets + np.random.normal(scale=0.1, size=train_set.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.20000005, -3.70000005, -2.0999999 ,  1.60000002,  0.60000002], dtype=float32),\n",
       " array([ 2.11051623, -3.64067068, -2.10724395,  1.64481148,  0.62672574]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets[:5], targets_aug[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361698, 40)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([train_set.features, features_aug], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = build_emb_model(train_set.features.shape[1], 2, [100], 3, max_id, \n",
    "                            compile=True, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180849 samples, validate on 182218 samples\n",
      "Epoch 1/50\n",
      "180849/180849 [==============================] - 1s - loss: 3.2343 - val_loss: 2.0386\n",
      "Epoch 2/50\n",
      "180849/180849 [==============================] - 1s - loss: 1.5920 - val_loss: 1.2551\n",
      "Epoch 3/50\n",
      "180849/180849 [==============================] - 1s - loss: 1.1297 - val_loss: 1.0354\n",
      "Epoch 4/50\n",
      "180849/180849 [==============================] - 1s - loss: 1.0138 - val_loss: 0.9986\n",
      "Epoch 5/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.9730 - val_loss: 0.9816\n",
      "Epoch 6/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.9539 - val_loss: 0.9707\n",
      "Epoch 7/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.9516 - val_loss: 0.9200\n",
      "Epoch 8/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.9236 - val_loss: 0.9069\n",
      "Epoch 9/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.9131 - val_loss: 0.9088\n",
      "Epoch 10/50\n",
      "180849/180849 [==============================] - 2s - loss: 0.9202 - val_loss: 0.8905\n",
      "Epoch 11/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8831 - val_loss: 0.8939\n",
      "Epoch 12/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8870 - val_loss: 0.8651\n",
      "Epoch 13/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8858 - val_loss: 0.9388\n",
      "Epoch 14/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8664 - val_loss: 0.9250\n",
      "Epoch 15/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8639 - val_loss: 0.8529\n",
      "Epoch 16/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8575 - val_loss: 0.8634\n",
      "Epoch 17/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8542 - val_loss: 0.8478\n",
      "Epoch 18/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8571 - val_loss: 0.8445\n",
      "Epoch 19/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8558 - val_loss: 0.8832\n",
      "Epoch 20/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8462 - val_loss: 0.8447\n",
      "Epoch 21/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8489 - val_loss: 0.8766\n",
      "Epoch 22/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8635 - val_loss: 0.8424\n",
      "Epoch 23/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8373 - val_loss: 0.8437\n",
      "Epoch 24/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8529 - val_loss: 0.8776\n",
      "Epoch 25/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8384 - val_loss: 0.8514\n",
      "Epoch 26/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8314 - val_loss: 0.8553\n",
      "Epoch 27/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8375 - val_loss: 0.8580\n",
      "Epoch 28/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8350 - val_loss: 0.8668\n",
      "Epoch 29/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8404 - val_loss: 0.8421\n",
      "Epoch 30/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8461 - val_loss: 0.8423\n",
      "Epoch 31/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8248 - val_loss: 0.8341\n",
      "Epoch 32/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8283 - val_loss: 0.8523\n",
      "Epoch 33/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8198 - val_loss: 0.8482\n",
      "Epoch 34/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8310 - val_loss: 0.8331\n",
      "Epoch 35/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8222 - val_loss: 0.8319\n",
      "Epoch 36/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8189 - val_loss: 0.8382\n",
      "Epoch 37/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8121 - val_loss: 0.8333\n",
      "Epoch 38/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8218 - val_loss: 0.8762\n",
      "Epoch 39/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8200 - val_loss: 0.8326\n",
      "Epoch 40/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8174 - val_loss: 0.8369\n",
      "Epoch 41/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8083 - val_loss: 0.9549\n",
      "Epoch 42/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8314 - val_loss: 0.8978\n",
      "Epoch 43/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8168 - val_loss: 0.8370\n",
      "Epoch 44/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8087 - val_loss: 0.8263\n",
      "Epoch 45/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8095 - val_loss: 0.8355\n",
      "Epoch 46/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8147 - val_loss: 0.8359\n",
      "Epoch 47/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8046 - val_loss: 0.8279\n",
      "Epoch 48/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8052 - val_loss: 0.8314\n",
      "Epoch 49/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8076 - val_loss: 0.8436\n",
      "Epoch 50/50\n",
      "180849/180849 [==============================] - 1s - loss: 0.8062 - val_loss: 0.8263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b5c9630>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.fit([train_set.features, train_set.cont_ids], train_set.targets, \n",
    "              epochs=50, batch_size=4096, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361698 samples, validate on 182218 samples\n",
      "Epoch 1/50\n",
      "361698/361698 [==============================] - 2s - loss: 2.1479 - val_loss: 1.1412\n",
      "Epoch 2/50\n",
      "361698/361698 [==============================] - 1s - loss: 1.0383 - val_loss: 0.9549\n",
      "Epoch 3/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.9582 - val_loss: 0.9531\n",
      "Epoch 4/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.9008 - val_loss: 0.8797\n",
      "Epoch 5/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8987 - val_loss: 0.8848\n",
      "Epoch 6/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8709 - val_loss: 0.8621\n",
      "Epoch 7/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8752 - val_loss: 0.8749\n",
      "Epoch 8/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8617 - val_loss: 0.8502\n",
      "Epoch 9/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8640 - val_loss: 0.8468\n",
      "Epoch 10/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8564 - val_loss: 0.8448\n",
      "Epoch 11/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8538 - val_loss: 0.8694\n",
      "Epoch 12/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8438 - val_loss: 0.9038\n",
      "Epoch 13/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8354 - val_loss: 0.8437\n",
      "Epoch 14/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8413 - val_loss: 0.8433\n",
      "Epoch 15/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8383 - val_loss: 0.8457\n",
      "Epoch 16/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8365 - val_loss: 0.8394\n",
      "Epoch 17/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8243 - val_loss: 0.8495\n",
      "Epoch 18/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8340 - val_loss: 0.8458\n",
      "Epoch 19/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8226 - val_loss: 0.8358\n",
      "Epoch 20/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8305 - val_loss: 0.8430\n",
      "Epoch 21/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8223 - val_loss: 0.8531\n",
      "Epoch 22/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8124 - val_loss: 0.8298\n",
      "Epoch 23/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8186 - val_loss: 0.8460\n",
      "Epoch 24/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8160 - val_loss: 0.8631\n",
      "Epoch 25/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8179 - val_loss: 0.8536\n",
      "Epoch 26/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8146 - val_loss: 0.8309\n",
      "Epoch 27/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8036 - val_loss: 0.8502\n",
      "Epoch 28/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8138 - val_loss: 0.8647\n",
      "Epoch 29/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8110 - val_loss: 0.8750\n",
      "Epoch 30/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8002 - val_loss: 0.8296\n",
      "Epoch 31/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8122 - val_loss: 0.8344\n",
      "Epoch 32/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8066 - val_loss: 0.8359\n",
      "Epoch 33/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8050 - val_loss: 0.8449\n",
      "Epoch 34/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8078 - val_loss: 0.9519\n",
      "Epoch 35/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.8057 - val_loss: 0.8442\n",
      "Epoch 36/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7934 - val_loss: 0.8372\n",
      "Epoch 37/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7956 - val_loss: 0.8692\n",
      "Epoch 38/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8037 - val_loss: 0.8927\n",
      "Epoch 39/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.8000 - val_loss: 0.8506\n",
      "Epoch 40/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.7956 - val_loss: 0.8389\n",
      "Epoch 41/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.7931 - val_loss: 0.8425\n",
      "Epoch 42/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7926 - val_loss: 0.8395\n",
      "Epoch 43/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7954 - val_loss: 0.9836\n",
      "Epoch 44/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7983 - val_loss: 0.8360\n",
      "Epoch 45/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7962 - val_loss: 0.8373\n",
      "Epoch 46/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7904 - val_loss: 0.8407\n",
      "Epoch 47/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7930 - val_loss: 0.8393\n",
      "Epoch 48/50\n",
      "361698/361698 [==============================] - 2s - loss: 0.7886 - val_loss: 0.8699\n",
      "Epoch 49/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7915 - val_loss: 0.8483\n",
      "Epoch 50/50\n",
      "361698/361698 [==============================] - 1s - loss: 0.7899 - val_loss: 0.8451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1288b6a20>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.fit([\n",
    "    np.concatenate([train_set.features, features_aug], axis=0), \n",
    "    np.concatenate([train_set.cont_ids, train_set.cont_ids])\n",
    "                   ], np.concatenate([train_set.targets, targets_aug]), \n",
    "              epochs=50, batch_size=4096, \n",
    "              validation_data=[[test_set.features, test_set.cont_ids], test_set.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set contains 365 days\n",
      "test set contains 366 days\n"
     ]
    }
   ],
   "source": [
    "# Test current error\n",
    "train_dates = ['2015-01-01', '2016-01-01']\n",
    "test_dates =  ['2016-01-01', '2017-01-01']\n",
    "train_set, test_set = get_train_test_sets(DATA_DIR, train_dates, test_dates,\n",
    "                                          add_current_error=True, \n",
    "                                          current_error_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195929, 11)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t2m_fc_mean',\n",
       " 't2m_fc_std',\n",
       " 'curr_t2m_fc_mean',\n",
       " 'curr_t2m_fc_obs',\n",
       " 'curr_err',\n",
       " 'curr_t2m_fc_mean_m1',\n",
       " 'curr_t2m_fc_obs_m1',\n",
       " 'curr_err_m1',\n",
       " 'curr_t2m_fc_mean_m2',\n",
       " 'curr_t2m_fc_obs_m2',\n",
       " 'curr_err_m2']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
