{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate = 0.005, used convolution for extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "from collections import Counter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('../../Glove/word_embedding_glove', 'rb')\n",
    "word_embedding = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word_embedding = word_embedding[: len(word_embedding)-1]\n",
    "\n",
    "f = open('../../Glove/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "# Model Description\n",
    "model_name = 'model-aw-lex-1-3'\n",
    "model_dir = '../output/all-word/' + model_name\n",
    "save_dir = os.path.join(model_dir, \"save/\")\n",
    "log_dir = os.path.join(model_dir, \"log\")\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "with open('/data/aviraj/dataset/train_val_data_fine/all_word_lex','rb') as f:\n",
    "    train_data, val_data = pickle.load(f)    \n",
    "    \n",
    "# Parameters\n",
    "mode = 'train'\n",
    "num_senses = 45\n",
    "num_pos = 12\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab)\n",
    "unk_vocab_size = 1\n",
    "word_emb_size = len(word_embedding[0])\n",
    "max_sent_size = 200\n",
    "hidden_size = 256\n",
    "num_filter = 256\n",
    "kernel_size = 5\n",
    "keep_prob = 0.3\n",
    "l2_lambda = 0.001\n",
    "init_lr = 0.001\n",
    "decay_steps = 500\n",
    "decay_rate = 0.99\n",
    "clip_norm = 1\n",
    "clipping = True\n",
    "moving_avg_deacy = 0.999\n",
    "num_gpus = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "device_num = 0\n",
    "tower_grads = []\n",
    "losses = []\n",
    "predictions = []\n",
    "predictions_pos = []\n",
    "\n",
    "x = tf.placeholder('int32', [num_gpus, batch_size, max_sent_size], name=\"x\")\n",
    "y = tf.placeholder('int32', [num_gpus, batch_size, max_sent_size], name=\"y\")\n",
    "y_pos = tf.placeholder('int32', [num_gpus, batch_size, max_sent_size], name=\"y\")\n",
    "x_mask  = tf.placeholder('bool', [num_gpus, batch_size, max_sent_size], name='x_mask') \n",
    "sense_mask  = tf.placeholder('bool', [num_gpus, batch_size, max_sent_size], name='sense_mask')\n",
    "is_train = tf.placeholder('bool', [], name='is_train')\n",
    "word_emb_mat = tf.placeholder('float', [None, word_emb_size], name='emb_mat')\n",
    "input_keep_prob = tf.cond(is_train,lambda:keep_prob, lambda:tf.constant(1.0))\n",
    "pretrain = tf.placeholder('bool', [], name=\"pretrain\")\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(init_lr, global_step, decay_steps, decay_rate, staircase=True)\n",
    "summaries = []\n",
    "\n",
    "def global_attention(input_x, input_mask, W_att):\n",
    "    h_masked = tf.boolean_mask(input_x, input_mask)\n",
    "    h_tanh = tf.tanh(h_masked)\n",
    "    u = tf.matmul(h_tanh, W_att)\n",
    "    a = tf.nn.softmax(u)\n",
    "    c = tf.reduce_sum(tf.multiply(h_tanh, a), 0)  \n",
    "    return c\n",
    "\n",
    "with tf.variable_scope(\"word_embedding\"):\n",
    "    unk_word_emb_mat = tf.get_variable(\"word_emb_mat\", dtype='float', shape=[unk_vocab_size, word_emb_size], initializer=tf.contrib.layers.xavier_initializer(uniform=True, seed=0, dtype=tf.float32))\n",
    "    final_word_emb_mat = tf.concat([word_emb_mat, unk_word_emb_mat], 0)\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope()):\n",
    "    for gpu_idx in range(num_gpus):\n",
    "        if gpu_idx>int(num_gpus/2)-1:\n",
    "            device_num = 1\n",
    "        with tf.name_scope(\"model_{}\".format(gpu_idx)) as scope, tf.device('/gpu:%d' % device_num):\n",
    "\n",
    "            if gpu_idx > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            with tf.name_scope(\"word\"):\n",
    "                Wx = tf.nn.embedding_lookup(final_word_emb_mat, x[gpu_idx])  \n",
    "\n",
    "            x_len = tf.reduce_sum(tf.cast(x_mask[gpu_idx], 'int32'), 1)\n",
    "\n",
    "            with tf.variable_scope(\"lstm1\"):\n",
    "                cell_fw1 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "                cell_bw1 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "\n",
    "                d_cell_fw1 = tf.contrib.rnn.DropoutWrapper(cell_fw1, input_keep_prob=input_keep_prob)\n",
    "                d_cell_bw1 = tf.contrib.rnn.DropoutWrapper(cell_bw1, input_keep_prob=input_keep_prob)\n",
    "\n",
    "                (fw_h1, bw_h1), _ = tf.nn.bidirectional_dynamic_rnn(d_cell_fw1, d_cell_bw1, Wx, sequence_length=x_len, dtype='float', scope='lstm1')\n",
    "                h1 = tf.concat([fw_h1, bw_h1], 2)\n",
    "\n",
    "            with tf.variable_scope(\"lstm2\"):\n",
    "                cell_fw2 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "                cell_bw2 = tf.contrib.rnn.BasicLSTMCell(hidden_size,state_is_tuple=True)\n",
    "\n",
    "                d_cell_fw2 = tf.contrib.rnn.DropoutWrapper(cell_fw2, input_keep_prob=input_keep_prob)\n",
    "                d_cell_bw2 = tf.contrib.rnn.DropoutWrapper(cell_bw2, input_keep_prob=input_keep_prob)\n",
    "\n",
    "                (fw_h2, bw_h2), _ = tf.nn.bidirectional_dynamic_rnn(d_cell_fw2, d_cell_bw2, h1, sequence_length=x_len, dtype='float', scope='lstm2')\n",
    "                h = tf.concat([fw_h2, bw_h2], 2)\n",
    "\n",
    "            with tf.variable_scope(\"global_attention\"):\n",
    "                W_att = tf.get_variable(\"W_att\", shape=[2*hidden_size, 1], initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1, seed=gpu_idx*10))\n",
    "                c = tf.expand_dims(global_attention(h[0], x_mask[gpu_idx][0], W_att), 0)\n",
    "                for i in range(1, batch_size):\n",
    "                    c = tf.concat([c, tf.expand_dims(global_attention(h[i], x_mask[gpu_idx][i], W_att), 0)], 0)\n",
    "                cc = tf.expand_dims(c, 1)\n",
    "                c_final = tf.tile(cc, [1, max_sent_size, 1])\n",
    "\n",
    "            rev_bw_h2 = tf.reverse(bw_h2, [1])\n",
    "\n",
    "            with tf.variable_scope(\"convolution\"):\n",
    "                conv1_fw = tf.layers.conv1d(inputs=fw_h2, filters=num_filter, kernel_size=[kernel_size], padding='same', activation=tf.nn.relu)\n",
    "                conv2_fw = tf.layers.conv1d(inputs=conv1_fw, filters=num_filter, kernel_size=[kernel_size], padding='same')\n",
    "                conv1_bw_rev = tf.layers.conv1d(inputs=rev_bw_h2, filters=num_filter, kernel_size=[kernel_size], padding='same', activation=tf.nn.relu)\n",
    "                conv2_bw_rev = tf.layers.conv1d(inputs=conv1_bw_rev, filters=num_filter, kernel_size=[kernel_size], padding='same')\n",
    "                conv2_bw = tf.reverse(conv2_bw_rev, [1])\n",
    "\n",
    "            h_final = tf.concat([c_final, conv2_fw, conv2_bw], 2)\n",
    "            flat_h_final = tf.reshape(h_final, [-1, tf.shape(h_final)[2]])\n",
    "\n",
    "            with tf.variable_scope(\"hidden_layer\"):\n",
    "                W = tf.get_variable(\"W\", shape=[4*hidden_size, 2*hidden_size], initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1, seed=gpu_idx*20))\n",
    "                b = tf.get_variable(\"b\", shape=[2*hidden_size], initializer=tf.zeros_initializer())\n",
    "                drop_flat_h_final = tf.nn.dropout(flat_h_final, input_keep_prob)\n",
    "                flat_hl = tf.matmul(drop_flat_h_final, W) + b\n",
    "\n",
    "            with tf.variable_scope(\"softmax_layer\"):\n",
    "                W = tf.get_variable(\"W\", shape=[2*hidden_size, num_senses], initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1, seed=gpu_idx*20))\n",
    "                b = tf.get_variable(\"b\", shape=[num_senses], initializer=tf.zeros_initializer())\n",
    "                drop_flat_hl = tf.nn.dropout(flat_hl, input_keep_prob)\n",
    "                flat_logits_sense = tf.matmul(drop_flat_hl, W) + b\n",
    "                logits = tf.reshape(flat_logits_sense, [batch_size, max_sent_size, num_senses])\n",
    "                predictions.append(tf.arg_max(logits, 2))\n",
    "\n",
    "            with tf.variable_scope(\"softmax_layer_pos\"):\n",
    "                W = tf.get_variable(\"W\", shape=[2*hidden_size, num_pos], initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1, seed=gpu_idx*30))\n",
    "                b = tf.get_variable(\"b\", shape=[num_pos], initializer=tf.zeros_initializer())\n",
    "                drop_flat_hl = tf.nn.dropout(flat_hl, input_keep_prob)\n",
    "                flat_logits_pos = tf.matmul(drop_flat_hl, W) + b\n",
    "                logits_pos = tf.reshape(flat_logits_pos, [batch_size, max_sent_size, num_pos])\n",
    "                predictions_pos.append(tf.arg_max(logits_pos, 2))\n",
    "\n",
    "\n",
    "            float_sense_mask = tf.cast(sense_mask[gpu_idx], 'float')\n",
    "            float_x_mask = tf.cast(x_mask[gpu_idx], 'float')\n",
    "\n",
    "            loss = tf.contrib.seq2seq.sequence_loss(logits, y[gpu_idx], float_sense_mask, name=\"loss\")\n",
    "            loss_pos = tf.contrib.seq2seq.sequence_loss(logits_pos, y_pos[gpu_idx], float_x_mask, name=\"loss_\")\n",
    "\n",
    "            l2_loss = l2_lambda * tf.losses.get_regularization_loss()\n",
    "\n",
    "            total_loss = tf.cond(pretrain, lambda:loss_pos, lambda:loss + loss_pos + l2_loss)\n",
    "\n",
    "            summaries.append(tf.summary.scalar(\"loss_{}\".format(gpu_idx), loss))\n",
    "            summaries.append(tf.summary.scalar(\"loss_pos_{}\".format(gpu_idx), loss_pos))\n",
    "            summaries.append(tf.summary.scalar(\"total_loss_{}\".format(gpu_idx), total_loss))\n",
    "\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_vars = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "            clipped_grads = grads_vars\n",
    "            if(clipping == True):\n",
    "                clipped_grads = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in clipped_grads]\n",
    "\n",
    "            tower_grads.append(clipped_grads)\n",
    "            losses.append(total_loss)\n",
    "\n",
    "tower_grads = average_gradients(tower_grads)\n",
    "losses = tf.add_n(losses)/len(losses)\n",
    "apply_grad_op = optimizer.apply_gradients(tower_grads, global_step=global_step)\n",
    "summaries.append(tf.summary.scalar('total_loss', losses))\n",
    "summaries.append(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "variable_averages = tf.train.ExponentialMovingAverage(moving_avg_deacy, global_step)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "train_op = tf.group(apply_grad_op, variables_averages_op)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "summary = tf.summary.merge(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "# print (device_lib.list_local_devices())\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())                          # For initializing all the variables\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)          # For writing Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_period = 100\n",
    "log_period = 100\n",
    "\n",
    "def model(xx, yy, yy_pos, mask, smask, train_cond=True, pretrain_cond=False):\n",
    "    num_batches = int(len(xx)/(batch_size*num_gpus))\n",
    "    _losses = 0\n",
    "    temp_loss = 0\n",
    "    preds_sense = []\n",
    "    true_sense = []\n",
    "    preds_pos = []\n",
    "    true_pos = []\n",
    "    \n",
    "    for j in range(num_batches): \n",
    "        \n",
    "        s = j * batch_size * num_gpus\n",
    "        e = (j+1) * batch_size * num_gpus\n",
    "        xx_re = xx[s:e].reshape([num_gpus, batch_size, -1])\n",
    "        yy_re = yy[s:e].reshape([num_gpus, batch_size, -1])\n",
    "        yy_pos_re = yy_pos[s:e].reshape([num_gpus, batch_size, -1])\n",
    "        mask_re = mask[s:e].reshape([num_gpus, batch_size, -1])\n",
    "        smask_re = smask[s:e].reshape([num_gpus, batch_size, -1])\n",
    " \n",
    "        feed_dict = {x:xx_re, y:yy_re, y_pos:yy_pos_re, x_mask:mask_re, sense_mask:smask_re, pretrain:pretrain_cond, is_train:train_cond, input_keep_prob:keep_prob, word_emb_mat:word_embedding}\n",
    "        \n",
    "        if(train_cond==True):\n",
    "            _, _loss, step, _summary = sess.run([train_op, losses, global_step, summary], feed_dict)\n",
    "            summary_writer.add_summary(_summary, step)\n",
    "            \n",
    "            temp_loss += _loss\n",
    "            if((j+1)%log_period==0):\n",
    "                print(\"Steps: {}\".format(step), \"Loss:{0:.4f}\".format(temp_loss/log_period), \", Current Loss: {0:.4f}\".format(_loss))\n",
    "                temp_loss = 0\n",
    "            if((j+1)%save_period==0):\n",
    "                saver.save(sess, save_path=save_dir)                         \n",
    "                \n",
    "        else:\n",
    "            _loss, pred, pred_pos = sess.run([total_loss, predictions, predictions_pos], feed_dict)\n",
    "            for i in range(num_gpus):\n",
    "                preds_sense.append(pred[i][smask_re[i]])\n",
    "                true_sense.append(yy_re[i][smask_re[i]])\n",
    "                preds_pos.append(pred_pos[i][mask_re[i]])\n",
    "                true_pos.append(yy_pos_re[i][mask_re[i]])\n",
    "\n",
    "        _losses +=_loss\n",
    "\n",
    "    if(train_cond==False): \n",
    "        sense_preds = []\n",
    "        sense_true = []\n",
    "        pos_preds = []\n",
    "        pos_true = []\n",
    "        \n",
    "        for preds in preds_sense:\n",
    "            for ps in preds:      \n",
    "                sense_preds.append(ps)  \n",
    "        for trues in true_sense:\n",
    "            for ts in trues:\n",
    "                sense_true.append(ts)\n",
    "        \n",
    "        for preds in preds_pos:\n",
    "            for ps in preds:      \n",
    "                pos_preds.append(ps)      \n",
    "        for trues in true_pos:\n",
    "            for ts in trues:\n",
    "                pos_true.append(ts)\n",
    "                \n",
    "        return _losses/num_batches, sense_preds, sense_true, pos_preds, pos_true\n",
    "\n",
    "    return _losses/num_batches, step\n",
    "\n",
    "def eval_score(yy, pred, yy_pos, pred_pos):\n",
    "    f1 = f1_score(yy, pred, average='macro')\n",
    "    accu = accuracy_score(yy, pred)\n",
    "    f1_pos = f1_score(yy_pos, pred_pos, average='macro')\n",
    "    accu_pos = accuracy_score(yy_pos, pred_pos)\n",
    "    return f1*100, accu*100, f1_pos*100, accu_pos*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_id_train = train_data['x']\n",
    "mask_train = train_data['x_mask']\n",
    "sense_mask_train = train_data['sense_mask']\n",
    "y_train = train_data['y']\n",
    "y_pos_train = train_data['pos']\n",
    "\n",
    "x_id_val = val_data['x']\n",
    "mask_val = val_data['x_mask']\n",
    "sense_mask_val = val_data['sense_mask']\n",
    "y_val = val_data['y']\n",
    "y_pos_val = val_data['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 100 Loss:33.2750 , Current Loss: 0.8327\n",
      "Steps: 200 Loss:0.5268 , Current Loss: 0.3678\n",
      "Steps: 300 Loss:0.3184 , Current Loss: 0.2711\n",
      "Steps: 400 Loss:0.2548 , Current Loss: 0.2264\n",
      "Steps: 500 Loss:0.2194 , Current Loss: 0.2027\n",
      "Steps: 600 Loss:0.1966 , Current Loss: 0.1826\n",
      "Steps: 700 Loss:0.1808 , Current Loss: 0.1779\n",
      "Steps: 800 Loss:0.1699 , Current Loss: 0.1736\n",
      "Steps: 900 Loss:0.1617 , Current Loss: 0.1602\n",
      "Steps: 1000 Loss:0.1529 , Current Loss: 0.1488\n",
      "Steps: 1100 Loss:0.1489 , Current Loss: 0.1545\n",
      "Steps: 1200 Loss:0.1458 , Current Loss: 0.1462\n",
      "Steps: 1300 Loss:0.1404 , Current Loss: 0.1401\n",
      "Steps: 1400 Loss:0.1356 , Current Loss: 0.1399\n",
      "Steps: 1500 Loss:0.1314 , Current Loss: 0.1258\n",
      "Steps: 1600 Loss:0.1298 , Current Loss: 0.1300\n",
      "Steps: 1700 Loss:0.1274 , Current Loss: 0.1163\n",
      "Epoch: 1 , Step: 1771 , loss: 2.0612 , Time: 11504.6\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/btech/aviraj/envs/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:1.12 Accuracy:1.47  POS: F1 Score:93.27 Accuracy:95.85 Loss:11.7837 , Time: 2299.4\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "val_period = 1\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = True\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1871 Loss:3.9956 , Current Loss: 3.1696\n",
      "Steps: 1971 Loss:2.9681 , Current Loss: 2.8126\n",
      "Steps: 2071 Loss:2.7876 , Current Loss: 2.7860\n",
      "Steps: 2171 Loss:2.6289 , Current Loss: 2.5738\n",
      "Steps: 2271 Loss:2.5435 , Current Loss: 2.3657\n",
      "Steps: 2371 Loss:2.4744 , Current Loss: 2.5109\n",
      "Steps: 2471 Loss:2.4250 , Current Loss: 2.4612\n",
      "Steps: 2571 Loss:2.3814 , Current Loss: 2.3583\n",
      "Steps: 2671 Loss:2.3216 , Current Loss: 2.3235\n",
      "Steps: 2771 Loss:2.2793 , Current Loss: 2.3103\n",
      "Steps: 2871 Loss:2.2659 , Current Loss: 2.2289\n",
      "Steps: 2971 Loss:2.2372 , Current Loss: 2.2791\n",
      "Steps: 3071 Loss:2.2210 , Current Loss: 2.1201\n",
      "Steps: 3171 Loss:2.1979 , Current Loss: 2.1856\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:34.22 Accuracy:48.23  POS: F1 Score:83.57 Accuracy:92.12\n",
      "Val: F1 Score:42.38 Accuracy:53.07  POS: F1 Score:84.18 Accuracy:92.44\n",
      "Val: F1 Score:44.58 Accuracy:54.93  POS: F1 Score:84.83 Accuracy:92.85\n",
      "Val: F1 Score:46.84 Accuracy:56.83  POS: F1 Score:84.66 Accuracy:92.92\n",
      "Val: F1 Score:48.40 Accuracy:57.32  POS: F1 Score:84.70 Accuracy:92.87\n"
     ]
    }
   ],
   "source": [
    "for v in val_collection:\n",
    "    print(\"Val: F1 Score:{0:.2f}\".format(v[0]), \"Accuracy:{0:.2f}\".format(v[1]), \" POS: F1 Score:{0:.2f}\".format(v[2]), \"Accuracy:{0:.2f}\".format(v[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4655171245723575,\n",
       " 2.0027206975278848,\n",
       " 1.8724895704943605,\n",
       " 1.7916589952471027,\n",
       " 1.736836715716551,\n",
       " 1.6930595723200086,\n",
       " 1.6597260672247793,\n",
       " 1.6344640272475446,\n",
       " 1.6094474871908453,\n",
       " 1.594043997436644]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00099700305"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(learning_ratening_ratening_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 19581 Loss:1.5124 , Current Loss: 1.4568\n",
      "Steps: 19681 Loss:1.4886 , Current Loss: 1.5131\n",
      "Steps: 19781 Loss:1.4710 , Current Loss: 1.5145\n",
      "Steps: 19881 Loss:1.4741 , Current Loss: 1.4782\n",
      "Steps: 19981 Loss:1.4605 , Current Loss: 1.4133\n",
      "Steps: 20081 Loss:1.4476 , Current Loss: 1.5403\n",
      "Steps: 20181 Loss:1.4435 , Current Loss: 1.4998\n",
      "Steps: 20281 Loss:1.4296 , Current Loss: 1.3782\n",
      "Steps: 20381 Loss:1.4368 , Current Loss: 1.4933\n",
      "Steps: 20481 Loss:1.4205 , Current Loss: 1.3596\n",
      "Steps: 20581 Loss:1.4142 , Current Loss: 1.4263\n",
      "Steps: 20681 Loss:1.4230 , Current Loss: 1.4139\n",
      "Steps: 20781 Loss:1.4181 , Current Loss: 1.4190\n",
      "Steps: 20881 Loss:1.4126 , Current Loss: 1.3405\n",
      "Steps: 20981 Loss:1.4132 , Current Loss: 1.3976\n",
      "Steps: 21081 Loss:1.4086 , Current Loss: 1.3606\n",
      "Steps: 21181 Loss:1.3866 , Current Loss: 1.4127\n",
      "Epoch: 1 , Step: 21252 , loss: 1.4373 , Time: 11614.5\n",
      "Model Saved\n",
      "Steps: 21352 Loss:1.3863 , Current Loss: 1.4108\n",
      "Steps: 21452 Loss:1.3959 , Current Loss: 1.3552\n",
      "Steps: 21552 Loss:1.3910 , Current Loss: 1.3754\n",
      "Steps: 21652 Loss:1.4003 , Current Loss: 1.3478\n",
      "Steps: 21752 Loss:1.3811 , Current Loss: 1.3573\n",
      "Steps: 21852 Loss:1.3976 , Current Loss: 1.3263\n",
      "Steps: 21952 Loss:1.3810 , Current Loss: 1.3263\n",
      "Steps: 22052 Loss:1.3779 , Current Loss: 1.2748\n",
      "Steps: 22152 Loss:1.3662 , Current Loss: 1.4441\n",
      "Steps: 22252 Loss:1.3666 , Current Loss: 1.4177\n",
      "Steps: 22352 Loss:1.3753 , Current Loss: 1.3614\n",
      "Steps: 22452 Loss:1.3783 , Current Loss: 1.2715\n",
      "Steps: 22552 Loss:1.3847 , Current Loss: 1.3756\n",
      "Steps: 22652 Loss:1.3721 , Current Loss: 1.4389\n",
      "Steps: 22752 Loss:1.3754 , Current Loss: 1.3159\n",
      "Steps: 22852 Loss:1.3768 , Current Loss: 1.4215\n",
      "Steps: 22952 Loss:1.3683 , Current Loss: 1.3478\n",
      "Epoch: 2 , Step: 23023 , loss: 1.3805 , Time: 11557.3\n",
      "Model Saved\n",
      "Val: F1 Score:54.44 Accuracy:62.49  POS: F1 Score:86.17 Accuracy:94.09 Loss:1.3759 , Time: 2276.5\n",
      "Steps: 23123 Loss:1.3678 , Current Loss: 1.4040\n",
      "Steps: 23223 Loss:1.3655 , Current Loss: 1.3003\n",
      "Steps: 23323 Loss:1.3541 , Current Loss: 1.3281\n",
      "Steps: 23423 Loss:1.3629 , Current Loss: 1.3342\n",
      "Steps: 23523 Loss:1.3568 , Current Loss: 1.2630\n",
      "Steps: 23623 Loss:1.3603 , Current Loss: 1.3835\n",
      "Steps: 23723 Loss:1.3590 , Current Loss: 1.3890\n",
      "Steps: 23823 Loss:1.3540 , Current Loss: 1.2734\n",
      "Steps: 23923 Loss:1.3597 , Current Loss: 1.3219\n",
      "Steps: 24023 Loss:1.3428 , Current Loss: 1.2861\n",
      "Steps: 24123 Loss:1.3507 , Current Loss: 1.2423\n",
      "Steps: 24223 Loss:1.3545 , Current Loss: 1.4506\n",
      "Steps: 24323 Loss:1.3407 , Current Loss: 1.3197\n",
      "Steps: 24423 Loss:1.3553 , Current Loss: 1.5045\n",
      "Steps: 24523 Loss:1.3325 , Current Loss: 1.4493\n",
      "Steps: 24623 Loss:1.3550 , Current Loss: 1.3753\n",
      "Steps: 24723 Loss:1.3579 , Current Loss: 1.2265\n",
      "Epoch: 3 , Step: 24794 , loss: 1.3542 , Time: 11352.1\n",
      "Model Saved\n",
      "Steps: 24894 Loss:1.3332 , Current Loss: 1.4005\n",
      "Steps: 24994 Loss:1.3446 , Current Loss: 1.2932\n",
      "Steps: 25094 Loss:1.3475 , Current Loss: 1.3104\n",
      "Steps: 25194 Loss:1.3364 , Current Loss: 1.3216\n",
      "Steps: 25294 Loss:1.3340 , Current Loss: 1.3190\n",
      "Steps: 25394 Loss:1.3320 , Current Loss: 1.3025\n",
      "Steps: 25494 Loss:1.3424 , Current Loss: 1.3537\n",
      "Steps: 25594 Loss:1.3504 , Current Loss: 1.4487\n",
      "Steps: 25694 Loss:1.3337 , Current Loss: 1.2365\n",
      "Steps: 25794 Loss:1.3243 , Current Loss: 1.2990\n",
      "Steps: 25894 Loss:1.3320 , Current Loss: 1.2885\n",
      "Steps: 25994 Loss:1.3438 , Current Loss: 1.3419\n",
      "Steps: 26094 Loss:1.3398 , Current Loss: 1.2321\n",
      "Steps: 26194 Loss:1.3369 , Current Loss: 1.4533\n",
      "Steps: 26294 Loss:1.3418 , Current Loss: 1.3506\n",
      "Steps: 26394 Loss:1.3308 , Current Loss: 1.2060\n",
      "Steps: 26494 Loss:1.3371 , Current Loss: 1.4090\n",
      "Epoch: 4 , Step: 26565 , loss: 1.3372 , Time: 11374.6\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:55.43 Accuracy:63.29  POS: F1 Score:86.72 Accuracy:94.23 Loss:1.3318 , Time: 2293.5\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 26665 Loss:1.3179 , Current Loss: 1.2862\n",
      "Steps: 26765 Loss:1.3207 , Current Loss: 1.2462\n",
      "Steps: 26865 Loss:1.3363 , Current Loss: 1.2352\n",
      "Steps: 26965 Loss:1.3255 , Current Loss: 1.3320\n",
      "Steps: 27065 Loss:1.3253 , Current Loss: 1.3032\n",
      "Steps: 27165 Loss:1.3140 , Current Loss: 1.3353\n",
      "Steps: 27265 Loss:1.3248 , Current Loss: 1.3165\n",
      "Steps: 27365 Loss:1.3272 , Current Loss: 1.1715\n",
      "Steps: 27465 Loss:1.3228 , Current Loss: 1.2624\n",
      "Steps: 27565 Loss:1.3206 , Current Loss: 1.3002\n",
      "Steps: 27665 Loss:1.3233 , Current Loss: 1.2588\n",
      "Steps: 27765 Loss:1.3185 , Current Loss: 1.2974\n",
      "Steps: 27865 Loss:1.3174 , Current Loss: 1.2037\n",
      "Steps: 27965 Loss:1.3160 , Current Loss: 1.2747\n",
      "Steps: 28065 Loss:1.3163 , Current Loss: 1.2220\n",
      "Steps: 28165 Loss:1.3213 , Current Loss: 1.2834\n",
      "Steps: 28265 Loss:1.3144 , Current Loss: 1.3345\n",
      "Epoch: 1 , Step: 28336 , loss: 1.3213 , Time: 11358.2\n",
      "Model Saved\n",
      "Steps: 28436 Loss:1.3049 , Current Loss: 1.2685\n",
      "Steps: 28536 Loss:1.3143 , Current Loss: 1.2625\n",
      "Steps: 28636 Loss:1.3311 , Current Loss: 1.3607\n",
      "Steps: 28736 Loss:1.3146 , Current Loss: 1.2234\n",
      "Steps: 28836 Loss:1.3026 , Current Loss: 1.3722\n",
      "Steps: 28936 Loss:1.2950 , Current Loss: 1.2969\n",
      "Steps: 29036 Loss:1.3071 , Current Loss: 1.3211\n",
      "Steps: 29136 Loss:1.3061 , Current Loss: 1.3239\n",
      "Steps: 29236 Loss:1.3201 , Current Loss: 1.4525\n",
      "Steps: 29336 Loss:1.3131 , Current Loss: 1.2591\n",
      "Steps: 29436 Loss:1.3108 , Current Loss: 1.2587\n",
      "Steps: 29536 Loss:1.3166 , Current Loss: 1.4063\n",
      "Steps: 29636 Loss:1.2979 , Current Loss: 1.2291\n",
      "Steps: 29736 Loss:1.3135 , Current Loss: 1.3336\n",
      "Steps: 29836 Loss:1.3107 , Current Loss: 1.3218\n",
      "Steps: 29936 Loss:1.3155 , Current Loss: 1.2113\n",
      "Steps: 30036 Loss:1.2960 , Current Loss: 1.2133\n",
      "Epoch: 2 , Step: 30107 , loss: 1.3100 , Time: 11319.3\n",
      "Model Saved\n",
      "Val: F1 Score:56.20 Accuracy:63.70  POS: F1 Score:86.75 Accuracy:94.32 Loss:1.3178 , Time: 2329.6\n",
      "Steps: 30207 Loss:1.3000 , Current Loss: 1.2345\n",
      "Steps: 30307 Loss:1.2976 , Current Loss: 1.2282\n",
      "Steps: 30407 Loss:1.3121 , Current Loss: 1.2741\n",
      "Steps: 30507 Loss:1.2934 , Current Loss: 1.4442\n",
      "Steps: 30607 Loss:1.3051 , Current Loss: 1.2232\n",
      "Steps: 30707 Loss:1.3074 , Current Loss: 1.4512\n",
      "Steps: 30807 Loss:1.3066 , Current Loss: 1.3150\n",
      "Steps: 30907 Loss:1.3023 , Current Loss: 1.2061\n",
      "Steps: 31007 Loss:1.2973 , Current Loss: 1.2602\n",
      "Steps: 31107 Loss:1.2952 , Current Loss: 1.3436\n",
      "Steps: 31207 Loss:1.3002 , Current Loss: 1.2352\n",
      "Steps: 31307 Loss:1.3042 , Current Loss: 1.1764\n",
      "Steps: 31407 Loss:1.2853 , Current Loss: 1.3366\n",
      "Steps: 31507 Loss:1.3051 , Current Loss: 1.2302\n",
      "Steps: 31607 Loss:1.2960 , Current Loss: 1.2448\n",
      "Steps: 31707 Loss:1.3066 , Current Loss: 1.2769\n",
      "Steps: 31807 Loss:1.2994 , Current Loss: 1.2550\n",
      "Epoch: 3 , Step: 31878 , loss: 1.3001 , Time: 11328.5\n",
      "Model Saved\n",
      "Steps: 31978 Loss:1.2932 , Current Loss: 1.2774\n",
      "Steps: 32078 Loss:1.2834 , Current Loss: 1.3536\n",
      "Steps: 32178 Loss:1.3059 , Current Loss: 1.2974\n",
      "Steps: 32278 Loss:1.2873 , Current Loss: 1.1431\n",
      "Steps: 32378 Loss:1.2943 , Current Loss: 1.3257\n",
      "Steps: 32478 Loss:1.2905 , Current Loss: 1.2703\n",
      "Steps: 32578 Loss:1.2815 , Current Loss: 1.3229\n",
      "Steps: 32678 Loss:1.2902 , Current Loss: 1.5245\n",
      "Steps: 32778 Loss:1.2763 , Current Loss: 1.3245\n",
      "Steps: 32878 Loss:1.2937 , Current Loss: 1.3370\n",
      "Steps: 32978 Loss:1.2885 , Current Loss: 1.3202\n",
      "Steps: 33078 Loss:1.2873 , Current Loss: 1.2431\n",
      "Steps: 33178 Loss:1.3064 , Current Loss: 1.3067\n",
      "Steps: 33278 Loss:1.2942 , Current Loss: 1.4331\n",
      "Steps: 33378 Loss:1.2883 , Current Loss: 1.2538\n",
      "Steps: 33478 Loss:1.2759 , Current Loss: 1.3253\n",
      "Steps: 33578 Loss:1.2896 , Current Loss: 1.4233\n",
      "Epoch: 4 , Step: 33649 , loss: 1.2895 , Time: 11362.1\n",
      "Model Saved\n",
      "Val: F1 Score:56.86 Accuracy:64.31  POS: F1 Score:87.05 Accuracy:94.42 Loss:1.2887 , Time: 2370.5\n",
      "Steps: 33749 Loss:1.2774 , Current Loss: 1.2831\n",
      "Steps: 33849 Loss:1.2835 , Current Loss: 1.2178\n",
      "Steps: 33949 Loss:1.2869 , Current Loss: 1.3400\n",
      "Steps: 34049 Loss:1.2666 , Current Loss: 1.3315\n",
      "Steps: 34149 Loss:1.2806 , Current Loss: 1.3830\n",
      "Steps: 34249 Loss:1.2848 , Current Loss: 1.2666\n",
      "Steps: 34349 Loss:1.2830 , Current Loss: 1.2896\n",
      "Steps: 34449 Loss:1.2978 , Current Loss: 1.1623\n",
      "Steps: 34549 Loss:1.2847 , Current Loss: 1.3068\n",
      "Steps: 34649 Loss:1.2806 , Current Loss: 1.2940\n",
      "Steps: 34749 Loss:1.2839 , Current Loss: 1.2945\n",
      "Steps: 34849 Loss:1.2846 , Current Loss: 1.2371\n",
      "Steps: 34949 Loss:1.2923 , Current Loss: 1.3108\n",
      "Steps: 35049 Loss:1.2916 , Current Loss: 1.2759\n",
      "Steps: 35149 Loss:1.2818 , Current Loss: 1.4296\n",
      "Steps: 35249 Loss:1.2858 , Current Loss: 1.2774\n",
      "Steps: 35349 Loss:1.2772 , Current Loss: 1.2566\n",
      "Epoch: 5 , Step: 35420 , loss: 1.2835 , Time: 11326.8\n",
      "Model Saved\n",
      "Steps: 35520 Loss:1.2760 , Current Loss: 1.3944\n",
      "Steps: 35620 Loss:1.2817 , Current Loss: 1.4226\n",
      "Steps: 35720 Loss:1.2704 , Current Loss: 1.2998\n",
      "Steps: 35820 Loss:1.2819 , Current Loss: 1.3223\n",
      "Steps: 35920 Loss:1.2712 , Current Loss: 1.4378\n",
      "Steps: 36020 Loss:1.2785 , Current Loss: 1.1845\n",
      "Steps: 36120 Loss:1.2610 , Current Loss: 1.0700\n",
      "Steps: 36220 Loss:1.2682 , Current Loss: 1.1625\n",
      "Steps: 36320 Loss:1.2769 , Current Loss: 1.2784\n",
      "Steps: 36420 Loss:1.2776 , Current Loss: 1.4332\n",
      "Steps: 36520 Loss:1.2696 , Current Loss: 1.3017\n",
      "Steps: 36620 Loss:1.2787 , Current Loss: 1.3456\n",
      "Steps: 36720 Loss:1.2614 , Current Loss: 1.1735\n",
      "Steps: 36820 Loss:1.2632 , Current Loss: 1.3169\n",
      "Steps: 36920 Loss:1.2759 , Current Loss: 1.3843\n",
      "Steps: 37020 Loss:1.2703 , Current Loss: 1.3116\n",
      "Steps: 37120 Loss:1.2812 , Current Loss: 1.3684\n",
      "Epoch: 6 , Step: 37191 , loss: 1.2734 , Time: 11335.3\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:57.40 Accuracy:64.72  POS: F1 Score:87.31 Accuracy:94.46 Loss:1.2828 , Time: 2283.2\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 37291 Loss:1.2675 , Current Loss: 1.3723\n",
      "Steps: 37391 Loss:1.2758 , Current Loss: 1.2771\n",
      "Steps: 37491 Loss:1.2539 , Current Loss: 1.2049\n",
      "Steps: 37591 Loss:1.2661 , Current Loss: 1.2917\n",
      "Steps: 37691 Loss:1.2715 , Current Loss: 1.2075\n",
      "Steps: 37791 Loss:1.2647 , Current Loss: 1.2314\n",
      "Steps: 37891 Loss:1.2737 , Current Loss: 1.2460\n",
      "Steps: 37991 Loss:1.2699 , Current Loss: 1.2344\n",
      "Steps: 38091 Loss:1.2749 , Current Loss: 1.1636\n",
      "Steps: 38191 Loss:1.2595 , Current Loss: 1.2154\n",
      "Steps: 38291 Loss:1.2720 , Current Loss: 1.3131\n",
      "Steps: 38391 Loss:1.2749 , Current Loss: 1.3101\n",
      "Steps: 38491 Loss:1.2565 , Current Loss: 1.2913\n",
      "Steps: 38591 Loss:1.2671 , Current Loss: 1.3980\n",
      "Steps: 38691 Loss:1.2660 , Current Loss: 1.3388\n",
      "Steps: 38791 Loss:1.2650 , Current Loss: 1.3399\n",
      "Steps: 38891 Loss:1.2570 , Current Loss: 1.3982\n",
      "Epoch: 1 , Step: 38962 , loss: 1.2666 , Time: 11799.0\n",
      "Model Saved\n",
      "Steps: 39062 Loss:1.2543 , Current Loss: 1.2514\n",
      "Steps: 39162 Loss:1.2598 , Current Loss: 1.3367\n",
      "Steps: 39262 Loss:1.2555 , Current Loss: 1.2105\n",
      "Steps: 39362 Loss:1.2610 , Current Loss: 1.3313\n",
      "Steps: 39462 Loss:1.2563 , Current Loss: 1.2242\n",
      "Steps: 39562 Loss:1.2492 , Current Loss: 1.3701\n",
      "Steps: 39662 Loss:1.2612 , Current Loss: 1.3038\n",
      "Steps: 39762 Loss:1.2600 , Current Loss: 1.2431\n",
      "Steps: 39862 Loss:1.2672 , Current Loss: 1.3057\n",
      "Steps: 39962 Loss:1.2613 , Current Loss: 1.1649\n",
      "Steps: 40062 Loss:1.2535 , Current Loss: 1.3466\n",
      "Steps: 40162 Loss:1.2677 , Current Loss: 1.2363\n",
      "Steps: 40262 Loss:1.2583 , Current Loss: 1.2605\n",
      "Steps: 40362 Loss:1.2676 , Current Loss: 1.1958\n",
      "Steps: 40462 Loss:1.2595 , Current Loss: 1.2194\n",
      "Steps: 40562 Loss:1.2553 , Current Loss: 1.2485\n",
      "Steps: 40662 Loss:1.2602 , Current Loss: 1.2372\n",
      "Epoch: 2 , Step: 40733 , loss: 1.2589 , Time: 11765.3\n",
      "Model Saved\n",
      "Val: F1 Score:57.41 Accuracy:64.87  POS: F1 Score:86.97 Accuracy:94.45 Loss:1.2640 , Time: 2271.6\n",
      "Steps: 40833 Loss:1.2588 , Current Loss: 1.2207\n",
      "Steps: 40933 Loss:1.2550 , Current Loss: 1.2608\n",
      "Steps: 41033 Loss:1.2529 , Current Loss: 1.2212\n",
      "Steps: 41133 Loss:1.2573 , Current Loss: 1.2683\n",
      "Steps: 41233 Loss:1.2542 , Current Loss: 1.1908\n",
      "Steps: 41333 Loss:1.2574 , Current Loss: 1.2896\n",
      "Steps: 41433 Loss:1.2489 , Current Loss: 1.2762\n",
      "Steps: 41533 Loss:1.2537 , Current Loss: 1.2265\n",
      "Steps: 41633 Loss:1.2496 , Current Loss: 1.4420\n",
      "Steps: 41733 Loss:1.2491 , Current Loss: 1.0974\n",
      "Steps: 41833 Loss:1.2444 , Current Loss: 1.1833\n",
      "Steps: 41933 Loss:1.2416 , Current Loss: 1.2862\n",
      "Steps: 42033 Loss:1.2597 , Current Loss: 1.4291\n",
      "Steps: 42133 Loss:1.2460 , Current Loss: 1.2001\n",
      "Steps: 42233 Loss:1.2668 , Current Loss: 1.2728\n",
      "Steps: 42333 Loss:1.2472 , Current Loss: 1.2641\n",
      "Steps: 42433 Loss:1.2521 , Current Loss: 1.1885\n",
      "Epoch: 3 , Step: 42504 , loss: 1.2516 , Time: 11767.6\n",
      "Model Saved\n",
      "Steps: 42604 Loss:1.2489 , Current Loss: 1.3874\n",
      "Steps: 42704 Loss:1.2458 , Current Loss: 1.2897\n",
      "Steps: 42804 Loss:1.2559 , Current Loss: 1.2727\n",
      "Steps: 42904 Loss:1.2520 , Current Loss: 1.1456\n",
      "Steps: 43004 Loss:1.2479 , Current Loss: 1.3622\n",
      "Steps: 43104 Loss:1.2514 , Current Loss: 1.3052\n",
      "Steps: 43204 Loss:1.2470 , Current Loss: 1.2871\n",
      "Steps: 43304 Loss:1.2369 , Current Loss: 1.2565\n",
      "Steps: 43404 Loss:1.2409 , Current Loss: 1.2606\n",
      "Steps: 43504 Loss:1.2469 , Current Loss: 1.1790\n",
      "Steps: 43604 Loss:1.2499 , Current Loss: 1.2624\n",
      "Steps: 43704 Loss:1.2412 , Current Loss: 1.2285\n",
      "Steps: 43804 Loss:1.2398 , Current Loss: 1.1001\n",
      "Steps: 43904 Loss:1.2530 , Current Loss: 1.3678\n",
      "Steps: 44004 Loss:1.2460 , Current Loss: 1.1831\n",
      "Steps: 44104 Loss:1.2400 , Current Loss: 1.1158\n",
      "Steps: 44204 Loss:1.2368 , Current Loss: 1.3211\n",
      "Epoch: 4 , Step: 44275 , loss: 1.2454 , Time: 11767.2\n",
      "Model Saved\n",
      "Val: F1 Score:58.60 Accuracy:65.34  POS: F1 Score:87.63 Accuracy:94.54 Loss:1.2424 , Time: 2275.2\n",
      "Steps: 44375 Loss:1.2522 , Current Loss: 1.2622\n",
      "Steps: 44475 Loss:1.2281 , Current Loss: 1.1889\n",
      "Steps: 44575 Loss:1.2459 , Current Loss: 1.1437\n",
      "Steps: 44675 Loss:1.2394 , Current Loss: 1.1483\n",
      "Steps: 44775 Loss:1.2409 , Current Loss: 1.1779\n",
      "Steps: 44875 Loss:1.2499 , Current Loss: 1.1673\n",
      "Steps: 44975 Loss:1.2400 , Current Loss: 1.1170\n",
      "Steps: 45075 Loss:1.2407 , Current Loss: 1.2329\n",
      "Steps: 45175 Loss:1.2469 , Current Loss: 1.1815\n",
      "Steps: 45275 Loss:1.2442 , Current Loss: 1.2121\n",
      "Steps: 45375 Loss:1.2395 , Current Loss: 1.1121\n",
      "Steps: 45475 Loss:1.2404 , Current Loss: 1.2296\n",
      "Steps: 45575 Loss:1.2455 , Current Loss: 1.2801\n",
      "Steps: 45675 Loss:1.2350 , Current Loss: 1.1656\n",
      "Steps: 45775 Loss:1.2329 , Current Loss: 1.2617\n",
      "Steps: 45875 Loss:1.2399 , Current Loss: 1.1537\n",
      "Steps: 45975 Loss:1.2244 , Current Loss: 1.2508\n",
      "Epoch: 5 , Step: 46046 , loss: 1.2404 , Time: 11918.3\n",
      "Model Saved\n",
      "Steps: 46146 Loss:1.2280 , Current Loss: 1.3143\n",
      "Steps: 46246 Loss:1.2388 , Current Loss: 1.2347\n",
      "Steps: 46346 Loss:1.2477 , Current Loss: 1.2911\n",
      "Steps: 46446 Loss:1.2372 , Current Loss: 1.1313\n",
      "Steps: 46546 Loss:1.2305 , Current Loss: 1.2389\n",
      "Steps: 46646 Loss:1.2307 , Current Loss: 1.2746\n",
      "Steps: 46746 Loss:1.2409 , Current Loss: 1.2562\n",
      "Steps: 46846 Loss:1.2294 , Current Loss: 1.3241\n",
      "Steps: 46946 Loss:1.2285 , Current Loss: 1.1447\n",
      "Steps: 47046 Loss:1.2329 , Current Loss: 1.1755\n",
      "Steps: 47146 Loss:1.2270 , Current Loss: 1.1102\n",
      "Steps: 47246 Loss:1.2396 , Current Loss: 1.2584\n",
      "Steps: 47346 Loss:1.2342 , Current Loss: 1.1136\n",
      "Steps: 47446 Loss:1.2427 , Current Loss: 1.2242\n",
      "Steps: 47546 Loss:1.2352 , Current Loss: 1.2195\n",
      "Steps: 47646 Loss:1.2150 , Current Loss: 1.1152\n",
      "Steps: 47746 Loss:1.2330 , Current Loss: 1.1861\n",
      "Epoch: 6 , Step: 47817 , loss: 1.2332 , Time: 11949.4\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 47917 Loss:1.2285 , Current Loss: 1.1608\n",
      "Steps: 48017 Loss:1.2170 , Current Loss: 1.2742\n",
      "Steps: 48117 Loss:1.2296 , Current Loss: 1.2536\n",
      "Steps: 48217 Loss:1.2181 , Current Loss: 1.2226\n",
      "Steps: 48317 Loss:1.2240 , Current Loss: 1.2340\n",
      "Steps: 48417 Loss:1.2253 , Current Loss: 1.1124\n",
      "Steps: 48517 Loss:1.2299 , Current Loss: 1.1854\n",
      "Steps: 48617 Loss:1.2215 , Current Loss: 1.2742\n",
      "Steps: 48717 Loss:1.2125 , Current Loss: 1.2555\n",
      "Steps: 48817 Loss:1.2219 , Current Loss: 1.3055\n",
      "Steps: 48917 Loss:1.2280 , Current Loss: 1.3099\n",
      "Steps: 49017 Loss:1.2145 , Current Loss: 1.1509\n",
      "Steps: 49117 Loss:1.2237 , Current Loss: 1.2367\n",
      "Steps: 49217 Loss:1.2122 , Current Loss: 1.2078\n",
      "Steps: 49317 Loss:1.2217 , Current Loss: 1.1881\n",
      "Steps: 49417 Loss:1.2199 , Current Loss: 1.1954\n",
      "Steps: 49517 Loss:1.2260 , Current Loss: 1.1849\n",
      "Epoch: 1 , Step: 49588 , loss: 1.2224 , Time: 12097.6\n",
      "Model Saved\n",
      "Steps: 49688 Loss:1.2229 , Current Loss: 1.2027\n",
      "Steps: 49788 Loss:1.2132 , Current Loss: 1.3340\n",
      "Steps: 49888 Loss:1.2134 , Current Loss: 1.2435\n",
      "Steps: 49988 Loss:1.2122 , Current Loss: 1.1160\n",
      "Steps: 50088 Loss:1.2107 , Current Loss: 1.1419\n",
      "Steps: 50188 Loss:1.2221 , Current Loss: 1.1929\n",
      "Steps: 50288 Loss:1.2182 , Current Loss: 1.1210\n",
      "Steps: 50388 Loss:1.2122 , Current Loss: 1.2644\n",
      "Steps: 50488 Loss:1.2256 , Current Loss: 1.1048\n",
      "Steps: 50588 Loss:1.2104 , Current Loss: 1.2871\n",
      "Steps: 50688 Loss:1.2179 , Current Loss: 1.2641\n",
      "Steps: 50788 Loss:1.2129 , Current Loss: 1.2797\n",
      "Steps: 50888 Loss:1.2179 , Current Loss: 1.2820\n",
      "Steps: 50988 Loss:1.2191 , Current Loss: 1.1344\n",
      "Steps: 51088 Loss:1.2114 , Current Loss: 1.1213\n",
      "Steps: 51188 Loss:1.2275 , Current Loss: 1.1330\n",
      "Steps: 51288 Loss:1.2114 , Current Loss: 1.2865\n",
      "Epoch: 2 , Step: 51359 , loss: 1.2168 , Time: 12034.0\n",
      "Model Saved\n",
      "Val: F1 Score:59.05 Accuracy:66.10  POS: F1 Score:87.51 Accuracy:94.64 Loss:1.2172 , Time: 2310.8\n",
      "Steps: 51459 Loss:1.2153 , Current Loss: 1.2058\n",
      "Steps: 51559 Loss:1.2046 , Current Loss: 1.2939\n",
      "Steps: 51659 Loss:1.2134 , Current Loss: 1.1706\n",
      "Steps: 51759 Loss:1.2073 , Current Loss: 1.3309\n",
      "Steps: 51859 Loss:1.2273 , Current Loss: 1.1872\n",
      "Steps: 51959 Loss:1.2173 , Current Loss: 1.2242\n",
      "Steps: 52059 Loss:1.2006 , Current Loss: 1.2469\n",
      "Steps: 52159 Loss:1.2125 , Current Loss: 1.1363\n",
      "Steps: 52259 Loss:1.2201 , Current Loss: 1.2746\n",
      "Steps: 52359 Loss:1.2110 , Current Loss: 1.1447\n",
      "Steps: 52459 Loss:1.1939 , Current Loss: 1.1860\n",
      "Steps: 52559 Loss:1.2226 , Current Loss: 1.1791\n",
      "Steps: 52659 Loss:1.2166 , Current Loss: 1.1930\n",
      "Steps: 52759 Loss:1.1998 , Current Loss: 1.2732\n",
      "Steps: 52859 Loss:1.2049 , Current Loss: 1.3179\n",
      "Steps: 52959 Loss:1.1982 , Current Loss: 1.1878\n",
      "Steps: 53059 Loss:1.2152 , Current Loss: 1.2229\n",
      "Epoch: 3 , Step: 53130 , loss: 1.2104 , Time: 12034.3\n",
      "Model Saved\n",
      "Steps: 53230 Loss:1.2158 , Current Loss: 1.1145\n",
      "Steps: 53330 Loss:1.2083 , Current Loss: 1.2298\n",
      "Steps: 53430 Loss:1.2033 , Current Loss: 1.0950\n",
      "Steps: 53530 Loss:1.2174 , Current Loss: 1.2679\n",
      "Steps: 53630 Loss:1.2071 , Current Loss: 1.1591\n",
      "Steps: 53730 Loss:1.2060 , Current Loss: 1.2052\n",
      "Steps: 53830 Loss:1.2080 , Current Loss: 1.1701\n",
      "Steps: 53930 Loss:1.2103 , Current Loss: 1.2986\n",
      "Steps: 54030 Loss:1.2122 , Current Loss: 1.2960\n",
      "Steps: 54130 Loss:1.2001 , Current Loss: 1.1529\n",
      "Steps: 54230 Loss:1.2072 , Current Loss: 1.1976\n",
      "Steps: 54330 Loss:1.2058 , Current Loss: 1.2856\n",
      "Steps: 54430 Loss:1.1887 , Current Loss: 1.2427\n",
      "Steps: 54530 Loss:1.2012 , Current Loss: 1.2506\n",
      "Steps: 54630 Loss:1.2058 , Current Loss: 1.2669\n",
      "Steps: 54730 Loss:1.1867 , Current Loss: 1.1544\n",
      "Steps: 54830 Loss:1.2019 , Current Loss: 1.1921\n",
      "Epoch: 4 , Step: 54901 , loss: 1.2054 , Time: 12031.5\n",
      "Model Saved\n",
      "Val: F1 Score:59.59 Accuracy:66.34  POS: F1 Score:87.91 Accuracy:94.70 Loss:1.1995 , Time: 2313.1\n",
      "Steps: 55001 Loss:1.1982 , Current Loss: 1.3022\n",
      "Steps: 55101 Loss:1.2087 , Current Loss: 1.1629\n",
      "Steps: 55201 Loss:1.2085 , Current Loss: 1.1833\n",
      "Steps: 55301 Loss:1.1956 , Current Loss: 1.2220\n",
      "Steps: 55401 Loss:1.2072 , Current Loss: 1.3348\n",
      "Steps: 55501 Loss:1.1989 , Current Loss: 1.1318\n",
      "Steps: 55601 Loss:1.2144 , Current Loss: 1.1205\n",
      "Steps: 55701 Loss:1.2073 , Current Loss: 1.1766\n",
      "Steps: 55801 Loss:1.1983 , Current Loss: 1.2636\n",
      "Steps: 55901 Loss:1.2016 , Current Loss: 1.1741\n",
      "Steps: 56001 Loss:1.1944 , Current Loss: 1.1841\n",
      "Steps: 56101 Loss:1.1871 , Current Loss: 1.1208\n",
      "Steps: 56201 Loss:1.1977 , Current Loss: 1.0495\n",
      "Steps: 56301 Loss:1.2039 , Current Loss: 1.1873\n",
      "Steps: 56401 Loss:1.1947 , Current Loss: 1.1202\n",
      "Steps: 56501 Loss:1.2061 , Current Loss: 1.1688\n",
      "Steps: 56601 Loss:1.2008 , Current Loss: 1.4690\n",
      "Epoch: 5 , Step: 56672 , loss: 1.2012 , Time: 12030.3\n",
      "Model Saved\n",
      "Steps: 56772 Loss:1.1939 , Current Loss: 1.1393\n",
      "Steps: 56872 Loss:1.1983 , Current Loss: 1.2971\n",
      "Steps: 56972 Loss:1.1999 , Current Loss: 1.1254\n",
      "Steps: 57072 Loss:1.1914 , Current Loss: 1.1766\n",
      "Steps: 57172 Loss:1.1874 , Current Loss: 1.0569\n",
      "Steps: 57272 Loss:1.2067 , Current Loss: 1.2657\n",
      "Steps: 57372 Loss:1.2055 , Current Loss: 1.1992\n",
      "Steps: 57472 Loss:1.1874 , Current Loss: 1.1948\n",
      "Steps: 57572 Loss:1.1855 , Current Loss: 1.1776\n",
      "Steps: 57672 Loss:1.2109 , Current Loss: 1.2349\n",
      "Steps: 57772 Loss:1.1919 , Current Loss: 1.1588\n",
      "Steps: 57872 Loss:1.1916 , Current Loss: 1.1031\n",
      "Steps: 57972 Loss:1.1907 , Current Loss: 1.1185\n",
      "Steps: 58072 Loss:1.1962 , Current Loss: 1.2748\n",
      "Steps: 58172 Loss:1.1907 , Current Loss: 1.2548\n",
      "Steps: 58272 Loss:1.1953 , Current Loss: 1.2977\n",
      "Steps: 58372 Loss:1.1942 , Current Loss: 1.2950\n",
      "Epoch: 6 , Step: 58443 , loss: 1.1954 , Time: 12034.9\n",
      "Model Saved\n",
      "Val: F1 Score:59.92 Accuracy:66.66  POS: F1 Score:87.65 Accuracy:94.76 Loss:1.2000 , Time: 2326.9\n",
      "Steps: 58543 Loss:1.2008 , Current Loss: 1.1363\n",
      "Steps: 58643 Loss:1.1880 , Current Loss: 1.1909\n",
      "Steps: 58743 Loss:1.1955 , Current Loss: 1.1949\n",
      "Steps: 58843 Loss:1.1807 , Current Loss: 1.0742\n",
      "Steps: 58943 Loss:1.1828 , Current Loss: 1.1145\n",
      "Steps: 59043 Loss:1.2027 , Current Loss: 1.2553\n",
      "Steps: 59143 Loss:1.1857 , Current Loss: 1.1585\n",
      "Steps: 59243 Loss:1.1956 , Current Loss: 1.0551\n",
      "Steps: 59343 Loss:1.2143 , Current Loss: 1.2862\n",
      "Steps: 59443 Loss:1.1823 , Current Loss: 1.2041\n",
      "Steps: 59543 Loss:1.1834 , Current Loss: 1.1381\n",
      "Steps: 59643 Loss:1.1943 , Current Loss: 1.1157\n",
      "Steps: 59743 Loss:1.1886 , Current Loss: 1.2567\n",
      "Steps: 59843 Loss:1.1905 , Current Loss: 1.1385\n",
      "Steps: 59943 Loss:1.1823 , Current Loss: 1.1034\n",
      "Steps: 60043 Loss:1.1849 , Current Loss: 1.1313\n",
      "Steps: 60143 Loss:1.1785 , Current Loss: 1.0950\n",
      "Epoch: 7 , Step: 60214 , loss: 1.1896 , Time: 12119.3\n",
      "Model Saved\n",
      "Steps: 60314 Loss:1.1908 , Current Loss: 1.2184\n",
      "Steps: 60414 Loss:1.1971 , Current Loss: 1.2357\n",
      "Steps: 60514 Loss:1.1836 , Current Loss: 1.1149\n",
      "Steps: 60614 Loss:1.1822 , Current Loss: 1.2604\n",
      "Steps: 60714 Loss:1.1811 , Current Loss: 1.1507\n",
      "Steps: 60814 Loss:1.1924 , Current Loss: 1.2366\n",
      "Steps: 60914 Loss:1.1844 , Current Loss: 1.1940\n",
      "Steps: 61014 Loss:1.1829 , Current Loss: 1.2294\n",
      "Steps: 61114 Loss:1.1790 , Current Loss: 1.2325\n",
      "Steps: 61214 Loss:1.1801 , Current Loss: 1.0904\n",
      "Steps: 61314 Loss:1.1895 , Current Loss: 1.2809\n",
      "Steps: 61414 Loss:1.1878 , Current Loss: 1.3664\n",
      "Steps: 61514 Loss:1.1919 , Current Loss: 1.1519\n",
      "Steps: 61614 Loss:1.1751 , Current Loss: 1.2594\n",
      "Steps: 61714 Loss:1.1895 , Current Loss: 1.1641\n",
      "Steps: 61814 Loss:1.1845 , Current Loss: 1.2763\n",
      "Steps: 61914 Loss:1.1938 , Current Loss: 1.2985\n",
      "Epoch: 8 , Step: 61985 , loss: 1.1858 , Time: 12113.0\n",
      "Model Saved\n",
      "Val: F1 Score:60.30 Accuracy:66.84  POS: F1 Score:88.24 Accuracy:94.76 Loss:1.1910 , Time: 2381.2\n",
      "Steps: 62085 Loss:1.1760 , Current Loss: 1.1094\n",
      "Steps: 62185 Loss:1.1718 , Current Loss: 1.1880\n",
      "Steps: 62285 Loss:1.1855 , Current Loss: 1.1088\n",
      "Steps: 62385 Loss:1.1818 , Current Loss: 1.0803\n",
      "Steps: 62485 Loss:1.1706 , Current Loss: 1.1832\n",
      "Steps: 62585 Loss:1.1733 , Current Loss: 1.1452\n",
      "Steps: 62685 Loss:1.1756 , Current Loss: 1.1493\n",
      "Steps: 62785 Loss:1.1817 , Current Loss: 1.1672\n",
      "Steps: 62885 Loss:1.1734 , Current Loss: 1.1555\n",
      "Steps: 62985 Loss:1.1908 , Current Loss: 1.0638\n",
      "Steps: 63085 Loss:1.1602 , Current Loss: 1.1258\n",
      "Steps: 63185 Loss:1.1803 , Current Loss: 1.1299\n",
      "Steps: 63285 Loss:1.1823 , Current Loss: 1.0118\n",
      "Steps: 63385 Loss:1.1863 , Current Loss: 1.0726\n",
      "Steps: 63485 Loss:1.1793 , Current Loss: 1.1041\n",
      "Steps: 63585 Loss:1.1857 , Current Loss: 1.1041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 63685 Loss:1.1847 , Current Loss: 1.1713\n",
      "Epoch: 9 , Step: 63756 , loss: 1.1787 , Time: 12027.2\n",
      "Model Saved\n",
      "Steps: 63856 Loss:1.1841 , Current Loss: 1.2301\n",
      "Steps: 63956 Loss:1.1785 , Current Loss: 1.1185\n",
      "Steps: 64056 Loss:1.1725 , Current Loss: 1.1739\n",
      "Steps: 64156 Loss:1.1750 , Current Loss: 1.1440\n",
      "Steps: 64256 Loss:1.1876 , Current Loss: 1.0063\n",
      "Steps: 64356 Loss:1.1794 , Current Loss: 1.1291\n",
      "Steps: 64456 Loss:1.1763 , Current Loss: 1.1623\n",
      "Steps: 64556 Loss:1.1606 , Current Loss: 1.2084\n",
      "Steps: 64656 Loss:1.1800 , Current Loss: 1.2594\n",
      "Steps: 64756 Loss:1.1724 , Current Loss: 1.1070\n",
      "Steps: 64856 Loss:1.1818 , Current Loss: 1.1023\n",
      "Steps: 64956 Loss:1.1778 , Current Loss: 1.2261\n",
      "Steps: 65056 Loss:1.1746 , Current Loss: 1.2680\n",
      "Steps: 65156 Loss:1.1798 , Current Loss: 1.0067\n",
      "Steps: 65256 Loss:1.1757 , Current Loss: 1.2404\n",
      "Steps: 65356 Loss:1.1727 , Current Loss: 1.0650\n",
      "Steps: 65456 Loss:1.1790 , Current Loss: 1.1002\n",
      "Epoch: 10 , Step: 65527 , loss: 1.1771 , Time: 12191.0\n",
      "Model Saved\n",
      "Val: F1 Score:60.43 Accuracy:67.07  POS: F1 Score:88.30 Accuracy:94.81 Loss:1.1861 , Time: 2304.8\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 65627 Loss:1.1703 , Current Loss: 1.1085\n",
      "Steps: 65727 Loss:1.1604 , Current Loss: 1.1488\n",
      "Steps: 65827 Loss:1.1683 , Current Loss: 1.1013\n",
      "Steps: 65927 Loss:1.1680 , Current Loss: 1.0642\n",
      "Steps: 66027 Loss:1.1671 , Current Loss: 1.0811\n",
      "Steps: 66127 Loss:1.1771 , Current Loss: 1.1177\n",
      "Steps: 66227 Loss:1.1537 , Current Loss: 1.0352\n",
      "Steps: 66327 Loss:1.1576 , Current Loss: 1.0934\n",
      "Steps: 66427 Loss:1.1563 , Current Loss: 1.0517\n",
      "Steps: 66527 Loss:1.1571 , Current Loss: 1.0961\n",
      "Steps: 66627 Loss:1.1623 , Current Loss: 1.1719\n",
      "Steps: 66727 Loss:1.1747 , Current Loss: 1.1284\n",
      "Steps: 66827 Loss:1.1650 , Current Loss: 1.0858\n",
      "Steps: 66927 Loss:1.1776 , Current Loss: 1.0603\n",
      "Steps: 67027 Loss:1.1716 , Current Loss: 1.1617\n",
      "Steps: 67127 Loss:1.1589 , Current Loss: 1.1192\n",
      "Steps: 67227 Loss:1.1554 , Current Loss: 1.1729\n",
      "Epoch: 1 , Step: 67298 , loss: 1.1650 , Time: 13973.3\n",
      "Model Saved\n",
      "Steps: 67398 Loss:1.1583 , Current Loss: 1.1437\n",
      "Steps: 67498 Loss:1.1657 , Current Loss: 1.0963\n",
      "Steps: 67598 Loss:1.1523 , Current Loss: 1.1193\n",
      "Steps: 67698 Loss:1.1605 , Current Loss: 1.1264\n",
      "Steps: 67798 Loss:1.1638 , Current Loss: 1.2133\n",
      "Steps: 67898 Loss:1.1582 , Current Loss: 1.1803\n",
      "Steps: 67998 Loss:1.1840 , Current Loss: 1.2525\n",
      "Steps: 68098 Loss:1.1670 , Current Loss: 1.2061\n",
      "Steps: 68198 Loss:1.1574 , Current Loss: 1.1446\n",
      "Steps: 68298 Loss:1.1601 , Current Loss: 1.1481\n",
      "Steps: 68398 Loss:1.1541 , Current Loss: 1.1019\n",
      "Steps: 68498 Loss:1.1545 , Current Loss: 1.1153\n",
      "Steps: 68598 Loss:1.1568 , Current Loss: 1.1748\n",
      "Steps: 68698 Loss:1.1666 , Current Loss: 1.2723\n",
      "Steps: 68798 Loss:1.1543 , Current Loss: 0.9969\n",
      "Steps: 68898 Loss:1.1518 , Current Loss: 1.1131\n",
      "Steps: 68998 Loss:1.1498 , Current Loss: 1.2111\n",
      "Epoch: 2 , Step: 69069 , loss: 1.1592 , Time: 11999.4\n",
      "Model Saved\n",
      "Val: F1 Score:61.37 Accuracy:67.46  POS: F1 Score:88.50 Accuracy:94.90 Loss:1.1751 , Time: 2297.2\n",
      "Steps: 69169 Loss:1.1466 , Current Loss: 1.1287\n",
      "Steps: 69269 Loss:1.1590 , Current Loss: 1.1169\n",
      "Steps: 69369 Loss:1.1479 , Current Loss: 1.0934\n",
      "Steps: 69469 Loss:1.1593 , Current Loss: 1.2011\n",
      "Steps: 69569 Loss:1.1394 , Current Loss: 1.1939\n",
      "Steps: 69669 Loss:1.1518 , Current Loss: 1.1022\n",
      "Steps: 69769 Loss:1.1432 , Current Loss: 1.1426\n",
      "Steps: 69869 Loss:1.1472 , Current Loss: 1.1307\n",
      "Steps: 69969 Loss:1.1499 , Current Loss: 1.1165\n",
      "Steps: 70069 Loss:1.1651 , Current Loss: 1.1859\n",
      "Steps: 70169 Loss:1.1499 , Current Loss: 1.1106\n",
      "Steps: 70269 Loss:1.1425 , Current Loss: 1.2055\n",
      "Steps: 70369 Loss:1.1474 , Current Loss: 1.0995\n",
      "Steps: 70469 Loss:1.1524 , Current Loss: 1.1677\n",
      "Steps: 70569 Loss:1.1585 , Current Loss: 1.2151\n",
      "Steps: 70669 Loss:1.1452 , Current Loss: 1.1255\n",
      "Steps: 70769 Loss:1.1563 , Current Loss: 1.0327\n",
      "Epoch: 3 , Step: 70840 , loss: 1.1507 , Time: 11983.0\n",
      "Model Saved\n",
      "Steps: 70940 Loss:1.1461 , Current Loss: 1.1205\n",
      "Steps: 71040 Loss:1.1501 , Current Loss: 1.1418\n",
      "Steps: 71140 Loss:1.1374 , Current Loss: 1.1550\n",
      "Steps: 71240 Loss:1.1484 , Current Loss: 1.2428\n",
      "Steps: 71340 Loss:1.1571 , Current Loss: 1.1085\n",
      "Steps: 71440 Loss:1.1525 , Current Loss: 1.1511\n",
      "Steps: 71540 Loss:1.1448 , Current Loss: 1.2692\n",
      "Steps: 71640 Loss:1.1563 , Current Loss: 1.3151\n",
      "Steps: 71740 Loss:1.1529 , Current Loss: 1.0869\n",
      "Steps: 71840 Loss:1.1538 , Current Loss: 1.2934\n",
      "Steps: 71940 Loss:1.1613 , Current Loss: 1.0273\n",
      "Steps: 72040 Loss:1.1371 , Current Loss: 1.0060\n",
      "Steps: 72140 Loss:1.1498 , Current Loss: 1.0696\n",
      "Steps: 72240 Loss:1.1372 , Current Loss: 1.0877\n",
      "Steps: 72340 Loss:1.1461 , Current Loss: 1.1230\n",
      "Steps: 72440 Loss:1.1630 , Current Loss: 1.1971\n",
      "Steps: 72540 Loss:1.1449 , Current Loss: 1.1332\n",
      "Epoch: 4 , Step: 72611 , loss: 1.1494 , Time: 12128.1\n",
      "Model Saved\n",
      "Val: F1 Score:61.48 Accuracy:67.64  POS: F1 Score:88.41 Accuracy:94.91 Loss:1.1608 , Time: 2311.1\n",
      "Steps: 72711 Loss:1.1436 , Current Loss: 1.2174\n",
      "Steps: 72811 Loss:1.1378 , Current Loss: 1.2620\n",
      "Steps: 72911 Loss:1.1461 , Current Loss: 1.1570\n",
      "Steps: 73011 Loss:1.1467 , Current Loss: 1.1363\n",
      "Steps: 73111 Loss:1.1458 , Current Loss: 1.2011\n",
      "Steps: 73211 Loss:1.1558 , Current Loss: 1.2856\n",
      "Steps: 73311 Loss:1.1462 , Current Loss: 1.2212\n",
      "Steps: 73411 Loss:1.1526 , Current Loss: 1.0122\n",
      "Steps: 73511 Loss:1.1430 , Current Loss: 1.1222\n",
      "Steps: 73611 Loss:1.1582 , Current Loss: 1.1402\n",
      "Steps: 73711 Loss:1.1424 , Current Loss: 1.1050\n",
      "Steps: 73811 Loss:1.1289 , Current Loss: 0.9752\n",
      "Steps: 73911 Loss:1.1480 , Current Loss: 1.1313\n",
      "Steps: 74011 Loss:1.1425 , Current Loss: 1.0660\n",
      "Steps: 74111 Loss:1.1357 , Current Loss: 1.1975\n",
      "Steps: 74211 Loss:1.1401 , Current Loss: 1.2475\n",
      "Steps: 74311 Loss:1.1409 , Current Loss: 1.1845\n",
      "Epoch: 5 , Step: 74382 , loss: 1.1445 , Time: 12119.8\n",
      "Model Saved\n",
      "Steps: 74482 Loss:1.1400 , Current Loss: 1.0542\n",
      "Steps: 74582 Loss:1.1383 , Current Loss: 1.1359\n",
      "Steps: 74682 Loss:1.1487 , Current Loss: 1.2241\n",
      "Steps: 74782 Loss:1.1544 , Current Loss: 1.1494\n",
      "Steps: 74882 Loss:1.1450 , Current Loss: 1.1502\n",
      "Steps: 74982 Loss:1.1432 , Current Loss: 1.0712\n",
      "Steps: 75082 Loss:1.1441 , Current Loss: 1.1051\n",
      "Steps: 75182 Loss:1.1426 , Current Loss: 1.1488\n",
      "Steps: 75282 Loss:1.1589 , Current Loss: 1.1678\n",
      "Steps: 75382 Loss:1.1373 , Current Loss: 1.2968\n",
      "Steps: 75482 Loss:1.1429 , Current Loss: 1.2538\n",
      "Steps: 75582 Loss:1.1449 , Current Loss: 1.1551\n",
      "Steps: 75682 Loss:1.1439 , Current Loss: 1.0695\n",
      "Steps: 75782 Loss:1.1352 , Current Loss: 1.3132\n",
      "Steps: 75882 Loss:1.1325 , Current Loss: 1.1960\n",
      "Steps: 75982 Loss:1.1383 , Current Loss: 1.0751\n",
      "Steps: 76082 Loss:1.1378 , Current Loss: 1.2350\n",
      "Epoch: 6 , Step: 76153 , loss: 1.1420 , Time: 12074.0\n",
      "Model Saved\n",
      "Val: F1 Score:61.81 Accuracy:67.84  POS: F1 Score:88.51 Accuracy:94.96 Loss:1.1559 , Time: 2298.6\n",
      "Steps: 76253 Loss:1.1354 , Current Loss: 1.1851\n",
      "Steps: 76353 Loss:1.1279 , Current Loss: 1.1747\n",
      "Steps: 76453 Loss:1.1469 , Current Loss: 1.1099\n",
      "Steps: 76553 Loss:1.1387 , Current Loss: 1.1246\n",
      "Steps: 76653 Loss:1.1407 , Current Loss: 1.1757\n",
      "Steps: 76753 Loss:1.1464 , Current Loss: 0.9818\n",
      "Steps: 76853 Loss:1.1487 , Current Loss: 1.0867\n",
      "Steps: 76953 Loss:1.1426 , Current Loss: 1.2108\n",
      "Steps: 77053 Loss:1.1392 , Current Loss: 1.0913\n",
      "Steps: 77153 Loss:1.1471 , Current Loss: 1.2232\n",
      "Steps: 77253 Loss:1.1213 , Current Loss: 1.0839\n",
      "Steps: 77353 Loss:1.1416 , Current Loss: 1.0745\n",
      "Steps: 77453 Loss:1.1352 , Current Loss: 1.0974\n",
      "Steps: 77553 Loss:1.1388 , Current Loss: 1.1957\n",
      "Steps: 77653 Loss:1.1321 , Current Loss: 1.1405\n",
      "Steps: 77753 Loss:1.1406 , Current Loss: 1.0651\n",
      "Steps: 77853 Loss:1.1211 , Current Loss: 1.0964\n",
      "Epoch: 7 , Step: 77924 , loss: 1.1377 , Time: 11965.9\n",
      "Model Saved\n",
      "Steps: 78024 Loss:1.1411 , Current Loss: 1.1067\n",
      "Steps: 78124 Loss:1.1441 , Current Loss: 1.1597\n",
      "Steps: 78224 Loss:1.1262 , Current Loss: 1.0886\n",
      "Steps: 78324 Loss:1.1348 , Current Loss: 1.0903\n",
      "Steps: 78424 Loss:1.1277 , Current Loss: 1.1493\n",
      "Steps: 78524 Loss:1.1245 , Current Loss: 1.1299\n",
      "Steps: 78624 Loss:1.1260 , Current Loss: 1.0471\n",
      "Steps: 78724 Loss:1.1345 , Current Loss: 1.0329\n",
      "Steps: 78824 Loss:1.1330 , Current Loss: 1.2116\n",
      "Steps: 78924 Loss:1.1547 , Current Loss: 1.1071\n",
      "Steps: 79024 Loss:1.1437 , Current Loss: 1.2122\n",
      "Steps: 79124 Loss:1.1259 , Current Loss: 1.2034\n",
      "Steps: 79224 Loss:1.1332 , Current Loss: 1.0789\n",
      "Steps: 79324 Loss:1.1487 , Current Loss: 1.2230\n",
      "Steps: 79424 Loss:1.1357 , Current Loss: 1.1988\n",
      "Steps: 79524 Loss:1.1304 , Current Loss: 1.2322\n",
      "Steps: 79624 Loss:1.1300 , Current Loss: 1.2196\n",
      "Epoch: 8 , Step: 79695 , loss: 1.1348 , Time: 11999.8\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 79795 Loss:1.1200 , Current Loss: 1.0775\n",
      "Steps: 79895 Loss:1.1391 , Current Loss: 1.1888\n",
      "Steps: 79995 Loss:1.1328 , Current Loss: 1.0539\n",
      "Steps: 80095 Loss:1.1264 , Current Loss: 0.9866\n",
      "Steps: 80195 Loss:1.1351 , Current Loss: 1.0090\n",
      "Steps: 80295 Loss:1.1246 , Current Loss: 1.0305\n",
      "Steps: 80395 Loss:1.1291 , Current Loss: 1.0609\n",
      "Steps: 80495 Loss:1.1463 , Current Loss: 1.1761\n",
      "Steps: 80595 Loss:1.1315 , Current Loss: 1.0556\n",
      "Steps: 80695 Loss:1.1244 , Current Loss: 1.2292\n",
      "Steps: 80795 Loss:1.1337 , Current Loss: 1.1745\n",
      "Steps: 80895 Loss:1.1125 , Current Loss: 0.9700\n",
      "Steps: 80995 Loss:1.1326 , Current Loss: 1.0512\n",
      "Steps: 81095 Loss:1.1278 , Current Loss: 1.1351\n",
      "Steps: 81195 Loss:1.1307 , Current Loss: 1.1419\n",
      "Steps: 81295 Loss:1.1348 , Current Loss: 1.2109\n",
      "Steps: 81395 Loss:1.1157 , Current Loss: 1.0764\n",
      "Epoch: 1 , Step: 81466 , loss: 1.1287 , Time: 12084.3\n",
      "Model Saved\n",
      "Steps: 81566 Loss:1.1190 , Current Loss: 1.1888\n",
      "Steps: 81666 Loss:1.1354 , Current Loss: 1.2150\n",
      "Steps: 81766 Loss:1.1276 , Current Loss: 1.1866\n",
      "Steps: 81866 Loss:1.1270 , Current Loss: 1.0741\n",
      "Steps: 81966 Loss:1.1217 , Current Loss: 1.2104\n",
      "Steps: 82066 Loss:1.1297 , Current Loss: 1.1195\n",
      "Steps: 82166 Loss:1.1207 , Current Loss: 1.1542\n",
      "Steps: 82266 Loss:1.1417 , Current Loss: 1.1439\n",
      "Steps: 82366 Loss:1.1317 , Current Loss: 1.0215\n",
      "Steps: 82466 Loss:1.1151 , Current Loss: 1.0085\n",
      "Steps: 82566 Loss:1.1232 , Current Loss: 1.2101\n",
      "Steps: 82666 Loss:1.1229 , Current Loss: 1.1944\n",
      "Steps: 82766 Loss:1.1278 , Current Loss: 1.1952\n",
      "Steps: 82866 Loss:1.1132 , Current Loss: 1.0703\n",
      "Steps: 82966 Loss:1.1271 , Current Loss: 1.2530\n",
      "Steps: 83066 Loss:1.1210 , Current Loss: 1.2639\n",
      "Steps: 83166 Loss:1.1248 , Current Loss: 1.1813\n",
      "Epoch: 2 , Step: 83237 , loss: 1.1253 , Time: 11984.7\n",
      "Model Saved\n",
      "Val: F1 Score:62.32 Accuracy:68.26  POS: F1 Score:88.83 Accuracy:95.01 Loss:1.1343 , Time: 2306.9\n",
      "Steps: 83337 Loss:1.1281 , Current Loss: 1.1242\n",
      "Steps: 83437 Loss:1.1234 , Current Loss: 1.2323\n",
      "Steps: 83537 Loss:1.1207 , Current Loss: 1.0365\n",
      "Steps: 83637 Loss:1.1233 , Current Loss: 1.0533\n",
      "Steps: 83737 Loss:1.1268 , Current Loss: 0.9340\n",
      "Steps: 83837 Loss:1.1160 , Current Loss: 1.1566\n",
      "Steps: 83937 Loss:1.1109 , Current Loss: 1.1362\n",
      "Steps: 84037 Loss:1.1246 , Current Loss: 1.1615\n",
      "Steps: 84137 Loss:1.1250 , Current Loss: 1.0639\n",
      "Steps: 84237 Loss:1.1287 , Current Loss: 1.0993\n",
      "Steps: 84337 Loss:1.1224 , Current Loss: 1.0805\n",
      "Steps: 84437 Loss:1.1190 , Current Loss: 1.0319\n",
      "Steps: 84537 Loss:1.1127 , Current Loss: 1.1625\n",
      "Steps: 84637 Loss:1.1391 , Current Loss: 1.0621\n",
      "Steps: 84737 Loss:1.1051 , Current Loss: 1.1064\n",
      "Steps: 84837 Loss:1.1117 , Current Loss: 1.0364\n",
      "Steps: 84937 Loss:1.1105 , Current Loss: 1.1350\n",
      "Epoch: 3 , Step: 85008 , loss: 1.1202 , Time: 12131.5\n",
      "Model Saved\n",
      "Steps: 85108 Loss:1.1144 , Current Loss: 1.1572\n",
      "Steps: 85208 Loss:1.1277 , Current Loss: 1.0594\n",
      "Steps: 85308 Loss:1.1091 , Current Loss: 1.1085\n",
      "Steps: 85408 Loss:1.1139 , Current Loss: 1.1011\n",
      "Steps: 85508 Loss:1.1089 , Current Loss: 1.0685\n",
      "Steps: 85608 Loss:1.1236 , Current Loss: 1.0823\n",
      "Steps: 85708 Loss:1.1156 , Current Loss: 1.0752\n",
      "Steps: 85808 Loss:1.1248 , Current Loss: 1.1203\n",
      "Steps: 85908 Loss:1.1244 , Current Loss: 1.1858\n",
      "Steps: 86008 Loss:1.1146 , Current Loss: 1.0006\n",
      "Steps: 86108 Loss:1.1337 , Current Loss: 1.1508\n",
      "Steps: 86208 Loss:1.1159 , Current Loss: 1.2222\n",
      "Steps: 86308 Loss:1.1215 , Current Loss: 1.0526\n",
      "Steps: 86408 Loss:1.1170 , Current Loss: 1.0331\n",
      "Steps: 86508 Loss:1.1090 , Current Loss: 1.0825\n",
      "Steps: 86608 Loss:1.1117 , Current Loss: 1.0568\n",
      "Steps: 86708 Loss:1.1148 , Current Loss: 1.0641\n",
      "Epoch: 4 , Step: 86779 , loss: 1.1176 , Time: 12132.1\n",
      "Model Saved\n",
      "Val: F1 Score:62.22 Accuracy:68.34  POS: F1 Score:88.68 Accuracy:95.01 Loss:1.1328 , Time: 2311.9\n",
      "Steps: 86879 Loss:1.1084 , Current Loss: 1.0763\n",
      "Steps: 86979 Loss:1.1114 , Current Loss: 1.0989\n",
      "Steps: 87079 Loss:1.1027 , Current Loss: 1.1079\n",
      "Steps: 87179 Loss:1.1183 , Current Loss: 1.0956\n",
      "Steps: 87279 Loss:1.1045 , Current Loss: 1.1038\n",
      "Steps: 87379 Loss:1.1269 , Current Loss: 1.2944\n",
      "Steps: 87479 Loss:1.1138 , Current Loss: 1.0372\n",
      "Steps: 87579 Loss:1.1086 , Current Loss: 1.1248\n",
      "Steps: 87679 Loss:1.1259 , Current Loss: 1.0687\n",
      "Steps: 87779 Loss:1.1252 , Current Loss: 1.1948\n",
      "Steps: 87879 Loss:1.1227 , Current Loss: 1.0520\n",
      "Steps: 87979 Loss:1.1144 , Current Loss: 1.0320\n",
      "Steps: 88079 Loss:1.1074 , Current Loss: 1.1047\n",
      "Steps: 88179 Loss:1.1222 , Current Loss: 1.0282\n",
      "Steps: 88279 Loss:1.1121 , Current Loss: 1.0220\n",
      "Steps: 88379 Loss:1.1076 , Current Loss: 1.1078\n",
      "Steps: 88479 Loss:1.1205 , Current Loss: 1.0303\n",
      "Epoch: 5 , Step: 88550 , loss: 1.1144 , Time: 12132.2\n",
      "Model Saved\n",
      "Steps: 88650 Loss:1.0959 , Current Loss: 0.9528\n",
      "Steps: 88750 Loss:1.1125 , Current Loss: 1.1620\n",
      "Steps: 88850 Loss:1.1187 , Current Loss: 1.0518\n",
      "Steps: 88950 Loss:1.1095 , Current Loss: 1.2323\n",
      "Steps: 89050 Loss:1.1126 , Current Loss: 1.0796\n",
      "Steps: 89150 Loss:1.1147 , Current Loss: 1.1726\n",
      "Steps: 89250 Loss:1.1137 , Current Loss: 1.1077\n",
      "Steps: 89350 Loss:1.1001 , Current Loss: 1.1073\n",
      "Steps: 89450 Loss:1.1206 , Current Loss: 1.0670\n",
      "Steps: 89550 Loss:1.1142 , Current Loss: 1.1473\n",
      "Steps: 89650 Loss:1.1127 , Current Loss: 1.0307\n",
      "Steps: 89750 Loss:1.1100 , Current Loss: 1.1423\n",
      "Steps: 89850 Loss:1.1150 , Current Loss: 1.2007\n",
      "Steps: 89950 Loss:1.1070 , Current Loss: 1.1971\n",
      "Steps: 90050 Loss:1.1131 , Current Loss: 1.1996\n",
      "Steps: 90150 Loss:1.1195 , Current Loss: 1.0545\n",
      "Steps: 90250 Loss:1.1199 , Current Loss: 1.0971\n",
      "Epoch: 6 , Step: 90321 , loss: 1.1122 , Time: 12141.5\n",
      "Model Saved\n",
      "Val: F1 Score:62.27 Accuracy:68.44  POS: F1 Score:88.74 Accuracy:95.07 Loss:1.1209 , Time: 2340.2\n",
      "Steps: 90421 Loss:1.0972 , Current Loss: 0.9858\n",
      "Steps: 90521 Loss:1.1130 , Current Loss: 1.3441\n",
      "Steps: 90621 Loss:1.1055 , Current Loss: 1.1023\n",
      "Steps: 90721 Loss:1.1173 , Current Loss: 1.1505\n",
      "Steps: 90821 Loss:1.1048 , Current Loss: 1.1211\n",
      "Steps: 90921 Loss:1.1064 , Current Loss: 1.1080\n",
      "Steps: 91021 Loss:1.1055 , Current Loss: 1.1316\n",
      "Steps: 91121 Loss:1.1138 , Current Loss: 1.1094\n",
      "Steps: 91221 Loss:1.1143 , Current Loss: 1.0121\n",
      "Steps: 91321 Loss:1.1024 , Current Loss: 1.1583\n",
      "Steps: 91421 Loss:1.1137 , Current Loss: 1.1096\n",
      "Steps: 91521 Loss:1.1073 , Current Loss: 1.0846\n",
      "Steps: 91621 Loss:1.1041 , Current Loss: 1.1824\n",
      "Steps: 91721 Loss:1.1057 , Current Loss: 0.9623\n",
      "Steps: 91821 Loss:1.1092 , Current Loss: 1.0697\n",
      "Steps: 91921 Loss:1.1171 , Current Loss: 1.0389\n",
      "Steps: 92021 Loss:1.1065 , Current Loss: 1.0131\n",
      "Epoch: 7 , Step: 92092 , loss: 1.1080 , Time: 12148.3\n",
      "Model Saved\n",
      "Steps: 92192 Loss:1.0999 , Current Loss: 1.0941\n",
      "Steps: 92292 Loss:1.1008 , Current Loss: 1.0998\n",
      "Steps: 92392 Loss:1.1061 , Current Loss: 1.2422\n",
      "Steps: 92492 Loss:1.1126 , Current Loss: 1.0666\n",
      "Steps: 92592 Loss:1.1090 , Current Loss: 1.0107\n",
      "Steps: 92692 Loss:1.1175 , Current Loss: 1.0885\n",
      "Steps: 92792 Loss:1.1038 , Current Loss: 1.0309\n",
      "Steps: 92892 Loss:1.1029 , Current Loss: 1.0843\n",
      "Steps: 92992 Loss:1.1059 , Current Loss: 1.1291\n",
      "Steps: 93092 Loss:1.1111 , Current Loss: 1.1208\n",
      "Steps: 93192 Loss:1.1147 , Current Loss: 1.1168\n",
      "Steps: 93292 Loss:1.1139 , Current Loss: 1.1549\n",
      "Steps: 93392 Loss:1.1058 , Current Loss: 1.1707\n",
      "Steps: 93492 Loss:1.1021 , Current Loss: 1.0707\n",
      "Steps: 93592 Loss:1.1115 , Current Loss: 1.1361\n",
      "Steps: 93692 Loss:1.1033 , Current Loss: 1.1537\n",
      "Steps: 93792 Loss:1.0988 , Current Loss: 1.0845\n",
      "Epoch: 8 , Step: 93863 , loss: 1.1072 , Time: 12141.6\n",
      "Model Saved\n",
      "Val: F1 Score:62.40 Accuracy:68.63  POS: F1 Score:88.96 Accuracy:95.08 Loss:1.1270 , Time: 2308.8\n",
      "Steps: 93963 Loss:1.0989 , Current Loss: 0.9469\n",
      "Steps: 94063 Loss:1.1146 , Current Loss: 1.1408\n",
      "Steps: 94163 Loss:1.1018 , Current Loss: 1.1721\n",
      "Steps: 94263 Loss:1.1112 , Current Loss: 1.1380\n",
      "Steps: 94363 Loss:1.0970 , Current Loss: 1.1495\n",
      "Steps: 94463 Loss:1.0870 , Current Loss: 1.0914\n",
      "Steps: 94563 Loss:1.1070 , Current Loss: 1.0385\n",
      "Steps: 94663 Loss:1.0980 , Current Loss: 1.0821\n",
      "Steps: 94763 Loss:1.1186 , Current Loss: 1.2489\n",
      "Steps: 94863 Loss:1.0916 , Current Loss: 1.1392\n",
      "Steps: 94963 Loss:1.1073 , Current Loss: 1.0886\n",
      "Steps: 95063 Loss:1.0974 , Current Loss: 1.0364\n",
      "Steps: 95163 Loss:1.1129 , Current Loss: 1.0733\n",
      "Steps: 95263 Loss:1.1014 , Current Loss: 1.1417\n",
      "Steps: 95363 Loss:1.0956 , Current Loss: 1.2146\n",
      "Steps: 95463 Loss:1.1061 , Current Loss: 1.1970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 95563 Loss:1.0942 , Current Loss: 1.2293\n",
      "Epoch: 9 , Step: 95634 , loss: 1.1024 , Time: 11893.5\n",
      "Model Saved\n",
      "Steps: 95734 Loss:1.0971 , Current Loss: 1.0361\n",
      "Steps: 95834 Loss:1.1053 , Current Loss: 1.1541\n",
      "Steps: 95934 Loss:1.1053 , Current Loss: 1.0345\n",
      "Steps: 96034 Loss:1.0991 , Current Loss: 1.0116\n",
      "Steps: 96134 Loss:1.1110 , Current Loss: 0.9568\n",
      "Steps: 96234 Loss:1.1031 , Current Loss: 1.1285\n",
      "Steps: 96334 Loss:1.0941 , Current Loss: 1.0702\n",
      "Steps: 96434 Loss:1.1066 , Current Loss: 1.0791\n",
      "Steps: 96534 Loss:1.1142 , Current Loss: 1.2335\n",
      "Steps: 96634 Loss:1.1107 , Current Loss: 1.0056\n",
      "Steps: 96734 Loss:1.1169 , Current Loss: 1.0617\n",
      "Steps: 96834 Loss:1.0999 , Current Loss: 1.0654\n",
      "Steps: 96934 Loss:1.0870 , Current Loss: 1.0647\n",
      "Steps: 97034 Loss:1.0903 , Current Loss: 1.1249\n",
      "Steps: 97134 Loss:1.0934 , Current Loss: 1.1321\n",
      "Steps: 97234 Loss:1.0999 , Current Loss: 1.2068\n",
      "Steps: 97334 Loss:1.0973 , Current Loss: 1.1554\n",
      "Epoch: 10 , Step: 97405 , loss: 1.1017 , Time: 11877.5\n",
      "Model Saved\n",
      "Val: F1 Score:62.96 Accuracy:68.92  POS: F1 Score:89.02 Accuracy:95.12 Loss:1.1130 , Time: 2277.6\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append(train_loss)\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:62.32 Accuracy:68.26  POS: F1 Score:88.83 Accuracy:95.01\n",
      "Val: F1 Score:62.22 Accuracy:68.34  POS: F1 Score:88.68 Accuracy:95.01\n",
      "Val: F1 Score:62.27 Accuracy:68.44  POS: F1 Score:88.74 Accuracy:95.07\n",
      "Val: F1 Score:62.40 Accuracy:68.63  POS: F1 Score:88.96 Accuracy:95.08\n",
      "Val: F1 Score:62.96 Accuracy:68.92  POS: F1 Score:89.02 Accuracy:95.12\n"
     ]
    }
   ],
   "source": [
    "for v in val_collection:\n",
    "    print(\"Val: F1 Score:{0:.2f}\".format(v[0]), \"Accuracy:{0:.2f}\".format(v[1]), \" POS: F1 Score:{0:.2f}\".format(v[2]), \"Accuracy:{0:.2f}\".format(v[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0952356004701504, 1.0919322563321627, 1.0906437579889474]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: F1 Score:62.59 Accuracy:68.87  POS: F1 Score:88.98 Accuracy:95.12\n"
     ]
    }
   ],
   "source": [
    "for v in val_collection:\n",
    "    print(\"Val: F1 Score:{0:.2f}\".format(v[0]), \"Accuracy:{0:.2f}\".format(v[1]), \" POS: F1 Score:{0:.2f}\".format(v[2]), \"Accuracy:{0:.2f}\".format(v[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 103519 Loss:1.0857 , Current Loss: 1.0955\n",
      "Steps: 103619 Loss:1.0943 , Current Loss: 1.0948\n",
      "Steps: 103719 Loss:1.0899 , Current Loss: 0.8898\n",
      "Steps: 103819 Loss:1.0767 , Current Loss: 1.0710\n",
      "Steps: 103919 Loss:1.1007 , Current Loss: 1.0328\n",
      "Steps: 104019 Loss:1.0872 , Current Loss: 1.0722\n",
      "Steps: 104119 Loss:1.0919 , Current Loss: 1.1361\n",
      "Steps: 104219 Loss:1.0784 , Current Loss: 1.0826\n",
      "Steps: 104319 Loss:1.0812 , Current Loss: 1.0122\n",
      "Steps: 104419 Loss:1.0748 , Current Loss: 1.1719\n",
      "Steps: 104519 Loss:1.0790 , Current Loss: 0.9485\n",
      "Steps: 104619 Loss:1.0880 , Current Loss: 1.0874\n",
      "Steps: 104719 Loss:1.0905 , Current Loss: 0.9633\n",
      "Steps: 104819 Loss:1.1007 , Current Loss: 1.0679\n",
      "Steps: 104919 Loss:1.0879 , Current Loss: 0.9702\n",
      "Steps: 105019 Loss:1.0761 , Current Loss: 1.1684\n",
      "Steps: 105119 Loss:1.0782 , Current Loss: 1.1231\n",
      "Epoch: 1 , Step: 105190 , loss: 1.0864 , Time: 16818.0\n",
      "Model Saved\n",
      "Steps: 105290 Loss:1.0955 , Current Loss: 1.2707\n",
      "Steps: 105390 Loss:1.0776 , Current Loss: 1.1164\n",
      "Steps: 105490 Loss:1.0880 , Current Loss: 1.1402\n",
      "Steps: 105590 Loss:1.0990 , Current Loss: 1.1127\n",
      "Steps: 105690 Loss:1.0863 , Current Loss: 1.1070\n",
      "Steps: 105790 Loss:1.0873 , Current Loss: 1.1109\n",
      "Steps: 105890 Loss:1.0850 , Current Loss: 1.0645\n",
      "Steps: 105990 Loss:1.0848 , Current Loss: 1.1022\n",
      "Steps: 106090 Loss:1.0799 , Current Loss: 1.1458\n",
      "Steps: 106190 Loss:1.0873 , Current Loss: 1.1537\n",
      "Steps: 106290 Loss:1.0678 , Current Loss: 1.1020\n",
      "Steps: 106390 Loss:1.0852 , Current Loss: 1.0384\n",
      "Steps: 106490 Loss:1.0926 , Current Loss: 1.1965\n",
      "Steps: 106590 Loss:1.0798 , Current Loss: 1.1091\n",
      "Steps: 106690 Loss:1.0803 , Current Loss: 1.0122\n",
      "Steps: 106790 Loss:1.0821 , Current Loss: 0.9924\n",
      "Steps: 106890 Loss:1.0897 , Current Loss: 1.1758\n",
      "Epoch: 2 , Step: 106961 , loss: 1.0849 , Time: 17498.0\n",
      "Model Saved\n",
      "Val: F1 Score:63.35 Accuracy:69.06  POS: F1 Score:89.04 Accuracy:95.15 Loss:1.1036 , Time: 3591.0\n",
      "Steps: 107061 Loss:1.0803 , Current Loss: 1.0628\n",
      "Steps: 107161 Loss:1.0804 , Current Loss: 1.0429\n",
      "Steps: 107261 Loss:1.0855 , Current Loss: 1.0522\n",
      "Steps: 107361 Loss:1.0806 , Current Loss: 1.1362\n",
      "Steps: 107461 Loss:1.0950 , Current Loss: 1.1235\n",
      "Steps: 107561 Loss:1.0970 , Current Loss: 1.0572\n",
      "Steps: 107661 Loss:1.0849 , Current Loss: 1.0435\n",
      "Steps: 107761 Loss:1.0932 , Current Loss: 1.0035\n",
      "Steps: 107861 Loss:1.0860 , Current Loss: 1.1905\n",
      "Steps: 107961 Loss:1.0818 , Current Loss: 1.1822\n",
      "Steps: 108061 Loss:1.0773 , Current Loss: 1.0738\n",
      "Steps: 108161 Loss:1.0788 , Current Loss: 1.0071\n",
      "Steps: 108261 Loss:1.0768 , Current Loss: 0.9683\n",
      "Steps: 108361 Loss:1.0946 , Current Loss: 1.1422\n",
      "Steps: 108461 Loss:1.0793 , Current Loss: 0.9957\n",
      "Steps: 108561 Loss:1.0853 , Current Loss: 1.0233\n",
      "Steps: 108661 Loss:1.0798 , Current Loss: 1.1163\n",
      "Epoch: 3 , Step: 108732 , loss: 1.0837 , Time: 17743.1\n",
      "Model Saved\n",
      "Steps: 108832 Loss:1.0830 , Current Loss: 1.1295\n",
      "Steps: 108932 Loss:1.0707 , Current Loss: 1.0586\n",
      "Steps: 109032 Loss:1.0824 , Current Loss: 1.0685\n",
      "Steps: 109132 Loss:1.0767 , Current Loss: 1.1934\n",
      "Steps: 109232 Loss:1.0874 , Current Loss: 1.0596\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append([step, train_loss])\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 109332 Loss:1.7847 , Current Loss: 1.7501\n",
      "Steps: 109432 Loss:1.7497 , Current Loss: 1.6552\n",
      "Steps: 109532 Loss:1.7298 , Current Loss: 1.8444\n",
      "Steps: 109632 Loss:1.7266 , Current Loss: 1.7739\n",
      "Steps: 109732 Loss:1.7245 , Current Loss: 1.8321\n",
      "Steps: 109832 Loss:1.7206 , Current Loss: 1.6995\n",
      "Steps: 109932 Loss:1.7060 , Current Loss: 1.5771\n",
      "Steps: 110032 Loss:1.7105 , Current Loss: 1.6508\n",
      "Steps: 110132 Loss:1.7066 , Current Loss: 1.6644\n",
      "Steps: 110232 Loss:1.6970 , Current Loss: 1.5904\n",
      "Steps: 110332 Loss:1.7139 , Current Loss: 1.8116\n",
      "Steps: 110432 Loss:1.6883 , Current Loss: 1.7113\n",
      "Steps: 110532 Loss:1.7016 , Current Loss: 1.7064\n",
      "Steps: 110632 Loss:1.7041 , Current Loss: 1.6901\n",
      "Steps: 110732 Loss:1.7024 , Current Loss: 1.5544\n",
      "Steps: 110832 Loss:1.7028 , Current Loss: 1.7255\n",
      "Steps: 110932 Loss:1.6965 , Current Loss: 1.5244\n",
      "Epoch: 1 , Step: 111003 , loss: 1.7145 , Time: 11848.3\n",
      "Model Saved\n",
      "Steps: 111103 Loss:1.6831 , Current Loss: 1.8166\n",
      "Steps: 111203 Loss:1.6968 , Current Loss: 1.7384\n",
      "Steps: 111303 Loss:1.6850 , Current Loss: 1.9071\n",
      "Steps: 111403 Loss:1.6878 , Current Loss: 1.7307\n",
      "Steps: 111503 Loss:1.6772 , Current Loss: 1.6832\n",
      "Steps: 111603 Loss:1.6850 , Current Loss: 1.6804\n",
      "Steps: 111703 Loss:1.6758 , Current Loss: 1.6832\n",
      "Steps: 111803 Loss:1.6695 , Current Loss: 1.6211\n",
      "Steps: 111903 Loss:1.6745 , Current Loss: 1.7811\n",
      "Steps: 112003 Loss:1.6699 , Current Loss: 1.6192\n",
      "Steps: 112103 Loss:1.6700 , Current Loss: 1.6354\n",
      "Steps: 112203 Loss:1.6730 , Current Loss: 1.7709\n",
      "Steps: 112303 Loss:1.6700 , Current Loss: 1.6203\n",
      "Steps: 112403 Loss:1.6678 , Current Loss: 1.6114\n",
      "Steps: 112503 Loss:1.6712 , Current Loss: 1.7116\n",
      "Steps: 112603 Loss:1.6729 , Current Loss: 1.6597\n",
      "Steps: 112703 Loss:1.6594 , Current Loss: 1.5689\n",
      "Epoch: 2 , Step: 112774 , loss: 1.6755 , Time: 11760.5\n",
      "Model Saved\n",
      "Val: F1 Score:48.66 Accuracy:56.87  POS: F1 Score:82.46 Accuracy:91.02 Loss:1.6845 , Time: 2263.7\n",
      "Steps: 112874 Loss:1.6715 , Current Loss: 1.6948\n",
      "Steps: 112974 Loss:1.6643 , Current Loss: 1.8534\n",
      "Steps: 113074 Loss:1.6666 , Current Loss: 1.6205\n",
      "Steps: 113174 Loss:1.6664 , Current Loss: 1.6460\n",
      "Steps: 113274 Loss:1.6574 , Current Loss: 1.7178\n",
      "Steps: 113374 Loss:1.6662 , Current Loss: 1.7013\n",
      "Steps: 113474 Loss:1.6683 , Current Loss: 1.6725\n",
      "Steps: 113574 Loss:1.6586 , Current Loss: 1.5804\n",
      "Steps: 113674 Loss:1.6560 , Current Loss: 1.8875\n",
      "Steps: 113774 Loss:1.6522 , Current Loss: 1.4565\n",
      "Steps: 113874 Loss:1.6524 , Current Loss: 1.6717\n",
      "Steps: 113974 Loss:1.6516 , Current Loss: 1.6951\n",
      "Steps: 114074 Loss:1.6544 , Current Loss: 1.5956\n",
      "Steps: 114174 Loss:1.6470 , Current Loss: 1.7122\n",
      "Steps: 114274 Loss:1.6497 , Current Loss: 1.7072\n",
      "Steps: 114374 Loss:1.6478 , Current Loss: 1.6293\n",
      "Steps: 114474 Loss:1.6540 , Current Loss: 1.6596\n",
      "Epoch: 3 , Step: 114545 , loss: 1.6579 , Time: 11764.3\n",
      "Model Saved\n",
      "Steps: 114645 Loss:1.6440 , Current Loss: 1.7015\n",
      "Steps: 114745 Loss:1.6660 , Current Loss: 1.6497\n",
      "Steps: 114845 Loss:1.6529 , Current Loss: 1.7650\n",
      "Steps: 114945 Loss:1.6474 , Current Loss: 1.6679\n",
      "Steps: 115045 Loss:1.6413 , Current Loss: 1.6400\n",
      "Steps: 115145 Loss:1.6517 , Current Loss: 1.8056\n",
      "Steps: 115245 Loss:1.6506 , Current Loss: 1.6073\n",
      "Steps: 115345 Loss:1.6441 , Current Loss: 1.7414\n",
      "Steps: 115445 Loss:1.6550 , Current Loss: 1.6111\n",
      "Steps: 115545 Loss:1.6383 , Current Loss: 1.6403\n",
      "Steps: 115645 Loss:1.6483 , Current Loss: 1.6986\n",
      "Steps: 115745 Loss:1.6355 , Current Loss: 1.5869\n",
      "Steps: 115845 Loss:1.6551 , Current Loss: 1.7229\n",
      "Steps: 115945 Loss:1.6655 , Current Loss: 1.7300\n",
      "Steps: 116045 Loss:1.6448 , Current Loss: 1.4811\n",
      "Steps: 116145 Loss:1.6481 , Current Loss: 1.7523\n",
      "Steps: 116245 Loss:1.6365 , Current Loss: 1.5730\n",
      "Epoch: 4 , Step: 116316 , loss: 1.6492 , Time: 11783.1\n",
      "Model Saved\n",
      "Val: F1 Score:49.05 Accuracy:57.22  POS: F1 Score:82.69 Accuracy:91.25 Loss:1.6644 , Time: 2266.5\n",
      "Steps: 116416 Loss:1.6551 , Current Loss: 1.6981\n",
      "Steps: 116516 Loss:1.6425 , Current Loss: 1.6943\n",
      "Steps: 116616 Loss:1.6479 , Current Loss: 1.6184\n",
      "Steps: 116716 Loss:1.6524 , Current Loss: 1.5703\n",
      "Steps: 116816 Loss:1.6441 , Current Loss: 1.8608\n",
      "Steps: 116916 Loss:1.6511 , Current Loss: 1.5664\n",
      "Steps: 117016 Loss:1.6435 , Current Loss: 1.7044\n",
      "Steps: 117116 Loss:1.6473 , Current Loss: 1.5883\n",
      "Steps: 117216 Loss:1.6391 , Current Loss: 1.5809\n",
      "Steps: 117316 Loss:1.6497 , Current Loss: 1.6931\n",
      "Steps: 117416 Loss:1.6430 , Current Loss: 1.5918\n",
      "Steps: 117516 Loss:1.6386 , Current Loss: 1.4935\n",
      "Steps: 117616 Loss:1.6293 , Current Loss: 1.5691\n",
      "Steps: 117716 Loss:1.6343 , Current Loss: 1.6163\n",
      "Steps: 117816 Loss:1.6350 , Current Loss: 1.6850\n",
      "Steps: 117916 Loss:1.6411 , Current Loss: 1.5957\n",
      "Steps: 118016 Loss:1.6356 , Current Loss: 1.5900\n",
      "Epoch: 5 , Step: 118087 , loss: 1.6427 , Time: 11785.0\n",
      "Model Saved\n",
      "Steps: 118187 Loss:1.6503 , Current Loss: 1.6529\n",
      "Steps: 118287 Loss:1.6432 , Current Loss: 1.5442\n",
      "Steps: 118387 Loss:1.6365 , Current Loss: 1.5613\n",
      "Steps: 118487 Loss:1.6364 , Current Loss: 1.6217\n",
      "Steps: 118587 Loss:1.6230 , Current Loss: 1.6126\n",
      "Steps: 118687 Loss:1.6334 , Current Loss: 1.5148\n",
      "Steps: 118787 Loss:1.6426 , Current Loss: 1.5906\n",
      "Steps: 118887 Loss:1.6407 , Current Loss: 1.6164\n",
      "Steps: 118987 Loss:1.6176 , Current Loss: 1.5932\n",
      "Steps: 119087 Loss:1.6242 , Current Loss: 1.5053\n",
      "Steps: 119187 Loss:1.6485 , Current Loss: 1.7065\n",
      "Steps: 119287 Loss:1.6354 , Current Loss: 1.6060\n",
      "Steps: 119387 Loss:1.6419 , Current Loss: 1.5428\n",
      "Steps: 119487 Loss:1.6367 , Current Loss: 1.6497\n",
      "Steps: 119587 Loss:1.6415 , Current Loss: 1.6820\n",
      "Steps: 119687 Loss:1.6255 , Current Loss: 1.6106\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,200,512]\n\t [[Node: model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_0/gradients/model_0/global_attention/strided_slice_grad/Shape, model_0/global_attention/strided_slice/stack, model_0/global_attention/strided_slice/stack_1, model_0/global_attention/strided_slice/stack_2, model_0/gradients/model_0/global_attention/boolean_mask/Reshape_grad/Reshape)]]\n\t [[Node: Adam/update/_9206 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_88100_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-d02798036098>\", line 127, in <module>\n    grads_vars = optimizer.compute_gradients(total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 346, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py\", line 243, in _StridedSliceGrad\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3594, in strided_slice_grad\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'model_0/global_attention/strided_slice', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-d02798036098>\", line 70, in <module>\n    c = tf.expand_dims(global_attention(h[0], x_mask[gpu_idx][0], W_att), 0)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 499, in _SliceHelper\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 663, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3515, in strided_slice\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,200,512]\n\t [[Node: model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_0/gradients/model_0/global_attention/strided_slice_grad/Shape, model_0/global_attention/strided_slice/stack, model_0/global_attention/strided_slice/stack_1, model_0/global_attention/strided_slice/stack_2, model_0/gradients/model_0/global_attention/boolean_mask/Reshape_grad/Reshape)]]\n\t [[Node: Adam/update/_9206 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_88100_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,200,512]\n\t [[Node: model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_0/gradients/model_0/global_attention/strided_slice_grad/Shape, model_0/global_attention/strided_slice/stack, model_0/global_attention/strided_slice/stack_1, model_0/global_attention/strided_slice/stack_2, model_0/gradients/model_0/global_attention/boolean_mask/Reshape_grad/Reshape)]]\n\t [[Node: Adam/update/_9206 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_88100_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5a52bf325877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_id_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pos_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msense_mask_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain_cond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_train_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-84bb7f86a63d>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(xx, yy, yy_pos, mask, smask, train_cond, pretrain_cond)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cond\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,200,512]\n\t [[Node: model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_0/gradients/model_0/global_attention/strided_slice_grad/Shape, model_0/global_attention/strided_slice/stack, model_0/global_attention/strided_slice/stack_1, model_0/global_attention/strided_slice/stack_2, model_0/gradients/model_0/global_attention/boolean_mask/Reshape_grad/Reshape)]]\n\t [[Node: Adam/update/_9206 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_88100_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-d02798036098>\", line 127, in <module>\n    grads_vars = optimizer.compute_gradients(total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 346, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 540, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py\", line 243, in _StridedSliceGrad\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3594, in strided_slice_grad\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'model_0/global_attention/strided_slice', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"/users/btech/aviraj/envs/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-d02798036098>\", line 70, in <module>\n    c = tf.expand_dims(global_attention(h[0], x_mask[gpu_idx][0], W_att), 0)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 499, in _SliceHelper\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 663, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3515, in strided_slice\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,200,512]\n\t [[Node: model_0/gradients/model_0/global_attention/strided_slice_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](model_0/gradients/model_0/global_attention/strided_slice_grad/Shape, model_0/global_attention/strided_slice/stack, model_0/global_attention/strided_slice/stack_1, model_0/global_attention/strided_slice/stack_2, model_0/gradients/model_0/global_attention/boolean_mask/Reshape_grad/Reshape)]]\n\t [[Node: Adam/update/_9206 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_88100_Adam/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "val_period = 2\n",
    "loss_collection = []\n",
    "val_collection = []\n",
    "pre_train_cond = False\n",
    "for i in range(num_epochs):\n",
    "    random = np.random.choice(len(y_train), size=(len(y_train)), replace=False)\n",
    "    x_id_train = x_id_train[random]\n",
    "    y_train = y_train[random]\n",
    "    mask_train = mask_train[random]    \n",
    "    sense_mask_train = sense_mask_train[random]\n",
    "    y_pos_train = y_pos_train[random]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, step = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, pretrain_cond=pre_train_cond)\n",
    "    time_taken = time.time() - start_time\n",
    "    loss_collection.append([step, train_loss])\n",
    "    print(\"Epoch: {}\".format(i+1),\", Step: {}\".format(step), \", loss: {0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))\n",
    "    saver.save(sess, save_path=save_dir)                         \n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    if((i+1)%val_period==0):\n",
    "        start_time = time.time()\n",
    "        val_loss, val_pred, val_true, val_pred_pos, val_true_pos = model(x_id_val, y_val, y_pos_val, mask_val, sense_mask_val, train_cond=False)        \n",
    "        f1_, accu_, f1_pos_, accu_pos_ = eval_score(val_true, val_pred, val_true_pos, val_pred_pos)\n",
    "        time_taken = time.time() - start_time\n",
    "        val_collection.append([f1_, accu_, f1_pos_, accu_pos_])\n",
    "        print(\"Val: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(val_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_loss, train_pred, train_true, train_pred_pos, train_true_pos = model(x_id_train, y_train, y_pos_train, mask_train, sense_mask_train, train_cond=False)        \n",
    "f1_, accu_, f1_pos_, accu_pos_ = etrain_score(train_true, train_pred, train_true_pos, train_pred_pos)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"train: F1 Score:{0:.2f}\".format(f1_), \"Accuracy:{0:.2f}\".format(accu_), \" POS: F1 Score:{0:.2f}\".format(f1_pos_), \"Accuracy:{0:.2f}\".format(accu_pos_), \"Loss:{0:.4f}\".format(train_loss), \", Time: {0:.1f}\".format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "cs771"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
