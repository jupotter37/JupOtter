{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq1sQRE0jIkr"
      },
      "outputs": [],
      "source": [
        "# 引入依赖包\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "pd.options.display.max_colwidth = 200\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV9Re7fejhwK"
      },
      "outputs": [],
      "source": [
        "# 初始化文本集与标签集\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "          'I love green eggs, ham, sausages and bacon!',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but the brown fox is quick!'    \n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox97pHZsjpJt"
      },
      "outputs": [],
      "source": [
        "# 将文本集和标签集标识为数据框架的格式\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus,'Category': labels})\n",
        "corpus_df = corpus_df[['Document','Category']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "_th2-isVkGbz",
        "outputId": "f2da5fd3-a85a-4a35-d874-70368a9fc487"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                             Document Category\n",
              "0                                      The sky is blue and beautiful.  weather\n",
              "1                                   Love this blue and beautiful sky!  weather\n",
              "2                        The quick brown fox jumps over the lazy dog.  animals\n",
              "3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n",
              "4                         I love green eggs, ham, sausages and bacon!     food\n",
              "5                    The brown fox is quick and the blue dog is lazy!  animals\n",
              "6            The sky is very blue and the sky is very beautiful today  weather\n",
              "7                         The dog is lazy but the brown fox is quick!  animals"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d64e8a2c-5621-40ae-96f9-2b106a7d1ac6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sky is blue and beautiful.</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love this blue and beautiful sky!</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
              "      <td>food</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The sky is very blue and the sky is very beautiful today</td>\n",
              "      <td>weather</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The dog is lazy but the brown fox is quick!</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d64e8a2c-5621-40ae-96f9-2b106a7d1ac6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d64e8a2c-5621-40ae-96f9-2b106a7d1ac6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d64e8a2c-5621-40ae-96f9-2b106a7d1ac6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "corpus_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrA19vFRkJD-",
        "outputId": "770dd9dd-a179-4b11-9ef2-95b39c904d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# 简单的文本预处理流程介绍\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  # 全小写化\n",
        "  # 移除特殊字符\n",
        "  # 移除空格\n",
        "  # 删除停用词\n",
        "def normalize_document(doc):\n",
        "  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "  # 对文档进行分词\n",
        "  tokens = wpt.tokenize(doc)\n",
        "  # 删除停用词\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  # 重新构建文档\n",
        "  doc = ' '.join(filtered_tokens)\n",
        "  return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1BgRVozlroY",
        "outputId": "dc394226-7712-481b-9c26-0dc494bf1f49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sky blue beautiful', 'love blue beautiful sky',\n",
              "       'quick brown fox jumps lazy dog',\n",
              "       'kings breakfast sausages ham bacon eggs toast beans',\n",
              "       'love green eggs ham sausages bacon',\n",
              "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
              "       'dog lazy brown fox quick'], dtype='<U51')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "norm_corpus = normalize_corpus(corpus)\n",
        "norm_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVySAkIOlzW5",
        "outputId": "be697c43-34a0-41dd-cf56-f7cd2e596b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# 词嵌入流程\n",
        "# 使用圣经作为输入文本集的样本\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from string import punctuation\n",
        "\n",
        "bible = gutenberg.sents('bible-kjv.txt') \n",
        "remove_terms = punctuation + '0123456789'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nObwK4rLmQ7d",
        "outputId": "3d96ddcc-b5b4-429f-c722-b6336092299b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines: 30103\n",
            "\n",
            "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
            "\n",
            "Processed line: god said let firmament midst waters let divide waters waters\n"
          ]
        }
      ],
      "source": [
        "# 移除标点符号和数字的同时进行小写转化\n",
        "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "# 文本内容拼接\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "# 使用normalize_corpus方法进行预处理操作\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "# 将通过分词处理后的圣经文本进行筛选，只留下长度大于2的圣经文本\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
        "\n",
        "print('Total lines:', len(bible))\n",
        "print('\\nSample line:', bible[10])\n",
        "print('\\nProcessed line:', norm_bible[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb8xUplRmX8U",
        "outputId": "4b4a3e16-87b3-4333-d4c0-7c25098fbd25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 12425\n",
            "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
          ]
        }
      ],
      "source": [
        "# 使用CBOW实现word2vect模型\n",
        "\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "\n",
        "# tokenizer 对象首先将所有文本转换为小写\n",
        "# 并去除标点符号等特殊字符\n",
        "# 然后将文本拆分成单词，并为每个单词分配一个唯一的整数 ID\n",
        "# 这些整数 ID 从 1 开始连续编号\n",
        "# 最后tokenizer 对象会构建一个词汇表，其中包含所有出现过的单词及其对应的整数 ID\n",
        "# 即使我们在之前已经对文本进行了预处理操作，这里主要是对文本转化为数字化的第一步\n",
        "# 也就是构建字典转化为 one-hot vector\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "word2id = tokenizer.word_index\n",
        "# 给字典赋予一个pad标签用于填充\n",
        "word2id['PAD'] = 0\n",
        "# 将word与id构建键值对\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "# 双重for循环\n",
        "# 首先从处理好的bible中拿到单条的doc文档\n",
        "# 对doc文档进行分词处理，不能说很有意义，因为我们已经做过很多步了\n",
        "# 将分词之后的序列转化为基于字典的数字，也就是句子向量化\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2\n",
        "print('Vocabulary Size:', vocab_size) # 展示字典大小\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10]) # 展示字典前十项"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRgC5tNysgyI"
      },
      "outputs": [],
      "source": [
        "# 构建生成 上下文与目标词的文本对 函数\n",
        "\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            # 基于window_size 填充context数组\n",
        "            context_words.append([words[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            # 原数组的全部文本就是 target words\n",
        "            label_word.append(word)\n",
        "            # 对边缘的文本场景所导致的上下文长度缺失问题，使用填充的方式解决\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            # 将target/center word 转化为one-hot encoding\n",
        "            y = np_utils.to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laVnq9k4uheE",
        "outputId": "7d26bb8d-57cc-462e-b216-771d47e41afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
            "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
            "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
            "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
            "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
            "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
            "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
            "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
            "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
            "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
            "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikajNu5aumPi",
        "outputId": "c58972e3-f45b-4b63-8d03-1acdba320bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 100)            1242500   \n",
            "                                                                 \n",
            " lambda_1 (Lambda)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 12425)             1254925   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,497,425\n",
            "Trainable params: 2,497,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# 构建 CBOW NN网络结构\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "cbow = Sequential()\n",
        "# input_dim 是词汇表的大小\n",
        "# output_dim 是输出向量的维度\n",
        "# input_length 是输入序列的长度\n",
        "# 对应网络结构中的 input layer 层\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "# 将上下文单词的嵌入向量相加\n",
        "# 除以上下文单词的数量，得到一个平均嵌入向量，表示当前中心词的词向量\n",
        "# 对应 网络结构中的 Hidden layer 层\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "# 将平均嵌入向量映射为一个概率分布\n",
        "# 代表了词汇表中每个单词作为中心词的可能性\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "# 使用交叉熵损失函数作为Loss\n",
        "# 并使用rmsprop作为优化函数\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "print(cbow.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "31WahjDSxTH8",
        "outputId": "3e9369eb-ba28-4d4e-d3b8-474090b43f90"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"355pt\" height=\"405pt\" viewBox=\"0.00 0.00 266.00 304.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.75 0.75) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-300 262,-300 262,4 -4,4\"/>\n<!-- 140320661655808 -->\n<g id=\"node1\" class=\"node\">\n<title>140320661655808</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-249.5 0,-295.5 258,-295.5 258,-249.5 0,-249.5\"/>\n<text text-anchor=\"middle\" x=\"62.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">embedding_1_input</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"0,-272.5 125,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"62.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"125,-249.5 125,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"152.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"125,-272.5 180,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"152.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"180,-249.5 180,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"219\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 4)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"180,-272.5 258,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"219\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 4)]</text>\n</g>\n<!-- 140320661741424 -->\n<g id=\"node2\" class=\"node\">\n<title>140320661741424</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"7.5,-166.5 7.5,-212.5 250.5,-212.5 250.5,-166.5 7.5,-166.5\"/>\n<text text-anchor=\"middle\" x=\"53\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">embedding_1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"7.5,-189.5 98.5,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"53\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">Embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"98.5,-166.5 98.5,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"126\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"98.5,-189.5 153.5,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"126\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"153.5,-166.5 153.5,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"202\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"153.5,-189.5 250.5,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"202\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4, 100)</text>\n</g>\n<!-- 140320661655808&#45;&gt;140320661741424 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140320661655808-&gt;140320661741424</title>\n<path fill=\"none\" stroke=\"black\" d=\"M129,-249.37C129,-241.15 129,-231.66 129,-222.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"132.5,-222.61 129,-212.61 125.5,-222.61 132.5,-222.61\"/>\n</g>\n<!-- 140320661735504 -->\n<g id=\"node3\" class=\"node\">\n<title>140320661735504</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"17.5,-83.5 17.5,-129.5 240.5,-129.5 240.5,-83.5 17.5,-83.5\"/>\n<text text-anchor=\"middle\" x=\"53\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">lambda_1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"17.5,-106.5 88.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"53\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">Lambda</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88.5,-83.5 88.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"116\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88.5,-106.5 143.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"116\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143.5,-83.5 143.5,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"192\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143.5,-106.5 240.5,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"192\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n</g>\n<!-- 140320661741424&#45;&gt;140320661735504 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140320661741424-&gt;140320661735504</title>\n<path fill=\"none\" stroke=\"black\" d=\"M129,-166.37C129,-158.15 129,-148.66 129,-139.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"132.5,-139.61 129,-129.61 125.5,-139.61 132.5,-139.61\"/>\n</g>\n<!-- 140320661737376 -->\n<g id=\"node4\" class=\"node\">\n<title>140320661737376</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"23,-0.5 23,-46.5 235,-46.5 235,-0.5 23,-0.5\"/>\n<text text-anchor=\"middle\" x=\"53.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">dense_1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"23,-23.5 84,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"53.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"84,-0.5 84,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"111.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"84,-23.5 139,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"111.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"139,-0.5 139,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"139,-23.5 235,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"187\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 12425)</text>\n</g>\n<!-- 140320661735504&#45;&gt;140320661737376 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140320661735504-&gt;140320661737376</title>\n<path fill=\"none\" stroke=\"black\" d=\"M129,-83.37C129,-75.15 129,-65.66 129,-56.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"132.5,-56.61 129,-46.61 125.5,-56.61 132.5,-56.61\"/>\n</g>\n</g>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 绘制网络结构图\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=True, \n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yix-peDJzmj_",
        "outputId": "192e9d68-4db3-4c93-a90d-098fd11859a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10000 (context, word) pairs\n",
            "Processed 20000 (context, word) pairs\n",
            "Processed 30000 (context, word) pairs\n",
            "Processed 40000 (context, word) pairs\n",
            "Processed 50000 (context, word) pairs\n",
            "Processed 60000 (context, word) pairs\n",
            "Processed 70000 (context, word) pairs\n",
            "Processed 80000 (context, word) pairs\n",
            "Processed 90000 (context, word) pairs\n",
            "Processed 100000 (context, word) pairs\n",
            "Processed 110000 (context, word) pairs\n",
            "Processed 120000 (context, word) pairs\n",
            "Processed 130000 (context, word) pairs\n",
            "Processed 140000 (context, word) pairs\n",
            "Processed 150000 (context, word) pairs\n",
            "Processed 160000 (context, word) pairs\n",
            "Processed 170000 (context, word) pairs\n",
            "Processed 180000 (context, word) pairs\n",
            "Processed 190000 (context, word) pairs\n",
            "Processed 200000 (context, word) pairs\n",
            "Processed 210000 (context, word) pairs\n",
            "Processed 220000 (context, word) pairs\n"
          ]
        }
      ],
      "source": [
        "# 模型训练\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpQdY42c0CZL"
      },
      "outputs": [],
      "source": [
        "# 取出CBOW 中训练好的词向量\n",
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdAuF1B62lgQ"
      },
      "outputs": [],
      "source": [
        "# 构建文本距离矩阵，观察最相似的文本\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# 基于词向量矩阵构建欧氏距离矩阵\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# 查询 words 的相似文本内容\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
        "\n",
        "similar_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q96h0fpE6ADG"
      },
      "source": [
        "## 使用 Skip-Gram 实现 Word2Vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHVDJQXH5HFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06d2f91-f7a1-4066-9e23-743a00784db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 12425\n",
            "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
          ]
        }
      ],
      "source": [
        "# 构建字典\n",
        "from keras.preprocessing import text\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "vocab_size = len(word2id) + 1 \n",
        "embed_size = 100\n",
        "\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3EIwlCx6sBM"
      },
      "outputs": [],
      "source": [
        "# 构建基于skipgram的上下文/目标词的文本对\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "# 生成skipgram文本对\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          id2word[pairs[i][0]], pairs[i][0], \n",
        "          id2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejq15nKl78T6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "435bd9b9-186f-4b8e-a214-b61bb914cd23"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-43e77b829b26>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 其中limit的值为sqrt(6 / (fan_in + fan_out))，fan_in和fan_out分别是输入和输出的单元数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Glorot Uniform初始化可以有效地避免梯度消失和梯度爆炸等问题，并且可以帮助加速模型的收敛\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m word_model.add(Embedding(vocab_size, embed_size,\n\u001b[0m\u001b[1;32m     18\u001b[0m                          \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"glorot_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                          input_length=1))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ],
      "source": [
        "# 构建Skip-gram神经网络\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.core import Dense, Reshape\n",
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "# 将center word 和 context words 分别作为模型的层来处理\n",
        "# skip gram 中我们使用 center word 预测 context word\n",
        "# 这是一个1toN的问题\n",
        "word_model = Sequential()\n",
        "# 将输入的文本进行词嵌入处理，将文本数据中的每一个单词表示为一个向量\n",
        "# 每一个单词会映射到一个对应的词向量上\n",
        "# 词向量矩阵的初始化过程使用glorot uniform进行初始化 - Xavier 初始化\n",
        "# “Xavier初始化” 是一种通过均匀分布来随机初始化权重参数的方法，\n",
        "# 这个分布的范围被设计为[-limit, limit]，\n",
        "# 其中limit的值为sqrt(6 / (fan_in + fan_out))，fan_in和fan_out分别是输入和输出的单元数\n",
        "# Glorot Uniform初始化可以有效地避免梯度消失和梯度爆炸等问题，并且可以帮助加速模型的收敛\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "# 将 Embedding 层的输出转换为一个一维向量，用于后续处理\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "# \n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "model = Sequential()\n",
        "# 创建一个合并模型，将center word model和context model 通过 dot 运算进行了拼接\n",
        "# 使用dot运算来计算center word 和 context words 的 similarity\n",
        "model.add(Concatenate([word_model, context_model], mode=\"dot\"))\n",
        "model.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gZTLzu18Z4e"
      },
      "outputs": [],
      "source": [
        "## 绘制模型结构图\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gIyn2ep8fvz"
      },
      "outputs": [],
      "source": [
        "# 训练模型\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrXNITnc8jUs"
      },
      "outputs": [],
      "source": [
        "# 取出词向量矩阵\n",
        "\n",
        "merge_layer = model.layers[0]\n",
        "word_model = merge_layer.layers[0]\n",
        "word_embed_layer = word_model.layers[0]\n",
        "weights = word_embed_layer.get_weights()[0][1:]\n",
        "\n",
        "print(weights.shape)\n",
        "pd.DataFrame(weights, index=id2word.values()).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwkrJh-e8nQn"
      },
      "outputs": [],
      "source": [
        "# 构建欧式距离矩阵，查找近义词\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
        "\n",
        "similar_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQgJfNX38u0N"
      },
      "outputs": [],
      "source": [
        "# 词向量矩阵可视化\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "words_ids = [word2id[w] for w in words]\n",
        "word_vectors = np.array([weights[idx] for idx in words_ids])\n",
        "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(word_vectors)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用Genism 构建 Word2Vect 模型"
      ],
      "metadata": {
        "id": "YCCV8Gqr1HI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# 对文本内容进行分词处理\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "tokenized_corpus = [wpt.tokenize(document) for document in norm_bible]\n",
        "\n",
        "# 参数设置\n",
        "feature_size = 100    # 词向量的表达维度 dim\n",
        "window_context = 30          # 上下文的大小 也就是 window_size                                                                           \n",
        "min_word_count = 1   # 最小单词数量                        \n",
        "sample = 1e-3   # 对高频词的下采样比例参数\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
        "                          window=window_context, min_count=min_word_count,\n",
        "                          sample=sample, iter=50)\n",
        "\n",
        "# 观察基于 genism 生成的词向量相似性关系\n",
        "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
        "                  for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
        "similar_words"
      ],
      "metadata": {
        "id": "igsjRrbF1LIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 词向量可视化"
      ],
      "metadata": {
        "id": "FlMg1EBt1ufB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "wvs = w2v_model.wv[words]\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "metadata": {
        "id": "65JgWD0q1uJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}