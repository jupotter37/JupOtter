{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccf8b6a-7076-43b9-82ee-d84fdfd75522",
   "metadata": {},
   "source": [
    "<img src=\"./images/logo.svg\" alt=\"lakeFS logo\" width=300/> <img src=\"https://www.apache.org/logos/res/iceberg/iceberg.png\" alt=\"Apache Iceberg logo\" width=300/>  \n",
    "\n",
    "## lakeFS ‚ù§Ô∏è Apache Iceberg - an example using NYC Film Permits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ba792-9948-4d21-8346-8dac596063ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config\n",
    "\n",
    "**_If you're not using the provided lakeFS server and MinIO storage then change these values to match your environment_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cb169-f6ad-4927-abbc-cc740d7e1a82",
   "metadata": {},
   "source": [
    "### lakeFS endpoint and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418e64da-c208-49f3-963d-9bf9c488b52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefsEndPoint = 'http://lakefs:8000' # e.g. 'https://username.aws_region_name.lakefscloud.io' \n",
    "lakefsAccessKey = 'AKIAIOSFOLKFSSAMPLES'\n",
    "lakefsSecretKey = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f01e2d-b8a5-4820-bed0-efa56d784b80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce9539c-ca43-466c-a820-ee8f3f6fa312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "storageNamespace = 's3://example' # e.g. \"s3://bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9f0c9-e77f-4d4f-a5d5-4b3fcff9bf65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b052ac3-055a-4b05-8f6a-cc39747ae5d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup\n",
    "\n",
    "**(you shouldn't need to change anything in this section, just run it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d51253-2d89-4dc0-9879-1104a5228b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_name = \"lakefs-iceberg-nyc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1ff89-480c-4421-8ed5-1b2e33a1fda9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create lakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde3e9b6-2da3-4ae2-968a-acf874867a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lakefs_client\n",
    "from lakefs_client.models import *\n",
    "from lakefs_client.client import LakeFSClient\n",
    "\n",
    "# lakeFS credentials and endpoint\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.username = lakefsAccessKey\n",
    "configuration.password = lakefsSecretKey\n",
    "configuration.host = lakefsEndPoint\n",
    "\n",
    "lakefs = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42373c-9c28-47ce-b060-204825fbce34",
   "metadata": {},
   "source": [
    "#### Verify lakeFS credentials by getting lakeFS version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d9debc-e52e-4b42-8e56-b52e7d798410",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying lakeFS credentials‚Ä¶\n",
      "‚Ä¶‚úÖlakeFS credentials verified\n",
      "\n",
      "‚ÑπÔ∏èlakeFS version 0.104.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Verifying lakeFS credentials‚Ä¶\")\n",
    "try:\n",
    "    v=lakefs.config.get_config()\n",
    "except:\n",
    "    print(\"üõë failed to get lakeFS version\")\n",
    "else:\n",
    "    print(f\"‚Ä¶‚úÖlakeFS credentials verified\\n\\n‚ÑπÔ∏èlakeFS version {v['version_config']['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2ebb0-b842-4584-8776-ef89976feb9a",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf73bf83-546b-4cc0-a920-cfcb16d87a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository lakefs-iceberg-nyc does not exist, so going to try and create it now.\n",
      "Created new repo lakefs-iceberg-nyc using storage namespace s3://example/lakefs-iceberg-nyc\n"
     ]
    }
   ],
   "source": [
    "from lakefs_client.exceptions import NotFoundException\n",
    "\n",
    "try:\n",
    "    repo=lakefs.repositories.get_repository(repo_name)\n",
    "    print(f\"Found existing repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "except NotFoundException as f:\n",
    "    print(f\"Repository {repo_name} does not exist, so going to try and create it now.\")\n",
    "    try:\n",
    "        repo=lakefs.repositories.create_repository(repository_creation=RepositoryCreation(name=repo_name,\n",
    "                                                                                                storage_namespace=f\"{storageNamespace}/{repo_name}\"))\n",
    "        print(f\"Created new repo {repo.id} using storage namespace {repo.storage_namespace}\")\n",
    "    except lakefs_client.ApiException as e:\n",
    "        print(f\"Error creating repo {repo_name}. Error is {e}\")\n",
    "        os._exit(00)\n",
    "except lakefs_client.ApiException as e:\n",
    "    print(f\"Error getting repo {repo_name}: {e}\")\n",
    "    os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7403de6-88df-4aed-a8eb-61e6d5899859",
   "metadata": {},
   "source": [
    "### Set up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098faed5-149a-4c4d-a52a-3c8e3f98c36b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIceberg / Jupyter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0,io.lakefs:lakefs-iceberg:0.1.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3.impl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop.fs.s3a.S3AFileSystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.endpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlakefsEndPoint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.path.style.access\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.access.key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlakefsAccessKey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.secret.key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlakefsSecretKey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.lakefs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.SparkCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.lakefs.catalog-impl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio.lakefs.iceberg.LakeFSCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.lakefs.warehouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlakefs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.lakefs.uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlakefsEndPoint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.lakefs.cache-enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.defaultCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlakefs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.extensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 417\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Iceberg / Jupyter\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0,io.lakefs:lakefs-iceberg:0.1.1\") \\\n",
    "        .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", lakefsEndPoint) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", lakefsAccessKey) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", lakefsSecretKey) \\\n",
    "        .config(\"spark.sql.catalog.lakefs\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs.catalog-impl\", \"io.lakefs.iceberg.LakeFSCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs.warehouse\", f\"lakefs://{repo_name}\") \\\n",
    "        .config(\"spark.sql.catalog.lakefs.uri\", lakefsEndPoint) \\\n",
    "        .config(\"spark.sql.catalog.lakefs.cache-enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.defaultCatalog\", \"lakefs\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4b709-7631-4fec-b75b-0fc4a4c417f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226c33f-5a90-4b19-a037-56966130aa6c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a827676-a2bc-48f2-9f4b-c4b2d4ba8ff3",
   "metadata": {},
   "source": [
    "# Main demo starts here üö¶ üëáüèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead44c0",
   "metadata": {},
   "source": [
    "# Load some Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a9f41",
   "metadata": {},
   "source": [
    "For this demo, we will use the [New York City Film Permits dataset](https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p) available as part of the NYC Open Data initiative. We're using a locally saved copy of a 1000 record sample, but feel free to download the entire dataset to use in this notebook!\n",
    "\n",
    "We'll save the sample dataset into an Iceberg table called `permits`, using lakeFS for the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5241a-a1f2-48ac-9ebb-54ab57f778e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\",\"true\").option(\"multiline\",\"true\").json(\"/data/nyc_film_permits.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c6bf0-746d-46d9-b4f8-0d17dc3c9e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"lakefs.main.nyc.permits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee3857-f878-404a-9f94-c0ef723631a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "if lakefsEndPoint=='http://lakefs:8000':\n",
    "    lakeFSWebUI='http://localhost:8000'\n",
    "else:\n",
    "    lakeFSWebUI=lakefsEndPoint\n",
    "\n",
    "md(f\"#### üëâüèª Optionally, go and view the objects in [lakeFS web UI]({lakeFSWebUI}/repositories/{repo.id}/objects?ref=main&path=nyc%2Fpermits%2F)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cf187",
   "metadata": {},
   "source": [
    "Taking a quick peek at the data, you can see that there are a number of permits for different boroughs in New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3170161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT borough, count(*) AS permit_cnt\n",
    "FROM lakefs.main.nyc.permits\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24d569-83ca-4c04-8435-2f51d55778dc",
   "metadata": {},
   "source": [
    "### Commit the new table and its data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec33a1-6332-4fc9-a003-a4b93914d218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repo.id, \"main\", CommitCreation(\n",
    "    message=\"Initial data load\",\n",
    "    metadata={'author': 'rmoff',\n",
    "              'data source': 'https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p'}\n",
    ") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825889e-49bf-4eac-a23e-9250ab0b53e0",
   "metadata": {},
   "source": [
    "# Create a new branch\n",
    "\n",
    "_This is copy-on-write; we're not duplicating the data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d1de1-eac2-4cf3-9fc9-6975f01201ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.create_branch(repo.id, \n",
    "                              BranchCreation(name=\"dev\",\n",
    "                                             source=\"main\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81878b-3cca-457b-b344-35ef476ce3fb",
   "metadata": {},
   "source": [
    "### Confirm that we can see the data on the `dev` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591df18-ba42-47a8-bf1b-38374f0cfe79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM lakefs.dev.nyc.permits;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437088f6",
   "metadata": {},
   "source": [
    "# Making [and reverting] changes on the dev branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdece16-eb56-4459-94e5-9d0da350b235",
   "metadata": {},
   "source": [
    "Let's go big! Let's see what happens when we delete the contents of the table with a careless `DELETE` omitting an all-important predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d066bae-77d2-4ead-a04b-ded869e96ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql DELETE FROM lakefs.dev.nyc.permits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f890ff-3353-4e87-9c5d-8a1f682d4ab1",
   "metadata": {},
   "source": [
    "How's that data looking now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba978c-b4f7-4c7f-9dfa-f51386072f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM lakefs.dev.nyc.permits;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9425b-ce20-44c3-a134-096d0fb9250c",
   "metadata": {},
   "source": [
    "But `main` is safe and unsullied üòå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea4d24-d556-41f6-9ad4-708f603916c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM lakefs.main.nyc.permits;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada3e38-bd04-4639-b7c0-a0edf50b0263",
   "metadata": {},
   "source": [
    "## Reverting changes to the `dev` branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ab7c3-0ddc-44f1-b4f0-6c9343994d5c",
   "metadata": {},
   "source": [
    "### Uncommitted objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a563f-2c69-4615-b4de-f8618504afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repo.id, \"dev\").results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae7c8a-2f9c-4e63-8a47-c3cdddfb7953",
   "metadata": {},
   "source": [
    "### Reset the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf7826-5a4e-4d9c-889d-582d3b918cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.branches.reset_branch(repo.id, \n",
    "                             \"dev\",\n",
    "                             ResetCreation(type=\"common_prefix\", \n",
    "                                           path=\"nyc/permits/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc566732-891b-4f88-9aa1-d8d02a0e98ad",
   "metadata": {},
   "source": [
    "_This just resets the changes to the files for this table. To reset the whole branch use_:\n",
    "\n",
    "```python\n",
    "lakefs.branches.reset_branch(repo.id, \n",
    "                             \"dev\",\n",
    "                             ResetCreation(type=\"reset\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fe4a8-f178-46b3-8f6a-5161f69e6cf4",
   "metadata": {},
   "source": [
    "### Uncommitted objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac271de0-44dd-4aa6-93be-d814cb034111",
   "metadata": {},
   "outputs": [],
   "source": [
    "lakefs.branches.diff_branch(repo.id, \"dev\").results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57d467-fade-415d-ba87-2ac72eb7927e",
   "metadata": {},
   "source": [
    "## Our data's back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f4a09-52ac-439e-993a-f408de64b94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*)\n",
    "FROM lakefs.dev.nyc.permits;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5a375-c3af-42d6-a12a-00ef37469a9d",
   "metadata": {},
   "source": [
    "# Making changes to the `dev` branch as a collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe94d73-cbfa-4570-9736-e82498441cab",
   "metadata": {},
   "source": [
    "## Delete all rows for permits in `Manhattan` from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14843243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql DELETE FROM lakefs.dev.nyc.permits WHERE borough='Manhattan'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891a7c9-d235-408c-be70-25fa701f923f",
   "metadata": {},
   "source": [
    "## Build an aggregate of the data to show how many permits we issued by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22154ae-e74d-4ae2-ad89-496dedfbcfe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE OR REPLACE TABLE lakefs.dev.nyc.agg_permit_category AS\n",
    "SELECT category, count(*) permit_cnt\n",
    "FROM lakefs.dev.nyc.permits\n",
    "GROUP BY category;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068734a-80e9-446a-a6ab-ccbe67f952a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM lakefs.dev.nyc.agg_permit_category LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb688c8",
   "metadata": {},
   "source": [
    "# Compare `main` and `dev`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d6217b-9963-46f6-8311-d1bc9286a889",
   "metadata": {},
   "source": [
    "## `dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd0d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM lakefs.dev.nyc.permits\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85332bf5",
   "metadata": {},
   "source": [
    "## `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df15e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT borough, count(*) permit_cnt\n",
    "FROM lakefs.main.nyc.permits\n",
    "GROUP BY borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa771262-2dde-4ecc-b1a1-3dac9f580cfa",
   "metadata": {},
   "source": [
    "# Commit the changes to the `dev` branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa91847-569b-4547-bcdc-f7ff60e5d6fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.commits.commit(repo.id, \"dev\", \n",
    "                      CommitCreation(\n",
    "                          message=\"Remove data for Manhattan from permits dataset, build category aggregate\",\n",
    "                          metadata={\"etl job name\": \"etl_job_42\",\n",
    "                                    \"author\": \"rmoff\"}\n",
    "                      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17435550-2212-4caf-a17e-d534ad9df31e",
   "metadata": {},
   "source": [
    "# Merge the branch back into `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0375e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lakefs.refs.merge_into_branch(repository=repo.id, \n",
    "                              source_ref=\"dev\", \n",
    "                              destination_branch=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea713c-8489-43e0-9968-9f4fd16e2d49",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2d46-d9f8-4880-a924-aaeb7312ed42",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a77a15-7f39-4707-a364-99451c6fe0f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44dd985-dae3-4521-b0b2-3164ecf9cca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "if lakefsEndPoint=='http://lakefs:8000':\n",
    "    lakeFSWebUI='http://localhost:8000'\n",
    "else:\n",
    "    lakeFSWebUI=lakefsEndPoint\n",
    "\n",
    "md(f\"### üëâüèª View the objects in [lakeFS web UI]({lakeFSWebUI}/repositories/{repo.id}/objects)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
