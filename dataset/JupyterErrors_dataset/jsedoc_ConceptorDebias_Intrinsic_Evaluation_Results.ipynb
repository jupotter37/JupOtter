{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intrinsic Evaluation Results.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/ACL-cleanup/Conceptors/Intrinsic_Evaluation_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "_kdv2Qs76F4T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Post-processing of Word Vectors via Conceptor Negation\n",
        "\n",
        "In this notebook, we presents the experiment results reported in [1]/\n",
        "\n",
        "[1] Unsupervised Post-processing of Word Vectors via Conceptor Negation. Tianlin Liu, Lyle Ungar, and Jo√£o Sedoc, Unsupervised Post-processing of Word Vectors via Conceptor Negation, AAAI 2019.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ibrQUHG_6F4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d8fbbd9-77c3-47b2-ff68-2cba1baa40e1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import tensorflow as tf\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "import functools as ft\n",
        "\n",
        "import numpy as np\n",
        "from itertools import combinations, filterfalse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# resourceFile = '/Users/liutianlin/Desktop/Academics/NLP/data/' \n",
        "resourceFile = '/data/' # the address of the datasets\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "aMXY_zd56F4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "!gdown https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM # download a small subset of glove\n",
        "!gdown https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB   # download a small subset of word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQXzUabN6F4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Word2Vec and GloVe word embeddings\n",
        " \n",
        "We provide a small word2vec and small glove word embedding in this repository -- their words appear at least 200 times in wikipedia (see the list provided by Arora et al https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt"
      ]
    },
    {
      "metadata": {
        "id": "OGmvUn837op4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1091
        },
        "outputId": "20892377-ec96-4a7c-956b-6cfc2e5586a1"
      },
      "cell_type": "code",
      "source": [
        "# our code for debiasing -- also includes word lists\n",
        "!rm -r ConceptorDebias\n",
        "!git clone https://github.com/jsedoc/ConceptorDebias\n",
        "!cd ConceptorDebias; git checkout ACL-cleanup\n",
        "\n",
        "sys.path.append('/content/ConceptorDebias')\n",
        "\n",
        "from Conceptors.conceptor_fxns import *\n",
        "\n",
        "def process_cn_matrix(subspace, alpha = 2):\n",
        "  \"\"\"Returns the conceptor negation matrix\n",
        "  Arguments\n",
        "           subspace : n x d matrix of word vectors from a oarticular subspace\n",
        "           alpha : Tunable parameter\n",
        "  \"\"\"\n",
        "  # Compute the conceptor matrix\n",
        "  C,_ = train_Conceptor(subspace, alpha)\n",
        "  \n",
        "  # Calculate the negation of the conceptor matrix\n",
        "  negC = NOT(C)\n",
        "  \n",
        "  return negC\n",
        "\n",
        "def apply_conceptor(x, C):\n",
        "  \"\"\"Returns the conceptored embeddings\n",
        "  Arguments\n",
        "           x : n x d matrix of all words to be conceptored\n",
        "           C : d x d conceptor matrix\n",
        "  \"\"\"\n",
        "  # Post-process the vocab matrix\n",
        "  newX = (C @ x).T\n",
        "  \n",
        "  return newX\n",
        "\n",
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  \"\"\"Loads all word vectors for all words in the list of words as a matrix\n",
        "  Arguments\n",
        "           embd : Dictonary of word-to-embedding for all words\n",
        "           wikiWordsPath : URL to the path where all embeddings are stored\n",
        "  Returns\n",
        "          all_words_index : Dictonary of words to the row-number of the corresponding word in the matrix\n",
        "          all_words_mat : Matrix of word vectors stored row-wise\n",
        "  \"\"\"\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  \"\"\"Loads all word vectors for the particular subspace in the list of words as a matrix\n",
        "  Arguments\n",
        "           embd : Dictonary of word-to-embedding for all words\n",
        "           subspace_words : List of words representing a particular subspace\n",
        "  Returns\n",
        "          subspace_embd_mat : Matrix of word vectors stored row-wise\n",
        "  \"\"\"\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat\n",
        "\n",
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20190320-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "\n",
        "from lists.load_word_lists import *\n",
        "\n",
        "\"\"\"Load list of pronouns representing the 'Pronoun' subspace for gender debiasing\"\"\"\n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "\"\"\"Load an extended list of words representing the gender subspace for gender debiasing\"\"\"\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "\"\"\"Load list of proper nouns representing the 'Proper Noun' subspace for gender debiasing\"\"\"\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "\"\"\"Load list of all representing the gender subspace for gender debiasing\"\"\"\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "\"\"\"Load list of common black and white names for racial debiasing\"\"\"\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'ConceptorDebias': No such file or directory\n",
            "Cloning into 'ConceptorDebias'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 388 (delta 39), reused 5 (delta 2), pack-reused 308\u001b[K\n",
            "Receiving objects: 100% (388/388), 3.68 MiB | 9.38 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "Branch 'ACL-cleanup' set up to track remote branch 'ACL-cleanup' from 'origin'.\n",
            "Switched to a new branch 'ACL-cleanup'\n",
            "--2019-04-17 04:05:03--  https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20190320-words-frequency.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27465330 (26M) [text/plain]\n",
            "Saving to: ‚Äòenwiki-20190320-words-frequency.txt‚Äô\n",
            "\n",
            "enwiki-20190320-wor 100%[===================>]  26.19M   132MB/s    in 0.2s    \n",
            "\n",
            "2019-04-17 04:05:04 (132 MB/s) - ‚Äòenwiki-20190320-words-frequency.txt‚Äô saved [27465330/27465330]\n",
            "\n",
            "Cloning into 'SIF'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 2.80 MiB | 6.63 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "Cloning into 'gn_glove'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 162 (delta 18), reused 25 (delta 9), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (162/162), 73.36 KiB | 1.22 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "Cloning into 'corefBias'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 471 (delta 3), reused 0 (delta 0), pack-reused 457\u001b[K\n",
            "Receiving objects: 100% (471/471), 84.18 MiB | 28.23 MiB/s, done.\n",
            "Resolving deltas: 100% (273/273), done.\n",
            "--2019-04-17 04:05:16--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35751 (35K) [text/plain]\n",
            "Saving to: ‚Äòfemale.txt‚Äô\n",
            "\n",
            "female.txt          100%[===================>]  34.91K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-04-17 04:05:16 (303 KB/s) - ‚Äòfemale.txt‚Äô saved [35751/35751]\n",
            "\n",
            "--2019-04-17 04:05:17--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20466 (20K) [text/plain]\n",
            "Saving to: ‚Äòmale.txt‚Äô\n",
            "\n",
            "male.txt            100%[===================>]  19.99K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-04-17 04:05:17 (270 KB/s) - ‚Äòmale.txt‚Äô saved [20466/20466]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gIc9TbF-8NdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ab08e831-7b47-4643-e86a-70c7b58eae92"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'Glove' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/gensim_glove.840B.300d.txt.bin ]; then gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "if 'glove' not in dir():\n",
        "  glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "  print('The glove embedding has been loaded!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
            "To: /content/gensim_glove.840B.300d.txt.bin\n",
            "2.65GB [00:47, 55.8MB/s]\n",
            "The glove embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "htHqj0I48Oma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a7e1ab18-a874-41f3-deb2-4ef8a2d845ed"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'Word2Vec' embeddings if not downloaded\"\"\"\n",
        "!if test -e /content/GoogleNews-vectors-negative300.bin.gz || test -e /content/GoogleNews-vectors-negative300.bin; then echo 'file already downloaded'; else echo 'starting download'; gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM; fi\n",
        "!if [ ! -f /content/GoogleNews-vectors-negative300.bin ]; then gunzip GoogleNews-vectors-negative300.bin.gz; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "if 'word2vec' not in dir():\n",
        "  word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "  print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting download\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "1.65GB [00:11, 149MB/s]\n",
            "The word2vec embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8tj2fArb8Q17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "59d975ba-ff52-4510-8575-15531667639f"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'Fasttext' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/fasttext.bin ]; then gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "if 'fasttext' not in dir():\n",
        "  fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
        "  print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh\n",
            "To: /content/fasttext.bin\n",
            "2.42GB [00:41, 58.4MB/s]\n",
            "The fasttext embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sGTlRMRE8kJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "161821dc-697f-4896-f4bd-0bfedf66b9dd"
      },
      "cell_type": "code",
      "source": [
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "\"\"\"Set the embedding to be used\"\"\"\n",
        "embd = 'word2vec'\n",
        "\n",
        "\"\"\"Set the subspace to be tested on\"\"\"\n",
        "subspace = 'gender_list_all' \n",
        "\n",
        "curr_embd = eval(embd)\n",
        "  \n",
        "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
        "if embd == 'elmo':\n",
        "  all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "if embd == 'bert':\n",
        "  all_words_index, all_words_mat = load_bert(all_dict, subspace)\n",
        "else:\n",
        "  all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
        "if subspace != 'without_conceptor':\n",
        "  subspace_words_list = eval(subspace)\n",
        "  if subspace == 'gender_list_and':\n",
        "    if embd == 'elmo':\n",
        "      subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
        "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
        "\n",
        "      subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
        "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
        "\n",
        "      subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
        "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
        "\n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "    elif embd == 'bert':\n",
        "      cn1 = load_bert_conceptor(all_dict, gender_list_pronouns)\n",
        "      \n",
        "      cn2 = load_bert_conceptor(all_dict, gender_list_extended)\n",
        "      \n",
        "      cn3 = load_bert_conceptor(all_dict, gender_list_propernouns)\n",
        "      \n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "    else:\n",
        "      subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
        "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "      subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
        "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "      subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
        "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "  else: \n",
        "    if embd == 'elmo':\n",
        "      subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "      cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
        "    elif embd == 'bert':\n",
        "      cn = load_bert_conceptor(all_dict, subspace)\n",
        "    else:\n",
        "      subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "      cn = process_cn_matrix(np.array(subspace_words_mat).T)\n",
        "      \n",
        "\"\"\"Conceptor all embeddings\"\"\"\n",
        "all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "\n",
        "\"\"\"Store all conceptored words in a dictonary\"\"\"\n",
        "all_words = {}\n",
        "for word, index in all_words_index.items():\n",
        "  if embd == 'elmo':\n",
        "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "  else:\n",
        "    all_words[word] = all_words_cn[index,:]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting...\n",
            "(300, 7117)\n",
            "R calculated\n",
            "C calculated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0lvu0V-6F4c",
        "colab_type": "code",
        "outputId": "ca91baa6-2a82-4a9f-fffa-87e5a5c2b680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "cell_type": "code",
      "source": [
        "def loadWordVecs(model_str):\n",
        "    word_dictionary = {}\n",
        "    \n",
        "    input_file_destination = 'small_' + model_str + '.txt'\n",
        "\n",
        "    f = codecs.open(input_file_destination, 'r', 'utf-8') \n",
        "\n",
        "    for line in f:\n",
        "\n",
        "        line = line.split(\" \", 1)   \n",
        "        transformed_key = line[0].lower()\n",
        "\n",
        "        try:\n",
        "            transformed_key = str(transformed_key)\n",
        "\n",
        "        except:\n",
        "            print(\"Can't convert the key to unicode:\", transformed_key)\n",
        "\n",
        "        word_dictionary[transformed_key] = np.fromstring(line[1], dtype=\"float32\", sep=\" \")\n",
        "\n",
        "        if word_dictionary[transformed_key].shape[0] != 300:\n",
        "            print(transformed_key, word_dictionary[transformed_key].shape)\n",
        "\n",
        "    return  word_dictionary     \n",
        "\n",
        "\n",
        "\n",
        "orig_word2vec = loadWordVecs('word2vec')\n",
        "print(\"loaded Word2vec!\")\n",
        "\n",
        "orig_glove = loadWordVecs('glove')\n",
        "print(\"loaded GloVe Common Crawl!\")\n",
        "\n",
        "\n",
        "orig_model = {}\n",
        "orig_model['word2vec'] = orig_word2vec\n",
        "orig_model['glove'] = orig_glove\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ab85ca65e8b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0morig_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadWordVecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded Word2vec!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ab85ca65e8b1>\u001b[0m in \u001b[0;36mloadWordVecs\u001b[0;34m(model_str)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_file_destination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'small_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_destination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'small_word2vec.txt'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "OyxhJUVw6F4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Post-process Word2Vec and GloVe with Conceptor Negation (CN)\n"
      ]
    },
    {
      "metadata": {
        "id": "nct55Ukt6F4h",
        "colab_type": "code",
        "outputId": "be625ae6-c859-413e-e1d8-993db6d3a80b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ensemble_cn_dict(wordVecModel_str, alpha = 2, orig_model = orig_model):\n",
        "    \n",
        "    \n",
        "    # put the word vectors in columns\n",
        "    x_collector = np.array(list(orig_model[wordVecModel_str].values())).T       \n",
        "        \n",
        "    \n",
        "    nrWords = x_collector.shape[1] # number of total words\n",
        "    \n",
        "    \n",
        "    R = x_collector.dot(x_collector.T) / nrWords # calculate the un-centered correlation matrix\n",
        "    \n",
        "    C = R @ np.linalg.inv(R + alpha ** (-2) * np.eye(300))# calculate the conceptor matrix\n",
        "    \n",
        "    vecMatrix = ((np.eye(300) - C) @ x_collector).T \n",
        "\n",
        "    cn_dict = {}\n",
        "        \n",
        "    for word_index in np.arange(0, len(orig_model[wordVecModel_str].keys())):\n",
        "        \n",
        "        word = list(orig_model[wordVecModel_str].keys())[word_index]\n",
        "        cn_dict[word] = vecMatrix[word_index,:]\n",
        "    \n",
        "    return cn_dict\n",
        "\n",
        "print(\"Post-processing Word2vec with CN\")\n",
        "cn_word2vec = ensemble_cn_dict('word2vec', orig_model = orig_model)\n",
        "\n",
        "print(\"Post-processing GloVe with CN\")\n",
        "cn_glove = ensemble_cn_dict('glove', orig_model = orig_model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post-processing Word2vec with CN\n",
            "Post-processing GloVe with CN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oj8BXC3y6F4j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Word similarity evaluation\n",
        "We evaluate the CN post-processed word vectors with 7 standard word similarity datasets: the RG65 (Rubenstein and Goodenough, 1965), the WordSim-353 (WS) (Finkelstein et al., 2002), the rare- words (RW) (Luong, Socher, and Manning, 2013), the MEN dataset (Bruni, Tran, and Baroni, 2014), the MTurk (Radinsky et al., 2011), the SimLex-999 (SimLex) (Hill, Reichart, and Korhonen, 2015), and the SimVerb-3500 (Gerz et al., 2016). \n",
        "\n",
        "To evaluate the word similarity, we calculate the cosine distance between vectors of two words. We report the Spearman‚Äôs rank correlation coefficient (Myers and Well, 1995) of the estimated rankings against the rankings given by human annotators.\n"
      ]
    },
    {
      "metadata": {
        "id": "5LK5EZ6qTwHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bc17ffa0-260c-42ae-9d5e-34c8e11df780"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mfaruqui/eval-word-vectors\n",
        "\n",
        "!mkdir wordSimData\n",
        "!mv eval-word-vectors/data/word-sim/ wordSimData/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'eval-word-vectors'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "Unpacking objects:   1% (1/54)   \rUnpacking objects:   3% (2/54)   \rUnpacking objects:   5% (3/54)   \rUnpacking objects:   7% (4/54)   \rUnpacking objects:   9% (5/54)   \rUnpacking objects:  11% (6/54)   \rUnpacking objects:  12% (7/54)   \rUnpacking objects:  14% (8/54)   \rUnpacking objects:  16% (9/54)   \rUnpacking objects:  18% (10/54)   \rUnpacking objects:  20% (11/54)   \rUnpacking objects:  22% (12/54)   \rUnpacking objects:  24% (13/54)   \rUnpacking objects:  25% (14/54)   \rUnpacking objects:  27% (15/54)   \rUnpacking objects:  29% (16/54)   \rUnpacking objects:  31% (17/54)   \rUnpacking objects:  33% (18/54)   \rUnpacking objects:  35% (19/54)   \rUnpacking objects:  37% (20/54)   \rUnpacking objects:  38% (21/54)   \rUnpacking objects:  40% (22/54)   \rUnpacking objects:  42% (23/54)   \rUnpacking objects:  44% (24/54)   \rUnpacking objects:  46% (25/54)   \rremote: Total 54 (delta 0), reused 0 (delta 0), pack-reused 54\u001b[K\n",
            "Unpacking objects:  48% (26/54)   \rUnpacking objects:  50% (27/54)   \rUnpacking objects:  51% (28/54)   \rUnpacking objects:  53% (29/54)   \rUnpacking objects:  55% (30/54)   \rUnpacking objects:  57% (31/54)   \rUnpacking objects:  59% (32/54)   \rUnpacking objects:  61% (33/54)   \rUnpacking objects:  62% (34/54)   \rUnpacking objects:  64% (35/54)   \rUnpacking objects:  66% (36/54)   \rUnpacking objects:  68% (37/54)   \rUnpacking objects:  70% (38/54)   \rUnpacking objects:  72% (39/54)   \rUnpacking objects:  74% (40/54)   \rUnpacking objects:  75% (41/54)   \rUnpacking objects:  77% (42/54)   \rUnpacking objects:  79% (43/54)   \rUnpacking objects:  81% (44/54)   \rUnpacking objects:  83% (45/54)   \rUnpacking objects:  85% (46/54)   \rUnpacking objects:  87% (47/54)   \rUnpacking objects:  88% (48/54)   \rUnpacking objects:  90% (49/54)   \rUnpacking objects:  92% (50/54)   \rUnpacking objects:  94% (51/54)   \rUnpacking objects:  96% (52/54)   \rUnpacking objects:  98% (53/54)   \rUnpacking objects: 100% (54/54)   \rUnpacking objects: 100% (54/54), done.\n",
            "mkdir: cannot create directory ‚ÄòwordSimData‚Äô: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "STzbhiYX6F4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
        "\n",
        "\n",
        "\n",
        "def similarity_eval(dataSetAddress, wordVecModel_str, all_word):\n",
        "    wordVecModel = eval(wordVecModel_str)\n",
        "    all_word = eval(all_word)\n",
        "    vocab = set(list(all_word.keys()))\n",
        "    \n",
        "    fread_simlex = open(dataSetAddress, \"r\")\n",
        "    \n",
        "    pair_list = []\n",
        "\n",
        "    line_number = 0\n",
        "    for line in fread_simlex:\n",
        "#         if line_number > 0:\n",
        "        tokens = line.split()\n",
        "        word_i = tokens[0]\n",
        "        word_j = tokens[1]\n",
        "        score = float(tokens[2])\n",
        "        if word_i in vocab and word_j in vocab:\n",
        "            pair_list.append( ((word_i, word_j), score) )\n",
        "#         line_number += 1\n",
        "\n",
        "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
        "\n",
        "\n",
        "    extracted_scores = {}\n",
        "\n",
        "    extracted_list = []\n",
        "    \n",
        "               \n",
        "    for (x,y) in pair_list:\n",
        "        (word_i, word_j) = x\n",
        "        \n",
        "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
        "\n",
        "        extracted_scores[(word_i, word_j)] = current_distance\n",
        "        extracted_list.append(((word_i, word_j), current_distance))\n",
        "\n",
        "    extracted_list.sort(key=lambda x: x[1])\n",
        "\n",
        "    spearman_original_list = []\n",
        "    spearman_target_list = []\n",
        "\n",
        "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
        "        score_2 = extracted_scores[word_pair]\n",
        "        position_2 = extracted_list.index((word_pair, score_2))\n",
        "        spearman_original_list.append(position_1)\n",
        "        spearman_target_list.append(position_2)\n",
        "\n",
        "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
        "    \n",
        "    return spearman_rho[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cCdTpkjsWwKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "81d51b9b-abbe-4c75-a197-b5252b309d22"
      },
      "cell_type": "code",
      "source": [
        "vocab = set(list(glove.wv.vocab))\n",
        "print(vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DITNFo__6F4m",
        "colab_type": "code",
        "outputId": "537c86cc-b65f-4b10-8f3a-64ca30b4afc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "wordSimResult = {}\n",
        "\n",
        "\n",
        "for dataset in dataSets:\n",
        "    dataSetAddress = resourceFile + 'wordSimData/word-sim/' +  dataset\n",
        "    print('evaluating the data set', dataset)\n",
        "    \n",
        "    print('Glove : %.4f' %  similarity_eval(dataSetAddress, 'fasttext', 'all_words_index'))\n",
        "    print('Glove + CN : %.4f' %  similarity_eval(dataSetAddress, 'all_words', 'all_words'))\n",
        "        \n",
        "    print('\\n')\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluating the data set EN-RG-65.txt\n",
            "Glove : 0.8587\n",
            "Glove + CN : 0.8594\n",
            "\n",
            "\n",
            "evaluating the data set EN-WS-353-ALL.txt\n",
            "Glove : 0.7882\n",
            "Glove + CN : 0.7744\n",
            "\n",
            "\n",
            "evaluating the data set EN-RW-STANFORD.txt\n",
            "Glove : 0.6217\n",
            "Glove + CN : 0.6248\n",
            "\n",
            "\n",
            "evaluating the data set EN-MEN-TR-3k.txt\n",
            "Glove : 0.8364\n",
            "Glove + CN : 0.8264\n",
            "\n",
            "\n",
            "evaluating the data set EN-MTurk-287.txt\n",
            "Glove : 0.7245\n",
            "Glove + CN : 0.7134\n",
            "\n",
            "\n",
            "evaluating the data set EN-SIMLEX-999.txt\n",
            "Glove : 0.5055\n",
            "Glove + CN : 0.5078\n",
            "\n",
            "\n",
            "evaluating the data set EN-SimVerb-3500.txt\n",
            "Glove : 0.4275\n",
            "Glove + CN : 0.4272\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "MnPSOAFD6F4p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment 2:  Semantic Textual Similarity (STS) tasks\n",
        "\n",
        "We use standard semantic textual similarity (STS) benchmarks to evaluate the post-processed word vectors: we use 2012-2015 SemEval STS tasks (Agirre et al., 2012, 2013, 2014, 2015) and 2012 SemEval Semantic Related task (SICK) (Marelli et al., 2014). \n",
        "\n",
        "We reuse the codes provided in  https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "y7hYLax76F4q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load datasets"
      ]
    },
    {
      "metadata": {
        "id": "UHa17No4oynx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "ebee3845-48b5-4edb-c3a5-32599b00b5b2"
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-dev.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-mt.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-other.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-test.csv\n",
        "!wget https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-train.csv"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-17 04:43:51--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-dev.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 255680 (250K) [text/plain]\n",
            "Saving to: ‚Äòsts-dev.csv‚Äô\n",
            "\n",
            "\rsts-dev.csv           0%[                    ]       0  --.-KB/s               \rsts-dev.csv         100%[===================>] 249.69K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-04-17 04:43:51 (9.67 MB/s) - ‚Äòsts-dev.csv‚Äô saved [255680/255680]\n",
            "\n",
            "--2019-04-17 04:43:52--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-mt.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 513141 (501K) [text/plain]\n",
            "Saving to: ‚Äòsts-mt.csv‚Äô\n",
            "\n",
            "sts-mt.csv          100%[===================>] 501.11K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-04-17 04:43:52 (14.0 MB/s) - ‚Äòsts-mt.csv‚Äô saved [513141/513141]\n",
            "\n",
            "--2019-04-17 04:43:54--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-other.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 661674 (646K) [text/plain]\n",
            "Saving to: ‚Äòsts-other.csv‚Äô\n",
            "\n",
            "sts-other.csv       100%[===================>] 646.17K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-04-17 04:43:54 (14.2 MB/s) - ‚Äòsts-other.csv‚Äô saved [661674/661674]\n",
            "\n",
            "--2019-04-17 04:43:57--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 282419 (276K) [text/plain]\n",
            "Saving to: ‚Äòsts-test.csv‚Äô\n",
            "\n",
            "sts-test.csv        100%[===================>] 275.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-04-17 04:43:57 (8.99 MB/s) - ‚Äòsts-test.csv‚Äô saved [282419/282419]\n",
            "\n",
            "--2019-04-17 04:43:59--  https://raw.githubusercontent.com/liutianlin0121/Conceptor-Negation-WV/master/data/stsbenchmark/sts-train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 901177 (880K) [text/plain]\n",
            "Saving to: ‚Äòsts-train.csv‚Äô\n",
            "\n",
            "sts-train.csv       100%[===================>] 880.06K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-04-17 04:43:59 (17.0 MB/s) - ‚Äòsts-train.csv‚Äô saved [901177/901177]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yr09h51R6F4r",
        "colab_type": "code",
        "outputId": "446d58df-d57b-427d-d017-94859caaee59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "def load_sts_dataset(filename):\n",
        "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
        "    sent_pairs = []\n",
        "    with tf.gfile.GFile(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            ts = line.strip().split(\"\\t\")\n",
        "            if len(ts) == 7 or len(ts) == 9:\n",
        "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[2]) + '-' + ts[1] , ts[5], ts[6], float(ts[4])))\n",
        "            elif len(ts) == 6 or len(ts) == 8:\n",
        "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[1]) + '-' + ts[0] , ts[4], ts[5], float(ts[3])))\n",
        "            else:\n",
        "                print('data format is wrong!!!')\n",
        "    return pd.DataFrame(sent_pairs, columns=[\"year-task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "def load_all_sts_dataset():\n",
        "    # Loads all of the STS datasets \n",
        "    stsbenchmarkDir = ''\n",
        "    stscompanionDir = ''\n",
        "    sts_train = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-train.csv\"))    \n",
        "    sts_dev = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-dev.csv\"))\n",
        "    sts_test = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-test.csv\"))\n",
        "    sts_other = load_sts_dataset(os.path.join(stscompanionDir, \"sts-other.csv\"))\n",
        "    sts_mt = load_sts_dataset(os.path.join(stscompanionDir, \"sts-mt.csv\"))\n",
        "    \n",
        "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
        "    \n",
        "    return sts_all\n",
        "\n",
        "sts_all = load_all_sts_dataset()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_sts_by_year_task():\n",
        "    # Divide STS datasets based on their year and tasks\n",
        "    sts_by_year_task = {}\n",
        "    \n",
        "    for year_task in sts_all['year-task'].unique():\n",
        "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x == year_task]\n",
        "        \n",
        "        pairs = sts_all.iloc[indices]\n",
        "        \n",
        "        sts_by_year_task[year_task] = pairs\n",
        "        \n",
        "    return sts_by_year_task\n",
        "\n",
        "sts_by_year_task = load_sts_by_year_task()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_sts_by_year():\n",
        "    # Divide STS datasets ONLY based on their year (different tasks in that year are merged).\n",
        "\n",
        "    sts_by_year = {}\n",
        "    \n",
        "    for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
        "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x.startswith(year)]\n",
        "        \n",
        "        pairs = sts_all.iloc[indices]\n",
        "        pairs = pairs.copy()\n",
        "        pairs['year-task'] = year\n",
        "        sts_by_year[year] = pairs\n",
        "        \n",
        "    return sts_by_year\n",
        "\n",
        "sts_by_year_task = load_sts_by_year_task()\n",
        "\n",
        "sts_by_year = load_sts_by_year()\n",
        "\n",
        "\n",
        "# filename = resourceFile + '2015-answers-students.test.tsv'\n",
        "# sent_pairs = []\n",
        "# with tf.gfile.GFile(filename, \"r\") as f:\n",
        "#     for line in f:\n",
        "#         ts = line.strip().split(\"\\t\")\n",
        "#         if len(ts) == 3:\n",
        "#             sent_pairs.append((ts[1], ts[2], float(ts[0])))\n",
        "# answers_students_2015 =  pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
        "\n",
        "\n",
        "# show some sample sts data    \n",
        "sts_all[:5] \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year-task</th>\n",
              "      <th>sent_1</th>\n",
              "      <th>sent_2</th>\n",
              "      <th>sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012-MSRvid</td>\n",
              "      <td>A plane is taking off.</td>\n",
              "      <td>An air plane is taking off.</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-MSRvid</td>\n",
              "      <td>A man is playing a large flute.</td>\n",
              "      <td>A man is playing a flute.</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012-MSRvid</td>\n",
              "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
              "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-MSRvid</td>\n",
              "      <td>Three men are playing chess.</td>\n",
              "      <td>Two men are playing chess.</td>\n",
              "      <td>2.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2012-MSRvid</td>\n",
              "      <td>A man is playing the cello.</td>\n",
              "      <td>A man seated is playing the cello.</td>\n",
              "      <td>4.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     year-task                                         sent_1  \\\n",
              "0  2012-MSRvid                         A plane is taking off.   \n",
              "1  2012-MSRvid                A man is playing a large flute.   \n",
              "2  2012-MSRvid  A man is spreading shreded cheese on a pizza.   \n",
              "3  2012-MSRvid                   Three men are playing chess.   \n",
              "4  2012-MSRvid                    A man is playing the cello.   \n",
              "\n",
              "                                              sent_2   sim  \n",
              "0                        An air plane is taking off.  5.00  \n",
              "1                          A man is playing a flute.  3.80  \n",
              "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
              "3                         Two men are playing chess.  2.60  \n",
              "4                 A man seated is playing the cello.  4.25  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "MG1Dgu1x6F4t",
        "colab_type": "code",
        "outputId": "23476e73-c092-4b3a-f8bb-b9c9da2bea60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def download_sick(f): \n",
        "\n",
        "    response = requests.get(f).text\n",
        "\n",
        "    lines = response.split(\"\\n\")[1:]\n",
        "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
        "    lines = [l for l in lines if len(l) == 5]\n",
        "\n",
        "    df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
        "    df['sim'] = pd.to_numeric(df['sim'])\n",
        "    return df\n",
        "    \n",
        "sick_all = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")\n",
        "\n",
        "sick_all[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>sent_1</th>\n",
              "      <th>sent_2</th>\n",
              "      <th>sim</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>There is no boy playing outdoors and there is ...</td>\n",
              "      <td>A group of kids is playing in a yard and an ol...</td>\n",
              "      <td>3.300</td>\n",
              "      <td>NEUTRAL\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>A group of boys in a yard is playing and a man...</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>3.700</td>\n",
              "      <td>NEUTRAL\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>A group of children is playing in the house an...</td>\n",
              "      <td>The young boys are playing outdoors and the ma...</td>\n",
              "      <td>3.000</td>\n",
              "      <td>NEUTRAL\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>4.900</td>\n",
              "      <td>ENTAILMENT\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>A brown dog is attacking another animal in fro...</td>\n",
              "      <td>A brown dog is helping another animal in front...</td>\n",
              "      <td>3.665</td>\n",
              "      <td>NEUTRAL\\r</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  idx                                             sent_1  \\\n",
              "0   6  There is no boy playing outdoors and there is ...   \n",
              "1   7  A group of boys in a yard is playing and a man...   \n",
              "2   8  A group of children is playing in the house an...   \n",
              "3  10  A brown dog is attacking another animal in fro...   \n",
              "4  11  A brown dog is attacking another animal in fro...   \n",
              "\n",
              "                                              sent_2    sim         label  \n",
              "0  A group of kids is playing in a yard and an ol...  3.300     NEUTRAL\\r  \n",
              "1  The young boys are playing outdoors and the ma...  3.700     NEUTRAL\\r  \n",
              "2  The young boys are playing outdoors and the ma...  3.000     NEUTRAL\\r  \n",
              "3  A brown dog is attacking another animal in fro...  4.900  ENTAILMENT\\r  \n",
              "4  A brown dog is helping another animal in front...  3.665     NEUTRAL\\r  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "bZehJIX_6F4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Some preparation for STS evaluation"
      ]
    },
    {
      "metadata": {
        "id": "ZZBsKDqg6F4x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class Sentence:\n",
        "    \n",
        "    def __init__(self, sentence):\n",
        "        self.raw = sentence\n",
        "        normalized_sentence = sentence.replace(\"‚Äò\", \"'\").replace(\"‚Äô\", \"'\")\n",
        "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
        "        \n",
        "def run_conceptor_benchmark(sentences1, sentences2, model_str): \n",
        "    \n",
        "    model = eval(model_str)\n",
        "    embeddings = []\n",
        "\n",
        "\n",
        "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
        "\n",
        "        tokens1 =  sent1.tokens\n",
        "        tokens2 =  sent2.tokens\n",
        "\n",
        "        tokens1 = [token for token in tokens1 if token in model and token.islower()]\n",
        "        tokens2 = [token for token in tokens2 if token in model and token.islower()]\n",
        "\n",
        "        embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
        "        embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "        if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
        "            embeddings.append(np.zeros(300))\n",
        "            embeddings.append(np.zeros(300))\n",
        "        else:\n",
        "            embeddings.append(embedding1)\n",
        "            embeddings.append(embedding2)\n",
        "\n",
        "\n",
        "\n",
        "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), embeddings[idx*2+1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings)/2))]\n",
        "    return sims\n",
        "\n",
        "def run_experiment(df, benchmarks): \n",
        "    \n",
        "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
        "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
        "    \n",
        "    pearson_cors, spearman_cors = [], []\n",
        "    for label, method in benchmarks:\n",
        "        sims = method(sentences1, sentences2)\n",
        "        pearson_correlation = round(scipy.stats.pearsonr(sims, df['sim'])[0] * 100,2)\n",
        "        #print(label, pearson_correlation)\n",
        "        pearson_cors.append(pearson_correlation)\n",
        "        \n",
        "    return pearson_cors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NV3JhYF66F4z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Do STS evaluation\n",
        "\n",
        "Note that results below are a bit different from what has been reported in the appendix of our paper because we are using a small word2vec for demonstration purpose here -- there are more out-of-vocabulary words. "
      ]
    },
    {
      "metadata": {
        "id": "ENVQoDN1rF0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "eb6c34b9-44d9-4333-d8f3-fdd35497c2b9"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "A5QcWPTn6F40",
        "colab_type": "code",
        "outputId": "0bea2ae0-a794-404f-fca6-e00aadc489f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "benchmarks = [(\"Word2vec\", ft.partial(run_conceptor_benchmark, model_str= 'word2vec')),    \n",
        "             (\"CN-Word2vec\", ft.partial(run_conceptor_benchmark, model_str= 'all_words'))]\n",
        "\n",
        "pearson_results_year_task = {}\n",
        "\n",
        "for year_task in sts_all['year-task'].unique():\n",
        "    print('STS-' + year_task)\n",
        "    pearson_results_year_task['STS-' + year_task] = run_experiment(sts_by_year_task[year_task], benchmarks)  \n",
        "    \n",
        "pearson_results_year_task['SICK'] = run_experiment(sick_all, benchmarks) \n",
        "# pearson_results_year_task['TWITTER'] = run_experiment(twitter_all, benchmarks) \n",
        "\n",
        "# pearson_results_year_task['2015-answers_students'] = run_experiment(answers_students_2015, benchmarks) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STS-2012-MSRvid\n",
            "STS-2014-images\n",
            "STS-2015-images\n",
            "STS-2014-deft-forum\n",
            "STS-2012-MSRpar\n",
            "STS-2014-deft-news\n",
            "STS-2013-headlines\n",
            "STS-2014-headlines\n",
            "STS-2015-headlines\n",
            "STS-2016-headlines\n",
            "STS-2017-track5.en-en\n",
            "STS-2015-answers-forums\n",
            "STS-2016-answer-answer\n",
            "STS-2012-surprise.OnWN\n",
            "STS-2013-FNWN\n",
            "STS-2013-OnWN\n",
            "STS-2014-OnWN\n",
            "STS-2014-tweet-news\n",
            "STS-2015-belief\n",
            "STS-2016-plagiarism\n",
            "STS-2016-question-question\n",
            "STS-2012-SMTeuroparl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:392: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:392: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "STS-2012-surprise.SMTnews\n",
            "STS-2016-postediting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ye8UYDKD6F46",
        "colab_type": "code",
        "outputId": "69776cb6-ef85-4370-f966-96aad8d2b0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.rcParams['figure.figsize'] = (10,5)\n",
        "\n",
        "pearson_results_year_task_df = pd.DataFrame(pearson_results_year_task)\n",
        "pearson_results_year_task_df = pearson_results_year_task_df.transpose()\n",
        "pearson_results_year_task_df = pearson_results_year_task_df.rename(columns={i:b[0] for i, b in enumerate(benchmarks)})\n",
        "\n",
        "pearson_results_year_task_df.reindex(['STS-2012-MSRpar', 'STS-2012-MSRvid', 'STS-2012-surprise.OnWN', 'STS-2012-SMTeuroparl', 'STS-2012-surprise.SMTnews','STS-2013-FNWN', 'STS-2013-OnWN', 'STS-2013-headlines',  'STS-2014-OnWN', 'STS-2014-deft-forum','STS-2014-deft-news', 'STS-2014-headlines', 'STS-2014-tweet-news',  'STS-2014-images', 'STS-2015-answers-forums', '2015-answers_students', 'STS-2015-belief',  'STS-2015-headlines', 'STS-2015-images', 'SICK'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word2vec</th>\n",
              "      <th>CN-Word2vec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>STS-2012-MSRpar</th>\n",
              "      <td>41.61</td>\n",
              "      <td>41.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2012-MSRvid</th>\n",
              "      <td>76.44</td>\n",
              "      <td>75.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2012-surprise.OnWN</th>\n",
              "      <td>70.85</td>\n",
              "      <td>70.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2012-SMTeuroparl</th>\n",
              "      <td>31.48</td>\n",
              "      <td>32.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2012-surprise.SMTnews</th>\n",
              "      <td>53.25</td>\n",
              "      <td>53.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2013-FNWN</th>\n",
              "      <td>40.92</td>\n",
              "      <td>40.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2013-OnWN</th>\n",
              "      <td>68.17</td>\n",
              "      <td>68.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2013-headlines</th>\n",
              "      <td>64.71</td>\n",
              "      <td>64.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2014-OnWN</th>\n",
              "      <td>75.09</td>\n",
              "      <td>75.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2014-deft-forum</th>\n",
              "      <td>40.12</td>\n",
              "      <td>41.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2014-deft-news</th>\n",
              "      <td>67.19</td>\n",
              "      <td>66.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2014-headlines</th>\n",
              "      <td>61.12</td>\n",
              "      <td>60.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2014-tweet-news</th>\n",
              "      <td>73.28</td>\n",
              "      <td>73.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2014-images</th>\n",
              "      <td>77.47</td>\n",
              "      <td>77.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2015-answers-forums</th>\n",
              "      <td>51.23</td>\n",
              "      <td>53.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-answers_students</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2015-belief</th>\n",
              "      <td>60.06</td>\n",
              "      <td>61.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2015-headlines</th>\n",
              "      <td>68.65</td>\n",
              "      <td>68.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STS-2015-images</th>\n",
              "      <td>80.22</td>\n",
              "      <td>79.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SICK</th>\n",
              "      <td>72.71</td>\n",
              "      <td>72.24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Word2vec  CN-Word2vec\n",
              "STS-2012-MSRpar               41.61        41.00\n",
              "STS-2012-MSRvid               76.44        75.61\n",
              "STS-2012-surprise.OnWN        70.85        70.85\n",
              "STS-2012-SMTeuroparl          31.48        32.61\n",
              "STS-2012-surprise.SMTnews     53.25        53.20\n",
              "STS-2013-FNWN                 40.92        40.68\n",
              "STS-2013-OnWN                 68.17        68.74\n",
              "STS-2013-headlines            64.71        64.34\n",
              "STS-2014-OnWN                 75.09        75.52\n",
              "STS-2014-deft-forum           40.12        41.88\n",
              "STS-2014-deft-news            67.19        66.24\n",
              "STS-2014-headlines            61.12        60.29\n",
              "STS-2014-tweet-news           73.28        73.14\n",
              "STS-2014-images               77.47        77.16\n",
              "STS-2015-answers-forums       51.23        53.64\n",
              "2015-answers_students           NaN          NaN\n",
              "STS-2015-belief               60.06        61.06\n",
              "STS-2015-headlines            68.65        68.02\n",
              "STS-2015-images               80.22        79.69\n",
              "SICK                          72.71        72.24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "K5rviiop6F48",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment 3: Concept Categorization\n",
        "\n",
        "\n",
        "In the concept categorization task, we used k-means to cluster words into concept cate- gories based on their vector representations (for example, ‚Äúbear‚Äù and ‚Äúcat‚Äù belong to the concept category of animals). We use three standard datasets: (i) a rather small dataset ESSLLI 2008 (Baroni, Evert, and Lenci, 2008) that contains 44 concepts in 9 categories; (ii) the Almuhareb-Poesio (AP) (Poesio and Almuhareb, 2005), which contains 402 concepts divided into 21 categories; and (iii) the BM dataset (Bat- tig and Montague, 1969) that 5321 concepts divided into 56 categories. Note that the datasets of ESSLLI, AP, and BM are increasingly challenging for clustering algorithms, due to the increasing numbers of words and categories.\n"
      ]
    },
    {
      "metadata": {
        "id": "BnBoPoEJ6F49",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_purity(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate purity for given true and predicted cluster labels.\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: array, shape: (n_samples, 1)\n",
        "      True cluster labels\n",
        "    y_pred: array, shape: (n_samples, 1)\n",
        "      Cluster assingment.\n",
        "    Returns\n",
        "    -------\n",
        "    purity: float\n",
        "      Calculated purity.\n",
        "    \"\"\"\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    true_clusters = np.zeros(shape=(len(set(y_true)), len(y_true)))\n",
        "    pred_clusters = np.zeros_like(true_clusters)\n",
        "    for id, cl in enumerate(set(y_true)):\n",
        "        true_clusters[id] = (y_true == cl).astype(\"int\")\n",
        "    for id, cl in enumerate(set(y_pred)):\n",
        "        pred_clusters[id] = (y_pred == cl).astype(\"int\")\n",
        "\n",
        "    M = pred_clusters.dot(true_clusters.T)\n",
        "    return 1. / len(y_true) * np.sum(np.max(M, axis=1))\n",
        "\n",
        "def evaluateCategorization(thisDict_str, testDataset_csv, method = 'fixed'):\n",
        "    \n",
        "    categorizationFile = resourceFile + 'word-categorization/monolingual/en/' + testDataset_csv\n",
        "\n",
        "    \n",
        "    thisDict = eval(thisDict_str)\n",
        "    modelVocab = list(thisDict.keys())\n",
        "\n",
        "    categorty_list = []\n",
        "    word_list = []\n",
        "\n",
        "    with open(categorizationFile, newline='') as csvfile:\n",
        "        next(csvfile)\n",
        "        reader = csv.reader(csvfile, quotechar='|')\n",
        "        for row in reader:\n",
        "            if len(row[2]) != 0 and row[2] in modelVocab:\n",
        "                categorty_list.append(row[1])\n",
        "                word_list.append(row[2])\n",
        "\n",
        "\n",
        "    wordVectorsMat = np.array([thisDict[word] for word in word_list])\n",
        "\n",
        "    initCentroids = []\n",
        "    for category in set(categorty_list):\n",
        "        indicesCategory = [i for i in range(len(categorty_list)) if categorty_list[i]== category]\n",
        "        initCentroid = np.mean(wordVectorsMat[indicesCategory, :], axis = 0)\n",
        "        initCentroids.append(initCentroid)\n",
        "\n",
        "    initCentroids = np.array(initCentroids)\n",
        "\n",
        "    if method == 'fixed':\n",
        "    \n",
        "        predClusters = KMeans(init = initCentroids, n_clusters=len(set(categorty_list))).fit_predict(wordVectorsMat)\n",
        "        purity = calculate_purity(np.array(categorty_list), predClusters)\n",
        "\n",
        "    else:\n",
        "        \n",
        "        predClusters = KMeans(n_init=10000, n_clusters=len(set(categorty_list))).fit_predict(wordVectorsMat)\n",
        "        purity= calculate_purity(np.array(categorty_list), predClusters)\n",
        "        \n",
        "        \n",
        "    return purity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V-z7fNF6F5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wordVecBrands_methods = ['cn_word2vec', 'cn_glove'] \n",
        "csvFile = ['battig.csv', 'ap.csv', 'essli-2008.csv']\n",
        "\n",
        "\n",
        "c = list(itertools.product(csvFile, wordVecBrands_methods))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "04a0TLCA6F5E",
        "colab_type": "code",
        "outputId": "99ee47dd-8c99-426b-87d7-9b22690a1c63",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_purity = []\n",
        "for (csvFile, wordVecBrand_method) in c:\n",
        "    print(wordVecBrand_method + '-' + csvFile)\n",
        "    print(round(evaluateCategorization(wordVecBrand_method, csvFile) * 100,2) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cn_word2vec-battig.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/Users/liutianlin/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:896: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
            "  return_n_iter=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60.19\n",
            "cn_glove-battig.csv\n",
            "67.63\n",
            "cn_word2vec-ap.csv\n",
            "89.31\n",
            "cn_glove-ap.csv\n",
            "90.95\n",
            "cn_word2vec-essli-2008.csv\n",
            "100.0\n",
            "cn_glove-essli-2008.csv\n",
            "100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n-0unXpT6F5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eb95PgVi6F5L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PlILLvyq6F5N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}