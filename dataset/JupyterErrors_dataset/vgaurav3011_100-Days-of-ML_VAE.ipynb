{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPxK/Z9JM/+jgQvDSKZHXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgaurav3011/100-Days-of-ML/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhQo3UzcZZIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3rybzrEbDLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "30e355ad-00ef-4017-fc95-fde5dd9d40b7"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
        "n_samples = mnist.train.num_examples"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0b05e0e5868b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MNIST_data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-zYPj8bbNLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xavier_init(fan_in, fan_out, constant=1): \n",
        "    \"\"\" Xavier initialization of network weights\"\"\"\n",
        "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
        "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
        "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
        "    return tf.random_uniform((fan_in, fan_out), \n",
        "                             minval=low, maxval=high, \n",
        "                             dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCuS6e0PoUmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(object):\n",
        "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
        "    \n",
        "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
        "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
        "    end-to-end.\n",
        "    \n",
        "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
        "    \"\"\"\n",
        "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
        "                 learning_rate=0.001, batch_size=100):\n",
        "        self.network_architecture = network_architecture\n",
        "        self.transfer_fct = transfer_fct\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # tf Graph input\n",
        "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
        "        \n",
        "        # Create autoencoder network\n",
        "        self._create_network()\n",
        "        # Define loss function based variational upper-bound and \n",
        "        # corresponding optimizer\n",
        "        self._create_loss_optimizer()\n",
        "        \n",
        "        # Initializing the tensor flow variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        # Launch the session\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(init)\n",
        "    \n",
        "    def _create_network(self):\n",
        "        # Initialize autoencode network weights and biases\n",
        "        network_weights = self._initialize_weights(**self.network_architecture)\n",
        "\n",
        "        # Use recognition network to determine mean and \n",
        "        # (log) variance of Gaussian distribution in latent\n",
        "        # space\n",
        "        self.z_mean, self.z_log_sigma_sq = \\\n",
        "            self._recognition_network(network_weights[\"weights_recog\"], \n",
        "                                      network_weights[\"biases_recog\"])\n",
        "\n",
        "        # Draw one sample z from Gaussian distribution\n",
        "        n_z = self.network_architecture[\"n_z\"]\n",
        "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
        "                               dtype=tf.float32)\n",
        "        # z = mu + sigma*epsilon\n",
        "        self.z = tf.add(self.z_mean, \n",
        "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
        "\n",
        "        # Use generator to determine mean of\n",
        "        # Bernoulli distribution of reconstructed input\n",
        "        self.x_reconstr_mean = \\\n",
        "            self._generator_network(network_weights[\"weights_gener\"],\n",
        "                                    network_weights[\"biases_gener\"])\n",
        "            \n",
        "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
        "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
        "                            n_input, n_z):\n",
        "        all_weights = dict()\n",
        "        all_weights['weights_recog'] = {\n",
        "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
        "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
        "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
        "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
        "        all_weights['biases_recog'] = {\n",
        "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
        "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
        "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
        "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
        "        all_weights['weights_gener'] = {\n",
        "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
        "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
        "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
        "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
        "        all_weights['biases_gener'] = {\n",
        "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
        "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
        "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
        "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
        "        return all_weights\n",
        "            \n",
        "    def _recognition_network(self, weights, biases):\n",
        "        # Generate probabilistic encoder (recognition network), which\n",
        "        # maps inputs onto a normal distribution in latent space.\n",
        "        # The transformation is parametrized and can be learned.\n",
        "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
        "                                           biases['b1'])) \n",
        "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
        "                                           biases['b2'])) \n",
        "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
        "                        biases['out_mean'])\n",
        "        z_log_sigma_sq = \\\n",
        "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
        "                   biases['out_log_sigma'])\n",
        "        return (z_mean, z_log_sigma_sq)\n",
        "\n",
        "    def _generator_network(self, weights, biases):\n",
        "        # Generate probabilistic decoder (decoder network), which\n",
        "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
        "        # The transformation is parametrized and can be learned.\n",
        "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
        "                                           biases['b1'])) \n",
        "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
        "                                           biases['b2'])) \n",
        "        x_reconstr_mean = \\\n",
        "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
        "                                 biases['out_mean']))\n",
        "        return x_reconstr_mean\n",
        "            \n",
        "    def _create_loss_optimizer(self):\n",
        "        # The loss is composed of two terms:\n",
        "        # 1.) The reconstruction loss (the negative log probability\n",
        "        #     of the input under the reconstructed Bernoulli distribution \n",
        "        #     induced by the decoder in the data space).\n",
        "        #     This can be interpreted as the number of \"nats\" required\n",
        "        #     for reconstructing the input when the activation in latent\n",
        "        #     is given.\n",
        "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
        "        reconstr_loss = \\\n",
        "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
        "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
        "                           1)\n",
        "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
        "        ##    between the distribution in latent space induced by the encoder on \n",
        "        #     the data and some prior. This acts as a kind of regularizer.\n",
        "        #     This can be interpreted as the number of \"nats\" required\n",
        "        #     for transmitting the the latent space distribution given\n",
        "        #     the prior.\n",
        "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
        "                                           - tf.square(self.z_mean) \n",
        "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
        "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
        "        # Use ADAM optimizer\n",
        "        self.optimizer = \\\n",
        "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
        "        \n",
        "    def partial_fit(self, X):\n",
        "        \"\"\"Train model based on mini-batch of input data.\n",
        "        \n",
        "        Return cost of mini-batch.\n",
        "        \"\"\"\n",
        "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
        "                                  feed_dict={self.x: X})\n",
        "        return cost\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
        "        # Note: This maps to mean of distribution, we could alternatively\n",
        "        # sample from Gaussian distribution\n",
        "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
        "    \n",
        "    def generate(self, z_mu=None):\n",
        "        \"\"\" Generate data by sampling from latent space.\n",
        "        \n",
        "        If z_mu is not None, data for this point in latent space is\n",
        "        generated. Otherwise, z_mu is drawn from prior in latent \n",
        "        space.        \n",
        "        \"\"\"\n",
        "        if z_mu is None:\n",
        "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
        "        # Note: This maps to mean of distribution, we could alternatively\n",
        "        # sample from Gaussian distribution\n",
        "        return self.sess.run(self.x_reconstr_mean, \n",
        "                             feed_dict={self.z: z_mu})\n",
        "    \n",
        "    def reconstruct(self, X):\n",
        "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
        "        return self.sess.run(self.x_reconstr_mean, \n",
        "                             feed_dict={self.x: X})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c2JRE6PiXSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(network_architecture, learning_rate=0.001,\n",
        "          batch_size=100, training_epochs=10, display_step=5):\n",
        "    vae = VariationalAutoencoder(network_architecture, \n",
        "                                 learning_rate=learning_rate, \n",
        "                                 batch_size=batch_size)\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(n_samples / batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
        "\n",
        "            # Fit training using batch data\n",
        "            cost = vae.partial_fit(batch_xs)\n",
        "            # Compute average loss\n",
        "            avg_cost += cost / n_samples * batch_size\n",
        "\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
        "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "    return vae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gENAruwziZm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "8a31c60b-2cac-42a1-c549-0848963b0da5"
      },
      "source": [
        "network_architecture = \\\n",
        "    dict(n_hidden_recog_1=500, # 1st layer encoder neurons\n",
        "         n_hidden_recog_2=500, # 2nd layer encoder neurons\n",
        "         n_hidden_gener_1=500, # 1st layer decoder neurons\n",
        "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
        "         n_input=784, # MNIST data input (img shape: 28*28)\n",
        "         n_z=20)  # dimensionality of latent space\n",
        "\n",
        "vae = train(network_architecture, training_epochs=75)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-9c366d8f5834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m          n_z=20)  # dimensionality of latent space\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-5a7bf82ab7dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network_architecture, learning_rate, batch_size, training_epochs, display_step)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mavg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtotal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Loop over all batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmkczdSbibvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}