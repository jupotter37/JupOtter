{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48596917",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24708c93",
   "metadata": {},
   "source": [
    "# 1.Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf02b9",
   "metadata": {},
   "source": [
    "In this notebook, we want to provide a tutorial about how to use the Hierarchical Parameter Server(HPS) backend to look up the embedding keys for inference service, and combine with pytorch and TernsorRT Triton backend "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439614f2",
   "metadata": {},
   "source": [
    "1. <a href='#section1'>Overview</a> \n",
    "2. [Generate sythetic datasets to train native Pytorch DNN model and deploy the pytorch model using pytorch triton backend](#section2) \n",
    "3. [Separate the trained DNN model graph into two, embedding lookup and dense model graph](#section3)  \n",
    "    3.1 [Deploy dense part model using pytorch Triton backend](#section3.1)  \n",
    "    3.2 [Deploy the embedding part using HPS Triton Backend](#section3.2)   \n",
    "    3.3 [Configure \"ensemble_model\" Triton backend for Embedding and Dense model](#section3.3) \n",
    "4. <a href='#section4'>Use TensorRT to speed up dense model inference and combine with HPS Backend</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb14d1",
   "metadata": {},
   "source": [
    "<a id='#section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dff752",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cbaaf-d9f9-40ed-83b0-44b8e56bed4a",
   "metadata": {},
   "source": [
    "# 2. Train Pytorch DNN Model Based on Sythetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffdcbc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bcc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "# define model training settings\n",
    "args[\"gpu_num\"] = 4                               # the number of available GPUs\n",
    "args[\"num_sample\"] = 64                           # the number of training sample\n",
    "args[\"iter_num\"] = 20                             # the number of training iteration\n",
    "args[\"embed_vec_size\"] = 32                       # the dimension of embedding vectors\n",
    "args[\"global_batch_size\"] = 32                 # the globally batchsize for all GPUs\n",
    "args[\"max_vocabulary_size\"] = 1000              # the num of embeddings in embedding table\n",
    "args[\"vocabulary_range_per_slot\"] = [[0,1000]]  # the range of embedding keys in embedding table\n",
    "# define data type\n",
    "args[\"np_key_type\"]    = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"]    = torch.int64\n",
    "args[\"tf_vector_type\"] = torch.float32\n",
    "\n",
    "# GPU environment configuration for model training\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da11c6",
   "metadata": {},
   "source": [
    "## 2.1 Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae1b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, key_dtype = np.int64):\n",
    "    \"\"\"\n",
    "    Data generator\n",
    "    \n",
    "    Returns a randomly generated set of values for keys and labels\n",
    "    \"\"\"\n",
    "    keys = list()\n",
    "    for vocab_range in vocabulary_range_per_slot:\n",
    "        keys_per_slot = np.random.randint(low=vocab_range[0], \n",
    "                                          high=vocab_range[1], \n",
    "                                          size=(num_samples, 1), \n",
    "                                          dtype=key_dtype)\n",
    "        keys.append(keys_per_slot)\n",
    "    keys = np.concatenate(np.array(keys), axis = 1)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return keys, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30d3ad",
   "metadata": {},
   "source": [
    "## 2.2 Define a Naive Pytorch DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eca596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.user_Embedding = nn.Embedding(user_num,user_dim)\n",
    "        self.mlp = nn.Sequential()\n",
    "        for id in range(1,len(layer)):\n",
    "            self.mlp.add_module(\"Linear_layer_%d\" % id, nn.Linear(layer[id-1],layer[id]))\n",
    "            self.mlp.add_module(\"Relu_layer_%d\" % id, nn.ReLU(inplace=True))\n",
    "        self.predict =  nn.Sequential(nn.Linear(layer[-1],1),nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        user = self.user_Embedding(x)\n",
    "        user = self.mlp(user)\n",
    "        score = self.predict(user)\n",
    "        return score\n",
    "\n",
    "model = MLP(1000,32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db100b94",
   "metadata": {},
   "source": [
    "## 2.3 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "keys, labels = generate_random_samples(args[\"num_sample\"], args[\"vocabulary_range_per_slot\"], args[\"np_key_type\"])\n",
    "x_train = torch.from_numpy(keys)\n",
    "y_train = torch.from_numpy(labels).float()\n",
    "x_dataloader = DataLoader(x_train, batch_size=args[\"global_batch_size\"], shuffle=True, num_workers=1, pin_memory=False, drop_last=False)\n",
    "y_dataloader = DataLoader(y_train, batch_size=args[\"global_batch_size\"], shuffle=True, num_workers=1, pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
    "for epoch in range(args[\"iter_num\"]):\n",
    "    iterations_per_epoch = len(x_dataloader)\n",
    "    x_iterator = iter(x_dataloader)\n",
    "    y_iterator = iter(y_dataloader)\n",
    "    for _ in range(iterations_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        x_train = next(x_iterator)\n",
    "        y_train = next(y_iterator)\n",
    "        x, y = x_train, y_train\n",
    "        preds = model(x).squeeze(1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd41c2",
   "metadata": {},
   "source": [
    "## 2.4 Print model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor)\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cdf536",
   "metadata": {},
   "source": [
    "## 2.5 Save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/torch_test/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420081d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=torch.jit.script(model)\n",
    "save_model.save(\"model/torch_test/0/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0030e50",
   "metadata": {},
   "source": [
    "## 2.6 Deploye the model using Pytorch Triton Backend\n",
    "Configure \"torch_test\" model with pytorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/torch_test/config.pbtxt\n",
    "name: \"torch_test\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "input: [\n",
    "   {\n",
    "      name: \"user_Embedding\"\n",
    "      data_type: TYPE_INT64\n",
    "      dims: [-1]\n",
    "   }\n",
    "]\n",
    "output: [\n",
    "   {\n",
    "      name: \"prediction\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1]\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7231998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Triton Server\n",
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=torch_test --model-control-mode=explicit  --allow-gpu-metrics=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe7d97",
   "metadata": {},
   "source": [
    "## 2.6 Send the inference request to Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0584a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'torch_test'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array([[123,456]],dtype='int64')\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"user_Embedding\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"prediction\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"prediction\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38feea0",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f626acf",
   "metadata": {},
   "source": [
    "# 3 Separate the trained navie DNN model graph into  embedding and dense(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513ba00",
   "metadata": {},
   "source": [
    "<a id='section3.1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac4b4cd",
   "metadata": {},
   "source": [
    "## 3.1 Depoly the Dense Model using Triton Backend\n",
    "### 3.1.1 Define Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46044843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OnlyMLP(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        \n",
    "        super(OnlyMLP, self).__init__()\n",
    "        #self.user_Embedding = nn.Embedding(user_num,user_dim)\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.emb_dim = user_dim\n",
    "        for id in range(1,len(layer)):\n",
    "            self.mlp.add_module(\"Linear_layer_%d\" % id, nn.Linear(layer[id-1],layer[id]))\n",
    "            self.mlp.add_module(\"Relu_layer_%d\" % id, nn.ReLU(inplace=True))\n",
    "        self.predict =  nn.Sequential(nn.Linear(layer[-1],1),nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #user = self.user_Embedding(x)\n",
    "        user = x.reshape(-1,self.emb_dim)\n",
    "        user = self.mlp(user)\n",
    "        score = self.predict(user)\n",
    "        return score\n",
    "\n",
    "dense_model = OnlyMLP(1000,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dense model layers\n",
    "for param_tensor in dense_model.state_dict():\n",
    "    print(param_tensor)\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187741d",
   "metadata": {},
   "source": [
    "### 3.1.2 Load complete pre-trained navie pytorch model( Step 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = torch.load(\"model/torch_test/0/model.pt\")\n",
    "# get pre-trained model state dict\n",
    "pretrain_dict=pretrain_model.state_dict()\n",
    "# get dense model state dict\n",
    "new_dict=  dense_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be21bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = torch.load(\"model/torch_test/0/model.pt\")\n",
    "pretrain_dict=pretrain_model.state_dict()\n",
    "for param_tensor in pretrain_model.state_dict():\n",
    "    print(param_tensor)\n",
    "    print( model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbff31",
   "metadata": {},
   "source": [
    "##3.3 Remove the embedding layer from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2745fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dict = {k:v for k,v in pretrain_dict.items() if k in new_dict}\n",
    "new_dict.update(pretrain_dict)\n",
    "#update dense model\n",
    "dense_model.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94962d",
   "metadata": {},
   "source": [
    "### 3.1.3 Save the dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e473f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/dense_test/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ed9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model=torch.jit.script(dense_model)\n",
    "save_model.save(\"model/dense_test/0/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be7bb5",
   "metadata": {},
   "source": [
    "### 3.1.4 Deploye the dense model using Pytorch Triton Backend\n",
    "Configure \"dense_test\" model with pytorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a317df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/dense_test/config.pbtxt\n",
    "name: \"dense_test\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 0\n",
    "input: [\n",
    "   {\n",
    "      name: \"mlp.Linear_layer_1\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1]\n",
    "   }\n",
    "]\n",
    "output: [\n",
    "   {\n",
    "      name: \"prediction\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1]\n",
    "   }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=dense_test --model-control-mode=explicit --allow-gpu-metrics=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b09e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the inference request to dense model\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'dense_test'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array([[np.random.uniform(0.0,1) for i in range(32)]],dtype='float32')\n",
    "    \n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"mlp.Linear_layer_1\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"prediction\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"prediction\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79365d89",
   "metadata": {},
   "source": [
    "<a id='section3.2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be241fc",
   "metadata": {},
   "source": [
    "## 3.2 Deploy the embedding part using HPS Triton Backend\n",
    "### 3.2.1 Configure HPS backend for Embedding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/hps_test/0/hps_sparse.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd522e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/hps_test/config.pbtxt\n",
    "name: \"hps_test\"\n",
    "backend: \"hps\"\n",
    "max_batch_size:32,\n",
    "input [\n",
    "  {\n",
    "    name: \"KEYS\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [-1]\n",
    "  },\n",
    "  {\n",
    "    name: \"NUMKEYS\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"OUTPUT0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 0}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus:[0]\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/hps.json\n",
    "{\n",
    "    \"models\": [{\n",
    "    \"model\": \"hps_test\",\n",
    "    \"sparse_files\": [\"model/hps_test/0/hps_sparse.model\"],\n",
    "    \"num_of_worker_buffer_in_pool\": 3,\n",
    "    \"embedding_table_names\":[\"0\"],\n",
    "    \"num_of_refresher_buffer_in_pool\":1,\n",
    "    \"embedding_vecsize_per_table\":[32],\n",
    "    \"num_of_refresher_buffer_in_pool\":0,\n",
    "    \"maxnum_catfeature_query_per_table_per_sample\":[1],\n",
    "    \"deployed_device_list\":[0],\n",
    "    \"max_batch_size\":32,\n",
    "    \"default_value_for_each_table\":[0.0],\n",
    "    \"cache_refresh_percentage_per_iteration\":0,\n",
    "    \"hit_rate_threshold\":1.1,\n",
    "    \"gpucacheper\":1.0,\n",
    "    \"gpucache\":true\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04173872",
   "metadata": {},
   "source": [
    "### 3.2.2 Conver the torch-format embedding file to HPS-format embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b704f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = torch.load(\"model/torch_test/0/model.pt\")\n",
    "pretrain_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c847c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse_model(embeddings_weights, embedding_table_path, embedding_vec_size):\n",
    "    \"\"\"\n",
    "    Convert the lookup part of the model to a format supported by HPS (key-vector pair files),\n",
    "    the embedding weights of the trained dense model will be reloaded.\n",
    "    \n",
    "    Outputs(key-vector pair files) will be saved to defined sparse model path\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"{}/key\".format(embedding_table_path), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(embedding_table_path), 'wb') as vec_file:\n",
    "        for key in range(embeddings_weights.shape[0]):\n",
    "            vec = embeddings_weights[key].data.tolist()\n",
    "            key_struct = struct.pack('q', key)\n",
    "            vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "            key_file.write(key_struct)\n",
    "            vec_file.write(vec_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ddeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_sparse_model(pretrain_model.state_dict()['user_Embedding.weight'], \"model/hps_test/0/hps_sparse.model\", 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f23cae",
   "metadata": {},
   "source": [
    "### 3.2.3 Launch Triton Server to verify HPS Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=hps_test --model-control-mode=explicit --backend-directory=/usr/local/hugectr/backends --backend-config=hps,ps=/hugectr_backend/model/hps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send embedding key to HPS backend\n",
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'hps_test'\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "\n",
    "    embedding_columns = np.array(torch.randint(low=0, high=999, size=(1,32)).numpy().tolist(),dtype='int64')\n",
    "    row_ptrs = np.array([[32]],dtype='int32')\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"KEYS\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"NUMKEYS\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    inputs[1].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"OUTPUT0\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"OUTPUT0\"))\n",
    "    print(response.as_numpy(\"OUTPUT0\").shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f394105",
   "metadata": {},
   "source": [
    "<a id='section3.3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97440c8b",
   "metadata": {},
   "source": [
    "## 3.3 Configure \"ensemble_model\" Triton backend for Embedding and Dense model\n",
    "### 3.3.1 Configure \"ensemble_model\" Triton backend for Embedding deployment(HPS backend) and Dense model(Pytorch backend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d18a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p model/ensemble_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad90d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/ensemble_model/config.pbtxt\n",
    "name: \"ensemble_model\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"EMB_KEY\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"EMB_N_KEY\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"DENSE_OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"hps_test\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"KEYS\"\n",
    "        value: \"EMB_KEY\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"NUMKEYS\"\n",
    "        value: \"EMB_N_KEY\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"dense_test\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"mlp.Linear_layer_1\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"prediction\"\n",
    "        value: \"DENSE_OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d8d4e0",
   "metadata": {},
   "source": [
    "### 3.3.2 Launch Triton Server to verify ensemble Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5122180",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=ensemble_model --model-control-mode=explicit  --allow-gpu-metrics=true --backend-config=hps,ps=/hugectr_backend/model/hps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'ensemble_model'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array([[123]],dtype='int64')\n",
    "    row_ptrs = np.array([[1]],dtype='int32')\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"EMB_KEY\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"EMB_N_KEY\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    inputs[1].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"DENSE_OUTPUT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692fc5e3",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790f23b",
   "metadata": {},
   "source": [
    "# 4 Use TensorRT to speed up dense model inference and combine with HPS Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46808665-c08d-4c91-9adf-f16105058918",
   "metadata": {},
   "source": [
    "## 4.1 Convert Dense model to Onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2212af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "dense_model = torch.load(\"model/dense_test/0/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ea62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dense_model(torch.from_numpy(np.array([np.random.uniform(0.0,1) for i in range(32)])).float())\n",
    "dense_model(torch.randn(1,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "dummy_input=torch.randn(BATCH_SIZE,32)\n",
    "import torch.onnx\n",
    "torch.onnx.export(dense_model, dummy_input,\"model/dense_onnx_model.onnx\", verbose = True, input_names = [\"vectors\"], output_names = [\"prediction\"],dynamic_axes = {'vectors' : {0 : 'BATCH_SIZE'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957462f",
   "metadata": {},
   "source": [
    "## 4.2 Conver onnx to tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PATH=/usr/src/tensorrt/bin/:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6181b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model/dense_trt/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5456c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --onnx=model/dense_onnx_model.onnx  --saveEngine=model/dense_trt/0/dense_dynamic.trt --optShapes=vectors:32x32 --minShapes=vectors:1x32 --maxShapes=vectors:32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7d580",
   "metadata": {},
   "source": [
    "### If the tensorrt is not installed in the merlin container, you need to run the following command in the host to get the tensorrt engine of dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker run --runtime=nvidia --cap-add SYS_NICE --gpus=all --net=host -u root -v $(pwd):/hugectr_backend -w /hugectr_backend nvcr.io/nvidia/tensorrt:22.11-py3 trtexec --onnx=/hugectr_backend/model/dense_onnx_model.onnx  --saveEngine=/hugectr_backend/model/dense_trt/0/dense_dynamic.trt --optShapes=vectors:32x32 --minShapes=vectors:1x32 --maxShapes=vectors:32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba41f3",
   "metadata": {},
   "source": [
    "### Create the Tensorrt backend configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e335d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/dense_trt/config.pbtxt\n",
    "platform: \"tensorrt_plan\"\n",
    "default_model_filename: \"dense_dynamic.trt\"\n",
    "backend: \"tensorrt\"\n",
    "max_batch_size: 0\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"vectors\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1024]\n",
    "    reshape: { shape: [32,32] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "      name: \"prediction\"\n",
    "      data_type: TYPE_FP32\n",
    "      dims: [-1, 1]\n",
    "  }\n",
    "]\n",
    "\n",
    "instance_group [\n",
    "  {\n",
    "    kind: KIND_GPU\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e37af",
   "metadata": {},
   "source": [
    "# HPS + TensorRT ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47670f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model/ensemble_model/config.pbtxt\n",
    "name: \"ensemble_model\"\n",
    "platform: \"ensemble\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  {\n",
    "    name: \"EMB_KEY\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [ -1 ]\n",
    "  },\n",
    "  {\n",
    "    name: \"EMB_N_KEY\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [ -1 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"DENSE_OUTPUT\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [-1]\n",
    "  }\n",
    "]\n",
    "ensemble_scheduling {\n",
    "  step [\n",
    "    {\n",
    "      model_name: \"hps_test\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"KEYS\"\n",
    "        value: \"EMB_KEY\"\n",
    "      }\n",
    "      input_map {\n",
    "        key: \"NUMKEYS\"\n",
    "        value: \"EMB_N_KEY\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"OUTPUT0\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      model_name: \"dense_trt\"\n",
    "      model_version: -1\n",
    "      input_map {\n",
    "        key: \"vectors\"\n",
    "        value: \"LOOKUP_VECTORS\"\n",
    "      }\n",
    "      output_map {\n",
    "        key: \"prediction\"\n",
    "        value: \"DENSE_OUTPUT\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60898258",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tritonserver --model-repository=/hugectr_backend/model/ --load-model=ensemble_model --model-control-mode=explicit  --allow-gpu-metrics=true --backend-config=hps,ps=/hugectr_backend/hps.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import *\n",
    "import tritonclient.http  as httpclient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "model_name = 'ensemble_model'\n",
    "\n",
    "with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "    embedding_columns = np.array(torch.randint(low=0, high=999, size=(1,32)).numpy().tolist(),dtype='int64')\n",
    "    row_ptrs = np.array([[32]],dtype='int32')\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"EMB_KEY\", embedding_columns.shape,\n",
    "                              np_to_triton_dtype(embedding_columns.dtype)),\n",
    "        httpclient.InferInput(\"EMB_N_KEY\", row_ptrs.shape,\n",
    "                              np_to_triton_dtype(row_ptrs.dtype)),\n",
    "\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(embedding_columns)\n",
    "    inputs[1].set_data_from_numpy(row_ptrs)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"DENSE_OUTPUT\")\n",
    "    ]\n",
    "\n",
    "    response = client.infer(model_name,\n",
    "                            inputs,\n",
    "                            request_id=str(1),\n",
    "                            outputs=outputs)\n",
    "\n",
    "    result = response.get_response()\n",
    "    print(result)\n",
    "    print(\"Prediction Result:\")\n",
    "    print(response.as_numpy(\"DENSE_OUTPUT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41427cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a712f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= torch.load(\"/hugectr/uber_model/EtaFitExperiment/0/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15551f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d8a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.user_Embedding = nn.Embedding(user_num,user_dim)\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.mlp2 = nn.Sequential()\n",
    "        self.mlp2.add_module(\"Relu_layer_%d\" % 0, nn.ReLU(inplace=True))\n",
    "        for id in range(1,len(layer)):\n",
    "            self.mlp.add_module(\"Linear_layer_%d\" % id, nn.Linear(layer[id-1],layer[id]))\n",
    "            self.mlp.add_module(\"Relu_layer_%d\" % id, nn.ReLU(inplace=True))\n",
    "        self.predict =  nn.Sequential(nn.Linear(layer[-1],1),nn.Sigmoid())\n",
    "     \n",
    "    \n",
    "    def forward(self,x:Dict[str,torch.Tensor]):\n",
    "        user = self.user_Embedding(x[\"embedding\"])\n",
    "        user2 = self.user_Embedding(x[\"embedding1\"])\n",
    "        user2 = self.mlp(user2)\n",
    "        user = self.mlp(user)\n",
    "        score = self.predict(user)\n",
    "        score2 = self.predict(user2)\n",
    "        return torch.add(score,score2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a999d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.user_Embedding = nn.Embedding(user_num,user_dim)\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.mlp2 = nn.Sequential()\n",
    "        self.mlp2.add_module(\"Relu_layer_%d\" % 0, nn.ReLU(inplace=True))\n",
    "        for id in range(1,len(layer)):\n",
    "            self.mlp.add_module(\"Linear_layer_%d\" % id, nn.Linear(layer[id-1],layer[id]))\n",
    "            self.mlp.add_module(\"Relu_layer_%d\" % id, nn.ReLU(inplace=True))\n",
    "        self.predict =  nn.Sequential(nn.Linear(layer[-1],1),nn.Sigmoid())\n",
    "     \n",
    "    \n",
    "    def forward(self,x:Dict[str,torch.Tensor]):\n",
    "        user = self.user_Embedding(x[\"embedding\"])\n",
    "        user2 = self.user_Embedding(x[\"embedding1\"])\n",
    "        user2 = self.mlp(user2)\n",
    "        user = self.mlp(user)\n",
    "        score = self.predict(user)\n",
    "        score2 = self.predict(user2)\n",
    "        return torch.add(score,score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3543a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(1000,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f774fb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (user_Embedding): Embedding(1000, 32)\n",
       "  (mlp): Sequential(\n",
       "    (Linear_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (Relu_layer_1): ReLU(inplace=True)\n",
       "    (Linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (Relu_layer_2): ReLU(inplace=True)\n",
       "  )\n",
       "  (mlp2): Sequential(\n",
       "    (Relu_layer_0): ReLU(inplace=True)\n",
       "  )\n",
       "  (predict): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa37315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "keys, labels = generate_random_samples(args[\"num_sample\"], args[\"vocabulary_range_per_slot\"], args[\"np_key_type\"])\n",
    "x_train = torch.from_numpy(keys)\n",
    "y_train = torch.from_numpy(labels).float()\n",
    "x_dataloader = DataLoader(x_train, batch_size=args[\"global_batch_size\"], shuffle=True, num_workers=1, pin_memory=False, drop_last=False)\n",
    "y_dataloader = DataLoader(y_train, batch_size=args[\"global_batch_size\"], shuffle=True, num_workers=1, pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ae5a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n",
      "MLP(\n",
      "  (user_Embedding): Embedding(1000, 32)\n",
      "  (mlp): Sequential(\n",
      "    (Linear_layer_1): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (Relu_layer_1): ReLU(inplace=True)\n",
      "    (Linear_layer_2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (Relu_layer_2): ReLU(inplace=True)\n",
      "  )\n",
      "  (mlp2): Sequential(\n",
      "    (Relu_layer_0): ReLU(inplace=True)\n",
      "  )\n",
      "  (predict): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "0 tensor(-0., grad_fn=<DivBackward1>)\n",
      "1 tensor(-0., grad_fn=<DivBackward1>)\n",
      "2 tensor(-0., grad_fn=<DivBackward1>)\n",
      "3 tensor(-0., grad_fn=<DivBackward1>)\n",
      "4 tensor(-0., grad_fn=<DivBackward1>)\n",
      "5 tensor(-0., grad_fn=<DivBackward1>)\n",
      "6 tensor(-0., grad_fn=<DivBackward1>)\n",
      "7 tensor(-0., grad_fn=<DivBackward1>)\n",
      "8 tensor(-0., grad_fn=<DivBackward1>)\n",
      "9 tensor(-0., grad_fn=<DivBackward1>)\n",
      "10 tensor(-0., grad_fn=<DivBackward1>)\n",
      "11 tensor(-0., grad_fn=<DivBackward1>)\n",
      "12 tensor(-0., grad_fn=<DivBackward1>)\n",
      "13 tensor(-0., grad_fn=<DivBackward1>)\n",
      "14 tensor(-0., grad_fn=<DivBackward1>)\n",
      "15 tensor(-0., grad_fn=<DivBackward1>)\n",
      "16 tensor(-0., grad_fn=<DivBackward1>)\n",
      "17 tensor(-0., grad_fn=<DivBackward1>)\n",
      "18 tensor(-0., grad_fn=<DivBackward1>)\n",
      "19 tensor(-0., grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
    "for epoch in range(args[\"iter_num\"]):\n",
    "    iterations_per_epoch = len(x_dataloader)\n",
    "    x_iterator = iter(x_dataloader)\n",
    "    y_iterator = iter(y_dataloader)\n",
    "    for _ in range(iterations_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        x_train = next(x_iterator)\n",
    "        y_train = next(y_iterator)\n",
    "        x, y = x_train, y_train\n",
    "        x={\"embedding\":x_train}\n",
    "        x[\"embedding1\"]=x_train\n",
    "        preds = model(x).squeeze(1)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d745563e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0917]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "my_dict: Dict[str, torch.Tensor] ={'embedding': torch.tensor([1],dtype=torch.int,device=device),'embedding1': torch.tensor([1],dtype=torch.int,device=device)}\n",
    "model.to(device)\n",
    "model(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7675fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm = torch.jit.trace(model,my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fa18609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0917]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmm(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd2f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch IR graph at exception: graph(%x : Dict(str, Tensor),\n",
      "      %user_Embedding.weight : Int(1, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %mlp.Linear_layer_1.bias : Float(1000, 32, strides=[32, 1], requires_grad=0, device=cuda:0),\n",
      "      %mlp.Linear_layer_1.weight : Float(16, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %mlp.Linear_layer_2.bias : Float(16, 32, strides=[32, 1], requires_grad=0, device=cuda:0),\n",
      "      %mlp.Linear_layer_2.weight : Float(8, strides=[1], requires_grad=0, device=cuda:0),\n",
      "      %predict.0.bias : Float(8, 16, strides=[16, 1], requires_grad=0, device=cuda:0),\n",
      "      %predict.0.weight : Float(1, strides=[1], requires_grad=0, device=cuda:0)):\n",
      "  %61 : Long(device=cpu) = prim::Constant[value={1}](), scope: MLP::\n",
      "  %62 : Bool(device=cpu) = prim::Constant[value={0}](), scope: MLP::/torch.nn.modules.sparse.Embedding::user_Embedding\n",
      "  %63 : Long(device=cpu) = prim::Constant[value={-1}](), scope: MLP::/torch.nn.modules.sparse.Embedding::user_Embedding\n",
      "  %12 : str = prim::Constant[value=\"embedding1\"](), scope: MLP::\n",
      "  %13 : str = prim::Constant[value=\"embedding\"](), scope: MLP::\n",
      "  %input.1 : Tensor = aten::__getitem__(%x, %13), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:976:0\n",
      "  %input.3 : Tensor = aten::__getitem__(%x, %12), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:976:0\n",
      "  %input.13 : Tensor = aten::embedding(%user_Embedding.weight, %input.1, %63, %62, %62), scope: MLP::/torch.nn.modules.sparse.Embedding::user_Embedding # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2210:0\n",
      "  %input.5 : Tensor = aten::embedding(%user_Embedding.weight, %input.3, %63, %62, %62), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2210:0\n",
      "  %input.7 : Tensor = aten::linear(%input.5, %mlp.Linear_layer_1.weight, %mlp.Linear_layer_1.bias), scope: MLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::Linear_layer_1 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:0\n",
      "  %45 : Tensor = aten::relu(%input.7), scope: MLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::Relu_layer_1 # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1455:0\n",
      "  %input.11 : Tensor = aten::linear(%45, %mlp.Linear_layer_2.weight, %mlp.Linear_layer_2.bias), scope: MLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::Linear_layer_2 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:0\n",
      "  %46 : Tensor = aten::relu(%input.11), scope: MLP::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::Relu_layer_2 # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1455:0\n",
      "  %input.15 : Tensor = aten::linear(%input.13, %mlp.Linear_layer_1.weight, %mlp.Linear_layer_1.bias), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:0\n",
      "  %47 : Tensor = aten::relu(%input.15), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1455:0\n",
      "  %input.19 : Tensor = aten::linear(%47, %mlp.Linear_layer_2.weight, %mlp.Linear_layer_2.bias), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:0\n",
      "  %48 : Tensor = aten::relu(%input.19), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1455:0\n",
      "  %input.23 : Tensor = aten::linear(%48, %predict.0.weight, %predict.0.bias), scope: MLP::/torch.nn.modules.container.Sequential::predict/torch.nn.modules.linear.Linear::0 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:0\n",
      "  %score : Tensor = aten::sigmoid(%input.23), scope: MLP::/torch.nn.modules.container.Sequential::predict/torch.nn.modules.activation.Sigmoid::1 # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py:294:0\n",
      "  %input : Tensor = aten::linear(%46, %predict.0.weight, %predict.0.bias), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114:0\n",
      "  %score2 : Tensor = aten::sigmoid(%input), scope: MLP:: # /usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py:294:0\n",
      "  %43 : Tensor = aten::add(%score, %score2, %61), scope: MLP:: # /tmp/ipykernel_9137/2137023801.py:27:0\n",
      "  return (%43)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:823: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f1212973ea0>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ScalarType UNKNOWN_SCALAR is an unexpected tensor scalar type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint,device\u001b[38;5;241m=\u001b[39mdevice),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding1\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint,device\u001b[38;5;241m=\u001b[39mdevice)}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdict_new_onnx_model.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvectors\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBATCH_SIZE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1115\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1112\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1126\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    685\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GLOBALS\u001b[38;5;241m.\u001b[39monnx_shape_inference:\n\u001b[0;32m--> 687\u001b[0m     \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx_graph_shape_type_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLOBALS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_onnx_opset_version\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ScalarType UNKNOWN_SCALAR is an unexpected tensor scalar type"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "dummy_input=torch.randn(BATCH_SIZE,32)\n",
    "x: Dict[str, torch.Tensor] ={'embedding': torch.tensor([1],dtype=torch.int,device=device),'embedding1': torch.tensor([1],dtype=torch.int,device=device)}\n",
    "import torch.onnx\n",
    "torch.onnx.export(tmm, (x,{}), \"dict_new_onnx_model.onnx\", verbose = True, input_names = [\"input\",\"embedding\"], output_names = [\"prediction\"],dynamic_axes = {'vectors' : {0 : 'BATCH_SIZE'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "757669d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 354126\r\n",
      "-rw-r--r-- 1 38458 dip     34783 Dec 14 04:15 01_model_training.ipynb\r\n",
      "-rw-r--r-- 1 38458 dip     22935 Dec 14 04:15 02_model_inference_hps_tf_ensemble.ipynb\r\n",
      "-rw-r--r-- 1 38458 dip     20214 Dec 14 04:15 03_model_inference_hps_trt_ensemble.ipynb\r\n",
      "-rwxrwxrwx 1 root  dip       136 Nov 10 14:21 \u001b[0m\u001b[01;32m32.json\u001b[0m*\r\n",
      "-rw-r--r-- 1 root  dip    153761 Feb  1 06:08 Dict_dense_onnx_model.onnx\r\n",
      "-rw-r--r-- 1 38458 dip   1031320 Nov 29 07:31 HPS_Pytorch.ipynb\r\n",
      "-rwxrwxrwx 1 root  dip     59509 Feb  1 07:07 \u001b[01;32mHPS_Pytorch_ensemble_deployment.ipynb\u001b[0m*\r\n",
      "-rw-r--r-- 1 38458 dip      5441 Dec 14 09:58 README.md\r\n",
      "-rw-r--r-- 1 root  dip    153769 Feb  1 06:07 dense_onnx_model.onnx\r\n",
      "-rw-r--r-- 1 root  dip    179361 Feb  1 07:08 dict_onnx_model.onnx\r\n",
      "-rw-rw-r-- 1 root  dip  72400431 Nov 21 05:01 hps_trt_report.nsys-rep\r\n",
      "-rw-r--r-- 1 root  dip 251457536 Nov 21 05:01 hps_trt_report.sqlite\r\n",
      "drwxrwxrwx 7 root  dip        10 Nov 29 08:22 \u001b[34;42mmodel\u001b[0m/\r\n",
      "drwxr-xr-x 2 38458 dip         4 Dec 14 04:15 \u001b[01;34mpic\u001b[0m/\r\n",
      "-rwxr-xr-x 1 38458 dip        37 Dec 14 05:35 \u001b[01;32mrequirements.txt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f130b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/serialization.py:779: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "user_model= torch.load(\"/hugectr/uber_model/EtaFitExperiment/0/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c7c2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=TorchGraphScriptWrapper\n",
       "  (model): RecursiveScriptModule(\n",
       "    original_name=LightningGraph\n",
       "    (embeddings): RecursiveScriptModule(\n",
       "      original_name=TorchGraph\n",
       "      (emb0_derived_city_id): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb1_derived_minute_of_week): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb2_indexed_derived_geohash_begin_4): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb3_indexed_derived_geohash_end_4): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb4_indexed_derived_geohash_begin_5): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb5_indexed_derived_geohash_end_5): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb6_derived_geohash_begin_6_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb7_derived_geohash_end_6_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb8_derived_geohash_begin_6shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb9_derived_geohash_end_6shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb10_derived_geohash_begin_7_fh18): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb11_derived_geohash_end_7_fh18): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb12_derived_geohash_od_56_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb13_derived_geohash_od_56shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb14_derived_geohash_od_65_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb15_derived_geohash_od_65shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb16_derived_segment_countrycode_fh12): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb17_derived_is_eats_record): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb18_derived_map_data_vendor): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb19_derived_route_type): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb20_derived_waypoint_tasktype): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb21_derived_unmodified_eta_sec_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb22_derived_estimated_distance_m_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb23_derived_haversine_distance_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb24_derived_historical_seconds_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb25_derived_umm_default_and_limit_seconds_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb26_derived_realtime_seconds_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb27_derived_historical_seconds_ratio_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb28_derived_umm_default_and_limit_seconds_ratio_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb29_derived_realtime_seconds_ratio_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "    )\n",
       "    (65OPG50ZN7): RecursiveScriptModule(\n",
       "      original_name=FastTransformer\n",
       "      (attention_layer): RecursiveScriptModule(\n",
       "        original_name=AttentionLayer\n",
       "        (inner_attention): RecursiveScriptModule(\n",
       "          original_name=LinearAttention\n",
       "          (feature_map): RecursiveScriptModule(original_name=ActivationFunctionFeatureMap)\n",
       "        )\n",
       "        (query_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        (key_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        (value_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        (out_projection): RecursiveScriptModule(original_name=Linear)\n",
       "      )\n",
       "      (layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "    )\n",
       "    (EZP8SZWHSF): RecursiveScriptModule(\n",
       "      original_name=MLP\n",
       "      (MLP_layer_0): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (model): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(original_name=Linear)\n",
       "          (1): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "          (2): RecursiveScriptModule(original_name=ReLU)\n",
       "          (3): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "      )\n",
       "      (MLP_layer_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (model): RecursiveScriptModule(original_name=Linear)\n",
       "      )\n",
       "    )\n",
       "    (2KTQC58ZIJ): RecursiveScriptModule(\n",
       "      original_name=BiasAdjustment\n",
       "      (bias_adjuster): RecursiveScriptModule(original_name=Linear)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    )\n",
       "  )\n",
       "  (gpu_model): RecursiveScriptModule(\n",
       "    original_name=LightningGraph\n",
       "    (embeddings): RecursiveScriptModule(\n",
       "      original_name=TorchGraph\n",
       "      (emb0_derived_city_id): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb1_derived_minute_of_week): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb2_indexed_derived_geohash_begin_4): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb3_indexed_derived_geohash_end_4): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb4_indexed_derived_geohash_begin_5): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb5_indexed_derived_geohash_end_5): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb6_derived_geohash_begin_6_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb7_derived_geohash_end_6_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb8_derived_geohash_begin_6shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb9_derived_geohash_end_6shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb10_derived_geohash_begin_7_fh18): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb11_derived_geohash_end_7_fh18): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb12_derived_geohash_od_56_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb13_derived_geohash_od_56shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb14_derived_geohash_od_65_fh21): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb15_derived_geohash_od_65shift_fh21b): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb16_derived_segment_countrycode_fh12): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb17_derived_is_eats_record): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb18_derived_map_data_vendor): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb19_derived_route_type): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb20_derived_waypoint_tasktype): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb21_derived_unmodified_eta_sec_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb22_derived_estimated_distance_m_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb23_derived_haversine_distance_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb24_derived_historical_seconds_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb25_derived_umm_default_and_limit_seconds_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb26_derived_realtime_seconds_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb27_derived_historical_seconds_ratio_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb28_derived_umm_default_and_limit_seconds_ratio_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "      (emb29_derived_realtime_seconds_ratio_after_QuantileDiscretizer_0): RecursiveScriptModule(\n",
       "        original_name=Embedding\n",
       "        (my_module): RecursiveScriptModule(original_name=Embedding)\n",
       "      )\n",
       "    )\n",
       "    (65OPG50ZN7): RecursiveScriptModule(\n",
       "      original_name=FastTransformer\n",
       "      (attention_layer): RecursiveScriptModule(\n",
       "        original_name=AttentionLayer\n",
       "        (inner_attention): RecursiveScriptModule(\n",
       "          original_name=LinearAttention\n",
       "          (feature_map): RecursiveScriptModule(original_name=ActivationFunctionFeatureMap)\n",
       "        )\n",
       "        (query_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        (key_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        (value_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        (out_projection): RecursiveScriptModule(original_name=Linear)\n",
       "      )\n",
       "      (layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "    )\n",
       "    (EZP8SZWHSF): RecursiveScriptModule(\n",
       "      original_name=MLP\n",
       "      (MLP_layer_0): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (model): RecursiveScriptModule(\n",
       "          original_name=Sequential\n",
       "          (0): RecursiveScriptModule(original_name=Linear)\n",
       "          (1): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "          (2): RecursiveScriptModule(original_name=ReLU)\n",
       "          (3): RecursiveScriptModule(original_name=Dropout)\n",
       "        )\n",
       "      )\n",
       "      (MLP_layer_1): RecursiveScriptModule(\n",
       "        original_name=Linear\n",
       "        (model): RecursiveScriptModule(original_name=Linear)\n",
       "      )\n",
       "    )\n",
       "    (2KTQC58ZIJ): RecursiveScriptModule(\n",
       "      original_name=BiasAdjustment\n",
       "      (bias_adjuster): RecursiveScriptModule(original_name=Linear)\n",
       "      (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "user_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90a8013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict ={'derived_unmodified_eta_sec': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_estimated_distance_m': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_city_id': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_minute_of_week': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'indexed_derived_geohash_begin_4': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'indexed_derived_geohash_end_4': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'indexed_derived_geohash_begin_5': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'indexed_derived_geohash_end_5': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_begin_6_fh21': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_end_6_fh21': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_begin_6shift_fh21b': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_end_6shift_fh21b': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_begin_7_fh18': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_end_7_fh18': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_od_56_fh21': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_od_56shift_fh21b': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_od_65_fh21': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_geohash_od_65shift_fh21b': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_segment_countrycode_fh12': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_is_eats_record': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_map_data_vendor': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_route_type': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_waypoint_tasktype': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_unmodified_eta_sec_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_estimated_distance_m_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_haversine_distance_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_historical_seconds_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_umm_default_and_limit_seconds_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_realtime_seconds_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_historical_seconds_ratio_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_umm_default_and_limit_seconds_ratio_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device),\n",
    "'derived_realtime_seconds_ratio_after_QuantileDiscretizer_0': torch.tensor([1],dtype=torch.float64,device=device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6549cb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(prediction=tensor([[19.3254]], device='cuda:0', grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_model(user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d33bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name=[\"derived_unmodified_eta_sec\",\"derived_estimated_distance_m\",\"derived_city_id\",\"derived_minute_of_week\",\"indexed_derived_geohash_begin_4\",\"indexed_derived_geohash_end_4\",\"indexed_derived_geohash_begin_5\",\"indexed_derived_geohash_end_5\",\n",
    "\"derived_geohash_begin_6_fh21\",\n",
    "\"derived_geohash_end_6_fh21\",\n",
    "\"derived_geohash_begin_6shift_fh21b\",\n",
    "\"derived_geohash_end_6shift_fh21b\",\n",
    "\"derived_geohash_begin_7_fh18\",\n",
    "\"derived_geohash_end_7_fh18\",\n",
    "\"derived_geohash_od_56_fh21\",\n",
    "\"derived_geohash_od_56shift_fh21b\",\n",
    "\"derived_geohash_od_65_fh21\",\n",
    "\"derived_geohash_od_65shift_fh21b\",\n",
    "\"derived_segment_countrycode_fh12\",\n",
    "\"derived_is_eats_record\",\n",
    "\"derived_map_data_vendor\",\n",
    "\"derived_route_type\",\n",
    "\"derived_waypoint_tasktype\",\n",
    "\"derived_unmodified_eta_sec_after_QuantileDiscretizer_0\",\n",
    "\"derived_estimated_distance_m_after_QuantileDiscretizer_0\",\n",
    "\"derived_haversine_distance_after_QuantileDiscretizer_0\",\n",
    "\"derived_historical_seconds_after_QuantileDiscretizer_0\",\n",
    "\"derived_umm_default_and_limit_seconds_after_QuantileDiscretizer_0\",\n",
    "\"derived_realtime_seconds_after_QuantileDiscretizer_0\",\n",
    "\"derived_historical_seconds_ratio_after_QuantileDiscretizer_0\",\n",
    "\"derived_umm_default_and_limit_seconds_ratio_after_QuantileDiscretizer_0\",\n",
    "\"derived_realtime_seconds_ratio_after_QuantileDiscretizer_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a483737",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(user_model, (user_dict,{}),\"user__onnx_model.onnx\", verbose = True, input_names = input_name, output_names = [\"output_spec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb14d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = [\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device),\n",
    "    torch.tensor([1],dtype=torch.float64,device=device)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2488db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(user_model, dummy_input,\"user__onnx_model.onnx\", verbose = True, input_names = input_name, output_names = [\"output_spec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5861fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test=(dict(zip(input_name,dummy_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de4bfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class MOCK(nn.Module):\n",
    "    def __init__(self,user_num,user_dim,layer=[32,16,8]):\n",
    "        super(MOCK, self).__init__()\n",
    "        self.predict= torch.load(\"/hugectr/uber_model/EtaFitExperiment/0/model.pt\")\n",
    "    \n",
    "    def forward(self,x:List[str],y:List[torch.Tensor]):\n",
    "        input={}\n",
    "        for i in range(len(x)):\n",
    "            input[x[i]]=y[i]\n",
    "        score = self.predict(input)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4796902",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model=MOCK(1000,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd89e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmm = torch.jit.script(test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f28f2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(prediction=tensor([[19.3254]], device='cuda:0', grad_fn=<DifferentiableGraphBackward>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmm(input_name,dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2901f6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(prediction=tensor([[19.3254]], device='cuda:0', grad_fn=<DifferentiableGraphBackward>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3dab533e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "args contained 32 None's after flattening. When exporting a ScriptModule or ScriptFunction, no args may be None because that breaks type propagation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser__onnx_model.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_spec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[38;5;28mbool\u001b[39m, Collection[Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1111\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1110\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1111\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:955\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, (torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule)):\n\u001b[1;32m    954\u001b[0m     flattened_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_flatten(\u001b[38;5;28mtuple\u001b[39m(args))[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 955\u001b[0m     \u001b[43m_check_flatten_did_not_remove\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m     torch_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:943\u001b[0m, in \u001b[0;36m_check_flatten_did_not_remove\u001b[0;34m(original, jit_flattened)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m num_none \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_none:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs contained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_none\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m None\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms after flattening. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen exporting a ScriptModule or ScriptFunction, no args may \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe None because that breaks type propagation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    947\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: args contained 32 None's after flattening. When exporting a ScriptModule or ScriptFunction, no args may be None because that breaks type propagation."
     ]
    }
   ],
   "source": [
    "torch.onnx.export(tmm, (input_name,dummy_input),\"user__onnx_model.onnx\", verbose = True, input_names = input_name, output_names = [\"output_spec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d76cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47dce4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8503,  0.9634, -0.7635, -0.2634, -0.4005,  0.0717,  0.6939,  0.9627,\n",
       "         0.2049,  0.1340, -0.2913, -0.3506,  1.2504,  0.8410,  1.4949,  0.2318,\n",
       "         0.0424,  1.0670,  0.1694, -0.3616, -1.3760,  0.0022, -0.5034,  0.5992,\n",
       "         1.1692,  0.2213,  0.6310, -1.3881, -0.4218, -0.0618, -1.7155, -1.1066],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972e42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
