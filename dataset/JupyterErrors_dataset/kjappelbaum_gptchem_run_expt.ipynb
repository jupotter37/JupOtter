{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from loguru import logger\n",
    "from matbench.bench import MatbenchBenchmark\n",
    "from matbench.constants import CLF_KEY\n",
    "\n",
    "from gptchem.gpt_regressor import GPTRegressor\n",
    "from gptchem.tuner import Tuner\n",
    "\n",
    "logger.enable(\"gptchem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import decorator\n",
    "\n",
    "\n",
    "def retry(howmany, *exception_types, **kwargs):\n",
    "    timeout = kwargs.get(\"timeout\", 0.0)  # seconds\n",
    "\n",
    "    @decorator.decorator\n",
    "    def tryIt(func, *fargs, **fkwargs):\n",
    "        for _ in range(howmany):\n",
    "            try:\n",
    "                return func(*fargs, **fkwargs)\n",
    "            except exception_types or Exception as e:\n",
    "                print(e)\n",
    "                if timeout is not None:\n",
    "                    time.sleep(timeout)\n",
    "\n",
    "    return tryIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 08:45:19 INFO     Initialized benchmark 'matbench_v0.1' with 1 tasks: \n",
      "['matbench_expt_gap']\n"
     ]
    }
   ],
   "source": [
    "mb = MatbenchBenchmark(\n",
    "    autoload=True,\n",
    "    subset=[\n",
    "        \"matbench_expt_gap\",\n",
    "        # \"matbench_steels\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(3, timeout=5)\n",
    "def train_test_fold(task, fold):\n",
    "    regressor = GPTRegressor(\n",
    "        task.metadata[\"target\"],\n",
    "        Tuner(n_epochs=8, learning_rate_multiplier=0.02, wandb_sync=False),\n",
    "        querier_settings={\"max_tokens\": 10},\n",
    "    )\n",
    "    train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # train and validate your model\n",
    "    regressor.fit(train_inputs, train_outputs.values)\n",
    "\n",
    "    # Get testing data\n",
    "    test_inputs = task.get_test_data(fold, include_target=False)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    # Your output should be a pandas series, numpy array, or python iterable\n",
    "    # where the array elements are floats or bools\n",
    "    predictions = regressor.predict(test_inputs)\n",
    "\n",
    "    # Record your data!\n",
    "    task.record(fold, predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'matbench_expt_gap': [array([ 1.8 ,  0.  ,  0.  ,  0.21,  0.  , 10.4 ,  2.4 ,  2.6 ,  0.  ,\n",
       "                      1.6 ,  2.  ,  1.7 ,  2.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  3.76,  0.  ,  1.8 ,  0.  ,  0.  ,\n",
       "                      2.27,  0.  ,  3.9 ,  5.17,  0.  ,  0.  ,  1.92,  1.2 ,  3.2 ,\n",
       "                      2.37,  1.72,  1.35,  2.4 ,  2.3 ,  2.04,  3.17,  0.  ,  3.17,\n",
       "                      1.72,  2.32,  4.17,  0.  ,  0.  ,  1.72,  0.  ,  0.  ,  0.  ,\n",
       "                      1.3 ,  0.  ,  2.3 ,  2.3 ,  0.  ,  2.3 ,  0.  ,  1.58,  2.53,\n",
       "                      1.88,  0.  ,  3.89,  3.3 ,  0.  ,  0.  ,  5.13,  0.  ,  1.76,\n",
       "                      2.8 ,  2.3 ,  2.53,  2.17,  3.6 ,  0.  ,  3.2 ,  0.  ,  3.76,\n",
       "                      0.  ,  0.  ,  0.  ,  3.3 ,  0.  ,  0.3 ,  0.  ,  2.6 ,  3.04,\n",
       "                      2.3 ,  0.  ,  0.15,  0.  ,  0.  ,  2.  ,  0.  ,  0.  ,  2.3 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  3.6 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.8 ,  0.12,  0.28,  0.8 ,  2.3 ,  3.76,\n",
       "                      0.  ,  2.65,  3.76,  4.9 ,  2.47,  4.6 ,  0.  ,  1.04,  3.8 ,\n",
       "                      0.  ,  0.  ,  2.  ,  0.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  1.76,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  3.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.5 ,  0.  ,  0.  ,  0.  ,  0.  ,  2.3 ,  2.53,  1.35,\n",
       "                      2.43,  2.03,  1.  ,  0.  ,  2.3 ,  1.85,  2.32,  2.76,  1.75,\n",
       "                      6.53,  2.3 ,  0.  ,  0.8 ,  3.04,  4.91,  2.56,  0.  ,  3.3 ,\n",
       "                      1.6 ,  2.12,  1.8 ,  3.6 ,  0.  ,  2.6 ,  0.  ,  0.  ,  2.3 ,\n",
       "                      1.5 ,  1.6 ,  4.9 ,  2.56,  2.31,  2.12,  0.  ,  2.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.2 ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  1.8 ,  0.  ,  2.8 ,  1.18,\n",
       "                      1.  ,  0.  ,  0.  ,  0.  ,  1.8 ,  1.  ,  0.6 ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.  ,  0.  ,  1.5 ,  0.18,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  3.6 ,  0.  ,  0.  ,  2.3 ,  0.38,  0.48,  0.7 ,  1.38,\n",
       "                      0.68,  1.84,  1.84,  0.68,  1.3 ,  1.92,  1.98,  2.  ,  2.3 ,\n",
       "                      4.3 ,  0.  ,  0.  ,  2.3 ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "                      1.2 ,  0.  ,  2.26,  1.  ,  1.6 ,  0.  ,  0.32,  0.48,  0.35,\n",
       "                      0.38,  0.48,  0.  ,  1.3 ,  2.4 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,\n",
       "                      0.  ,  3.3 ,  3.04,  0.  ,  3.6 ,  0.  ,  1.8 ,  1.21,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  3.5 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.3 ,\n",
       "                      1.5 ,  1.3 ,  1.3 ,  1.3 ,  0.85,  1.8 ,  0.85,  0.85,  1.15,\n",
       "                      1.15,  1.06,  0.16,  0.12,  0.12,  0.16,  1.02,  0.  ,  0.  ,\n",
       "                      1.02,  0.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.3 ,  0.  ,\n",
       "                      0.9 ,  2.32,  3.32,  1.8 ,  2.9 ,  2.3 ,  3.2 ,  1.96,  0.85,\n",
       "                      1.8 ,  0.  ,  1.02,  2.3 ,  0.  ,  2.  ,  2.  ,  2.3 ,  1.35,\n",
       "                      0.7 ,  0.  ,  1.88,  2.  ,  2.3 ,  2.13,  1.8 ,  2.3 ,  3.2 ,\n",
       "                      2.53,  1.2 ,  0.  ,  1.8 ,  1.8 ,  0.  ,  0.  ,  0.  ,  2.  ,\n",
       "                      1.35,  2.8 ,  4.9 ,  1.6 ,  0.  ,  4.53,  0.  ,  1.21,  0.  ,\n",
       "                      2.3 ,  0.  ,  3.17,  1.6 ,  0.  ,  0.  ,  1.8 ,  0.  ,  2.53,\n",
       "                      1.  ,  0.  ,  0.  ,  0.  ,  2.02,  0.  ,  0.  ,  1.76,  0.  ,\n",
       "                      0.  ,  2.3 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.8 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.32,  0.  ,  3.6 ,  0.  ,  0.  ,  0.  ,  3.17,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.9 ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.76,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  4.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "                      3.4 ,  0.  ,  0.  ,  0.  ,  3.4 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  4.9 ,  1.76,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.2 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.32,  0.  ,  2.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.3 ,  0.  ,  0.  ,  1.58,  0.  ,  3.9 ,  0.  ,\n",
       "                      2.3 ,  2.3 ,  2.3 ,  0.  ,  4.9 ,  0.  ,  0.38,  0.  ,  0.  ,\n",
       "                      2.3 ,  2.8 ,  4.3 ,  0.  ,  0.23,  0.  ,  1.3 ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.28,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  1.8 ,  0.  ,  1.8 ,  0.  ,  0.  ,\n",
       "                      0.  ,  4.89,  0.  ,  0.  ,  0.  ,  3.  ,  2.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  1.8 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.3 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      1.18,  1.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.3 ,  0.  ,  0.  ,\n",
       "                      0.  ,  2.02,  2.3 ,  2.3 ,  0.  ,  1.6 ,  1.6 ,  1.06,  2.6 ,\n",
       "                      2.6 ,  1.7 ,  0.  ,  0.  ,  2.  ,  0.  ,  2.3 ,  0.43,  1.5 ,\n",
       "                      0.  ,  3.  ,  2.3 ,  0.  ,  3.53,  0.  ,  2.  ,  4.3 ,  0.  ,\n",
       "                      2.17,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,\n",
       "                      0.  ,  2.  ,  0.  ,  1.2 ,  0.4 ,  0.  ,  1.18,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      1.04,  0.93,  0.94,  0.93,  1.04,  1.8 ,  1.8 ,  0.  ,  1.2 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.76,  3.6 ,  2.  ,\n",
       "                      3.  ,  1.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.21,  0.21,\n",
       "                      0.32,  0.16,  0.45,  0.35,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,\n",
       "                      0.  ,  4.53,  0.  ,  2.9 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.28,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  6.53,  3.  ,  0.  ,\n",
       "                      2.67,  0.  ,  0.  ,  4.  ,  2.  ,  0.  ,  0.  ,  4.9 ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,\n",
       "                      2.23,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.2 ,  0.2 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.8 ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.  ,  3.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.  ,  0.  ,  0.  ,  2.6 ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.3 ,  0.95,  1.8 ,  1.6 ,  1.85,  0.  ,  0.  ,  0.  ,  0.23,\n",
       "                      1.5 ,  1.6 ,  0.  ,  0.  ,  0.  ,  2.17,  0.  ,  0.  ,  1.9 ,\n",
       "                      1.02,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  2.6 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      2.6 ,  1.8 ,  3.9 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.8 ,  0.  ,  1.2 ,\n",
       "                      0.  ,  0.  ,  1.  ,  0.4 ,  3.6 ,  0.  ,  2.65,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.2 ,  0.  ,  1.2 ,\n",
       "                      0.  ,  0.  ,  2.08,  1.8 ,  2.3 ,  1.8 ,  1.8 ,  1.71,  1.35,\n",
       "                      1.8 ,  1.5 ,  2.32,  0.  ,  3.1 ,  0.  ,  3.1 ,  2.8 ,  2.3 ,\n",
       "                      1.2 ,  4.65,  2.17,  0.  ,  0.  ,  2.63,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ]),\n",
       "              array([ 1.37,  0.  ,  1.6 ,  0.  ,  2.3 ,  1.3 ,  2.3 ,  1.  ,  2.4 ,\n",
       "                      2.3 ,  1.13,  2.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  3.7 ,  0.  ,  2.  ,  1.6 ,  0.  ,  2.  ,\n",
       "                      2.  ,  0.  ,  0.  ,  1.  ,  2.3 ,  0.  ,  3.76,  0.  ,  2.  ,\n",
       "                      0.  ,  0.  ,  1.45,  0.  ,  0.54,  2.23,  0.  ,  4.17,  0.  ,\n",
       "                      1.92,  0.  ,  1.37,  1.76,  2.13,  2.29,  0.  ,  1.37,  1.87,\n",
       "                      2.76,  1.37,  1.85,  3.08,  3.08,  0.  ,  1.99,  3.89,  2.2 ,\n",
       "                      3.17,  0.  ,  0.  ,  3.3 ,  3.2 ,  2.37,  0.  ,  0.  ,  3.8 ,\n",
       "                      2.3 ,  0.  ,  2.2 ,  3.6 ,  2.3 ,  0.  ,  0.  ,  3.  ,  2.1 ,\n",
       "                      2.6 ,  5.53,  0.  ,  0.  ,  0.  ,  0.  ,  0.12,  0.  ,  3.6 ,\n",
       "                      3.76,  3.76,  3.8 ,  1.6 ,  3.8 ,  0.28,  0.28,  0.2 ,  1.2 ,\n",
       "                      0.  ,  2.2 ,  2.4 ,  3.8 ,  1.3 ,  0.  ,  1.6 ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      3.3 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      4.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.13,  0.68,  0.28,  0.68,\n",
       "                      0.68,  0.68,  0.7 ,  0.7 ,  0.71,  3.17,  0.  ,  3.3 ,  2.7 ,\n",
       "                      0.  ,  4.76,  3.2 ,  1.8 ,  0.  ,  3.8 ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.6 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.6 ,  0.  ,\n",
       "                      0.  ,  0.34,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.  ,  1.25,  1.28,  0.  ,\n",
       "                      2.3 ,  1.4 ,  1.45,  1.98,  1.76,  2.  ,  1.64,  0.  ,  1.6 ,\n",
       "                      0.  ,  0.  ,  0.45,  2.13,  1.6 ,  2.4 ,  0.  ,  2.55,  0.  ,\n",
       "                      1.76,  2.9 ,  2.4 ,  0.  ,  4.17,  6.  ,  1.61,  1.43,  0.  ,\n",
       "                      0.  ,  0.  ,  4.42,  2.17,  0.  ,  0.  ,  2.08,  3.8 ,  4.  ,\n",
       "                      0.  ,  1.45,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.45,\n",
       "                      0.  ,  1.2 ,  2.6 ,  0.  ,  0.  ,  0.  ,  3.6 ,  1.6 ,  0.  ,\n",
       "                      0.  ,  2.46,  0.  ,  2.44,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.12,  0.  ,  0.  ,  0.  ,  3.8 ,  0.  ,  0.  ,  1.15,\n",
       "                      1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.5 ,  1.6 ,  0.  ,  0.  ,\n",
       "                      4.89,  0.  ,  3.  ,  3.8 ,  0.  ,  0.  ,  0.  ,  0.57,  0.7 ,\n",
       "                      0.7 ,  0.7 ,  1.85,  2.46,  2.46,  1.93,  2.2 ,  2.2 ,  1.5 ,\n",
       "                      1.08,  1.45,  1.  ,  2.75,  1.85,  0.  ,  4.8 ,  0.  ,  1.8 ,\n",
       "                      0.  ,  1.98,  0.  ,  4.6 ,  0.  ,  0.28,  0.4 ,  0.48,  0.34,\n",
       "                      0.  ,  0.  ,  2.3 ,  0.  ,  4.  ,  0.  ,  3.6 ,  1.6 ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  3.3 ,  0.  ,  1.85,  1.85,  2.67,  0.  ,  0.  ,\n",
       "                      2.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.28,  0.37,  1.85,  0.7 ,  1.29,\n",
       "                      1.45,  1.  ,  0.1 ,  0.16,  4.17,  1.63,  5.6 ,  1.9 ,  1.4 ,\n",
       "                      0.  ,  0.  ,  0.  ,  2.3 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.85,\n",
       "                      2.07,  0.  ,  2.2 ,  7.  ,  2.4 ,  2.75,  2.4 ,  3.6 ,  1.89,\n",
       "                      2.46,  2.37,  2.03,  2.08,  2.13,  2.07,  3.17,  1.98,  2.  ,\n",
       "                      2.08,  1.61,  0.  ,  2.4 ,  1.89,  1.76,  0.  ,  1.85,  1.85,\n",
       "                      1.54,  2.  ,  2.  ,  0.  ,  1.8 ,  1.  ,  2.5 , 10.  ,  1.85,\n",
       "                      0.  ,  1.85,  0.  ,  3.3 ,  1.8 ,  2.  ,  0.  ,  2.17,  0.  ,\n",
       "                      0.  ,  0.  ,  1.58,  0.  ,  0.  ,  0.  ,  0.  ,  2.6 ,  4.  ,\n",
       "                      0.  ,  1.76,  1.6 ,  0.  ,  2.  ,  0.  ,  0.  ,  0.  ,  2.  ,\n",
       "                      2.  ,  0.  ,  0.  ,  4.75,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,\n",
       "                      0.  ,  0.54,  0.67,  1.37,  0.  ,  0.  ,  0.  ,  5.  ,  3.6 ,\n",
       "                      0.  ,  0.  ,  3.2 ,  0.  ,  3.17,  2.26,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  3.6 ,  2.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.6 ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,  0.  , 10.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  4.6 ,  0.  ,  2.6 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.17,  0.  ,  1.12,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.06,\n",
       "                      0.  ,  0.  ,  2.5 ,  3.8 ,  0.  ,  2.45,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  2.  ,  0.  ,  3.  ,  0.64,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.17,  0.  ,  0.  ,  0.  ,  2.  ,  2.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  3.7 ,  0.  ,  0.  ,  2.54,  0.  ,  3.2 ,\n",
       "                      2.88,  2.26,  3.3 ,  2.6 ,  0.  ,  0.  ,  2.45,  0.  ,  3.89,\n",
       "                      3.5 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.6 ,  0.  ,  0.  ,\n",
       "                      0.  ,  4.17,  2.6 ,  1.15,  4.6 ,  0.  ,  1.76,  0.  ,  0.  ,\n",
       "                      1.76,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.6 ,  0.  ,  0.  ,\n",
       "                      2.  ,  1.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.28,  4.81,  3.7 ,\n",
       "                      0.  ,  4.  ,  2.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.35,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.  ,  3.6 ,  1.  ,  3.  ,  0.  ,  2.76,  5.53,  2.46,\n",
       "                      2.  ,  2.45,  1.76,  4.6 ,  2.3 ,  4.53,  0.  ,  3.3 ,  2.76,\n",
       "                      1.8 ,  5.53,  0.  ,  0.  ,  4.17,  3.17,  1.37,  2.1 ,  2.1 ,\n",
       "                      1.6 ,  1.45,  0.  ,  5.6 ,  1.28,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  3.  ,  0.  ,  3.6 ,  0.  ,  0.64,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      1.08,  0.85,  1.08,  0.85,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.4 ,  1.45,  1.11,  0.  ,  0.9 ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.8 ,  1.8 ,  3.37,  3.8 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.  ,  0.  ,  0.  ,  0.32,  0.22,  0.08,  0.04,  0.68,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,  0.  ,  0.  ,  3.  ,\n",
       "                      2.76,  0.  ,  0.  ,  3.76,  3.6 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  4.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      1.75,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.28,  0.8 ,  1.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  4.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  2.  ,  0.  ,  0.7 ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      1.9 ,  0.  ,  0.  ,  0.  ,  0.37,  0.71,  2.13,  0.  ,  2.  ,\n",
       "                      0.23,  1.6 ,  0.  ,  2.65,  3.1 ,  0.  ,  1.45,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  ,  1.8 ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.8 ,  0.  ,  1.98,  0.  ,  0.  ,  0.  ,  3.3 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  2.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.45,  1.8 ,  1.05,  1.45,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      2.45,  1.4 ,  1.68,  2.5 ,  1.35,  0.7 ,  1.4 ,  2.2 ,  2.2 ,\n",
       "                      1.8 ,  2.4 ,  1.  ,  1.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  2.6 ,\n",
       "                      2.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ]),\n",
       "              array([1.  , 0.  , 2.  , 0.  , 0.  , 0.  , 2.2 , 0.  , 1.2 , 1.25, 0.  ,\n",
       "                     0.  , 1.8 , 2.3 , 0.68, 0.  , 0.  , 0.  , 4.8 , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 5.6 , 4.  , 3.54, 0.  , 2.12, 5.53, 4.54, 1.32, 0.  , 1.32,\n",
       "                     1.62, 3.1 , 2.31, 1.62, 0.  , 0.  , 0.  , 1.35, 2.6 , 1.07, 0.  ,\n",
       "                     2.32, 0.  , 2.47, 3.3 , 4.9 , 3.2 , 3.99, 3.4 , 1.  , 1.64, 3.  ,\n",
       "                     1.8 , 3.9 , 1.64, 2.8 , 1.67, 0.  , 2.8 , 2.  , 0.  , 0.  , 0.  ,\n",
       "                     2.3 , 2.32, 2.3 , 1.45, 3.2 , 3.2 , 2.31, 0.  , 1.8 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 3.  , 0.  , 0.  , 4.8 , 0.  , 0.16,\n",
       "                     0.13, 0.21, 0.15, 0.15, 0.27, 3.3 , 0.28, 0.7 , 0.2 , 0.3 , 3.76,\n",
       "                     3.8 , 3.9 , 3.  , 0.  , 3.76, 0.  , 2.3 , 0.5 , 2.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 3.3 , 0.  , 6.3 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 2.8 , 0.69, 0.68, 0.21, 0.08, 0.7 , 0.38, 0.35, 0.7 , 0.68,\n",
       "                     2.12, 1.3 , 1.2 , 2.6 , 0.  , 3.  , 2.62, 2.32, 1.3 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 1.6 , 4.  , 0.  , 0.  , 3.  , 0.  , 0.  , 0.  , 3.76,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 1.98, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 1.5 , 1.05, 6.53, 0.  , 1.  , 0.  , 2.6 ,\n",
       "                     2.  , 2.3 , 2.25, 2.12, 0.  , 1.8 , 1.6 , 0.  , 2.3 , 2.3 , 1.6 ,\n",
       "                     0.  , 1.32, 3.9 , 2.6 , 0.  , 1.  , 0.  , 1.85, 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.11, 0.27, 0.3 ,\n",
       "                     0.  , 2.61, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.7 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 1.8 , 0.  , 1.9 , 1.8 , 0.  , 0.  , 0.13, 0.  , 0.  ,\n",
       "                     0.  , 1.8 , 0.8 , 3.53, 3.6 , 0.  , 1.35, 1.6 , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 3.  , 0.  , 0.  , 0.  , 0.  , 5.5 , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.3 , 0.71, 0.72, 0.69, 0.69, 1.58, 0.68, 2.58, 2.58,\n",
       "                     2.84, 2.54, 1.7 , 1.  , 1.8 , 0.68, 0.3 , 1.28, 0.7 , 3.  , 3.  ,\n",
       "                     1.71, 2.32, 0.  , 1.06, 2.43, 2.3 , 2.1 , 1.05, 0.  , 0.  , 2.  ,\n",
       "                     1.45, 0.  , 2.5 , 0.  , 1.21, 1.8 , 0.  , 0.  , 2.32, 1.45, 2.5 ,\n",
       "                     0.28, 0.28, 0.28, 0.32, 0.34, 0.34, 0.2 , 0.34, 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 2.  , 0.16, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 1.25, 1.25, 1.  ,\n",
       "                     1.25, 1.25, 1.  , 2.3 , 0.35, 0.34, 1.25, 1.3 , 1.25, 1.13, 1.06,\n",
       "                     1.85, 0.21, 0.12, 0.21, 2.54, 0.  , 1.  , 0.  , 2.73, 1.3 , 1.6 ,\n",
       "                     1.  , 1.8 , 1.5 , 1.  , 1.5 , 0.9 , 2.  , 1.  , 2.3 , 2.2 , 2.  ,\n",
       "                     2.3 , 1.71, 2.  , 3.3 , 1.  , 0.  , 0.  , 2.  , 3.4 , 4.4 , 2.3 ,\n",
       "                     2.2 , 0.  , 2.25, 0.  , 2.  , 3.2 , 0.  , 2.3 , 2.3 , 1.  , 1.45,\n",
       "                     0.  , 1.6 , 3.  , 0.  , 0.  , 2.3 , 2.  , 0.  , 2.  , 0.  , 2.3 ,\n",
       "                     1.8 , 2.63, 2.76, 2.4 , 0.  , 0.  , 1.9 , 0.  , 0.  , 1.6 , 2.  ,\n",
       "                     0.  , 0.  , 0.  , 3.64, 0.  , 0.  , 1.45, 0.  , 1.35, 2.25, 1.31,\n",
       "                     2.9 , 3.  , 3.3 , 0.  , 4.54, 0.  , 2.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.3 , 0.  , 1.62,\n",
       "                     0.  , 0.  , 3.2 , 0.  , 3.  , 2.3 , 2.31, 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 3.6 , 3.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.45, 0.  , 3.25, 0.  , 0.  ,\n",
       "                     2.3 , 0.  , 0.  , 1.05, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 1.07, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 3.75, 0.  ,\n",
       "                     0.  , 1.35, 0.  , 0.  , 0.  , 0.  , 0.  , 3.67, 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 1.8 , 0.  , 0.  , 0.  , 1.5 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 2.75, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 1.25, 0.  , 2.32, 2.28, 3.  , 2.13, 0.  , 0.  ,\n",
       "                     2.3 , 1.8 , 0.  , 0.  , 2.6 , 0.  , 0.  , 0.  , 0.  , 2.45, 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.2 , 4.25, 2.3 ,\n",
       "                     0.  , 2.  , 0.  , 0.  , 2.  , 0.75, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.06, 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 2.  , 2.3 , 0.  , 0.  , 1.  , 0.27, 0.  ,\n",
       "                     0.  , 0.  , 0.  , 1.6 , 0.  , 2.6 , 1.45, 0.  , 2.3 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.34, 4.76, 0.  ,\n",
       "                     2.3 , 0.  , 1.6 , 3.05, 6.53, 1.57, 0.  , 1.3 , 4.04, 0.  , 2.76,\n",
       "                     0.  , 0.  , 1.63, 0.  , 4.9 , 1.05, 2.3 , 3.  , 4.  , 0.  , 1.8 ,\n",
       "                     1.35, 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 3.3 , 0.  , 1.6 , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 3.76, 2.  , 2.06, 0.68,\n",
       "                     0.68, 1.  , 1.  , 0.  , 3.  , 0.  , 0.  , 3.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 1.5 , 0.  , 0.  , 0.  , 0.  , 3.8 , 0.  , 2.25, 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.13, 0.32, 0.02, 0.04, 0.13, 0.04, 0.13, 0.12, 0.32, 1.25, 3.  ,\n",
       "                     0.  , 0.  , 3.9 , 2.9 , 0.  , 0.  , 0.  , 0.  , 2.3 , 0.  , 0.  ,\n",
       "                     0.  , 2.13, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 3.9 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 1.15, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     2.  , 0.  , 0.  , 0.  , 0.  , 1.8 , 4.  , 0.  , 0.  , 1.8 , 0.  ,\n",
       "                     1.6 , 0.  , 0.  , 0.  , 2.06, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     3.76, 0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 1.6 , 0.  , 2.2 , 0.  ,\n",
       "                     0.  , 0.8 , 0.8 , 0.  , 0.8 , 0.95, 0.  , 0.  , 0.47, 2.2 , 1.9 ,\n",
       "                     1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.76,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 5.6 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 3.  , 0.  , 0.  , 0.  , 1.8 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     2.38, 2.3 , 1.8 , 1.5 , 1.  , 1.8 , 1.07, 2.3 , 0.  , 0.  , 0.  ,\n",
       "                     3.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 1.9 , 0.  , 0.  , 0.  , 0.  ]),\n",
       "              array([0.  , 0.65, 3.76, 0.  , 1.85, 0.  , 1.2 , 0.13, 0.  , 3.  , 1.28,\n",
       "                     2.3 , 1.6 , 1.5 , 2.3 , 1.4 , 0.  , 0.  , 0.  , 3.6 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 1.6 , 0.  , 0.  , 0.  , 0.  , 1.6 , 2.3 , 0.  ,\n",
       "                     2.53, 2.3 , 0.  , 0.  , 0.  , 2.64, 2.64, 0.  , 2.  , 0.  , 1.02,\n",
       "                     1.08, 1.35, 2.31, 1.35, 3.  , 1.22, 1.97, 2.9 , 1.35, 0.  , 2.37,\n",
       "                     0.  , 1.4 , 2.3 , 2.1 , 0.  , 3.3 , 0.  , 0.  , 0.  , 1.85, 2.21,\n",
       "                     2.85, 2.17, 0.  , 1.45, 0.  , 0.  , 2.22, 0.  , 0.  , 2.  , 1.85,\n",
       "                     0.  , 0.  , 2.  , 1.8 , 3.  , 3.76, 0.  , 3.  , 2.  , 0.  , 0.  ,\n",
       "                     1.8 , 1.85, 0.  , 0.  , 0.  , 1.5 , 0.  , 0.28, 0.26, 0.  , 3.9 ,\n",
       "                     0.  , 0.28, 0.28, 0.28, 0.2 , 3.1 , 0.  , 0.8 , 0.  , 0.  , 0.  ,\n",
       "                     4.3 , 0.  , 0.  , 0.  , 0.  , 0.  , 3.76, 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 4.  , 3.  , 2.45, 0.69, 0.93, 0.21, 0.12,\n",
       "                     0.29, 1.4 , 0.45, 2.3 , 0.  , 0.13, 1.07, 0.  , 0.  , 0.  , 2.45,\n",
       "                     1.8 , 2.17, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.93, 0.  ,\n",
       "                     0.  , 1.6 , 0.  , 0.  , 1.32, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 2.  , 0.  , 0.  , 3.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 2.3 , 6.53, 3.9 , 2.54, 1.8 , 1.45, 2.37, 0.  , 2.4 ,\n",
       "                     2.4 , 2.15, 1.45, 0.  , 2.2 , 2.3 , 2.6 , 2.21, 1.92, 2.35, 1.37,\n",
       "                     1.08, 5.54, 0.8 , 1.73, 1.85, 2.03, 2.  , 4.4 , 1.07, 0.  , 2.53,\n",
       "                     2.15, 1.58, 0.  , 0.  , 2.6 , 2.76, 3.8 , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     2.8 , 1.45, 1.2 , 1.2 , 3.  , 2.5 , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.9 , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.2 , 0.  , 1.15, 0.  , 1.2 ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 4.  , 0.5 , 0.  , 2.76, 1.8 , 0.  , 0.  , 0.  , 0.68,\n",
       "                     0.7 , 0.7 , 0.68, 1.45, 2.45, 1.85, 2.1 , 1.5 , 0.67, 1.3 , 1.8 ,\n",
       "                     2.45, 2.15, 2.4 , 1.85, 0.  , 2.  , 1.85, 0.  , 1.4 , 1.42, 0.  ,\n",
       "                     0.  , 2.17, 0.  , 0.  , 1.8 , 0.  , 0.  , 5.9 , 0.  , 0.  , 0.  ,\n",
       "                     3.2 , 0.32, 0.37, 0.37, 0.3 , 0.4 , 0.32, 0.8 , 2.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.85,\n",
       "                     0.  , 2.76, 1.6 , 2.4 , 6.  , 0.  , 1.5 , 1.5 , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.3 , 1.67, 2.2 , 1.5 , 1.45, 1.45, 1.15, 1.  , 1.22, 1.22, 2.13,\n",
       "                     0.23, 1.85, 0.38, 0.  , 0.  , 1.07, 1.8 , 1.6 , 0.55, 0.82, 1.07,\n",
       "                     3.76, 2.8 , 3.76, 2.15, 2.6 , 1.6 , 2.19, 2.64, 2.2 , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 2.2 , 2.7 , 5.76, 0.  , 0.  , 0.  , 1.  , 2.8 , 1.8 ,\n",
       "                     2.2 , 6.  , 0.  , 3.76, 5.  , 0.  , 0.  , 5.  , 0.  , 2.2 , 1.08,\n",
       "                     0.  , 1.  , 0.  , 2.5 , 5.  , 0.  , 0.  , 1.85, 0.  , 2.45, 1.85,\n",
       "                     1.98, 0.  , 0.  , 0.  , 3.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 2.47, 3.37, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 2.6 , 0.  , 0.  , 0.  , 2.9 , 0.  ,\n",
       "                     2.47, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 1.88, 0.  , 3.9 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 3.3 , 2.6 , 0.  , 0.  , 0.  , 0.  , 0.  , 2.3 ,\n",
       "                     1.5 , 0.  , 3.1 , 2.47, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 1.35, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.37, 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 3.3 , 2.35, 0.  , 2.54, 2.5 , 0.  , 0.  ,\n",
       "                     2.2 , 2.45, 0.  , 1.92, 0.  , 0.  , 3.  , 1.9 , 0.  , 9.6 , 4.  ,\n",
       "                     2.17, 2.  , 2.  , 0.  , 0.  , 0.  , 1.28, 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 1.85, 2.75, 5.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 1.8 , 0.  , 4.  , 0.  , 0.  , 0.  , 0.  , 4.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.53, 2.  ,\n",
       "                     2.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 1.15, 1.  , 0.  , 1.9 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     2.9 , 0.  , 0.  , 0.  , 1.76, 1.6 , 1.35, 0.  , 2.3 , 0.  , 2.45,\n",
       "                     1.8 , 2.3 , 2.3 , 2.45, 1.76, 4.89, 2.3 , 2.3 , 0.  , 0.  , 0.  ,\n",
       "                     1.98, 0.  , 0.  , 0.  , 0.  , 3.9 , 1.71, 0.  , 1.6 , 2.3 , 0.  ,\n",
       "                     2.3 , 0.  , 0.45, 0.  , 1.8 , 1.8 , 1.8 , 1.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 2.3 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.68,\n",
       "                     1.04, 0.7 , 0.  , 0.  , 0.  , 2.  , 0.  , 0.  , 0.  , 1.14, 0.  ,\n",
       "                     0.  , 0.  , 0.  , 1.8 , 0.  , 0.  , 0.  , 2.44, 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 2.  , 0.  , 0.06, 0.14, 0.  , 1.6 , 1.6 ,\n",
       "                     2.45, 1.8 , 0.  , 0.  , 0.  , 3.08, 3.2 , 0.  , 1.85, 4.63, 0.  ,\n",
       "                     0.  , 0.  , 0.  , 4.3 , 2.88, 0.  , 0.  , 0.  , 0.  , 2.  , 0.  ,\n",
       "                     2.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 2.37, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.26, 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.9 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     2.3 , 3.  , 0.  , 0.  , 1.  , 0.  , 1.8 , 1.35, 3.  , 1.8 , 0.  ,\n",
       "                     0.  , 2.9 , 0.  , 0.  , 0.85, 0.75, 1.8 , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     2.47, 2.45, 2.9 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 1.8 , 0.  , 4.76, 0.  , 0.  , 0.  , 0.  , 2.45, 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 3.  , 1.8 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.35, 0.  , 0.  ,\n",
       "                     0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.15,\n",
       "                     1.74, 1.6 , 1.5 , 1.5 , 2.15, 3.2 , 3.45, 2.9 , 2.15, 1.35, 0.  ,\n",
       "                     3.1 , 2.37, 2.31, 2.2 , 2.2 , 2.5 , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "                     0.  , 0.  , 3.3 , 0.  , 0.  , 0.  , 0.  , 0.  ]),\n",
       "              array([ 1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.3 ,  1.75,  2.3 ,  1.36,  2.25,  2.13,  1.6 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  4.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.8 ,  0.  ,  1.5 ,  3.  ,  0.  ,  4.  ,  2.12,  0.  ,\n",
       "                      0.  ,  0.  ,  1.88,  1.76,  1.36,  0.  ,  3.25,  2.64,  2.21,\n",
       "                      2.1 ,  2.3 ,  0.  ,  3.1 ,  2.21,  1.92,  3.  ,  2.25,  0.  ,\n",
       "                      0.  ,  0.  ,  2.32,  0.  ,  2.64,  0.  ,  1.88,  1.88,  2.6 ,\n",
       "                      0.  ,  2.06,  4.89,  1.92,  0.  ,  0.  ,  4.1 ,  0.  ,  0.  ,\n",
       "                      2.75,  0.  ,  2.4 ,  0.  ,  0.  ,  2.3 ,  0.  ,  0.  ,  3.64,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  1.5 ,  0.  ,  0.  ,  0.28,  0.  ,\n",
       "                      0.11,  0.  ,  0.7 ,  0.2 ,  0.28,  0.  ,  3.1 ,  2.6 ,  2.9 ,\n",
       "                      0.  ,  3.6 ,  0.  ,  0.  ,  2.75,  0.  ,  2.7 ,  0.  ,  3.1 ,\n",
       "                      3.9 ,  0.  ,  2.3 ,  0.  ,  0.  ,  0.  ,  2.35,  2.3 ,  0.  ,\n",
       "                      3.17,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      1.85,  0.  ,  0.68,  0.68,  0.21,  0.73,  0.8 ,  2.3 ,  0.  ,\n",
       "                      1.92,  2.4 ,  1.5 ,  0.  ,  2.12,  0.  ,  2.45,  3.9 ,  3.4 ,\n",
       "                      1.35, 10.9 ,  2.75,  0.  ,  3.67,  0.  ,  0.  ,  0.  ,  1.02,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  3.9 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  0.  ,\n",
       "                      4.19,  0.  ,  2.64,  2.64,  0.  ,  0.  ,  1.92,  1.85,  4.49,\n",
       "                      0.  ,  2.06,  2.35,  2.1 ,  4.9 ,  0.  ,  2.64,  4.89,  0.  ,\n",
       "                      0.  ,  2.64,  2.  ,  1.45,  0.89,  0.  ,  1.35,  1.36,  2.12,\n",
       "                      1.95,  2.45, 10.6 ,  2.35,  5.58,  4.9 ,  2.3 ,  2.5 ,  2.6 ,\n",
       "                      1.85,  4.3 ,  0.  ,  0.  ,  2.5 ,  2.36,  1.37,  1.37,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  1.85,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  2.9 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.08,  0.  ,  1.45,\n",
       "                      0.  ,  3.1 ,  1.6 ,  1.6 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.37,\n",
       "                      0.72,  0.72,  0.68,  1.75,  1.6 ,  1.45,  1.85,  1.05,  2.41,\n",
       "                      2.09,  0.  ,  0.  ,  0.  ,  0.  ,  0.68,  0.  ,  3.4 ,  0.  ,\n",
       "                      3.2 ,  0.  ,  4.99,  1.  ,  1.8 ,  0.  ,  0.37,  0.21,  0.35,\n",
       "                      4.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.64,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  2.4 ,  3.4 ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.53,  1.5 ,  1.5 ,  0.68,  1.28,  1.45,  0.45,  0.1 ,\n",
       "                      0.1 ,  4.  ,  0.  ,  1.6 ,  0.  ,  0.  ,  0.  ,  0.4 ,  1.6 ,\n",
       "                      2.9 ,  0.  ,  2.1 ,  2.3 ,  0.46,  0.  ,  0.  ,  2.3 ,  0.36,\n",
       "                      2.32,  3.25,  1.95,  1.75,  0.  ,  2.63,  2.  ,  3.1 ,  2.45,\n",
       "                      1.85,  0.  ,  1.88,  0.  ,  2.12,  2.2 ,  1.85,  2.2 ,  1.8 ,\n",
       "                      1.85,  0.  ,  1.6 ,  0.  ,  3.9 ,  2.2 ,  0.  ,  4.  ,  3.9 ,\n",
       "                      3.2 ,  1.85,  0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  1.85,  2.  ,  1.85,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  2.64,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  4.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  4.1 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  4.1 ,  1.6 ,  0.  ,  0.  ,  3.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  4.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.4 ,\n",
       "                      0.  ,  0.  ,  5.63,  5.89,  0.  ,  1.6 ,  3.  ,  4.  ,  2.75,\n",
       "                      0.  ,  4.9 ,  2.64,  0.  ,  0.  ,  1.6 ,  2.9 ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  4.  ,  0.  ,  0.  ,  0.  ,  3.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.3 ,  0.  ,  0.  ,  0.  ,  0.  ,  1.55,  1.6 ,\n",
       "                      0.  ,  0.  ,  2.3 ,  1.85,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.  ,  0.  ,\n",
       "                      0.  ,  0.27,  0.28,  5.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.5 ,  1.45,  1.6 ,  1.8 ,\n",
       "                      0.  ,  0.  ,  0.  ,  2.32,  2.3 ,  0.  ,  2.64,  3.1 ,  0.  ,\n",
       "                      2.3 ,  0.  ,  2.2 ,  0.  ,  0.  ,  2.  ,  0.  ,  0.  ,  4.99,\n",
       "                      3.64,  0.  ,  2.21,  1.85,  3.6 ,  0.  ,  4.1 ,  3.  ,  3.2 ,\n",
       "                      1.95,  0.  ,  0.  ,  0.  ,  0.  ,  0.  , 10.6 ,  0.  ,  0.  ,\n",
       "                      0.  ,  2.4 ,  0.  ,  0.  ,  1.85,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.04,  1.08,  1.04,\n",
       "                      1.  ,  1.8 ,  1.8 ,  0.  ,  0.  ,  1.1 ,  0.  ,  0.  ,  3.4 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.12,  0.12,  0.02,  0.12,  0.14,  0.13,  0.12,\n",
       "                      1.28,  1.28,  0.  ,  0.  ,  0.  ,  2.6 ,  4.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.45,  0.  ,  3.58,  3.1 ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  1.92,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  1.8 ,  0.  ,  2.8 ,  0.  ,  0.  ,  0.  ,\n",
       "                      4.9 ,  0.  ,  0.  ,  0.  ,  0.  ,  3.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.8 ,  0.2 ,  0.26,  0.  ,  0.  ,  0.  ,  0.  ,  1.5 ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.37,  0.32,  1.6 ,  0.  ,  1.8 ,  0.  ,  0.  ,  2.3 ,\n",
       "                      0.  ,  0.  ,  0.  ,  2.2 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.37,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  2.48,\n",
       "                      0.  ,  2.9 ,  2.4 ,  3.  ,  0.  ,  0.  ,  0.  ,  2.9 ,  0.  ,\n",
       "                      0.4 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  2.35,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  0.  ,  0.  ,  3.9 ,  0.  ,  0.  ,  3.06,  1.2 ,\n",
       "                      1.8 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.35,  0.  ,\n",
       "                      0.  ,  4.67,  0.  ,  0.  ,  1.85,  3.5 ,  1.37,  0.72,  0.68,\n",
       "                      1.6 ,  2.3 ,  2.18,  1.36,  1.36,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ,  2.63,  1.35,  0.  ,  4.45,  3.1 ,  0.  ,  0.  ,\n",
       "                      0.  ,  1.93,  1.92,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "                      0.  ,  0.  ])]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.xtras import save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\"expt_gap_predictions.pkl\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 08:45:23 INFO     Dataset matbench_expt_gap already loaded; not reloading dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 450Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_084523/train.jsonl: file-NWr8LeSxKwsDCTtHYgmFt1Ny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 08:45:27.296 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675410327,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675410327,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-yIz4t695ZKn7tXVQL4Cso2bI\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-yIz4t695ZKn7tXVQL4Cso2bI\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420861,\n",
      "      \"created_at\": 1675410327,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_084523/train.jsonl\",\n",
      "      \"id\": \"file-NWr8LeSxKwsDCTtHYgmFt1Ny\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675410327,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 08:45:27.554 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 08:47:28.070 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 08:49:29.202 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 08:51:29.699 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 08:53:30.245 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 08:55:30.749 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 08:57:31.249 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 08:59:31.733 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:01:32.213 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:03:32.705 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:05:33.207 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:07:33.702 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:09:34.180 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:11:34.679 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:13:35.169 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:15:35.654 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:17:36.157 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:19:36.648 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:21:37.145 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:23:37.650 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:25:38.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:27:38.654 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:29:39.157 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:31:39.720 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:33:40.825 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 09:35:41.389 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 09:35:41.449 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_084523', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_084523/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-08-35-02', 'ft_id': 'ft-yIz4t695ZKn7tXVQL4Cso2bI', 'date': '20230203_093541', 'train_file_id': 'file-NWr8LeSxKwsDCTtHYgmFt1Ny', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 09:37:41 INFO     Recorded fold matbench_expt_gap-0 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 302Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_093741/train.jsonl: file-bQqYKzdA9xzBQS8hG4FeFnJs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 09:37:44.407 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675413464,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675413464,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-eRKDdXkL8cYRIrxQqz69l1uq\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-eRKDdXkL8cYRIrxQqz69l1uq\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420507,\n",
      "      \"created_at\": 1675413463,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_093741/train.jsonl\",\n",
      "      \"id\": \"file-bQqYKzdA9xzBQS8hG4FeFnJs\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675413464,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 09:37:44.579 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:39:45.080 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:41:45.775 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:43:46.356 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:45:46.963 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:47:47.524 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:49:48.117 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:51:48.819 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:53:49.379 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:55:49.918 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:57:50.610 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 09:59:51.091 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:01:51.701 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:03:52.318 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:05:52.905 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:07:53.519 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:09:54.095 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:11:55.008 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:13:55.752 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:15:56.406 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:17:56.990 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:19:57.583 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:21:58.169 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:23:58.748 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:25:59.337 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:27:59.929 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:30:00.865 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 10:32:01.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:34:02.028 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:36:19.702 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:38:20.722 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:40:21.485 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:42:22.062 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:44:22.646 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:46:23.230 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:48:24.337 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:50:24.965 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:52:25.553 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:54:26.161 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:56:26.723 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 10:58:27.446 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:00:28.012 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:02:28.591 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:04:29.170 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:06:29.647 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:08:30.198 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:10:31.277 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:12:32.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:14:32.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:16:33.210 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:18:33.788 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 11:18:33.790 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_093741', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_093741/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-10-17-26', 'ft_id': 'ft-eRKDdXkL8cYRIrxQqz69l1uq', 'date': '20230203_111833', 'train_file_id': 'file-bQqYKzdA9xzBQS8hG4FeFnJs', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 11:20:34 INFO     Recorded fold matbench_expt_gap-1 successfully.\n",
      "Input contains NaN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 167Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_112039/train.jsonl: file-gOooc5Dse3YozVMdmduEdhpx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 11:20:41.235 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675419641,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675419641,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-rpcxotTLK7bCdEyNHzmHsf4R\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-rpcxotTLK7bCdEyNHzmHsf4R\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420507,\n",
      "      \"created_at\": 1675419640,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_112039/train.jsonl\",\n",
      "      \"id\": \"file-gOooc5Dse3YozVMdmduEdhpx\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675419641,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 11:20:41.541 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:22:42.124 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:24:42.707 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:26:43.294 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:28:43.873 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:30:44.747 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:32:45.605 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:34:46.201 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:36:46.759 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:38:47.351 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:40:47.948 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:42:48.425 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:44:49.018 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:46:49.606 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:48:50.160 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:50:50.738 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:52:51.291 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:54:51.867 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 11:56:52.439 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 11:58:53.235 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:00:53.773 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:02:54.392 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:04:54.954 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:06:55.557 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:08:56.122 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:10:56.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:12:57.696 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:14:58.266 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:16:58.850 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:18:59.441 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:21:00.025 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:23:01.144 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:25:01.770 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:27:02.363 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:29:03.119 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:31:03.685 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:33:04.291 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:35:04.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:37:05.992 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:39:06.678 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:41:07.272 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 12:43:07.849 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 12:43:07.852 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_112039', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_112039/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-11-42-04', 'ft_id': 'ft-rpcxotTLK7bCdEyNHzmHsf4R', 'date': '20230203_124307', 'train_file_id': 'file-gOooc5Dse3YozVMdmduEdhpx', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 12:45:08 ERROR    Fold number 1 already recorded! Aborting record...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 420k/420k [00:00<00:00, 571Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_124508/train.jsonl: file-HBe4wSdRdrFggVpGBiUqgbXr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 12:45:10.192 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675424710,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675424710,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-fLBXyQh9pCX9kUG0OUlZTVBk\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-fLBXyQh9pCX9kUG0OUlZTVBk\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420263,\n",
      "      \"created_at\": 1675424709,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_124508/train.jsonl\",\n",
      "      \"id\": \"file-HBe4wSdRdrFggVpGBiUqgbXr\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675424710,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 12:45:10.378 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:47:10.976 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:49:11.567 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:51:12.166 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:53:12.758 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:55:13.358 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:57:13.952 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 12:59:14.600 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 13:01:15.199 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:03:15.783 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:05:16.379 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:13:02.825 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:25:13.372 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:27:14.005 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:29:14.590 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:31:15.364 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:33:15.950 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:35:16.539 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:37:17.127 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:39:17.937 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:41:18.534 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:43:19.275 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:45:19.874 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:47:20.463 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:49:21.142 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 13:49:21.144 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_124508', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_124508/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-12-47-46', 'ft_id': 'ft-fLBXyQh9pCX9kUG0OUlZTVBk', 'date': '20230203_134921', 'train_file_id': 'file-HBe4wSdRdrFggVpGBiUqgbXr', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 13:51:21 INFO     Recorded fold matbench_expt_gap-2 successfully.\n",
      "Input contains NaN.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 420k/420k [00:00<00:00, 596Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_135126/train.jsonl: file-jnK2HBlAYazUFOonzyb3POn1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 13:51:28.472 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675428688,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675428688,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-teeVcLBbuZ9c6PbFoIEttvDg\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-teeVcLBbuZ9c6PbFoIEttvDg\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420263,\n",
      "      \"created_at\": 1675428688,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_135126/train.jsonl\",\n",
      "      \"id\": \"file-jnK2HBlAYazUFOonzyb3POn1\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675428688,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 13:51:28.684 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 13:53:29.271 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:55:30.046 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:57:30.850 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 13:59:32.091 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:01:32.625 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:03:33.131 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:05:33.691 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:07:34.274 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:09:35.118 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:11:35.586 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:13:36.159 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:15:37.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:17:37.713 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:19:38.426 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:21:39.055 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:23:39.644 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:25:40.152 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:27:40.899 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:29:41.478 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:31:42.464 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:33:43.164 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:35:43.756 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:37:44.480 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:39:45.618 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 14:39:45.634 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_135126', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_135126/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-13-38-54', 'ft_id': 'ft-teeVcLBbuZ9c6PbFoIEttvDg', 'date': '20230203_143945', 'train_file_id': 'file-jnK2HBlAYazUFOonzyb3POn1', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 14:41:45 ERROR    Fold number 2 already recorded! Aborting record...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 395Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_144145/train.jsonl: file-0chdGUBP1t0iIsuFGBpVsidy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 14:41:51.107 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675431711,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675431711,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-8Mj6ZnCdb8DcKKraPXcZ27cs\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-8Mj6ZnCdb8DcKKraPXcZ27cs\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420723,\n",
      "      \"created_at\": 1675431710,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_144145/train.jsonl\",\n",
      "      \"id\": \"file-0chdGUBP1t0iIsuFGBpVsidy\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675431711,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 14:41:51.403 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 14:43:51.986 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 14:45:52.572 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:47:53.155 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:49:53.734 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:51:54.320 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:53:55.469 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:55:56.036 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:57:56.572 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 14:59:57.213 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:01:57.810 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:03:58.505 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:05:59.084 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:07:59.633 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:10:00.573 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:12:02.829 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:14:03.967 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:16:04.622 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:18:05.235 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:20:06.047 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:22:06.624 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:24:07.209 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:26:07.831 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:28:08.331 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:30:08.919 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:32:09.674 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 15:32:09.676 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_144145', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_144145/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-14-30-43', 'ft_id': 'ft-8Mj6ZnCdb8DcKKraPXcZ27cs', 'date': '20230203_153209', 'train_file_id': 'file-0chdGUBP1t0iIsuFGBpVsidy', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 15:34:10 INFO     Recorded fold matbench_expt_gap-3 successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|██████████| 421k/421k [00:00<00:00, 235Mit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file from /Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_153410/train.jsonl: file-0DyHA941zMG5z60OCMTOtA3X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 15:34:14.019 | DEBUG    | gptchem.tuner:tune:188 - Requested fine tuning. {\n",
      "  \"created_at\": 1675434853,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"created_at\": 1675434853,\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-ubLeh9LTZYJSvLHFdlhEM6SN\",\n",
      "      \"object\": \"fine-tune-event\"\n",
      "    }\n",
      "  ],\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"hyperparams\": {\n",
      "    \"batch_size\": null,\n",
      "    \"learning_rate_multiplier\": 0.02,\n",
      "    \"n_epochs\": 8,\n",
      "    \"prompt_loss_weight\": 0.01\n",
      "  },\n",
      "  \"id\": \"ft-ubLeh9LTZYJSvLHFdlhEM6SN\",\n",
      "  \"model\": \"ada\",\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"organization_id\": \"org-TFRJXw3PPQocOWbu71eI2t9U\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"pending\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"bytes\": 420710,\n",
      "      \"created_at\": 1675434853,\n",
      "      \"filename\": \"/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_153410/train.jsonl\",\n",
      "      \"id\": \"file-0DyHA941zMG5z60OCMTOtA3X\",\n",
      "      \"object\": \"file\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"status\": \"uploaded\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"updated_at\": 1675434853,\n",
      "  \"validation_files\": []\n",
      "}\n",
      "2023-02-03 15:34:14.289 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 15:36:14.877 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: pending\n",
      "2023-02-03 15:38:15.462 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:40:16.052 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:42:16.642 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:44:17.229 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:46:17.816 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:48:18.420 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:50:18.959 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:52:20.074 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:54:20.725 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:56:21.310 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 15:58:21.894 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:00:22.409 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:02:23.004 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:04:23.887 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:10:30.724 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:12:31.336 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:14:32.059 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:16:32.615 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:18:33.213 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:20:33.983 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:22:34.500 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:24:35.022 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: running\n",
      "2023-02-03 16:26:35.576 | DEBUG    | gptchem.tuner:get_ft_model_name:29 - Fine tuning status: succeeded\n",
      "2023-02-03 16:26:35.580 | DEBUG    | gptchem.tuner:tune:207 - Fine tuning completed. {'base_model': 'ada', 'batch_size': None, 'n_epochs': 8, 'learning_rate_multiplier': 0.02, 'run_name': None, 'wandb_sync': False, 'outdir': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_153410', 'train_filename': '/Users/kevinmaikjablonka/git/kjappelbaum/gptchem/experiments/04_regression/matbench/out/20230203_153410/train.jsonl', 'valid_filename': 'None', 'model_name': 'ada:ft-lsmoepfl-2023-02-03-15-25-03', 'ft_id': 'ft-ubLeh9LTZYJSvLHFdlhEM6SN', 'date': '20230203_162635', 'train_file_id': 'file-0DyHA941zMG5z60OCMTOtA3X', 'valid_file_id': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 16:27:44 INFO     Recorded fold matbench_expt_gap-4 successfully.\n"
     ]
    }
   ],
   "source": [
    "predictions = defaultdict(list)\n",
    "\n",
    "for task in mb.tasks:\n",
    "    task.load()\n",
    "\n",
    "    for fold_ind, fold in enumerate(task.folds):\n",
    "        if task.is_recorded[fold_ind]:\n",
    "            print(f\"Skipping fold {fold_ind} of {task.dataset_name}\")\n",
    "            continue\n",
    "        pred = train_test_fold(task, fold)\n",
    "        predictions[task.dataset_name].append(pred)\n",
    "        train_inputs, train_outputs = task.get_train_and_val_data(fold)\n",
    "\n",
    "    # print(f\"{task.dataset_name}: MAE  {task.scores['mae']['mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid __array_struct__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task\u001b[39m.\u001b[39;49mscores\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/matbench/task.py:650\u001b[0m, in \u001b[0;36mMatbenchTask.scores\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m     raw_metrics_on_folds \u001b[39m=\u001b[39m [\n\u001b[1;32m    646\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults[fk][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_SCORES_KEY][mk]\n\u001b[1;32m    647\u001b[0m         \u001b[39mfor\u001b[39;00m fk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfolds_map\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    648\u001b[0m     ]\n\u001b[1;32m    649\u001b[0m     \u001b[39mfor\u001b[39;00m op \u001b[39min\u001b[39;00m FOLD_DIST_METRICS:\n\u001b[0;32m--> 650\u001b[0m         metric[op] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(np, op)(raw_metrics_on_folds)\n\u001b[1;32m    651\u001b[0m     scores[mk] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m RecursiveDotDict(scores)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3430\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3432\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3433\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptchem/lib/python3.9/site-packages/numpy/core/_methods.py:164\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 164\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    166\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid __array_struct__"
     ]
    }
   ],
   "source": [
    "task.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 18:53:04 INFO     Successfully wrote MatbenchBenchmark to file 'gpt_steel_bench.json.gz'.\n"
     ]
    }
   ],
   "source": [
    "mb.to_file(\"gpt_expt_gap_bench.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f3b9074e5baa1438c27e2ea813f7f53b7516c83bd70840b6d64eae6820ee5df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
