{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-671ccdeb75bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "def extract_image_patches(image, patch_dims, step):\n",
    "    patches = []\n",
    "    for ii in range(0,image.shape[0],step[0]):\n",
    "        for jj in range(0,image.shape[1],step[1]):\n",
    "            patch = (image[ii:(ii+patch_dims[0]),jj:(jj+patch_dims[1])]).tolist()\n",
    "            patches.append(patch)\n",
    "            \n",
    "    return np.asarray(patches)\n",
    "\n",
    "def combine_image_patches(patches, patch_dims, step, image_dims):\n",
    "    image = np.empty(image_dims, dtype=int)\n",
    "    cnt = 0\n",
    "    for ii in range(0,image.shape[0],step[0]):\n",
    "        for jj in range(0,image.shape[1],step[1]):\n",
    "            image[ii:(ii+patch_dims[0]),jj:(jj+patch_dims[1])] = np.maximum(\n",
    "                np.minimum(np.rint(patches[cnt]),256),0)\n",
    "            cnt = cnt+1\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "patch_dims = [8,8]\n",
    "image_dims = [512,512]\n",
    "input_image= imageio.imread('./data/0.bmp', flatten= 0)\n",
    "patches = extract_image_patches(input_image, patch_dims, patch_dims)\n",
    "print(patches.shape)\n",
    "patches_reshaped = patches.reshape(int(image_dims[0]/patch_dims[0]*image_dims[1]/patch_dims[1]),\n",
    "                                   patch_dims[0]*patch_dims[1])/256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_units = patch_dims[0]*patch_dims[1]\n",
    "hidden_units = 128\n",
    "display_step = 100\n",
    "X = tf.placeholder(\"float\", [None, input_units])\n",
    "weights = {\n",
    "    'encoder':tf.Variable(tf.random_normal([input_units, hidden_units])),\n",
    "    'decoder':tf.Variable(tf.random_normal([hidden_units, input_units]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder':tf.Variable(tf.random_normal([hidden_units])),\n",
    "    'decoder':tf.Variable(tf.random_normal([input_units]))\n",
    "}\n",
    "\n",
    "def encoder(x):\n",
    "    layer = tf.nn.softmax(tf.add(tf.matmul(x,weights['encoder']),biases['encoder']))\n",
    "    #layer = tf.add(tf.matmul(x,weights['encoder']),biases['encoder'])\n",
    "    return layer\n",
    "                          \n",
    "def decoder(x):\n",
    "    layer = tf.nn.sigmoid(tf.add(tf.matmul(x,weights['decoder']),biases['decoder']))\n",
    "    #layer = tf.add(tf.matmul(x,weights['decoder']),biases['decoder'])\n",
    "    return layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss: 0.110585\n",
      "Step 100: Minibatch Loss: 0.071156\n",
      "Step 200: Minibatch Loss: 0.051530\n",
      "Step 300: Minibatch Loss: 0.042964\n",
      "Step 400: Minibatch Loss: 0.039320\n",
      "Step 500: Minibatch Loss: 0.037662\n",
      "Step 600: Minibatch Loss: 0.036432\n",
      "Step 700: Minibatch Loss: 0.035094\n",
      "Step 800: Minibatch Loss: 0.033512\n",
      "Step 900: Minibatch Loss: 0.031685\n",
      "Step 1000: Minibatch Loss: 0.029666\n",
      "Step 1100: Minibatch Loss: 0.027533\n",
      "Step 1200: Minibatch Loss: 0.025364\n",
      "Step 1300: Minibatch Loss: 0.023234\n",
      "Step 1400: Minibatch Loss: 0.021219\n",
      "Step 1500: Minibatch Loss: 0.019384\n",
      "Step 1600: Minibatch Loss: 0.017775\n",
      "Step 1700: Minibatch Loss: 0.016409\n",
      "Step 1800: Minibatch Loss: 0.015273\n",
      "Step 1900: Minibatch Loss: 0.014338\n",
      "Step 2000: Minibatch Loss: 0.013572\n",
      "Step 2100: Minibatch Loss: 0.012943\n",
      "Step 2200: Minibatch Loss: 0.012424\n",
      "Step 2300: Minibatch Loss: 0.011993\n",
      "Step 2400: Minibatch Loss: 0.011632\n",
      "Step 2500: Minibatch Loss: 0.011327\n",
      "Step 2600: Minibatch Loss: 0.011066\n",
      "Step 2700: Minibatch Loss: 0.010840\n",
      "Step 2800: Minibatch Loss: 0.010643\n",
      "Step 2900: Minibatch Loss: 0.010469\n",
      "Step 3000: Minibatch Loss: 0.010315\n",
      "Step 3100: Minibatch Loss: 0.010175\n",
      "Step 3200: Minibatch Loss: 0.010049\n",
      "Step 3300: Minibatch Loss: 0.009934\n",
      "Step 3400: Minibatch Loss: 0.009828\n",
      "Step 3500: Minibatch Loss: 0.009730\n",
      "Step 3600: Minibatch Loss: 0.009638\n",
      "Step 3700: Minibatch Loss: 0.009553\n",
      "Step 3800: Minibatch Loss: 0.009472\n",
      "Step 3900: Minibatch Loss: 0.009396\n",
      "Step 4000: Minibatch Loss: 0.009324\n",
      "Step 4100: Minibatch Loss: 0.009255\n",
      "Step 4200: Minibatch Loss: 0.009189\n",
      "Step 4300: Minibatch Loss: 0.009126\n",
      "Step 4400: Minibatch Loss: 0.009066\n",
      "Step 4500: Minibatch Loss: 0.009007\n",
      "Step 4600: Minibatch Loss: 0.008951\n",
      "Step 4700: Minibatch Loss: 0.008896\n",
      "Step 4800: Minibatch Loss: 0.008843\n",
      "Step 4900: Minibatch Loss: 0.008791\n",
      "Step 5000: Minibatch Loss: 0.008740\n",
      "Step 5100: Minibatch Loss: 0.008691\n",
      "Step 5200: Minibatch Loss: 0.008642\n",
      "Step 5300: Minibatch Loss: 0.008595\n",
      "Step 5400: Minibatch Loss: 0.008548\n",
      "Step 5500: Minibatch Loss: 0.008502\n",
      "Step 5600: Minibatch Loss: 0.008457\n",
      "Step 5700: Minibatch Loss: 0.008412\n",
      "Step 5800: Minibatch Loss: 0.008368\n",
      "Step 5900: Minibatch Loss: 0.008325\n",
      "Step 6000: Minibatch Loss: 0.008282\n",
      "Step 6100: Minibatch Loss: 0.008240\n",
      "Step 6200: Minibatch Loss: 0.008198\n",
      "Step 6300: Minibatch Loss: 0.008156\n",
      "Step 6400: Minibatch Loss: 0.008115\n",
      "Step 6500: Minibatch Loss: 0.008074\n",
      "Step 6600: Minibatch Loss: 0.008034\n",
      "Step 6700: Minibatch Loss: 0.007994\n",
      "Step 6800: Minibatch Loss: 0.007954\n",
      "Step 6900: Minibatch Loss: 0.007914\n",
      "Step 7000: Minibatch Loss: 0.007875\n",
      "Step 7100: Minibatch Loss: 0.007836\n",
      "Step 7200: Minibatch Loss: 0.007797\n",
      "Step 7300: Minibatch Loss: 0.007759\n",
      "Step 7400: Minibatch Loss: 0.007720\n",
      "Step 7500: Minibatch Loss: 0.007682\n",
      "Step 7600: Minibatch Loss: 0.007645\n",
      "Step 7700: Minibatch Loss: 0.007607\n",
      "Step 7800: Minibatch Loss: 0.007570\n",
      "Step 7900: Minibatch Loss: 0.007533\n",
      "Step 8000: Minibatch Loss: 0.007496\n",
      "Step 8100: Minibatch Loss: 0.007459\n",
      "Step 8200: Minibatch Loss: 0.007423\n",
      "Step 8300: Minibatch Loss: 0.007387\n",
      "Step 8400: Minibatch Loss: 0.007351\n",
      "Step 8500: Minibatch Loss: 0.007315\n",
      "Step 8600: Minibatch Loss: 0.007280\n",
      "Step 8700: Minibatch Loss: 0.007245\n",
      "Step 8800: Minibatch Loss: 0.007210\n",
      "Step 8900: Minibatch Loss: 0.007175\n",
      "Step 9000: Minibatch Loss: 0.007141\n",
      "Step 9100: Minibatch Loss: 0.007107\n",
      "Step 9200: Minibatch Loss: 0.007074\n",
      "Step 9300: Minibatch Loss: 0.007040\n",
      "Step 9400: Minibatch Loss: 0.007007\n",
      "Step 9500: Minibatch Loss: 0.006975\n",
      "Step 9600: Minibatch Loss: 0.006942\n",
      "Step 9700: Minibatch Loss: 0.006910\n",
      "Step 9800: Minibatch Loss: 0.006879\n",
      "Step 9900: Minibatch Loss: 0.006847\n",
      "Step 10000: Minibatch Loss: 0.006816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aliaksei/anaconda3/lib/python3.6/site-packages/imageio/core/util.py:104: UserWarning: Conversion from int64 to uint8, range [29, 202]\n",
      "  'range [{2}, {3}]'.format(dtype_str, out_type.__name__, mi, ma))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "num_steps = 10000\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "y_pred = decoder_op\n",
    "y_true = X\n",
    "\n",
    "loss = tf.reduce_mean(tf.pow(y_pred - y_true, 2))\n",
    "#optimiser = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\n",
    "optimiser = tf.train.MomentumOptimizer(learning_rate=lr,momentum =0.99).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for ii in range(1,num_steps+1):\n",
    "        for jj in range(0,(len(patches_reshaped)-batch_size),batch_size):\n",
    "            end = jj+batch_size\n",
    "            batch = patches_reshaped[jj:end,:]\n",
    "            \n",
    "            _, l = sess.run([optimiser, loss], feed_dict={X:batch})\n",
    "\n",
    "        if ii % display_step == 0 or ii == 1:\n",
    "            print('Step %i: Minibatch Loss: %f' % (ii, l))\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    g = sess.run([decoder_op], feed_dict={X:patches_reshaped})\n",
    "    \n",
    "\n",
    "patches_new = g[0].reshape(int(image_dims[0]/patch_dims[0]*image_dims[1]/patch_dims[1]),\n",
    "                                   patch_dims[0],\n",
    "                                   patch_dims[1])*256\n",
    "image = combine_image_patches(patches_new, patch_dims, patch_dims,image_dims)\n",
    "imageio.imsave(\"rec.png\",im=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.625     ,  0.625     ,  0.625     , ...,  0.60546875,\n",
       "         0.6015625 ,  0.60546875],\n",
       "       [ 0.609375  ,  0.60546875,  0.625     , ...,  0.609375  ,\n",
       "         0.61328125,  0.625     ],\n",
       "       [ 0.59765625,  0.61328125,  0.60546875, ...,  0.65234375,\n",
       "         0.640625  ,  0.66015625],\n",
       "       ..., \n",
       "       [ 0.3125    ,  0.3359375 ,  0.30859375, ...,  0.47265625,\n",
       "         0.47265625,  0.45703125],\n",
       "       [ 0.421875  ,  0.4453125 ,  0.453125  , ...,  0.19921875,\n",
       "         0.19921875,  0.22265625],\n",
       "       [ 0.30078125,  0.28515625,  0.26953125, ...,  0.41796875,\n",
       "         0.421875  ,  0.4375    ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a  =pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_noisy</th>\n",
       "      <th>mask_train</th>\n",
       "      <th>mask_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_noisy</td>\n",
       "      <td>0_train_mask</td>\n",
       "      <td>0_val_mask</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_noisy    mask_train mask_validation\n",
       "0     0_noisy  0_train_mask      0_val_mask"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0_noisy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['image_noisy'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, 5, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
