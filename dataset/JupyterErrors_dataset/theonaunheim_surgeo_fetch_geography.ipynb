{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pathlib\n",
    "import urllib.request\n",
    "import tempfile\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background (links good as of 2019-12-10)\n",
    "\n",
    "<h2 style=\"color: red;\">This takes a fair amount of RAM and download time</h2>\n",
    "\n",
    "This script fetches ~50 .sf1 state and territory zip files from the census server at the following location:\n",
    "\n",
    "* https://www2.census.gov/census_2010/04-Summary_File_1/\n",
    "\n",
    "These each of these zip files contains (among other things) a geographic file and a population file containing.\n",
    "\n",
    "The fixed-width geo file is useful because it ties a  \"Logical Record\" to ZCTA codes. The CSV population file is useful because it contains the actual data we need for the logical records.\n",
    "\n",
    "Specifically, we are pulling our data from population table 9 (\"P9\" ... see Summary File 1 documentation page  184).\n",
    "\n",
    "    P5. HISPANIC OR LATINO ORIGIN BY RACE Universe: Total population (17)\n",
    "\n",
    "The data for this table is as follows:\n",
    "\n",
    "    P5 data:\n",
    "        Total:\n",
    "            Not Hispanic or Latino:\n",
    "                White alone\n",
    "                Black or African American alone\n",
    "                American Indian and Alaska Native alone\n",
    "                Asian alone\n",
    "                Native Hawaiian and Other Pacific Islander alone \n",
    "                Some Other Race alone\n",
    "                Two or More Races\n",
    "            Hispanic or Latino:\n",
    "                White alone\n",
    "                Black or African American alone \n",
    "                American Indian and Alaska Native alone\n",
    "                Asian alone\n",
    "                Native Hawaiian and Other Pacific Islander alone \n",
    "                Some Other Race alone\n",
    "                Two or More Races\n",
    "\n",
    "## Notes:\n",
    "* Geo table must be filtered by summary level == 871 (State-5-Digit ZIP Code Tabulation Area) so that it is limited to ZCTA records.\n",
    "* The cut down geo table is joined to the population table using the LOGRECNO (Log Record Number)\n",
    "* All Hispanic groups are condense to a single group to make consistent with surname data\n",
    "* Asian and API are combined to make consistent with surname data\n",
    "* \"Some Other Race\" is apporitoned abong the groups.\n",
    "\n",
    "For more details, the technical documentation for Summary File 1 can be found here:\n",
    "* https://www.census.gov/prod/cen2010/doc/sf1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the start/stop indices for the fixed width geo file.\n",
    "# It allows them to be easily converted to dataframes.\n",
    "GEO_MAP_2010 = {\n",
    "    'FILEID'  : (1  , 7  ),\n",
    "    'STUSAB'  : (7  , 9  ),\n",
    "    'SUMLEV'  : (9  , 12 ),\n",
    "    'GEOCOMP' : (12 , 14 ),\n",
    "    'CHARITER': (14 , 17 ),\n",
    "    'CIFSN'   : (17 , 19 ),\n",
    "    'LOGRECNO': (19 , 26 ),\n",
    "    'REGION'  : (26 , 27 ),\n",
    "    'DIVISION': (27 , 28 ),\n",
    "    'STATE'   : (28 , 30 ),\n",
    "    'COUNTY'  : (30 , 33 ),\n",
    "    'COUNTYCC': (33 , 35 ),\n",
    "    'COUNTYSC': (35 , 37 ),\n",
    "    'COUSUB'  : (37 , 42 ),\n",
    "    'COUSUBCC': (42 , 44 ),\n",
    "    'COUSUBSC': (44 , 46 ),\n",
    "    'PLACE'   : (46 , 51 ),\n",
    "    'PLACECC' : (51 , 53 ),\n",
    "    'PLACESC' : (53 , 55 ),\n",
    "    'TRACT'   : (55 , 61 ),\n",
    "    'BLKGRP'  : (61 , 62 ),\n",
    "    'BLOCK'   : (62 , 66 ),\n",
    "    'IUC'     : (66 , 68 ),\n",
    "    'CONCIT'  : (68 , 73 ),\n",
    "    'CONCITCC': (73 , 75 ),\n",
    "    'CONCITSC': (75 , 77 ),\n",
    "    'AIANHH'  : (77 , 81 ),\n",
    "    'AIANHHFP': (81 , 86 ),\n",
    "    'AIANHHCC': (86 , 88 ),\n",
    "    'AIHHTLI' : (88 , 89 ),\n",
    "    'AITSCE'  : (89 , 92 ),\n",
    "    'AITS'    : (92 , 97 ),\n",
    "    'AITSCC'  : (97 , 99 ),\n",
    "    'TTRACT'  : (99 , 105),\n",
    "    'TBLKGRP' : (105, 106),\n",
    "    'ANRC'    : (106, 111),\n",
    "    'ANRCCC'  : (111, 113),\n",
    "    'CBSA'    : (113, 118),\n",
    "    'CBSASC'  : (118, 120),\n",
    "    'METDIV'  : (120, 125),\n",
    "    'CSA'     : (125, 128),\n",
    "    'NECTA'   : (128, 133),\n",
    "    'NECTASC' : (133, 135),\n",
    "    'NECTADIV': (135, 140),\n",
    "    'CNECTA'  : (140, 143),\n",
    "    'CBSAPCI' : (143, 144),\n",
    "    'NECTAPCI': (144, 145),\n",
    "    'UA'      : (145, 150),\n",
    "    'UASC'    : (150, 152),\n",
    "    'UATYPE'  : (152, 153),\n",
    "    'UR'      : (153, 154),\n",
    "    'CD'      : (154, 156),\n",
    "    'SLDU'    : (156, 159),\n",
    "    'SLDL'    : (159, 162),\n",
    "    'VTD'     : (162, 168),\n",
    "    'VTDI'    : (168, 169),\n",
    "    'RESERVE2': (169, 172),\n",
    "    'ZCTA5'   : (172, 177),\n",
    "    'SUBMCD'  : (177, 182),\n",
    "    'SUBMCDCC': (182, 184),\n",
    "    'SDELM'   : (184, 189),\n",
    "    'SDSEC'   : (189, 194),\n",
    "    'SDUNI'   : (194, 199),\n",
    "    'AREALAND': (119, 213),\n",
    "    'AREAWATR': (213, 227),\n",
    "    'NAME'    : (227, 317),\n",
    "    'FUNCSTAT': (317, 318),\n",
    "    'GCUNI'   : (318, 319),\n",
    "    'POP100'  : (319, 328),\n",
    "    'HU100'   : (328, 337),\n",
    "    'INTPTLAT': (337, 348),\n",
    "    'INTPTLON': (348, 360),\n",
    "    'LSADC'   : (360, 362),\n",
    "    'PARTFLAG': (362, 363),\n",
    "    'RESERVE3': (363, 369),\n",
    "    'UGA'     : (369, 374),\n",
    "    'STATENS' : (374, 382),\n",
    "    'COUNTYNS': (382, 390),\n",
    "    'COUSUBNS': (390, 398),\n",
    "    'PLACENS' : (398, 406),\n",
    "    'CONCITNS': (406, 414),\n",
    "    'AIANHHNS': (414, 422),\n",
    "    'AITSNS'  : (422, 430),\n",
    "    'ANRCNS'  : (430, 438),\n",
    "    'SUBMCDNS': (438, 446),\n",
    "    'CD113'   : (446, 448),\n",
    "    'CD114'   : (448, 450),\n",
    "    'CD115'   : (450, 452),\n",
    "    'SLDU2'   : (452, 455),\n",
    "    'SLDU3'   : (455, 458),\n",
    "    'SLDU4'   : (458, 461),\n",
    "    'SLDL2'   : (461, 464),\n",
    "    'SLDL3'   : (464, 467),\n",
    "    'SLDL4'   : (467, 470),\n",
    "    'AIANHHSC': (470, 472),\n",
    "    'CSASC'   : (472, 476),\n",
    "    'CNECTASC': (474, 477),\n",
    "    'MEMI'    : (476, 478),\n",
    "    'NMEMI'   : (477, 478),\n",
    "    'PUMA'    : (478, 483),\n",
    "    'RESERVED': (483, 501),\n",
    "}\n",
    "\n",
    "# Our zip downloads have URLs that can be recreated with state/arrevs.\n",
    "STATES = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'DC': 'District_of_Columbia',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New_Hampshire',\n",
    "    'NJ': 'New_Jersey',\n",
    "    'NM': 'New_Mexico',\n",
    "    'NY': 'New_York',\n",
    "    'NC': 'North_Carolina',\n",
    "    'ND': 'North_Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'PR': 'Puerto_Rico',\n",
    "    'RI': 'Rhode_Island',\n",
    "    'SC': 'South_Carolina',\n",
    "    'SD': 'South_Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West_Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming',    \n",
    "}\n",
    "\n",
    "# This is the template for the URLs\n",
    "URL_TEMPLATE_ZIP = 'https://www2.census.gov/census_2010/04-Summary_File_1/{state}/{state_abbrev}2010.sf1.zip'\n",
    "\n",
    "TEMP_DIR = pathlib.Path(tempfile.gettempdir()) / 'surgeo_temp'\n",
    "TEMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# This creates a URL with each and every state in the STATES dictioanry\n",
    "urls = [\n",
    "    URL_TEMPLATE_ZIP.format(state_abbrev=code.lower(), state=name)\n",
    "    for code, name\n",
    "    in STATES.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_data(url, retries):\n",
    "    '''Helper that attempts to get file a number of times'''\n",
    "    tries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as r:\n",
    "                data = r.read()\n",
    "                return data\n",
    "        except Exception:\n",
    "            tries += 1\n",
    "            if tries >= retries:\n",
    "                raise\n",
    "        print('Retrying {url}...'.format(url))\n",
    "\n",
    "def dl_file(url, file_path):\n",
    "    '''Helper func: downloads zip from URL and stores it in local folder'''\n",
    "    # If it exsits do nothing\n",
    "    if file_path.exists():\n",
    "        print('{} is already present. Processing ...'.format(file_path))\n",
    "        pass\n",
    "    # Otherwise download file to dir\n",
    "    else:\n",
    "    # Open request\n",
    "        data = request_data(url, 3)\n",
    "        file_path.touch()\n",
    "        file_path.write_bytes(data)\n",
    "\n",
    "def make_geo_df(file_path, geo_level=\"ZCTA\"):\n",
    "    '''Helper func: takes zip and creates a geographic file from data'''\n",
    "    # Read zip data\n",
    "    with zipfile.ZipFile(file_path) as zf:\n",
    "        # Filter out everything except the ZipInfo (geo) for csv we want\n",
    "        target = zf.filelist[0]\n",
    "        # Read that CSV into BytesIO object\n",
    "        geo_data = io.BytesIO(zf.read(target))\n",
    "        # Read fixed-width file into dataframe\n",
    "        geo_df = pd.read_fwf(\n",
    "            geo_data, \n",
    "            header=None,\n",
    "            # Use the GEO MAP but subtract from column start/stop\n",
    "            colspecs=[\n",
    "                (tuple_[0] - 1, tuple_[1] - 1)\n",
    "                for tuple_\n",
    "                in GEO_MAP_2010.values()\n",
    "            ],\n",
    "            dtype=str,\n",
    "            encoding='latin',\n",
    "            engine='python'\n",
    "        )\n",
    "    # Give names to columns\n",
    "    geo_df.columns = tuple(GEO_MAP_2010.keys())\n",
    "    # Filter out all records that are not related to ZCTAs only\n",
    "    # e.g. get rid of census block data\n",
    "    if geo_level== \"ZCTA\":\n",
    "        geo_df = geo_df.loc[geo_df.SUMLEV == '871']\n",
    "        # Keep the STUSAB (state), LOGRECONO (join key), and ZCTA5 (zip code proxy)\n",
    "        geo_df = geo_df[['STUSAB', 'LOGRECNO', 'ZCTA5']]#.dropna(subset=['ZCTA5'])\n",
    "    elif geo_level == 'TRACT':\n",
    "        geo_df = geo_df.loc[geo_df.SUMLEV == '140']\n",
    "        geo_df = geo_df[['STUSAB', 'LOGRECNO', 'TRACT', 'STATE', 'COUNTY']]#.dropna(subset=['ZCTA5'])\n",
    "    return geo_df\n",
    "\n",
    "def make_pop_df(file_path ):\n",
    "    '''Helper func: Takes a zip and creates population df'''\n",
    "    # Read zip data\n",
    "    with zipfile.ZipFile(file_path) as zf:\n",
    "        # Filter out everything except the ZipInfo for csv we want\n",
    "        # This contains Table P5\n",
    "        target = zf.filelist[3]\n",
    "        # Read that CSV into BytesIO object\n",
    "        pop_data = io.BytesIO(zf.read(target))\n",
    "        pop_df = pd.read_csv(\n",
    "            pop_data, \n",
    "            header=None,\n",
    "            dtype=str,\n",
    "            encoding='latin',\n",
    "            engine='python'\n",
    "        )\n",
    "        # Keep only a subset of columns and renames them\n",
    "        pop_df = pop_df[[1, 4, 18, 19, 20, 21, 22, 23, 24, 25]]\n",
    "        pop_df.columns = [\n",
    "            'STUSAB',\n",
    "            'LOGRECNO',\n",
    "            'white',\n",
    "            'black',\n",
    "            'native',\n",
    "            'asian',\n",
    "            'pi',\n",
    "            'other',\n",
    "            'multiple',\n",
    "            'hispanic',\n",
    "        ]\n",
    "        return pop_df\n",
    "\n",
    "def merge_frames(geo_df, pop_df, geo_level=\"ZCTA\"):\n",
    "    '''Merges our GEO and POP frames'''\n",
    "    # Merges common STUSAB and LOGRECNO fields\n",
    "    merged = geo_df.merge(pop_df)\n",
    "    # Rename zctq5\n",
    "    if geo_level=='TRACT':\n",
    "        merged = merged.rename(columns={'TRACT': 'tract', 'STATE':'state', 'COUNTY':'county'})\n",
    "        merged = merged.set_index([\"state\",\"county\",\"tract\"])\n",
    "        merged = merged.sort_index()\n",
    "    else:\n",
    "        merged = merged.rename(columns={'ZCTA5': 'zcta5'})\n",
    "        # Set index to ZCTA5 and sort\n",
    "        merged = merged.set_index('zcta5')\n",
    "        merged = merged.sort_index()\n",
    "    return merged\n",
    "    \n",
    "def create_df(url, temp_dir, geo_level=\"ZCTA\"):\n",
    "    '''Main function to download, join, and clean data for single state'''\n",
    "    \n",
    "    file_name = url.rpartition('/')[2]\n",
    "    file_path = temp_dir / file_name \n",
    "    # Download\n",
    "    if not file_path.exists():\n",
    "        print(url)\n",
    "        data   = dl_file(url, file_path)\n",
    "    else:\n",
    "        print(file_name, \" Exists and Cached\")\n",
    "    # Make dfs\n",
    "    geo_df = make_geo_df(file_path, geo_level)\n",
    "    pop_df = make_pop_df(file_path)\n",
    "    # Join DFs, sort, trip, and process\n",
    "    df     = merge_frames(geo_df, pop_df, geo_level)\n",
    "    df = df.iloc[:, 2:]\n",
    "    df = df.astype(np.float64)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download.\n",
      "al2010.sf1.zip  Exists and Cached\n",
      "ak2010.sf1.zip  Exists and Cached\n",
      "az2010.sf1.zip  Exists and Cached\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Arkansas/ar2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/California/ca2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Colorado/co2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Connecticut/ct2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Delaware/de2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/District_of_Columbia/dc2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Florida/fl2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Georgia/ga2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Hawaii/hi2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Idaho/id2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Illinois/il2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Indiana/in2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Iowa/ia2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Kansas/ks2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Kentucky/ky2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Louisiana/la2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Maine/me2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Maryland/md2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Massachusetts/ma2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Michigan/mi2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Minnesota/mn2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Mississippi/ms2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Missouri/mo2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Montana/mt2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Nebraska/ne2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Nevada/nv2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/New_Hampshire/nh2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/New_Jersey/nj2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/New_Mexico/nm2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/New_York/ny2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/North_Carolina/nc2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/North_Dakota/nd2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Ohio/oh2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Oklahoma/ok2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Oregon/or2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Pennsylvania/pa2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Puerto_Rico/pr2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Rhode_Island/ri2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/South_Carolina/sc2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/South_Dakota/sd2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Tennessee/tn2010.sf1.zip\n",
      "https://www2.census.gov/census_2010/04-Summary_File_1/Texas/tx2010.sf1.zip\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 785. MiB for an array with shape (1018487, 101) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-de2ef41308e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEMP_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-de2ef41308e0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m data_zcta = [\n\u001b[1;32m      5\u001b[0m     \u001b[0mcreate_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEMP_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m ]\n",
      "\u001b[0;32m<ipython-input-11-227c7eadc3c3>\u001b[0m in \u001b[0;36mcreate_df\u001b[0;34m(url, temp_dir, geo_level)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Exists and Cached\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m# Make dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mgeo_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_geo_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mpop_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pop_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Join DFs, sort, trip, and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-227c7eadc3c3>\u001b[0m in \u001b[0;36mmake_geo_df\u001b[0;34m(file_path, geo_level)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Give names to columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/surgeo/venv/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_fwf\u001b[0;34m(filepath_or_buffer, colspecs, widths, infer_nrows, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"infer_nrows\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_nrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"engine\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"python-fwf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/surgeo/venv/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/surgeo/venv/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/surgeo/venv/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   2581\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m         \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rows_to_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exclude_implicit_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/surgeo/venv/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_rows_to_cols\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \u001b[0;31m# see gh-13320\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m         \u001b[0mzipped_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.to_object_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 785. MiB for an array with shape (1018487, 101) and data type object"
     ]
    }
   ],
   "source": [
    "print('Starting download.')\n",
    "\n",
    "# Create a dataframe for each URL and store in list\n",
    "data_zcta = [\n",
    "    create_df(url, TEMP_DIR)\n",
    "    for url\n",
    "    in urls\n",
    "]\n",
    "\n",
    "print('Download complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all data into single dataframe and sort index\n",
    "df = pd.concat(data_zcta)\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a ZCTA that crosses state lines\n",
    "df.loc['69201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/theonaunheim/surgeo/issues/10\n",
    "# Certain zctas cross state lines and must be added together.\n",
    "df = df.groupby(df.index).apply(sum)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck zip that crosses state lines\n",
    "df.loc['69201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store column totals\n",
    "totals = df.sum(axis=1)\n",
    "totals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store some other race so it can be divvyed up among other groups\n",
    "other = df['other']\n",
    "other.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Asian or Pacific Islander (this is what surname uses)\n",
    "df['api'] = df['asian'] + df['pi']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we will no longer use\n",
    "df = df.drop(columns=['other', 'asian', 'pi'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now determine what percent of the row each items makes up.\n",
    "percentages = df.divide(totals, axis='rows')\n",
    "percentages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up 'other' into the remaining rows based on the percents above\n",
    "apportioned_other = percentages.multiply(other, axis='rows')\n",
    "apportioned_other.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute 'other' to the remaining groups based on percentage makeup\n",
    "# quasi Iterative proportortional fit / matrix rake over single dimension\n",
    "df += apportioned_other\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconvert to percentage\n",
    "column_totals = df.sum(axis=0)\n",
    "ratio_by_column = df.divide(column_totals, axis='columns').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "ratio_by_column = ratio_by_column[[\n",
    "    'white',\n",
    "    'black',\n",
    "    'api',\n",
    "    'native',\n",
    "    'multiple',\n",
    "    'hispanic'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconvert to percentage\n",
    "row_totals = df.sum(axis=1)\n",
    "ratio_by_row = df.divide(row_totals, axis='index').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "ratio_by_row = ratio_by_row[[\n",
    "    'white',\n",
    "    'black',\n",
    "    'api',\n",
    "    'native',\n",
    "    'multiple',\n",
    "    'hispanic'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat Calculation for Tract Level if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zcta = []\n",
    "print(\"Gather the Tract Information\")\n",
    "data_tract = [\n",
    "    create_df(url, TEMP_DIR, \"TRACT\")\n",
    "    for url\n",
    "    in urls\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all data into single dataframe and sort index\n",
    "df_tract = pd.concat(data_tract)\n",
    "df_tract = df_tract.sort_index()\n",
    "\n",
    "# Store column totals\n",
    "totals_tract = df_tract.sum(axis=1)\n",
    "display(totals_tract.count())\n",
    "\n",
    "\n",
    "other = df_tract['other']\n",
    "df_tract['api'] = df_tract['asian'] + df_tract['pi']\n",
    "df_tract = df_tract.drop(columns=['other', 'asian', 'pi'])\n",
    "percentages = df_tract.divide(totals_tract, axis='rows')\n",
    "apportioned_other = percentages.multiply(other, axis='rows')\n",
    "df_tract += apportioned_other\n",
    "\n",
    "display(df_tract.count())\n",
    "\n",
    "column_totals = df_tract.sum(axis=0)\n",
    "ratio_by_column_tract = df_tract.divide(column_totals, axis='columns').copy()\n",
    "\n",
    "ratio_by_column_tract = ratio_by_column_tract[[\n",
    "    'white',\n",
    "    'black',\n",
    "    'api',\n",
    "    'native',\n",
    "    'multiple',\n",
    "    'hispanic'\n",
    "]]\n",
    "\n",
    "row_totals = df_tract.sum(axis=1)\n",
    "ratio_by_row_tract = df_tract.divide(row_totals, axis='index').copy()\n",
    "\n",
    "ratio_by_row_tract = ratio_by_row_tract[[\n",
    "    'white',\n",
    "    'black',\n",
    "    'api',\n",
    "    'native',\n",
    "    'multiple',\n",
    "    'hispanic'\n",
    "]]\n",
    "\n",
    "ratio_by_row_tract.sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write data to module as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = pathlib.Path().cwd()\n",
    "project_directory = current_directory.parents[0]\n",
    "data_directory    = project_directory / 'surgeo' / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob zcta given race\n",
    "rbc_path = data_directory / 'prob_zcta_given_race_2010.csv'\n",
    "ratio_by_column.to_csv(rbc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob race given block zcta\n",
    "rbr_path = data_directory / 'prob_race_given_zcta_2010.csv'\n",
    "ratio_by_row.to_csv(rbr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob zcta given race tract\n",
    "rbc_path = data_directory / 'prob_tract_given_race_2010.csv'\n",
    "ratio_by_column_tract.to_csv(rbc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob race given block tract\n",
    "rbr_path = data_directory / 'prob_race_given_tract_2010.csv'\n",
    "ratio_by_row_tract.to_csv(rbr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up TEMP_DIR\n",
    "\n",
    "If you turn this into comments, you cache the census files locally so you do not have to re-download everything (because the census FTP server drops connections like nobody's business). This comes at the expense of ~10GB disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete files\n",
    "for path in TEMP_DIR.rglob('*'):\n",
    "    path.unlink()\n",
    "\n",
    "# Delete dir\n",
    "TEMP_DIR.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
