{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp inference.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text.inference\n",
    "\n",
    "> Provides inference scripts specifically for text modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.text.all import *\n",
    "from fastinference.inference.inference import _decode_loss\n",
    "import matplotlib.cm as cm\n",
    "import html\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _decode_texts(dl, texts, outs, decoder=decode_spec_tokens):\n",
    "    \"Decode a list of tokenized `texts`\"\n",
    "    dec_texts, num = [], dl.dataset.numericalize\n",
    "    def _inner(o,num,dl):\n",
    "        tokens = [num.vocab[i] for i in o if num.vocab[i] not in [BOS,PAD]]\n",
    "        sep = dl.dataset.tokenizer[-1].sep\n",
    "        return sep.join(decoder(tokens))\n",
    "    for batch in texts:\n",
    "        for text in batch:\n",
    "            if isinstance(text, TensorText):\n",
    "                for text in batch[0]:\n",
    "                    dec_texts.append(_inner(text,num,dl))\n",
    "    outs.insert(len(outs), dec_texts)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_preds(x:LMLearner, ds_idx=1, dl=None, raw_outs=False, decoded_loss=True, fully_decoded=False, cat_dim=0,\n",
    "             decoder=decode_spec_tokens, **kwargs):\n",
    "    \"Get predictions with possible decoding\"\n",
    "    inps, outs, dec_out, raw = [], [], [], []\n",
    "    if dl is None: dl = x.dls[ds_idx].new(shuffle=False, drop_last=False)\n",
    "    x.model.eval()\n",
    "    for batch in dl:\n",
    "        with torch.no_grad():\n",
    "            inps.append(batch[:x.dls.n_inp])\n",
    "            if decoded_loss or fully_decoded:\n",
    "                out = x.model(*batch[:x.dls.n_inp])[0]\n",
    "                raw.append(out)\n",
    "                dec_out.append(x.loss_func.decodes(out))\n",
    "            else:\n",
    "                raw.append(x.model(*batch[:x.dls.n_inp])[0])\n",
    "    raw = torch.cat(raw, dim=cat_dim).cpu().numpy()\n",
    "    if decoded_loss or fully_decoded:\n",
    "        dec_out = torch.cat(dec_out, dim=0)\n",
    "    if not raw_outs:\n",
    "        try: outs.insert(0, x.loss_func.activation(tensor(raw)).numpy())\n",
    "        except: outs.insert(0, dec_out)\n",
    "    else:\n",
    "        outs.insert(0, raw)\n",
    "    if fully_decoded: outs = _decode_texts(x.dls[0], inps, outs, decoder=decoder)\n",
    "    if decoded_loss: outs = _decode_loss(x.dls.categorize.vocab, dec_out, outs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LMLearner.get_preds\" class=\"doc_header\"><code>LMLearner.get_preds</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LMLearner.get_preds</code>(**`x`**:`LMLearner`, **`ds_idx`**=*`1`*, **`dl`**=*`None`*, **`raw_outs`**=*`False`*, **`decoded_loss`**=*`True`*, **`fully_decoded`**=*`False`*, **`cat_dim`**=*`0`*, **`decoder`**=*`decode_spec_tokens`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Get predictions with possible decoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LMLearner.get_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_preds(x:TextLearner, ds_idx=1, dl=None, raw_outs=False, decoded_loss=True, fully_decoded=False,\n",
    "             decoder=decode_spec_tokens, **kwargs):\n",
    "    \"Get predictions with possible decoding\"\n",
    "    inps, outs, dec_out, raw = [], [], [], []\n",
    "    if dl is None: dl = x.dls[ds_idx].new(shuffle=False, drop_last=False)\n",
    "    x.model.eval()\n",
    "    for batch in dl:\n",
    "        with torch.no_grad():\n",
    "            inps.append(batch[:x.dls.n_inp])\n",
    "            if decoded_loss or fully_decoded:\n",
    "                out = x.model(*batch[:x.dls.n_inp])[0]\n",
    "                raw.append(out)\n",
    "                dec_out.append(x.loss_func.decodes(out))\n",
    "            else:\n",
    "                raw.append(x.model(*batch[:x.dls.n_inp])[0])\n",
    "    raw = torch.cat(raw, dim=0).cpu().numpy()\n",
    "    if decoded_loss or fully_decoded:\n",
    "        dec_out = torch.cat(dec_out, dim=0)\n",
    "    if not raw_outs:\n",
    "        try: outs.insert(0, x.loss_func.activation(tensor(raw)).numpy())\n",
    "        except: outs.insert(0, dec_out)\n",
    "    else:\n",
    "        outs.insert(0, raw)\n",
    "    if fully_decoded: outs = _decode_texts(x.dls[0], inps, outs, decoder=decoder)\n",
    "    if decoded_loss: outs = _decode_loss(x.dls.categorize.vocab, dec_out, outs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TextLearner.get_preds\" class=\"doc_header\"><code>TextLearner.get_preds</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TextLearner.get_preds</code>(**`x`**:`TextLearner`, **`ds_idx`**=*`1`*, **`dl`**=*`None`*, **`raw_outs`**=*`False`*, **`decoded_loss`**=*`True`*, **`fully_decoded`**=*`False`*, **`decoder`**=*`decode_spec_tokens`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Get predictions with possible decoding"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextLearner.get_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def predict(x:LMLearner, text, n_words=1, no_unk=True, temperature=1., min_p=None,\n",
    "                decoder=decode_spec_tokens, only_last_word=False):\n",
    "        \"Predict `n_words` from `text`\"\n",
    "        x.model.reset()\n",
    "        idxs = idxs_all = x.dls.test_dl([text]).items[0].to(x.dls.device)\n",
    "        unk_idx = x.dls.vocab.index(UNK)\n",
    "        for _ in (range(n_words)):\n",
    "            preds = x.get_preds(dl=[(idxs[None],)], decoded_loss=False)\n",
    "            res = preds[0][0][-1]\n",
    "            if no_unk: res[unk_idx] = 0.\n",
    "            if min_p is not None:\n",
    "                if (res >= min_p).float().sum() == 0:\n",
    "                    warn(f\"There is no item with probability >= {min_p}, try a lower value.\")\n",
    "                else: res[res < min_p] = 0.\n",
    "            if temperature != 1.: res.pow_(1 / temperature)\n",
    "            res = tensor(res)\n",
    "            idx = torch.multinomial(res, 1).item()\n",
    "            idxs = idxs_all = torch.cat([idxs_all, idxs.new([idx])])\n",
    "            if only_last_word: idxs = idxs[-1][None]\n",
    "        decoder=decode_spec_tokens\n",
    "        num = x.dls.train_ds.numericalize\n",
    "        tokens = [num.vocab[i] for i in idxs_all if num.vocab[i] not in [BOS, PAD]]\n",
    "        sep = x.dls.train_ds.tokenizer[-1].sep\n",
    "        return sep.join(decoder(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"LMLearner.predict\" class=\"doc_header\"><code>LMLearner.predict</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>LMLearner.predict</code>(**`x`**:`LMLearner`, **`text`**, **`n_words`**=*`1`*, **`no_unk`**=*`True`*, **`temperature`**=*`1.0`*, **`min_p`**=*`None`*, **`decoder`**=*`decode_spec_tokens`*, **`only_last_word`**=*`False`*)\n",
       "\n",
       "Predict `n_words` from `text`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(LMLearner.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "data_lm = TextDataLoaders.from_csv(path, 'texts.csv', text_col='text', is_lm=True)\n",
    "lm_learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "lm_learn.save_encoder('fine_tuned')\n",
    "blocks = (TextBlock.from_df('text', seq_len=data_lm.seq_len, vocab=data_lm.vocab), CategoryBlock())\n",
    "imdb_clas = DataBlock(blocks=blocks,\n",
    "                      get_x=ColReader('text'),\n",
    "                      get_y=ColReader('label'),\n",
    "                      splitter=ColSplitter())\n",
    "dls = imdb_clas.dataloaders(df, bs=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tokenizer' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f8e634a5fc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm_learn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my name is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_last_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-dd9683c530e6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x, text, n_words, no_unk, temperature, min_p, decoder, only_last_word)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs_all\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tokenizer' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "lm_learn.predict('my name is', n_words=2, only_last_word = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n",
    "learn.path = path\n",
    "learn.load_encoder('fine_tuned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = learn.dls.test_dl(df.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tokenizer' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-41bf6b2665f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minps\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfully_decoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-20e9c487499c>\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(x, ds_idx, dl, raw_outs, decoded_loss, fully_decoded, decoder, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mfully_decoded\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_decode_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoded_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_decode_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9e6e51496cb4>\u001b[0m in \u001b[0;36m_decode_texts\u001b[0;34m(dl, texts, outs, decoder)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mdec_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9e6e51496cb4>\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(o, num, dl)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBOS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tokenizer' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "classes, probs, inps  = learn.get_preds(dl=dl, fully_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _value2rgba(x, cmap=cm.RdYlGn, alpha_mult=1.0):\n",
    "    \"Convert a value `x` from 0 to 1 (inclusive) to an RGBA tuple according to `cmap` times transparency `alpha_mult`.\"\n",
    "    c = cmap(x)\n",
    "    rgb = (np.array(c[:-1]) * 255).astype(int)\n",
    "    a = c[-1] * alpha_mult\n",
    "    return tuple(rgb.tolist() + [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _eval_dropouts(mod):\n",
    "        module_name =  mod.__class__.__name__\n",
    "        if 'Dropout' in module_name or 'BatchNorm' in module_name: mod.training = False\n",
    "        for module in mod.children(): _eval_dropouts(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _piece_attn_html(pieces, attns, sep=' ', **kwargs):\n",
    "    html_code,spans = ['<span style=\"font-family: monospace;\">'], []\n",
    "    for p, a in zip(pieces, attns):\n",
    "        p = html.escape(p)\n",
    "        c = str(_value2rgba(a, alpha_mult=0.5, **kwargs))\n",
    "        spans.append(f'<span title=\"{a:.3f}\" style=\"background-color: rgba{c};\">{p}</span>')\n",
    "    html_code.append(sep.join(spans))\n",
    "    html_code.append('</span>')\n",
    "    return ''.join(html_code)\n",
    "\n",
    "def _show_piece_attn(*args, **kwargs):\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(_piece_attn_html(*args, **kwargs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _intrinsic_attention(learn, text, class_id=None):\n",
    "    \"Calculate the intrinsic attention of the input w.r.t to an output `class_id`, or the classification given by the model if `None`.\"\n",
    "    learn.model.train()\n",
    "    _eval_dropouts(learn.model)\n",
    "    learn.model.zero_grad()\n",
    "    learn.model.reset()\n",
    "    dl = learn.dls.test_dl([text])\n",
    "    batch = next(iter(dl))[0]\n",
    "    emb = learn.model[0].module.encoder(batch).detach().requires_grad_(True)\n",
    "    emb.retain_grad()\n",
    "    lstm = learn.model[0].module(emb, True)\n",
    "    learn.model.eval()\n",
    "    cl = learn.model[1]((lstm, torch.zeros_like(batch).bool(),))[0].softmax(dim=-1)\n",
    "    if class_id is None: class_id = cl.argmax()\n",
    "    cl[0][class_id].backward()\n",
    "    attn = emb.grad.squeeze().abs().sum(dim=-1)\n",
    "    attn /= attn.max()\n",
    "    tok, _ = learn.dls.decode_batch((*tuplify(batch), *tuplify(cl)))[0]\n",
    "    return tok, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def intrinsic_attention(x:TextLearner, text:str, class_id:int=None, **kwargs):\n",
    "    \"Shows the `intrinsic attention for `text`, optional `class_id`\"\n",
    "    if isinstance(x, LMLearner): raise Exception(\"Language models are not supported\")\n",
    "    text, attn = _intrinsic_attention(x, text, class_id)\n",
    "    return _show_piece_attn(text.split(), to_np(attn), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TextLearner.intrinsic_attention\" class=\"doc_header\"><code>TextLearner.intrinsic_attention</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TextLearner.intrinsic_attention</code>(**`x`**:`TextLearner`, **`text`**:`str`, **`class_id`**:`int`=*`None`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Shows the `intrinsic attention for `text`, optional `class_id`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TextLearner.intrinsic_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: monospace;\"><span title=\"0.556\" style=\"background-color: rgba(233, 245, 161, 0.5);\">xxbos</span> <span title=\"0.283\" style=\"background-color: rgba(251, 162, 91, 0.5);\">xxmaj</span> <span title=\"0.873\" style=\"background-color: rgba(45, 161, 84, 0.5);\">batman</span> <span title=\"0.301\" style=\"background-color: rgba(252, 172, 96, 0.5);\">is</span> <span title=\"1.000\" style=\"background-color: rgba(0, 104, 55, 0.5);\">rich</span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.intrinsic_attention('Batman is rich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
