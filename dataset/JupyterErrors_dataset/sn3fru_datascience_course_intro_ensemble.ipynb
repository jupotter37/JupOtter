{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/dh_logo.png\" align=\"right\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3.12 - Ensemble Theory\n",
    "\n",
    "Fala galera! Tudo bem? Continuamos nossas aulas de técnicas de *machine learning* para classificação. Até agora vimos como montar modelos isolados para tarefas de classificação. Cada modelo, por seu funcionamento e capacidade, acaba tendo seus prós e contras: alguns focam em certas métricas, outros encaram o trade-off de viés e variância de certo modo. Dessa forma, escolher um algoritmo acarreta diretamente no comportamento e trade-offs do seu modelo. Mas e se não usássemos apenas um, mas vários modelos que se completam. Enter Ensembles! Nas próximas 2 aulas, veremos como encarar problemas utilizando técnicas de Ensemble, que nada mais são do que juntar modelos diversos para termos uma melhor modelagem.\n",
    "\n",
    "## Dica de hoje - Principais algoritmos simples de classificação para um DS\n",
    "#### KNN (K-Nearest Neighbors)\n",
    "\n",
    "#### Logistic Regression\n",
    " - __[Artigo do Datacamp](https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)__\n",
    " - __[Tutorial do Andrew Ng](https://www.youtube.com/watch?v=-la3q9d7AKQ)__\n",
    " - __[Tutorial StatsQuest](https://www.youtube.com/watch?v=yIYKR4sgzI8)__\n",
    "\n",
    "#### Naive Bayes\n",
    " - __[Artigo do Datacamp](https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn)__\n",
    " - __[Teoria de Probabilidade](https://www.youtube.com/watch?v=PrkiRVcrxOs)__\n",
    " - __[Video Tutorial](https://www.youtube.com/watch?v=CPqOCI0ahss&t=236s)__\n",
    "\n",
    "#### SVM (Support Vector Machine)\n",
    " - __[Artigo do Datacamp](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python)__\n",
    " - __[Tutorial Codebasics](https://www.youtube.com/watch?v=FB5EdxAGxQg)__\n",
    "\n",
    "#### Decision Tree\n",
    " - __[Artigo Datacamp](https://www.datacamp.com/community/tutorials/decision-tree-classification-python)__\n",
    " - __[Tutorial StatQuest](https://datahackers.com.br/blog)__\n",
    " - __[Google Devs from zero](https://www.youtube.com/watch?v=LDRbO9a6XPU)__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T09:13:51.988369Z",
     "start_time": "2019-05-29T09:13:51.984278Z"
    }
   },
   "source": [
    "## Sabedoria da multidão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indico que vocês vejam o documentário \"The Code\", que tem apenas 3 episódios. Um episódio em especial trata exatamente desse assunto e eu vou mostrar um trechinho para vocês. EP03 41:49 até 47."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que tal um pouquinho de história da matemática? =]\n",
    "\n",
    "Francis Galton, em 1906, acompanhou participantes de uma feira que estavam tentando adivinhar o peso de um boi. Quem estava na feira podia escrever no ticket de entrada quanto achava que o boi pesava e quem chegasse mais perto ganharia um prêmio. \n",
    "\n",
    "O boi pesava 543,4 Kg. Individualmente os chutes das pessoas chegavam a ter erros de mais de 20Kg, mas tomando a média dos 787 tickets, Galton descobriu que o grupo tinha chegado ao valor de 547,4Kg. Eles acertaram com menos de 1% de erro! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos de Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/avengers_ensemble.jpg\" align=\"left\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não, eu não tive a maturidade pra deixar de fazer essa piada em época de EndGame.<br>\n",
    "\n",
    "Ensembles são agrupamentos de algoritmos treinados para a realização da mesma tarefa (nem que seja derrotar o Thanos). Imagine uma orquestra funcionando. Dificilmente você imaginou uma pessoa sozinha tocando todos os instrumentos, certo? O mesmo princípio é aplicado em machine learning: ao invés de termos um só algoritmo como classificador ou regressor, vamos utilizar o conjunto deles para melhor representar nosso *mapping*. Ou seja, pensando na Sabedoria da Multidão, nós estamos tentando reproduzir o maior número de \"chutes\" possível para que consigamos chegar o mais perto que conseguirmos do resultado que desejamos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os esembles podem ser homogênios, utilizando vários modelos gerados a partir do mesmo método (por exemplo, contendo apenas regressões lineares) ou heterogênios (utilizando vários tipos de modelos diferentes para chegar em um resultado). Todos eles partem do princípio que iremos usar *weak learners* (lembra dos chutes que falamos?) para atingir o objetivo final\n",
    "\n",
    "Existem essencialmente três tipos de ensembles:\n",
    "    \n",
    "    - Bagging\n",
    "    - Boosting\n",
    "    - Stacking\n",
    "    \n",
    "Para o *Bagging* e para a *Random Forest*, podemos agregar o resultado de algumas maneiras diferentes: Fazendo uma média dos valores, quando estamos nos referindo resultados de valores contínuos, fazendo a moda dos valores (votação) quando estamos falando de classificações  ou ainda atribuindo um peso a cada modelo e fazendo uma média ponderada dos seus resultados. Todos esses modelos podem ser feitos de forma paralela (ao mesmo tempo)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T10:09:35.638449Z",
     "start_time": "2019-05-29T10:09:35.629632Z"
    }
   },
   "source": [
    "Um exemplo de obtenção do resultado por votação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T10:13:54.661360Z",
     "start_time": "2019-05-29T10:13:54.651384Z"
    }
   },
   "source": [
    "<img src=\"imgs/votacao.jpg\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Boosting funciona de uma maneira um pouquinho diferente, já que seu resultado é construído sequencialmente a partir de um único weak learner inicial. Mas vamos explicar isso em detalhes mais adiante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Bagging ensemble \n",
    "\n",
    "A palavra *Bagging* é a abreviação de *Bootstrap Aggregation*, onde a primeira palavra indica a criação de um sample com reposição e a segunda palavra a agregação dos resultados obtidos por modelos treinados a partir dessas diferentes samples. Ou seja, primeiramente temos a etapa de *resampling* para cada modelo, para então fazermos seus treinos e por fim agregar suas respostas da maneira mais adequada.  \n",
    "\n",
    "Na criação das novas samples a partir da sample original, todos os exemplos da sample original, nesse caso, tem a mesma chance de aparecer na nova sample. Cada exemplo que é selecionado para entrar na nova sample sempre retorna ao \"banco de exemplos\" existentes na sample original, podendo assim ser escolhido novamente. As novas samples podem ser de tamahos menores ou do mesmo tamanho da sample original. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/Bagging_1.png\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com todas as novas samples geradas, podemos então treinar cada um dos seus respectivos modelos, como é ilustrado na imagem abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/Bagging_2.png\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, formamos os várias *weak learners* (modelos que podem ser de quaisquer algoritmos como regressões logísticas, árvores de decisão, KNN; geralmente escolhe-se um deles, onde o mais comum é o de Árvore de Decisão pelo fato de ser mais facilmente interpretável), que irão gerar os seus \"chutes\" (previsões) para cada um dos exemplos de validação/teste, que serão agregados da forma mais adequada para cada tipo de problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo *Random Forests* pode ser considerado uma evolução do método de *bagging*. Suas árvores podem ser geradas com ou sem resample dos examplos da base de dados~, e caso seja optado por não fazer o *bagging* com os exemplos, todas as árvores serão testadas com o mesmo sample. A novidade aqui está na escolha dos preditores (features, características...) da base de dados. O número de preditores que cada árvore terá (hiperparâmetro) pode ser determinado dependendo do tipo de resultado que pretendemos obter, como mostrado a seguir, sendo esta a sugestão do criador, mas também de diversos outras maneiras como usando o log:\n",
    "    - Quando uma regressão: teremos que o número de parâmetros de cada árvore será o número total de preditores sobre 3 (p/3)\n",
    "    - Quando uma classificação: teremos que o número de parâmetros de cada árvore será a raiz quadrada do número total de preditores (sqrt(p))\n",
    "    \n",
    "   \n",
    "A partir dessa determinação, cada árvore terá seus parâmetros selecionados randomicamente do total (por exemplo, se queremos resolver um problema de classificação e temos um total de 9 parâmetros, cada árvore terá 3 parâmetros, os quais serão selecionados aleatóriamente para cada uma). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Stacking\n",
    "\n",
    "A ideia por trás do stacking se divide em 2 partes. Queremos treinar diversos algoritmos nos nossos dados, e queremos que as visões deles sobre os algoritmos sejam complementares, ou seja, cada algoritmo aprenda algo a partir do anterior. Desse modo, o stacking se dá por 2 loops:\n",
    " - loop externo: treino de cada algoritmo de modo independente.\n",
    " - loop interno: quebre seu dataset de treino em 10 partes, você terá 10 combinações de 9:1. Treine cada algoritmo em cada uma dessas 10 combinações. Os outputs de cada algoritmo no dataset de teste servirão de features para o próximo algoritmo no loop externo\n",
    "De modo visual, o stacking se parece com isso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/stacking_0.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "1) Após ter feito o `train_test_split`, quebre seu dataset de treino em 10 partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/stacking_1.png\" align=\"left\" width=\"20%\"> \n",
    "\n",
    "2) Fazendo combinações de 9:1 partes, treine seu algoritmo (por ex, uma Decision Tree) nas 9 partes e use a décima parte com pré-teste. Repita isso para todas as combinações. Isso ajuda o algoritmo a entender pontos de vista diferentes sob seu dataset. O nome disso é **K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/stacking_2.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "3) Após o treino do primeiro algoritmo em seu loop interno, faça as predictions no dataset de teste. Esse output será utilizado como *feature* para os próximos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"imgs/stacking_3.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "4) Repita o passo desse loop externo com os próximos algoritmos, até alcançar a performance desejada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O legal do Stacking é que você pode criar as features que desejar com esse método! Por exemplo, antes de fazer um regressor para prever a quantidade de vendas de um determinado produto na próxima semana, você pode antes fazer um classificador que preve se as vendas vão aumentar ou diminuir, para então passar esse output como nova feature :D\n",
    "\n",
    "Algoritmos de Stacking:\n",
    "- quaisquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/stacking_5.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse método é amplamente utilizado nas competições do Kaggle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Boosting\n",
    "\n",
    "Ah, o crème de la crème dos ensembles. Imagina que estamos construindo um classificador. Ele terá um erro, correto? E se ao invés de treinar nossos modelos e cada um tivesse seu erro, nós treinássemos modelos especializados em corrigir os erros anteriores? Diferente dos ensembles anteriores, que são ensembles em paralelo, boosting é um ensemble sequencial.\n",
    "\n",
    "1) Primeiramente, criamos um classificador base. Ele vai conter erros, e precisamos corrigí-los\n",
    "<img src=\"imgs/boosting_0.png\" align=\"center\" width=\"20%\">\n",
    "\n",
    "2) Criamos um segundo classificador, mas que opera em cima dos erros do primeiro (wrong predictions).\n",
    "<img src=\"imgs/boosting_1.png\" align=\"center\" width=\"20%\">\n",
    "\n",
    "\n",
    "3) Continuamos esse loop até chegar na performance desejada (um threshold específico, acurácia,...). No momento de definir este *trashold* precisamos tomar cuidado para não overfitarmos os nossos dados. O output final é dado pela Weighted Average dos sub-modelos.\n",
    "\n",
    "<img src=\"imgs/boosting_2.jpeg\" align=\"center\" width=\"20%\">\n",
    "\n",
    "4) Nosso modelo final é uma combinação de todos os outros\n",
    "<img src=\"imgs/boosting_3.png\" align=\"center\" width=\"20%\">\n",
    "\n",
    "\n",
    "O Gradient Boosting e o XGBoost são os modelos de boosting mais utilizados por diversas razões, entretando entre os dois citados, o XGBoost costuma ter uma desempenho melhor por ter uma regularização diferenciada. \n",
    "\n",
    "\n",
    "Algoritmos baseados em boosting:\n",
    " - Adaboost\n",
    " - GBM (Gradient Boosting Machine)\n",
    " - XGBoost (Extreme Gradient Boost)\n",
    " - LightGBM\n",
    " - CatBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De um modo geral, os métodos de ensemble citados acima diminuem a variância do modelo único, já que eles combinam as estimativas dos diversos modelos. Isso faz com que o resultado seja mais estável (faça uma analogia com a Sabedoria da multidão). Se o problema do modelo único for baixa performance, o bagging será capaz de dar um bias melhor mas provavelmente a abordagem de boosting fará isso melhor. Entretando, se o modelo singular está overfitando, o bagging se torna a abordagem mais interessante. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, alguns códigos base para vocês guardarem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2cc7ade2ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Voting Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "# Voting Classifier\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))\n",
    "    \n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "def Stacking(model,train,y,test,n_fold):\n",
    "    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    test_pred=np.empty((test.shape[0],1),float)\n",
    "    train_pred=np.empty((0,1),float)\n",
    "    for train_indices,val_indices in folds.split(train,y.values):\n",
    "        x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "\n",
    "        model.fit(X=x_train,y=y_train)\n",
    "        train_pred=np.append(train_pred,model.predict(x_val))\n",
    "        test_pred=np.append(test_pred,model.predict(test))\n",
    "    return test_pred.reshape(-1,1),train_pred\n",
    "\n",
    "model1 = tree.DecisionTreeClassifier(random_state=1)\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)\n",
    "\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(df,y_train)\n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "# Na real que a gente só usa RandomForest mesmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "# Usamos amplamente o XGBoost. Para isso, usamos a biblioteca xgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
