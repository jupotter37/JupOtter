{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Psychologists - Session 7\n",
    "\n",
    "## handling dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From session 3, we are already familiar with handling data in **pandas** and how to create dataframes from scratch. However, most of the time, we don´t create our dataframe on our own, but rather get a table (i.e. a .xls or .csv or .txt) with data. Today we want to cover some basic steps from reading, cleaning and processing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load and read an existing datasheet into a dataframe we can e.g. use ```pd.read_csv(\"folderstructure/folder/file.csv\")``` . Keep in mind, that ```pd.read``` has more options to read in datafiles! Depending on your datafile, you also need to specify the *file seperator* that separates your values within your datasheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s load and read the **results1.csv** file from your data directory using\n",
    "- ```pd.read_csv(\"file\", sep= \"\\t\")```\n",
    "- ```pd.read_csv(\"file\", sep= \",\")```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7db424b4126f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/Dominik/Documents/WS19_Python_for_Psychologists/session7/daten_python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/Dominik/Documents/WS19_Python_for_Psychologists/session7/daten_python\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ```dataframe.head() ``` we can easily get a quick first glance of the first five rows of our dataframe, however, we can adjust this value by simply indicating another number in the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to display the whole dataframe, you usually end up with a section of your rows or columns, as pandas masks some of them by default. By using ```pd.options.display.max_rows/columns``` we can check the default value and, if needed adjust it with ```pd.options.set.max_rows/columns=int```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our dataframe only contains reaction times from a single participant. If we are not super duber interested in single-case studies, this is usually not the type of data we are handling. Most often, especially after conducting an experiment we usually (&hopefully) got data from more than 1 subject stored on our disk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time your data is somewhere stored on your disk, to interact with our operating system in jupyter notebook, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned above, we usually end up with more than one datafile from different subjects. \n",
    "\n",
    "To check e.g. for files in a given directory, we can use ```os.listdir(\"directory\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = os.listdir(\"/Users/Dominik/Documents/daten_python\")[1:] ### the ds_store file is special to iOS\n",
    "source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our easy example, we got data from 8 different participants. Conveniently, all our files are named in the same way, i.e. result**number**.csv \n",
    "\n",
    "To load the datafile from each subject iteratevily, we could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = \"/Users/Dominik/Documents/daten_python/results{}.csv\" #create a blank file directory with {} \n",
    "logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ```some_string{}.format``` to format our string by filling in the placeholder {} with a value, either fixed or in an iteratevily manner.\n",
    "\n",
    "Lets see with a single string how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with a whole sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s load the datasheet of each subject and combine it to one single dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                              # create a subject list \n",
    "             \n",
    "\n",
    "                                 # create an empty list for all dataframes\n",
    "\n",
    "\n",
    "                                 # we use the blank directory and .format to fill in the subject per for circle \n",
    "                                 # read/load the file and create a dataframe\n",
    "                                 # append each subjects dataframe to the list \n",
    "                                    # use pandas concatenate to create one final dataframe with all subjects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could do something like this, which might be a bit more handy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/Dominik/Documents/daten_python/\" \n",
    "\n",
    "all_dfs = []\n",
    "for i in source:\n",
    "    df = pd.read_csv(path+i)\n",
    "    all_dfs.append(df)\n",
    "df = pd.concat(all_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a first idea about your dataframe, its shape, its column names and some first descriptive values, we can use:\n",
    "\n",
    "- ```dataframe.shape```: contains your rows x columns\n",
    "- ```dataframe.columns```: contains your column names\n",
    "- ```dataframe.describe```: contains some basic, descriptive values (e.g. min, max) for numeric values\n",
    "\n",
    "Especially ```dataframe.describe()``` gives you a first idea whether there are missing values in your data. However it only works for columns with numeric values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ```dataframe.describe()```function only resolves columns with numeric values. To get a better idea what other columns contain, we could look for unique values by simply using ```dataframe[\"some_column\"].unique()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further check for missing values we can use ```dataframe.isnull()``` which returns a dataframe with a boolean statement (i.e. TRUE or FALSE) for each cell. To get a better and comprehensive overview, we just can add ```.sum()``` to see the added missing values per column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily we don´t have any missing values in our dataframe. However, if you have any in yours, don´t worry, there are ways to either fill or drop missing values \n",
    "\n",
    "- ```dataframe.fillna(some value, inplace=True)``` fills missing values with \"some value\"\n",
    "- ```dataframe.dropna(axis=0 , how=\"any\")``` drops rows with at least one missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s again have a look at our todays example. We have information about the stimulus material, the correct *answer*, the actual button press and the response/reaction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ```dataframe.describe()```above, we already know, that the mean RT was about 0.46 s - try to recreate the result using the ```.mean()```function and round it two decimals using ```round(some_number, number_of_decimals)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try also to recreate the *min* / *max* / *standard deviation* from the **describe** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that mean RT of the experiment and the SD is quite small (under 1s) - let´s try to display any RT above 1 s using ```dataframe.loc``` and a conditional (i.e. boolean) statement as a *potential* outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visually check if certain values might be an outlier or not, we can use a quick and tbh not very aesthetic vizualisation by using ```dataframe[some_colum].plot(kind=\"box\" or \"hist\" or \"bar\" or or or ...)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the RT, let´s investigate the accuracy of our participants. To do so, we need to use the given information in our dataframe.\n",
    "\n",
    "\n",
    "Let´s create a new column *iscorrect* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we assess the overall mean accuracy in our experiment? \n",
    "\n",
    "Why can we assess it in this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy is quite high, let´s see how each individual participant has performed. For this we can use ```dataframe.groupby([\"some_column\"]).method()```to split our dataframe, apply a function and combine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to add the accuracy from our aggregated dataframe back to our \"non-aggregated\" dataframe. For this purpose we need a list that matches the length of our original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose we can use a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list=[]\n",
    "\n",
    "  \n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list=[]\n",
    "\n",
    "for vpn in df[\"vpn_number\"]:\n",
    "    acc_list.append(accuracy[vpn])\n",
    "    \n",
    "df[\"accuracy\"] = acc_list    \n",
    "    \n",
    "df.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to add the error rate as simple as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s see whether the RT differs between correct and false responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already know that there is more than one correct way, let´s try to investigate the RT differences between correct and false responses by creating two new dataframes (i.e. df_corr and df_incorr) using ```dataframe.loc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also combine different statemens and e.g. group our dataframe by subjects and evaluate only the mean RT for correct trials. We can do this by using ```df.loc```+ ```df.groupby```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s visualize the RT for correct and incorrect trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further specify our plot, we can set hue (i.e. color coding in seaborn) to *condition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization suggests that there is no difference between RT_correct and RT_incorrect. However, for educational purposes only, let´s calculate a paired t-test.\n",
    "\n",
    "Most statistics are covered in **stats** from the **scipy** module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your new dataframe, you can use\n",
    "\n",
    "```pd.save(\"location_on_disk\", potential settings, such as, sep=) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"/Users/Dominik/Documents/daten_python/final.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
