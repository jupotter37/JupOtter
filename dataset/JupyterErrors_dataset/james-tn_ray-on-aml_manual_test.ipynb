{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Interactive use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1639464907468
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from azureml.core import Workspace, Experiment, Environment, Datastore, Dataset, ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "# from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import platform\n",
    "# from ray_on_azureml.ray_on_aml import getRay\n",
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade ray[default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install raydp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install raydp /azureml-envs/azureml_6cb52194be4f7fc697297312b8f55547/lib/python3.8/site-packages/ray/jars/ray_dist.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210615.v1\n",
      "ARG HTTP_PROXY\n",
      "ARG HTTPS_PROXY\n",
      "# set http_proxy & https_proxy\n",
      "ENV http_proxy=${HTTPS_PROXY}\n",
      "ENV https_proxy=${HTTPS_PROXY}\n",
      "RUN http_proxy=${HTTPS_PROXY} https_proxy=${HTTPS_PROXY} apt-get update -y \\\n",
      "    && mkdir -p /usr/share/man/man1 \\\n",
      "    && http_proxy=${HTTPS_PROXY} https_proxy=${HTTPS_PROXY} apt-get install -y openjdk-11-jdk \\\n",
      "    && mkdir /raydp \\\n",
      "    && pip --no-cache-dir install raydp\n",
      "WORKDIR /raydp\n",
      "# unset http_proxy & https_proxy\n",
      "ENV http_proxy=\n",
      "ENV https_proxy=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_image=\"FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210615.v1\"\n",
    "dockerfile = r\"\"\"\n",
    "{0}\n",
    "ARG HTTP_PROXY\n",
    "ARG HTTPS_PROXY\n",
    "# set http_proxy & https_proxy\n",
    "ENV http_proxy=${{HTTPS_PROXY}}\n",
    "ENV https_proxy=${{HTTPS_PROXY}}\n",
    "RUN http_proxy=${{HTTPS_PROXY}} https_proxy=${{HTTPS_PROXY}} apt-get update -y \\\n",
    "    && mkdir -p /usr/share/man/man1 \\\n",
    "    && http_proxy=${{HTTPS_PROXY}} https_proxy=${{HTTPS_PROXY}} apt-get install -y openjdk-11-jdk \\\n",
    "    && mkdir /raydp \\\n",
    "    && pip --no-cache-dir install raydp\n",
    "WORKDIR /raydp\n",
    "# unset http_proxy & https_proxy\n",
    "ENV http_proxy=\n",
    "ENV https_proxy=\n",
    "\"\"\".format(base_image)\n",
    "print(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1639464931994
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancel active AML runs if any\n",
      "Canceling active run  ray_on_aml_1644811136_d1923402 in ray_on_aml\n",
      "Shutting down ray if any\n",
      "Found existing cluster d15-v2\n",
      "Creating new Environment ray-0.0.7-1634135180135722568\n",
      "Waiting for cluster to start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accelerator_type:K80': 1.0,\n",
       " 'node:10.0.0.13': 1.0,\n",
       " 'CPU': 6.0,\n",
       " 'GPU': 1.0,\n",
       " 'object_store_memory': 16625394892.0,\n",
       " 'memory': 33250789787.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.ray_on_aml.core import Ray_On_AML\n",
    "# from ray_on_aml.core import Ray_On_AML\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"d15-v2\", additional_pip_packages=['torch==1.10.0', 'torchvision', 'sklearn'], maxnode=2)\n",
    "# ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"gpunc6\", base_pip_dep = ['ray[tune]==1.9.2', 'ray[rllib]==1.9.2','ray[serve]==1.9.2', 'xgboost_ray==0.1.6', 'dask==2021.12.0','pyarrow >= 5.0.0','fsspec==2021.10.1','fastparquet==0.7.2','tabulate==0.8.9','raydp'], maxnode=2)\n",
    "\n",
    "# ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"worker-cpu-v3\", base_pip_dep = ['ray[default]==1.8.0', 'xgboost_ray==0.1.6', 'dask==2021.12.0',\\\n",
    "# 'pyarrow >= 5.0.0','fsspec==2021.10.1','fastparquet==0.7.2','tabulate==0.8.9','pyspark'], maxnode=2)\n",
    "ray = ray_on_aml.getRay()\n",
    "\n",
    "# ray = ray_on_aml.getRay(gpu_support=True)\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accelerator_type:K80': 1.0,\n",
       " 'node:10.0.0.13': 1.0,\n",
       " 'memory': 238150763316.0,\n",
       " 'GPU': 1.0,\n",
       " 'object_store_memory': 104381099212.0,\n",
       " 'CPU': 46.0,\n",
       " 'node:10.0.0.5': 1.0,\n",
       " 'node:10.0.0.21': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run this code outside of the Ray cluster!\n",
    "import ray\n",
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_lib_path = sys.executable.split('/')[-3]+\"/lib/python\"+sys.version[:3]\n",
    "conda_lib_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /anaconda/envs/azureml_py38/lib/python3.8/site-packages/pyspark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1639464969085
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Dask on Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:07<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from adlfs import AzureBlobFileSystem\n",
    "\n",
    "abfs = AzureBlobFileSystem(account_name=\"azureopendatastorage\",  container_name=\"isdweatherdatacontainer\")\n",
    "#if read all years and months\n",
    "# data = ray.data.read_parquet(\"az://isdweatherdatacontainer/ISDWeather//\", filesystem=abfs)\n",
    "data =ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2015/\"], filesystem=abfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1639105974201
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 04:56:32,313\tERROR serialization.py:289 -- Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 287, in deserialize_objects\n",
      "    obj = self._deserialize_object(data, metadata, object_ref)\n",
      "  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 194, in _deserialize_object\n",
      "    return self._deserialize_msgpack_data(data, metadata_fields)\n",
      "  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 172, in _deserialize_msgpack_data\n",
      "    python_objects = self._deserialize_pickle5_data(pickle5_data)\n",
      "  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 160, in _deserialize_pickle5_data\n",
      "    obj = pickle.loads(in_band, buffers=buffers)\n",
      "AttributeError: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>\n"
     ]
    },
    {
     "ename": "RaySystemError",
     "evalue": "System error: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>\ntraceback: Traceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 287, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 194, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 172, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 160, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band, buffers=buffers)\nAttributeError: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRaySystemError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92d143f470b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         np.random.randint(0, 10000, size=(1024, 2)), columns=[\"age\", \"grade\"]),\n\u001b[1;32m     20\u001b[0m     npartitions=2)\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ray.shutdown()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/util/dask/scheduler.py\u001b[0m in \u001b[0;36mray_dask_get\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menable_progress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mpb_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_dask_on_ray_pb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_get_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_actor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpb_actor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mray_finish_cbs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mray_finish_cbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/util/dask/scheduler.py\u001b[0m in \u001b[0;36mray_get_unpack\u001b[0;34m(object_refs, progress_bar_actor)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# completes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_object_refs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mcomputed_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/util/dask/scheduler.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(object_refs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprogress_bar_actor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mrender_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_individual_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRaySystemError\u001b[0m: System error: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>\ntraceback: Traceback (most recent call last):\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 287, in deserialize_objects\n    obj = self._deserialize_object(data, metadata, object_ref)\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 194, in _deserialize_object\n    return self._deserialize_msgpack_data(data, metadata_fields)\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 172, in _deserialize_msgpack_data\n    python_objects = self._deserialize_pickle5_data(pickle5_data)\n  File \"/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/ray/serialization.py\", line 160, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band, buffers=buffers)\nAttributeError: Can't get attribute '_unpickle_block' on <module 'pandas._libs.internals' from '/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/pandas/_libs/internals.cpython-38-x86_64-linux-gnu.so'>\n"
     ]
    }
   ],
   "source": [
    "#Scaling up date with Dask dataframe API\n",
    "import dask\n",
    "from ray.util.dask import ray_dask_get,enable_dask_on_ray\n",
    "enable_dask_on_ray()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "storage_options = {'account_name': 'azureopendatastorage'}\n",
    "ddf = dd.read_parquet('az://nyctlc/green/puYear=2015/puMonth=*/*.parquet', storage_options=storage_options)\n",
    "ddf.count().compute()\n",
    "\n",
    "\n",
    "# Set the scheduler to ray_dask_get in your config so you don't have to\n",
    "# specify it on each compute call.\n",
    "\n",
    "df = dd.from_pandas(\n",
    "    pd.DataFrame(\n",
    "        np.random.randint(0, 10000, size=(1024, 2)), columns=[\"age\", \"grade\"]),\n",
    "    npartitions=2)\n",
    "df.groupby([\"age\"]).mean().compute()\n",
    "\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639104458150
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "storage_options = {'account_name': 'azureopendatastorage'}\n",
    "ddf = dd.read_parquet('az://nyctlc/green/puYear=2019/puMonth=*/*.parquet', storage_options=storage_options)\n",
    "ddf.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639106144831
    }
   },
   "outputs": [],
   "source": [
    "#dask\n",
    "\n",
    "# import ray\n",
    "from ray.util.dask import ray_dask_get\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from azureml.core import Workspace, Dataset, Model\n",
    "from adlfs import AzureBlobFileSystem\n",
    "account_key = ws.get_default_keyvault().get_secret(\"adls7-account-key\")\n",
    "account_name=\"adlsgen7\"\n",
    "abfs = AzureBlobFileSystem(account_name=\"adlsgen7\",account_key=account_key,  container_name=\"mltraining\")\n",
    "abfs2 = AzureBlobFileSystem(account_name=\"azureopendatastorage\",  container_name=\"isdweatherdatacontainer\")\n",
    "\n",
    "\n",
    "storage_options={'account_name': account_name, 'account_key': account_key}\n",
    "\n",
    "# ddf = dd.read_parquet('az://mltraining/ISDWeatherDelta/year2008', storage_options=storage_options)\n",
    "\n",
    "data = ray.data.read_parquet(\"az://isdweatherdatacontainer/ISDWeather/year=2009\", filesystem=abfs2)\n",
    "data2 = ray.data.read_parquet(\"az://mltraining/ISDWeatherDelta/year2008\", filesystem=abfs)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from azureml.core import Workspace, Dataset, Model\n",
    "from adlfs import AzureBlobFileSystem\n",
    "account_key = ws.get_default_keyvault().get_secret(\"adls7-account-key\")\n",
    "account_name=\"adlsgen7\"\n",
    "# abfs = AzureBlobFileSystem(account_name=\"adlsgen7\",account_key=account_key,  container_name=\"mltraining\")\n",
    "abfs2 = AzureBlobFileSystem(account_name=\"azureopendatastorage\",  container_name=\"isdweatherdatacontainer\")\n",
    "\n",
    "\n",
    "storage_options={'account_name': account_name, 'account_key': account_key}\n",
    "\n",
    "# ddf = dd.read_parquet('az://mltraining/ISDWeatherDelta/year2008', storage_options=storage_options)\n",
    "\n",
    "data = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2012/\"], filesystem=abfs2)\n",
    "data1 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2015/\"], filesystem=abfs2)\n",
    "data2 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2010/\"], filesystem=abfs2)\n",
    "data3 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2009/\"], filesystem=abfs2)\n",
    "data4 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2011/\"], filesystem=abfs2)\n",
    "data5 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2013/\"], filesystem=abfs2)\n",
    "data6 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2014/\"], filesystem=abfs2)\n",
    "all_data =data.union(data1).union(data2).union(data3).union(data4).union(data5).union(data6)\n",
    "all_data.count()\n",
    "start = time.time()\n",
    "#convert Ray dataset to Dask dataframe \n",
    "all_data_dask = data.to_dask().describe().compute()\n",
    "print(all_data_dask)\n",
    "stop = time.time()\n",
    "print(\"duration \", (stop-start))\n",
    "#717s for single machine nc6\n",
    "# duration  307.69699811935425s for CI as head and 4 workers of DS14_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ray Tune for distributed ML tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1639106657384
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/azureuser/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a744a80e0c4f968c26a647f090ec7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/azureuser/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/azureuser/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/azureuser/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838060163b2140b7beac08ce8e7b1a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/azureuser/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/azureuser/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/azureuser/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb799091d8d44acad5e867056da2870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/azureuser/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/azureuser/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/azureuser/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372cc468aabe4b0385cd987213ec6712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/azureuser/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/azureuser/data/MNIST/raw\n",
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 03:59:06 (running for 00:00:00.17)<br>Memory usage on this node: 6.2/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/18 CPUs, 0/3 GPUs, 0.0/105.64 GiB heap, 0.0/47.4 GiB objects (0.0/3.0 accelerator_type:K80)<br>Result logdir: /home/azureuser/ray_results/train_mnist_2022-01-30_03-59-06<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mnist_f81e8_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">1.22836e-09</td><td style=\"text-align: right;\"> 0.0819143</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-30 03:59:07,287\tERROR trial_runner.py:958 -- Trial train_mnist_f81e8_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 924, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 787, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ray/worker.py\", line 1715, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=208, ip=10.0.0.18)\n",
      "RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\n",
      "\u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=208, ip=10.0.0.18)\n",
      "  File \"/azureml-envs/azureml_b754358d546bd4cc08fdd901eb6aa8f0/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle.py\", line 679, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'torchvision'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_mnist_f81e8_00000:\n",
      "  trial_id: f81e8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-30 03:59:07 (running for 00:00:01.16)<br>Memory usage on this node: 6.3/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/18 CPUs, 0/3 GPUs, 0.0/105.64 GiB heap, 0.0/47.4 GiB objects (0.0/3.0 accelerator_type:K80)<br>Result logdir: /home/azureuser/ray_results/train_mnist_2022-01-30_03-59-06<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mnist_f81e8_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">1.22836e-09</td><td style=\"text-align: right;\"> 0.0819143</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_mnist_f81e8_00000</td><td style=\"text-align: right;\">           1</td><td>/home/azureuser/ray_results/train_mnist_2022-01-30_03-59-06/train_mnist_f81e8_00000_0_lr=1.2284e-09,momentum=0.081914_2022-01-30_03-59-06/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m 2022-01-30 03:59:07,297\tERROR worker.py:431 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=208, ip=10.0.0.18)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=208, ip=10.0.0.18)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m   File \"/azureml-envs/azureml_b754358d546bd4cc08fdd901eb6aa8f0/lib/python3.8/site-packages/ray/cloudpickle/cloudpickle.py\", line 679, in subimport\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m     __import__(name)\n",
      "\u001b[2m\u001b[36m(TemporaryActor pid=208, ip=10.0.0.18)\u001b[0m ModuleNotFoundError: No module named 'torchvision'\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_mnist_f81e8_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_74698/956064152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_mnist_f81e8_00000])"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "# import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # In this example, we don't change the model architecture\n",
    "        # due to simplicity.\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "# Change these values if you want the training to run quicker or slower.\n",
    "EPOCH_SIZE = 512\n",
    "TEST_SIZE = 256\n",
    "\n",
    "def train(model, optimizer, train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "            return\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(data) > TEST_SIZE:\n",
    "                break\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    for i in range(10):\n",
    "        train(model, optimizer, train_loader)\n",
    "        acc = test(model, test_loader)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "search_space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.01, 0.09)\n",
    "}\n",
    "\n",
    "# Uncomment this to enable distributed execution\n",
    "# ray.shutdown()\n",
    "# ray.init(address=\"auto\",ignore_reinit_error=True)\n",
    "# ray.init(address =f'ray://{headnode_private_ip}:10001',allow_multiple=True,ignore_reinit_error=True )\n",
    "# Download the dataset first\n",
    "datasets.MNIST(\"~/data\", train=True, download=True)\n",
    "\n",
    "analysis = tune.run(train_mnist, config=search_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639106681808
    }
   },
   "outputs": [],
   "source": [
    " import sklearn.datasets\n",
    " import sklearn.metrics\n",
    " from sklearn.model_selection import train_test_split\n",
    " import xgboost as xgb\n",
    "\n",
    " from ray import tune\n",
    "\n",
    "\n",
    " def train_breast_cancer(config):\n",
    "     # Load dataset\n",
    "     data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "     # Split into train and test set\n",
    "     train_x, test_x, train_y, test_y = train_test_split(\n",
    "         data, labels, test_size=0.25)\n",
    "     # Build input matrices for XGBoost\n",
    "     train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "     test_set = xgb.DMatrix(test_x, label=test_y)\n",
    "     # Train the classifier\n",
    "     results = {}\n",
    "     xgb.train(\n",
    "         config,\n",
    "         train_set,\n",
    "         evals=[(test_set, \"eval\")],\n",
    "         evals_result=results,\n",
    "         verbose_eval=False)\n",
    "     # Return prediction accuracy\n",
    "     accuracy = 1. - results[\"eval\"][\"error\"][-1]\n",
    "     tune.report(mean_accuracy=accuracy, done=True)\n",
    "\n",
    "\n",
    " config = {\n",
    "     \"objective\": \"binary:logistic\",\n",
    "     \"eval_metric\": [\"logloss\", \"error\"],\n",
    "     \"max_depth\": tune.randint(1, 9),\n",
    "     \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "     \"subsample\": tune.uniform(0.5, 1.0),\n",
    "     \"eta\": tune.loguniform(1e-4, 1e-1)\n",
    " }\n",
    " analysis = tune.run(\n",
    "     train_breast_cancer,\n",
    "     resources_per_trial={\"cpu\": 1},\n",
    "     config=config,\n",
    "     num_samples=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install raydp-nightly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "python_path = subprocess.check_output(\"which python\", shell=True).strip()\n",
    "python_path = python_path.decode('utf-8')\n",
    "python_path = python_path +\"/site-packages/raydp/jars/raydp-0.5.0-SNAPSHOT.jar\"\n",
    "os.listdir('/anaconda/envs/azureml_py38/bin//python/site-packages/raydp/jars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Spark on Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.copy(/anaconda/envs/azureml_py38/lib/python3.8/site-packages/raydp/jars/raydp-0.5.0-SNAPSHOT.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raydp\n",
    "os.path.abspath(os.path.join(os.path.abspath(raydp.__file__), \"../../jars/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /anaconda/envs/azureml_py38/lib/python3.8/site-packages/jars/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAYDP_CP = os.path.abspath(os.path.join(os.path.abspath(__file__), \"../../jars/*\"))\n",
    "import ray\n",
    "RAY_CP = os.path.abspath(os.path.join(os.path.dirname(ray.__file__), \"jars/*\"))\n",
    "RAY_CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment,ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "#Remember the AML job has to have distribted setings (MPI type) for ray-on-aml to work correctly.\n",
    "ws = Workspace.from_config()\n",
    "compute_cluster = 'd12-ssh-novnet' #This can be another cluster different from the interactive cluster. \n",
    "ray_cluster = ComputeTarget(workspace=ws, name=compute_cluster)\n",
    "\n",
    "aml_run_config_ml = RunConfiguration(communicator='OpenMpi')\n",
    "\n",
    "\n",
    "#Check the conda_env.yml, it has an entry of ray-on-aml\n",
    "rayEnv = Environment.from_conda_specification(name = \"RLEnv\",\n",
    "                                             file_path = \"conda_env.yml\")\n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210615.v1\n",
    "ARG HTTP_PROXY\n",
    "ARG HTTPS_PROXY\n",
    "# set http_proxy & https_proxy\n",
    "ENV http_proxy=${HTTP_PROXY}\n",
    "ENV https_proxy=${HTTPS_PROXY}\n",
    "RUN http_proxy=${HTTP_PROXY} https_proxy=${HTTPS_PROXY} apt-get update -y \\\n",
    "    && mkdir -p /usr/share/man/man1 \\\n",
    "    && http_proxy=${HTTP_PROXY} https_proxy=${HTTPS_PROXY} apt-get install -y openjdk-11-jdk \\\n",
    "    && mkdir /raydp \\\n",
    "    && pip --no-cache-dir install raydp\n",
    "WORKDIR /raydp\n",
    "# unset http_proxy & https_proxy\n",
    "ENV http_proxy=\n",
    "ENV https_proxy=\n",
    "\"\"\"\n",
    "\n",
    "# Set the base image to None, because the image is defined by Dockerfile.\n",
    "rayEnv.docker.base_image = None\n",
    "rayEnv.docker.base_dockerfile = dockerfile\n",
    "\n",
    "aml_run_config_ml.target = ray_cluster\n",
    "aml_run_config_ml.node_count = 3\n",
    "aml_run_config_ml.environment = rayEnv\n",
    "src = ScriptRunConfig(source_directory='job',\n",
    "                    script='aml_job.py',\n",
    "                    run_config = aml_run_config_ml,\n",
    "                   )\n",
    "\n",
    "run = Experiment(ws, \"rl_on_aml_job\").submit(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raydp\n",
    "raydp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:55:41.408 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 2 on 10.0.0.5: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 22:55:45,516\tERROR worker.py:478 -- print_logs: Connection closed by server.\n",
      "2022-01-29 22:55:45,517\tERROR worker.py:1247 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2022-01-29 22:55:45,520\tERROR import_thread.py:89 -- ImportThread: Connection closed by server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:55:45.453 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 0 on 10.0.0.13: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22:55:45.457 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 1 on 10.0.0.20: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-29 23:05:44,751 E 15477 15529] gcs_server_address_updater.cc:76: Failed to receive the GCS address for 600 times without success. The worker will exit ungracefully. It is because GCS has died. It could be because there was an issue that kills GCS, such as high memory usage triggering OOM killer to kill GCS. Cluster will be highly likely unavailable if you see this log. Please check the log from gcs_server.err.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "additional_spark_configs ={\"fs.azure.account.key.adlsdatalakegen6.blob.core.windows.net\":\"AcDil/MwM9KlDvJu0LBcBIQxogAncv306NMRYABtjphXfWgaDTV3yjZgoSNckUb/3nhG04ND2Nqn553fq36Pqw==\",\n",
    "          \"fs.azure.account.key.adlsdatalakegen6.dfs.core.windows.net\":\"AcDil/MwM9KlDvJu0LBcBIQxogAncv306NMRYABtjphXfWgaDTV3yjZgoSNckUb/3nhG04ND2Nqn553fq36Pqw==\"}\n",
    "other_configs = {\"raydp.executor.extraClassPath\":\"/azureml-envs/azureml_6cb52194be4f7fc697297312b8f55547/lib/python3.8/site-packages/pyspark/jars/*\"}\n",
    "spark = ray_on_aml.getSpark(executor_cores =3,num_executors =3 ,executor_memory='10GB', additional_spark_configs=additional_spark_configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ray\n",
    "# import raydp\n",
    "# # # # import os\n",
    "\n",
    "\n",
    "# # # ray.init(address ='ray://10.0.0.11:6379')\n",
    "# spark = raydp.init_spark(\n",
    "#   app_name = \"example\",\n",
    "#   num_executors = 2,\n",
    "#   executor_cores = 2,\n",
    "#   executor_memory = \"1GB\",\n",
    "# #     configs = {\"spark.jars\":\"jars/*.jar\"}\n",
    "#     configs = {\"spark.jars\":\"jars/azure-storage-8.6.6.jar,jars/hadoop-azure-3.3.1.jar,jars/jetty-util-ajax-11.0.7.jar,jars/jetty-util-9.3.24.v20180605.jar,jars/delta-core_2.12-1.1.0.jar,jars/mssql-jdbc-9.4.1.jre8.jar\",\n",
    "#               \"fs.azure.account.key.adlsdatalakegen6.blob.core.windows.net\":\"AcDil/MwM9KlDvJu0LBcBIQxogAncv306NMRYABtjphXfWgaDTV3yjZgoSNckUb/3nhG04ND2Nqn553fq36Pqw==\",\n",
    "#               \"fs.azure.account.key.adlsdatalakegen6.dfs.core.windows.net\":\"AcDil/MwM9KlDvJu0LBcBIQxogAncv306NMRYABtjphXfWgaDTV3yjZgoSNckUb/3nhG04ND2Nqn553fq36Pqw==\"}\n",
    "\n",
    "# #     configs = {\"spark.jars\":\"jars/mssql-jdbc-9.4.1.jre8.jar\"}\n",
    "# #     configs = {\n",
    "# #                \"spark.driver.extraClassPath\":\"jars/mssql-jdbc-9.4.1.jre8.jar\"}\n",
    "\n",
    "# #     configs = {\"spark.jars.packages\":\"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.6\"}\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raydp\n",
    "raydp.stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_data = spark.read.format(\"delta\").load(\"wasbs://mltraining@adlsdatalakegen6.blob.core.windows.net/ISDWeatherDelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_data = spark.read.format(\"delta\").load(\"abfss://mltraining@adlsdatalakegen6.dfs.core.windows.net/ISDWeatherDelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_data.groupby(\"stationName\").count().head(20)\n",
    "# 73,696,631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_data.write.format(\"delta\").mode(\"overwrite\").save(\"wasbs://mltraining@adlsdatalakegen6.blob.core.windows.net/FirstRaySave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adls_data.selectExpr(\"snowDepth\",\"stationName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_data = ray.data.from_spark(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synapse SQL Pool Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = \"jdbc:sqlserver://sy2qwhqqkv7eacsws1.sql.azuresynapse.net:1433\"\n",
    "database_name = \"sy2qwhqqkv7eacsws1p1\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"ISDWeatherDelta\"\n",
    "username = \"azureuser\"\n",
    "password = \"abcd@12345\" # Please specify password here\n",
    "# try:\n",
    "#     adls_data.write \\\n",
    "#     .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"url\", url) \\\n",
    "#     .option(\"dbtable\", table_name) \\\n",
    "#     .option(\"user\", username) \\\n",
    "#     .option(\"password\", password) \\\n",
    "#     .save()\n",
    "# except ValueError as error :\n",
    "#     print(\"Connector write failed\", error)\n",
    "\n",
    "    \n",
    "\n",
    "jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password).load()\n",
    "\n",
    "\n",
    "# sparkdf = spark.createDataFrame([(1, 2),(2, 9),(3, 7)],\n",
    "#                             (\"id\", \"name\")) \n",
    "# try:\n",
    "#     adls_data.selectExpr(\"snowDepth\",\"latitude\").write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"url\", url) \\\n",
    "#     .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "#     .option(\"dbtable\", table_name) \\\n",
    "#     .option(\"user\", username) \\\n",
    "#     .option(\"batchsize\", 10000) \\\n",
    "#     .option(\"password\", password) \\\n",
    "#     .save()\n",
    "# except ValueError as error :\n",
    "#     print(\"Connector write failed\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.format(\"delta\").mode(\"overwrite\").save(\"data/delta_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import raydp\n",
    "from raydp.tf import TFEstimator\n",
    "from raydp.utils import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(data):\n",
    "    \n",
    "    # Fill NA in column Fare, Age and Embarked\n",
    "    data = data.fillna({\"Embarked\": \"S\"})\n",
    "    \n",
    "    fare_avg = data.select(mean(col(\"Fare\")).alias(\"mean\")).collect()\n",
    "    data = data.na.fill({\"Fare\": fare_avg[0][\"mean\"]})\n",
    "    \n",
    "    age_avg = data.select(mean(col(\"Age\")).alias(\"mean\")).collect()\n",
    "    data = data.na.fill({'Age': age_avg[0][\"mean\"]})\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(data):\n",
    "    \n",
    "    # Add some new features\n",
    "    data = data.withColumn(\"name_length\", length(\"Name\"))\n",
    "    data = data.withColumn(\"has_cabin\", col(\"Cabin\").isNotNull().cast('int'))\n",
    "    \n",
    "    data = data.withColumn(\"family_size\", col(\"SibSp\") + col(\"Parch\") + 1)\n",
    "    data = data.withColumn(\"is_alone\", (col(\"family_size\") == 1).cast('int'))\n",
    "    \n",
    "    \n",
    "    # Add some features about passengers' title with spark udf\n",
    "    @udf(\"string\")\n",
    "    def get_title(name):\n",
    "        title = ''\n",
    "        title_match = re.search(' ([A-Za-z]+)\\.', name)\n",
    "        if (title_match):\n",
    "            title = title_match.group(1)\n",
    "            if (title in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n",
    "                          'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']):\n",
    "                title = 'Rare'\n",
    "            return title\n",
    "        return title\n",
    "    data = data.withColumn(\"Title\", get_title(col(\"Name\")))\n",
    "    data = data.withColumn(\"Title\", regexp_replace(\"Title\", \"Mlle|Ms\", \"Miss\"))\n",
    "    data = data.withColumn(\"Title\", regexp_replace(\"Title\", \"Mme\", \"Mrs\"))\n",
    "    \n",
    "    # Encode column Sex\n",
    "    sex_udf = udf(lambda x: 0 if x == \"female\" else 1)\n",
    "    data = data.withColumn(\"Sex\", sex_udf(col(\"Sex\")).cast('int'))\n",
    "    \n",
    "    # Encode column Title\n",
    "    title_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    title_udf = udf(lambda x: title_map[x])\n",
    "    data = data.withColumn(\"Title\", title_udf(col(\"Title\")).cast('int'))\n",
    "    \n",
    "    # Encode column Embarked\n",
    "    embarked_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "    embarked_udf = udf(lambda x: embarked_map[x])\n",
    "    data = data.withColumn(\"Embarked\", embarked_udf(col(\"Embarked\")).cast('int'))\n",
    "    \n",
    "    # Categorize column Fare\n",
    "    @udf(\"int\")\n",
    "    def fare_map(fare):\n",
    "        if (fare <= 7.91):\n",
    "            return 0\n",
    "        elif fare <= 14.454:\n",
    "            return 1\n",
    "        elif fare <= 31:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    data = data.withColumn(\"Fare\", fare_map(col(\"Fare\")))\n",
    "    \n",
    "    # Categorize column Age\n",
    "    @udf(\"int\")\n",
    "    def age_map(age):\n",
    "        if age <= 16:\n",
    "            return 0\n",
    "        elif age <= 32:\n",
    "            return 1\n",
    "        elif age <= 48:\n",
    "            return 2\n",
    "        elif age <= 64:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "    data = data.withColumn(\"Age\", age_map(col(\"Age\")))\n",
    "    \n",
    "    return data\n",
    "def drop_cols(data):\n",
    "    \n",
    "    # Drop useless columns\n",
    "    data = data.drop(\"PassengerId\") \\\n",
    "        .drop(\"Name\") \\\n",
    "        .drop(\"Ticket\") \\\n",
    "        .drop(\"Cabin\") \\\n",
    "        .drop(\"SibSp\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fill_na(train)\n",
    "\n",
    "train = do_features(train)\n",
    "\n",
    "train = drop_cols(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [field.name for field in list(train.schema) if field.name != \"Survived\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTensor = []\n",
    "for _ in range(len(features)):\n",
    "    inTensor.append(keras.Input((1,)))\n",
    "concatenated = keras.layers.concatenate(inTensor)\n",
    "fc1 = keras.layers.Dense(32, activation='relu')(concatenated)\n",
    "fc2 = keras.layers.Dense(32, activation='relu')(fc1)\n",
    "dp1 = keras.layers.Dropout(0.25)(fc2)\n",
    "fc3 = keras.layers.Dense(16, activation='relu')(dp1)\n",
    "dp2 = keras.layers.Dropout(0.25)(fc3)\n",
    "fc4 = keras.layers.Dense(1, activation='sigmoid')(dp2)\n",
    "model = keras.models.Model(inTensor, fc4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsp = keras.optimizers.RMSprop()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "estimator = TFEstimator(num_workers=3, model=model, optimizer=rmsp, loss=loss, metrics=[\"binary_accuracy\"],\n",
    "                        feature_columns=features, label_column=\"Survived\", batch_size=32, num_epochs=100,\n",
    "                        config={\"fit_config\": {\"steps_per_epoch\": train.count() // 32}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit_on_spark(train, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raydp.stop_spark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ray on Job Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     pyarrow >=6.0.1\n",
    "# dask >=2021.11.2\n",
    "# adlfs >=2021.10.0\n",
    "# fsspec==2021.10.1\n",
    "# ray[default]==1.9.0\n",
    "ws = Workspace.from_config()\n",
    "base_conda_dep =['adlfs==2021.10.0','pip']\n",
    "base_pip_dep = ['ray[tune]==1.9.1', 'xgboost_ray==0.1.5', 'dask==2021.12.0','pyarrow >= 5.0.0','fsspec==2021.10.1', 'torch','torchvision==0.8.1']\n",
    "\n",
    "compute_cluster = 'worker-cpu-v3'\n",
    "maxnode =5\n",
    "vm_size='STANDARD_DS3_V2'\n",
    "vnet='rayvnet'\n",
    "subnet='default'\n",
    "exp ='ray_on_aml_job'\n",
    "ws_detail = ws.get_details()\n",
    "ws_rg = ws_detail['id'].split(\"/\")[4]\n",
    "vnet_rg=None\n",
    "try:\n",
    "    ray_cluster = ComputeTarget(workspace=ws, name=compute_cluster)\n",
    "\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    if vnet_rg is None:\n",
    "        vnet_rg = ws_rg\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                        min_nodes=0, max_nodes=maxnode,\n",
    "                                                        vnet_resourcegroup_name=vnet_rg,\n",
    "                                                        vnet_name=vnet,\n",
    "                                                        subnet_name=subnet)\n",
    "    ray_cluster = ComputeTarget.create(ws, compute_cluster, compute_config)\n",
    "\n",
    "    ray_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "python_version = [\"python=\"+platform.python_version()]\n",
    "\n",
    "\n",
    "\n",
    "conda_packages = python_version+base_conda_dep\n",
    "pip_packages = base_pip_dep \n",
    "\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "rayEnv = Environment(name=\"rayEnv\")\n",
    "# rayEnv = Environment.get(ws, \"rayEnv\", version=16)\n",
    "for conda_package in conda_packages:\n",
    "    conda_dep.add_conda_package(conda_package)\n",
    "\n",
    "for pip_package in pip_packages:\n",
    "    conda_dep.add_pip_package(pip_package)\n",
    "\n",
    "# # Adds dependencies to PythonSection of myenv\n",
    "rayEnv.python.conda_dependencies=conda_dep\n",
    "\n",
    "src = ScriptRunConfig(source_directory='job',\n",
    "                script='aml_job.py',\n",
    "                environment=rayEnv,\n",
    "                compute_target=ray_cluster,\n",
    "                distributed_job_config=PyTorchConfiguration(node_count=maxnode),\n",
    "                    # arguments = [\"--master_ip\",master_ip]\n",
    "                )\n",
    "run = Experiment(ws, exp).submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "serve.start()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "def hello(request):\n",
    "    name = request.query_params[\"name\"]\n",
    "    return f\"Hello {name}!\"\n",
    "\n",
    "\n",
    "hello.deploy()\n",
    "\n",
    "# Query our endpoint over HTTP.\n",
    "response = requests.get(\"http://127.0.0.1:8000/hello?name=serve\").text\n",
    "assert response == \"Hello serve!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "serve.start()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        self.count += 1\n",
    "        return {\"count\": self.count}\n",
    "\n",
    "\n",
    "# Deploy our class.\n",
    "Counter.deploy()\n",
    "\n",
    "# Query our endpoint in two different ways: from HTTP and from Python.\n",
    "assert requests.get(\"http://127.0.0.1:8000/Counter\").json() == {\"count\": 1}\n",
    "assert ray.get(Counter.get_handle().remote()) == {\"count\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class Counter:\n",
    "  def __init__(self):\n",
    "      self.count = 0\n",
    "\n",
    "  @app.get(\"/\")\n",
    "  def get(self):\n",
    "      return {\"count\": self.count}\n",
    "\n",
    "  @app.get(\"/incr\")\n",
    "  def incr(self):\n",
    "      self.count += 1\n",
    "      return {\"count\": self.count}\n",
    "\n",
    "  @app.get(\"/decr\")\n",
    "  def decr(self):\n",
    "      self.count -= 1\n",
    "      return {\"count\": self.count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter.deploy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET localhost:8000/Counter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "azureml_py38_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
