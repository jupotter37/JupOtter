{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce23e8b0-f86d-4ece-9b9f-ab3b304fff9e",
   "metadata": {},
   "source": [
    "## Step 1: First trying to load an xarray via kerchunked zarr refs of a fake nwb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef311937-8230-451f-90da-6e7c525953b3",
   "metadata": {},
   "source": [
    "### create fake nwb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da68396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff5780e-15ad-4482-b2ed-4a2db7d25e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import kerchunk\n",
    "import kerchunk.hdf\n",
    "import fsspec\n",
    "import ujson\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f87003-d046-471e-b709-540813af11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_id = \"12345\"\n",
    "filename = f\"probe_{probe_id}_lfp.nwb\"\n",
    "data_group = 'data'\n",
    "ref_file = \"lfp_one_probe_ref_rand.json\"\n",
    "ref_filepath = os.getcwd() + '/' + ref_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68395764-eb8d-44f9-8881-280f1c3aee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File probe_12345_lfp.nwb created.\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(filename, 'w') as f:\n",
    "    probe_group = f.create_group(data_group)\n",
    "    \n",
    "    data_shape = (100, 10)\n",
    "    data = np.random.random(data_shape)\n",
    "    probe_data = probe_group.create_dataset(\"lfp\", data_shape, dtype='f', data=data, chunks=(10, 2))\n",
    "    probe_data.attrs['unit'] = 'volts'\n",
    "\n",
    "print(f\"File {filename} created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77830d51-3a3c-40c5-a6c5-f7f543be850b",
   "metadata": {},
   "source": [
    "### create kerchunk/zarr/json ref file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1434f0-4ca8-499a-a1ff-d2c0ec51e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(filename) as f:\n",
    "    h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, filename)\n",
    "    refs = h5chunks.translate()\n",
    "\n",
    "with open(ref_file, \"wb\") as f:\n",
    "    f.write(ujson.dumps(refs).encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd657f-f3e1-4941-b806-6b54cecbfc5b",
   "metadata": {},
   "source": [
    "### access the data into xarray with fsspecc mapping of ref file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca42f36-1c5d-456f-9d81-4e0f0408ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13700631, 0.29692534, 0.0986272 , 0.60131884, 0.8357998 ],\n",
       "       [0.03251839, 0.10793467, 0.07548545, 0.272498  , 0.29228824],\n",
       "       [0.40465006, 0.00370023, 0.21820004, 0.230124  , 0.9424046 ],\n",
       "       [0.99663234, 0.90835714, 0.17431271, 0.42966798, 0.39867067],\n",
       "       [0.2736057 , 0.8326975 , 0.5470424 , 0.426214  , 0.5854852 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = fsspec.filesystem(\"reference\", fo=ref_filepath)\n",
    "m = fs.get_mapper()\n",
    "ds = xr.open_dataset(\n",
    "    m,\n",
    "    group=data_group,\n",
    "    engine=\"zarr\",\n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "\n",
    "ds.lfp.values[:5,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cde02-bed1-4125-930f-e1b7797a6faa",
   "metadata": {},
   "source": [
    "ok that works.. next trying on the real data but using similar approach as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61f889-edfc-4086-86c8-cead32bd7a88",
   "metadata": {},
   "source": [
    "## Step 2: Now load the real .nwb file as is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2c896-7d17-4d70-971a-718f0742ea85",
   "metadata": {},
   "source": [
    "- note.. xarray needs an `_ARRAY_DIMENSIONS` attribute for each dataset in order to read it from zarr\n",
    "- I'm trying to just add this `_ARRAY_DIMENSIONS` attribute right into the lfp dataset reference.\n",
    "- This is slightly different than Ian's approach of creating a zarr construct with `_ARRAY_DIMENSIONS` attribute and then fitting the lfp refs into that.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc56c7bf-3df8-41a5-baa1-ecc4cc54dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_array_dimensions(refs, data_path, dimensions):\n",
    "    ref_path = data_path + \"/.zarray\"\n",
    "    if ref_path in refs['refs']:\n",
    "        ref_str = refs['refs'][ref_path]\n",
    "        ref_dict = json.loads(ref_str)  # Deserialize the JSON string\n",
    "        print(ref_path)\n",
    "        ref_dict['_ARRAY_DIMENSIONS'] = dimensions\n",
    "        ref_str = json.dumps(ref_dict)  # Serialize back to a JSON string\n",
    "        refs['refs'][ref_path] = ref_str  # Replace the reference in refs\n",
    "        print(ref_dict)  # Original reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b2101c-0880-45e4-8226-3d52d5ea40de",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/data/.zarray\n",
      "{'chunks': [41859, 1], 'compressor': {'id': 'zlib', 'level': 9}, 'dtype': '<f4', 'fill_value': 0.0, 'filters': None, 'order': 'C', 'shape': [10715666, 93], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['time', 'channel']}\n",
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/electrodes/.zarray\n",
      "{'chunks': [93], 'compressor': None, 'dtype': '<i8', 'fill_value': 0, 'filters': None, 'order': 'C', 'shape': [93], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['channel']}\n",
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/timestamps/.zarray\n",
      "{'chunks': [10465], 'compressor': {'id': 'zlib', 'level': 9}, 'dtype': '<f8', 'fill_value': 0.0, 'filters': None, 'order': 'C', 'shape': [10715666], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['time']}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(ujson\u001b[38;5;241m.\u001b[39mdumps(refs)\u001b[38;5;241m.\u001b[39mencode())\n\u001b[1;32m     26\u001b[0m fs \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mfilesystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m, fo\u001b[38;5;241m=\u001b[39mnwb_ref_filepath)\n\u001b[0;32m---> 27\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlfp_group_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# i think setting the group is one aspect that Ian's approach was missing.. \u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsolidated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_and_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m data \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mlfp[:\u001b[38;5;241m100\u001b[39m, :\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> data\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mcompute())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:980\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    978\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 980\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/store.py:58\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     44\u001b[0m encoding \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m     attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mset_coords(coord_names\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mvars\u001b[39m))\n\u001b[1;32m     60\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(filename_or_obj\u001b[38;5;241m.\u001b[39mclose)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/dataset.py:685\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    683\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39m_variables\n\u001b[0;32m--> 685\u001b[0m variables, coord_names, dims, indexes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_data_and_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs) \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/dataset.py:416\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data_vars, coords)\u001b[0m\n\u001b[1;32m    412\u001b[0m     coords \u001b[38;5;241m=\u001b[39m create_coords_with_default_indexes(coords, data_vars)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# exclude coords from alignment (all variables in a Coordinates object should\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# already be aligned together) and use coordinates' indexes to align data_vars\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbroadcast_equals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplicit_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxindexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpriority_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_align_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/merge.py:723\u001b[0m, in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value, skip_align_args)\u001b[0m\n\u001b[1;32m    718\u001b[0m prioritized \u001b[38;5;241m=\u001b[39m _get_priority_vars_and_indexes(aligned, priority_arg, compat\u001b[38;5;241m=\u001b[39mcompat)\n\u001b[1;32m    719\u001b[0m variables, out_indexes \u001b[38;5;241m=\u001b[39m merge_collected(\n\u001b[1;32m    720\u001b[0m     collected, prioritized, compat\u001b[38;5;241m=\u001b[39mcompat, combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs\n\u001b[1;32m    721\u001b[0m )\n\u001b[0;32m--> 723\u001b[0m dims \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m coord_names, noncoord_names \u001b[38;5;241m=\u001b[39m determine_coords(coerced)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m explicit_coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/variable.py:3257\u001b[0m, in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m   3255\u001b[0m             last_used[dim] \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m   3256\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m dims[dim] \u001b[38;5;241m!=\u001b[39m size:\n\u001b[0;32m-> 3257\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3258\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflicting sizes for dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3259\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims[dim]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_used\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3260\u001b[0m             )\n\u001b[1;32m   3261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dims\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}"
     ]
    }
   ],
   "source": [
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "probe_id = \"810755797\"\n",
    "nwb_filepath = os.path.join(data_dir, f\"probe_{probe_id}_lfp.nwb\")\n",
    "nwb_ref_filepath = os.path.join(data_dir, \"lfp_one_probe_ref.json\")\n",
    "lfp_group_path = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "lfp_data_path = lfp_group_path + \"/data\"\n",
    "electrodes_data_path = lfp_group_path + \"/electrodes\"\n",
    "time_data_path = lfp_group_path + \"/timestamps\"\n",
    "\n",
    "with fsspec.open(nwb_filepath) as f:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, nwb_filepath)\n",
    "        refs = h5chunks.translate()\n",
    "\n",
    "# Adding _ARRAY_DIMENSIONS\n",
    "set_array_dimensions(refs, lfp_data_path, [\"time\", \"channel\"])\n",
    "set_array_dimensions(refs, electrodes_data_path, [\"channel\"])\n",
    "set_array_dimensions(refs, time_data_path, [\"time\"])\n",
    "\n",
    "\n",
    "with open(nwb_ref_filepath, \"wb\") as f:\n",
    "    f.write(ujson.dumps(refs).encode())\n",
    "\n",
    "fs = fsspec.filesystem(\"reference\", fo=nwb_ref_filepath)\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(),\n",
    "    engine=\"zarr\",\n",
    "    group=lfp_group_path, # i think setting the group is one aspect that Ian's approach was missing.. \n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "\n",
    "data = ds.lfp[:100, :4]\n",
    "print(\"==> data\", data.compute())\n",
    "mean = data.mean().compute()\n",
    "print(\"==> mean\", mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55d22c-0890-4406-a07c-778d71913019",
   "metadata": {},
   "source": [
    "`RuntimeError: error during blosc decompression: -1`.. maybe the underlying data is not blosc..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "584952f4-99e4-4c4f-ad16-365e576fc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression method: gzip\n",
      "Compression options: 9\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(nwb_filepath, 'r') as f:\n",
    "    dataset = f[lfp_data_path]\n",
    "    \n",
    "    print(\"Compression method:\", dataset.compression)\n",
    "    print(\"Compression options:\", dataset.compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddad786-ee41-450a-b19c-a533fc57140d",
   "metadata": {},
   "source": [
    "I'm not sure how to deal with this information. would it go into the kerchunk references? or xarray reading?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149bb05-0fe5-4108-94c2-5b3e3683d64b",
   "metadata": {},
   "source": [
    "Now I'm getting: `ValueError: conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}`... which I don't undertstand because I'm setting the dims with the `_Array_Dimensions` field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb29bf-5184-44af-a0a1-7a1d8a99556d",
   "metadata": {},
   "source": [
    "## Step 3: Create New Zarr Directory with References \n",
    "- (create_zarr_lfp_one_probe_ref.py)\n",
    "- get input filename, sizes\n",
    "- create a skeleton Zarr mapping of datasets with lfp, time, and channel. populate the time array and sim the channel.\n",
    "- get references from kerchunked NWB and add the lfp references to the \n",
    "- write the references to a JSON file\n",
    "- Add references to lfp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95990b9c-8895-4559-afa6-e514a9e61484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import h5py\n",
    "import kerchunk.hdf\n",
    "import kerchunk.utils\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import ujson\n",
    "import warnings\n",
    "import zarr\n",
    "\n",
    "# Configuration\n",
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "probe_id = \"810755797\"\n",
    "\n",
    "input_directory = os.path.join(data_dir)\n",
    "output_json_file = os.path.join(data_dir, \"lfp_one_probe_ref.json\")\n",
    "\n",
    "\n",
    "def get_input_filename(probe_id):\n",
    "    return os.path.join(input_directory, f\"probe_{probe_id}_lfp.nwb\")\n",
    "\n",
    "\n",
    "def get_sizes():\n",
    "    chunk_size = dict(probe=1)\n",
    "\n",
    "    f = h5py.File(get_input_filename(probe_id), \"r\")\n",
    "    lfp = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/data\"]\n",
    "    ntime, nchannel = lfp.shape\n",
    "    print(ntime, nchannel)\n",
    "\n",
    "    lfp_dtype = lfp.dtype\n",
    "\n",
    "    time = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/timestamps\"]\n",
    "    time_dtype = time.dtype\n",
    "\n",
    "    chunk_size = dict(time=lfp.chunks[0], channel=lfp.chunks[1])\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print(\"ntime\", ntime, \"nchannel\", nchannel)\n",
    "    print(\"Chunk sizes\", chunk_size)\n",
    "    print(\"Number of time chunks\", math.ceil(ntime / chunk_size[\"time\"]))\n",
    "\n",
    "    return ntime, nchannel, lfp_dtype, time_dtype, chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943eaf36-494f-42c2-96a4-11db29b34ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10715666 93\n",
      "ntime 10715666 nchannel 93\n",
      "Chunk sizes {'time': 41859, 'channel': 1}\n",
      "Number of time chunks 256\n"
     ]
    }
   ],
   "source": [
    "ntime, nchannel, lfp_dtype, time_dtype, chunk_size = get_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5653c5ab-7e39-49f0-bc6c-e006e1cfe22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing time for probe 810755797\n",
      "==> CHECK k acquisition/probe_810755797_lfp/probe_810755797_lfp_data/data/20.2\n",
      "==>       v ['/Users/droumis/data/allen/probe_810755797_lfp.nwb', 163354739, 84801]\n",
      "==>       name lfp/20.2\n",
      "Writing /Users/droumis/data/allen/lfp_one_probe_ref.json\n"
     ]
    }
   ],
   "source": [
    "def create_zarr_file(ntime, nchannel, lfp_dtype, time_dtype, chunk_size):\n",
    "    shape = (ntime, nchannel)\n",
    "    refs = {}\n",
    "\n",
    "    root = zarr.open_group(refs, mode=\"w\")\n",
    "    \n",
    "    # LFP data\n",
    "    #### Probably needs to know compression of underlying data?????????\n",
    "    ## DR: compression of underlying data is gzip level 9.. but I don't know how to use this info\n",
    "    lfp_data = root.create_dataset(\n",
    "        name=\"lfp\",\n",
    "        shape=shape,\n",
    "        synchronizer=zarr.ThreadSynchronizer(),\n",
    "        chunks=(chunk_size[\"time\"], chunk_size[\"channel\"]),\n",
    "        dtype=lfp_dtype,\n",
    "    )\n",
    "    lfp_data.attrs[\"_ARRAY_DIMENSIONS\"] = [\"time\", \"channel\"]\n",
    "    \n",
    "    # Time coordinates, filled in later.\n",
    "    time_data = root.create_dataset(\n",
    "        name=\"time\",\n",
    "        chunks=chunk_size[\"time\"],  # Do I want this chunked or not?\n",
    "        shape=ntime,\n",
    "        dtype=time_dtype,\n",
    "    )\n",
    "    time_data.attrs[\"_ARRAY_DIMENSIONS\"] = [\"time\"]\n",
    "\n",
    "    # Channel coordinates are integers starting at zero.\n",
    "    channel = np.arange(nchannel, dtype=np.uint32)\n",
    "    coord = root.create_dataset(\n",
    "        name=\"channel\",\n",
    "        shape=nchannel,\n",
    "        dtype=channel.dtype,\n",
    "    )\n",
    "    coord[:] = channel\n",
    "    coord.attrs[\"_ARRAY_DIMENSIONS\"] = [\"channel\"]\n",
    "\n",
    "    return time_data, refs\n",
    "\n",
    "\n",
    "def get_kerchunk_refs(probe_id, refs):\n",
    "    filename = get_input_filename(probe_id)\n",
    "    so = dict(anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "\n",
    "    # Getting all the chunk references from the NWB file here, then filtering them.\n",
    "    # Can I just read the ones I am interested in?\n",
    "    with fsspec.open(filename, **so) as f:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, filename)\n",
    "            probe_refs = h5chunks.translate()[\"refs\"]\n",
    "\n",
    "    # Filter refs to only contain what we want\n",
    "    # Don't know if need .zarray and .zattrs or not.\n",
    "    match = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/data/\"\n",
    "    for ref in list(probe_refs):\n",
    "        if not ref.startswith(match):\n",
    "            probe_refs.pop(ref)\n",
    "\n",
    "    len_match = len(match)\n",
    "    for k, v in probe_refs.items():\n",
    "        suffix = k[len_match:]\n",
    "        # Don't think .zarray and .zattrs are needed as the create_dataset\n",
    "        # specifies the dtype, shape, etc\n",
    "        if suffix[0] == \".\":\n",
    "            continue\n",
    "        # before, this was refs[f\"lfo... typo?\n",
    "        refs[f\"lfp/{suffix}\"] = v\n",
    "\n",
    "        if suffix == \"20.2\":\n",
    "            print(\"==> CHECK k\", k)\n",
    "            print(\"==>       v\", v)\n",
    "            print(\"==>       name\", f\"lfp/{suffix}\")\n",
    "\n",
    "\n",
    "def load_and_store(time2d_data, refs):\n",
    "    input_filename = get_input_filename(probe_id)\n",
    "    f = h5py.File(input_filename, \"r\")\n",
    "\n",
    "    print(\"Writing time for probe\", probe_id)\n",
    "    time = f[f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data/timestamps\"]\n",
    "    time2d_data[:] = time[:]\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # LFP data kept in original files, referenced chunkwise from kerchunk-created JSON file\n",
    "    get_kerchunk_refs(probe_id, refs)\n",
    "\n",
    "    refs = kerchunk.utils.consolidate(refs)\n",
    "    return refs\n",
    "\n",
    "time2d_data, refs = create_zarr_file(ntime, nchannel, lfp_dtype, time_dtype, chunk_size)\n",
    "\n",
    "refs = load_and_store(time2d_data, refs)\n",
    "\n",
    "print(f\"Writing {output_json_file}\")\n",
    "with open(output_json_file, \"w\") as f:\n",
    "    ujson.dump(refs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f40ad6-26d7-48fb-b0ed-11f81fdae4bd",
   "metadata": {},
   "source": [
    "## Step 3.1: KERCHUNK! Data Access and Calculations\n",
    "- (test_zarr_lfp_one_probe_ref.py)\n",
    "- Open the JSON/Zarr file created in the previous step and print the dataset's structure.\n",
    "- Try some data access and calculations on the LFP data read from the JSON/Zarr file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d7ce2-d9df-4ad2-8f04-3bb5f994efa7",
   "metadata": {},
   "source": [
    "I'm stuck at: \"RuntimeError: error during blosc decompression: -1\".. Maybe we do need to use the compression of underlying data in some way (gzip level 9).. I need Ian's help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "842acc3e-fb0c-40ac-9750-c187ac47f9e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Zarr object is missing the attribute `_ARRAY_DIMENSIONS` and the NCZarr metadata, which are required for xarray to determine variable dimensions.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:210\u001b[0m, in \u001b[0;36m_get_zarr_dims_and_attrs\u001b[0;34m(zarr_obj, dimension_key, try_nczarr)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# Xarray-Zarr\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     dimensions \u001b[38;5;241m=\u001b[39m \u001b[43mzarr_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdimension_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/zarr/attrs.py:73\u001b[0m, in \u001b[0;36mAttributes.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: '_ARRAY_DIMENSIONS'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:224\u001b[0m, in \u001b[0;36m_get_zarr_dims_and_attrs\u001b[0;34m(zarr_obj, dimension_key, try_nczarr)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# NCZarr uses Fully Qualified Names\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     dimensions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 224\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(dim) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[43mzarray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_NCZARR_ARRAY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimrefs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    225\u001b[0m     ]\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: '_NCZARR_ARRAY'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# input_json = os.path.join(directory, \"lfp_one_probe_ref.json\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Disable mask_and_scale otherwise dtypes are converted to floats.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m fs \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mfilesystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m, fo\u001b[38;5;241m=\u001b[39minput_json)\n\u001b[0;32m---> 10\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsolidated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_and_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(ds)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Try some data access and calculations\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:980\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    978\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 980\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/store.py:43\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, AbstractDataStore)\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mvars\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m filename_or_obj\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m         attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/common.py:210\u001b[0m, in \u001b[0;36mAbstractDataStore.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    This loads the variables and attributes simultaneously.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    A centralized loading function makes it easier to create\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    are requested, so care should be taken to make sure its fast.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     variables \u001b[38;5;241m=\u001b[39m FrozenDict(\n\u001b[0;32m--> 210\u001b[0m         (_decode_variable_name(k), v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    212\u001b[0m     attributes \u001b[38;5;241m=\u001b[39m FrozenDict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attrs())\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variables, attributes\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:517\u001b[0m, in \u001b[0;36mZarrStore.get_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFrozenDict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_store_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzarr_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/utils.py:471\u001b[0m, in \u001b[0;36mFrozenDict\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mFrozenDict\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Frozen:\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Frozen(\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:518\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FrozenDict(\n\u001b[0;32m--> 518\u001b[0m         (k, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_store_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzarr_group\u001b[38;5;241m.\u001b[39marrays()\n\u001b[1;32m    519\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:494\u001b[0m, in \u001b[0;36mZarrStore.open_store_variable\u001b[0;34m(self, name, zarr_array)\u001b[0m\n\u001b[1;32m    492\u001b[0m data \u001b[38;5;241m=\u001b[39m indexing\u001b[38;5;241m.\u001b[39mLazilyIndexedArray(ZarrArrayWrapper(name, \u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    493\u001b[0m try_nczarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 494\u001b[0m dimensions, attributes \u001b[38;5;241m=\u001b[39m \u001b[43m_get_zarr_dims_and_attrs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzarr_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDIMENSION_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_nczarr\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attributes)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# TODO: this should not be needed once\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# https://github.com/zarr-developers/zarr-python/issues/1269 is resolved.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:227\u001b[0m, in \u001b[0;36m_get_zarr_dims_and_attrs\u001b[0;34m(zarr_obj, dimension_key, try_nczarr)\u001b[0m\n\u001b[1;32m    223\u001b[0m         dimensions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    224\u001b[0m             os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(dim) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m zarray[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_NCZARR_ARRAY\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimrefs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    225\u001b[0m         ]\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZarr object is missing the attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdimension_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` and the NCZarr metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich are required for xarray to determine variable dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    232\u001b[0m nc_attrs \u001b[38;5;241m=\u001b[39m [attr \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m zarr_obj\u001b[38;5;241m.\u001b[39mattrs \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    233\u001b[0m attributes \u001b[38;5;241m=\u001b[39m HiddenKeyDict(zarr_obj\u001b[38;5;241m.\u001b[39mattrs, [dimension_key] \u001b[38;5;241m+\u001b[39m nc_attrs)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Zarr object is missing the attribute `_ARRAY_DIMENSIONS` and the NCZarr metadata, which are required for xarray to determine variable dimensions.'"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "input_json = output_json_file\n",
    "# input_json = os.path.join(directory, \"lfp_one_probe_ref.json\")\n",
    "\n",
    "# Disable mask_and_scale otherwise dtypes are converted to floats.\n",
    "fs = fsspec.filesystem(\"reference\", fo=input_json)\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(\"\"),\n",
    "    engine=\"zarr\",\n",
    "    group=\"\",\n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "print(ds)\n",
    "\n",
    "\n",
    "# Try some data access and calculations\n",
    "data = ds.lfp[:100, 0]\n",
    "print(\"==> data\", data.compute())\n",
    "mean = data.mean().compute()\n",
    "print(\"==> mean\", mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2918fa-6668-4669-bed8-6bce229f2249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b598266-11c6-4ac2-b6ae-a0fe974c5bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96644b-b2d5-4714-a4d5-9eebe623d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df637eb-ab37-4e3a-bd06-1d6dfa8fc04e",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759a28f-08de-4dd8-a668-52fb100acd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93dca8-d6d7-41af-acf7-772d26e715ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# probe_id = \"12345\"\n",
    "# filename = f\"probe_{probe_id}_lfp.nwb\"\n",
    "# # data_group = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "# data_group = 'lfp'\n",
    "\n",
    "# # Create a new HDF5 file\n",
    "# with h5py.File(filename, 'w') as f:\n",
    "#     # Create a group structure\n",
    "#     probe_group = f.create_group(data_group)\n",
    "    \n",
    "#     # Create a dataset with random data\n",
    "#     data_shape = (100, 10)\n",
    "#     data = np.random.random(data_shape)\n",
    "#     probe_data = probe_group.create_dataset(\"data\", data_shape, dtype='f', data=data, chunks=(10, 2))\n",
    "    \n",
    "#     # Add attributes (Optional)\n",
    "#     probe_data.attrs['unit'] = 'volts'\n",
    "\n",
    "# print(f\"File {filename} created.\")\n",
    "\n",
    "\n",
    "# import kerchunk.hdf\n",
    "# import fsspec\n",
    "# import ujson\n",
    "\n",
    "# # so = dict(anon=True, default_fill_cache=False, default_cache_type='first')\n",
    "\n",
    "# # Open the HDF5 file and translate the chunk information\n",
    "# # with fsspec.open(filename, **so) as f:\n",
    "# with fsspec.open(filename) as f:\n",
    "#     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, filename)\n",
    "#     refs = h5chunks.translate()#[\"refs\"] #translates content of the HDF5 file into the Zarr format\n",
    "\n",
    "# # Consolidate the references\n",
    "# # refs = kerchunk.utils.consolidate(refs)\n",
    "\n",
    "# # Print the references\n",
    "# # print(refs)\n",
    "\n",
    "# Save to a JSON file\n",
    "# output_json_file = \"lfp_one_probe_ref_rand.json\"\n",
    "# with open(output_json_file, \"wb\") as f:\n",
    "#     # ujson.dump(refs, f)\n",
    "#     f.write(ujson.dumps(refs).encode())\n",
    "\n",
    "# with open(output_json_file, \"r\") as f:\n",
    "#     content = f.read()\n",
    "#     print(content)\n",
    "\n",
    "# with fsspec.open(url, **so) as inf:\n",
    "#     h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, url, inline_threshold=100)\n",
    "#     h5chunks.translate()\n",
    "#     with open(\"single_file_kerchunk.json\", \"wb\") as f:\n",
    "#         f.write(ujson.dumps(h5chunks.translate()).encode()\n",
    "\n",
    "# # import os\n",
    "\n",
    "# # data_dir='~/data/allen/'\n",
    "# json_filepath = '/Users/droumis/src/neuro/workflows/ephys-viewer/dev/' + output_json_file\n",
    "# # data_dir = os.path.expanduser(data_dir)\n",
    "# # output_json_file = os.path.join(data_dir, output_json_file)\n",
    "# json_filepath\n",
    "\n",
    "# import fsspec\n",
    "# import xarray as xr\n",
    "\n",
    "# # data_group = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "# # ref_data_group = 'refs/refs/' + data_group\n",
    "# # Disable mask_and_scale otherwise dtypes are converted to floats.\n",
    "# fs = fsspec.filesystem(\"reference\", fo=json_filepath)\n",
    "# m = fs.get_mapper()\n",
    "# ds = xr.open_dataset(\n",
    "#     m,\n",
    "#     group='lfp',\n",
    "#     engine=\"zarr\",\n",
    "#     backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    "# )\n",
    "\n",
    "# print(ds)\n",
    "\n",
    "# # Try some data access and calculations\n",
    "# data = ds.data[:100, 0]  # Update this line according to the actual structure of your data\n",
    "# print(\"==> data\", data.compute())\n",
    "# mean = data.mean().compute()\n",
    "# print(\"==> mean\", mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
