{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(COMPPARALELOGPUSSMC)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Cómputo en paralelo usando GPUs en un sistema de memoria compartida (SMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Notas para contenedor de docker:\n",
    "\n",
    "Comando de docker para ejecución de la nota de forma local:\n",
    "\n",
    "nota: cambiar `<ruta a mi directorio>` por la ruta de directorio que se desea mapear a `/datos` dentro del contenedor de docker y `<versión imagen de docker>` por la versión más actualizada que se presenta en la documentación.\n",
    "\n",
    "`docker run --rm -v <ruta a mi directorio>:/datos --name jupyterlab_optimizacion_2 -p 8888:8888 -d palmoreck/jupyterlab_optimizacion_2:<versión imagen de docker>`\n",
    "\n",
    "password para jupyterlab: `qwerty`\n",
    "\n",
    "Detener el contenedor de docker:\n",
    "\n",
    "`docker stop jupyterlab_optimizacion_2`\n",
    "\n",
    "Documentación de la imagen de docker `palmoreck/jupyterlab_optimizacion_2:<versión imagen de docker>` en [liga](https://github.com/palmoreck/dockerfiles/tree/master/jupyterlab/optimizacion_2).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota generada a partir de [liga](https://www.dropbox.com/s/yjijtfuky3s5dfz/2.5.Compute_Unified_Device_Architecture.pdf?dl=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Al final de esta nota la comunidad lectora:\n",
    ":class: tip\n",
    "\n",
    "* Aprenderá un poco de historia y arquitectura de la GPU.\n",
    "\n",
    "* Se familiarizará con la sintaxis de *CUDA-C* para cómputo en la GPU con ejemplos sencillos y los relacionará con el modelo de programación CUDA.\n",
    "\n",
    "* Utilizará el paquete *CuPy* de *Python* para cómputo en la GPU.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se presentan códigos y sus ejecuciones en una máquina `p2.xlarge` con una AMI `ubuntu 20.04 - ami-042e8287309f5df03` de la nube de [AWS](https://aws.amazon.com/). Se utilizó en la sección de `User data` el [script_cuda_and_tools.sh](https://github.com/palmoreck/scripts_for_useful_tools_installations/blob/main/AWS/ubuntu_20.04/optimizacion_2/script_cuda_and_tools.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La máquina `p2.xlarge` tiene las siguientes características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                    x86_64\n",
      "CPU op-mode(s):                  32-bit, 64-bit\n",
      "Byte Order:                      Little Endian\n",
      "Address sizes:                   46 bits physical, 48 bits virtual\n",
      "CPU(s):                          4\n",
      "On-line CPU(s) list:             0-3\n",
      "Thread(s) per core:              2\n",
      "Core(s) per socket:              2\n",
      "Socket(s):                       1\n",
      "NUMA node(s):                    1\n",
      "Vendor ID:                       GenuineIntel\n",
      "CPU family:                      6\n",
      "Model:                           79\n",
      "Model name:                      Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz\n",
      "Stepping:                        1\n",
      "CPU MHz:                         2701.377\n",
      "CPU max MHz:                     3000.0000\n",
      "CPU min MHz:                     1200.0000\n",
      "BogoMIPS:                        4600.15\n",
      "Hypervisor vendor:               Xen\n",
      "Virtualization type:             full\n",
      "L1d cache:                       64 KiB\n",
      "L1i cache:                       64 KiB\n",
      "L2 cache:                        512 KiB\n",
      "L3 cache:                        45 MiB\n",
      "NUMA node0 CPU(s):               0-3\n",
      "Vulnerability Itlb multihit:     KVM: Vulnerable\n",
      "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
      "Vulnerability Mds:               Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
      "Vulnerability Meltdown:          Mitigation; PTI\n",
      "Vulnerability Spec store bypass: Vulnerable\n",
      "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:        Mitigation; Full generic retpoline, STIBP disabled, RSB filling\n",
      "Vulnerability Srbds:             Not affected\n",
      "Vulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
      "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx xsaveopt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *-firmware\n",
      "       description: BIOS\n",
      "       vendor: Xen\n",
      "       physical id: 0\n",
      "       version: 4.2.amazon\n",
      "       date: 08/24/2006\n",
      "       size: 96KiB\n",
      "       capabilities: pci edd\n",
      "  *-memory\n",
      "       description: System Memory\n",
      "       physical id: 1000\n",
      "       size: 61GiB\n",
      "       capabilities: ecc\n",
      "       configuration: errordetection=multi-bit-ecc\n",
      "     *-bank:0\n",
      "          description: DIMM RAM\n",
      "          physical id: 0\n",
      "          slot: DIMM 0\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:1\n",
      "          description: DIMM RAM\n",
      "          physical id: 1\n",
      "          slot: DIMM 1\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:2\n",
      "          description: DIMM RAM\n",
      "          physical id: 2\n",
      "          slot: DIMM 2\n",
      "          size: 16GiB\n",
      "          width: 64 bits\n",
      "     *-bank:3\n",
      "          description: DIMM RAM\n",
      "          physical id: 3\n",
      "          slot: DIMM 3\n",
      "          size: 13GiB\n",
      "          width: 64 bits\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo lshw -C memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#execute next line to have in the book output of cell\n",
    "sudo lshw -C display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  *-display:0 UNCLAIMED\n",
    "       description: VGA compatible controller\n",
    "       product: GD 5446\n",
    "       vendor: Cirrus Logic\n",
    "       physical id: 2\n",
    "       bus info: pci@0000:00:02.0\n",
    "       version: 00\n",
    "       width: 32 bits\n",
    "       clock: 33MHz\n",
    "       capabilities: vga_controller\n",
    "       configuration: latency=0\n",
    "       resources: memory:80000000-81ffffff memory:86004000-86004fff memory:c0000-dffff\n",
    "  *-display:1 UNCLAIMED\n",
    "       description: 3D controller\n",
    "       product: GK210GL [Tesla K80]\n",
    "       vendor: NVIDIA Corporation\n",
    "       physical id: 1e\n",
    "       bus info: pci@0000:00:1e.0\n",
    "       version: a1\n",
    "       width: 64 bits\n",
    "       clock: 33MHz\n",
    "       capabilities: pm msi pciexpress cap_list\n",
    "       configuration: latency=0\n",
    "       resources: iomemory:100-ff memory:84000000-84ffffff memory:1000000000-13ffffffff memory:82000000-83ffffff\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux ip-10-0-0-128 5.4.0-1045-aws #47-Ubuntu SMP Tue Apr 13 07:02:25 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "uname -ar #r for kernel, a for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "En la celda anterior se utilizó el comando de *magic* `%%bash`. Algunos comandos de *magic* los podemos utilizar también con `import`. Ver [ipython-magics](https://ipython.readthedocs.io/en/stable/interactive/magics.html#)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Compute Unified Device Architecture* (CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un poco de historia..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "GPGPU es un término que se utilizó para referirse a la programación en unidades de procesamiento gráfico de forma general. Hoy en día se conoce simplemente como *GPU programming*. Ver [General-purpose computing on graphics processing units](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La industria de videojuegos impulsó el desarrollo de las tarjetas gráficas a una velocidad sin precedente a partir del año 1999 para incrementar el nivel de detalle visual en los juegos de video. Alrededor del 2003 se planteó la posibilidad de utilizar las unidades de procesamiento gráfico para procesamiento en paralelo relacionado con aplicaciones distintas al ambiente de gráficas. A partir del 2006 la empresa [NVIDIA](https://www.nvidia.com/en-us/about-nvidia/) introdujo CUDA, una plataforma GPGPU y un modelo de programación que facilita el procesamiento en paralelo en las GPU's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde el 2006, las tarjetas gráficas muestran una brecha significativa con las unidades de procesamiento CPU's. Ver por ejemplo las gráficas que *NVIDIA* publica año tras año y que están relacionadas con el número de operaciones en punto flotante por segundo (FLOPS) y la transferencia de datos en la memoria RAM de la GPU: [gráficas cpu vs gpu en imágenes de google](https://www.google.com/search?q=plot+gflops+gpu+cpu+nvidia&tbm=isch&ved=2ahUKEwjKk7Le_bzwAhUUaKwKHX9-AP8Q2-cCegQIABAA&oq=plot+gflops+gpu+cpu+nvidia&gs_lcp=CgNpbWcQA1C_W1i_W2DhXGgAcAB4AIABX4gBX5IBATGYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=xAiYYMqhL5TQsQX__IH4Dw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "La GPU y la CPU están conectadas por una interconexión de nombre [PCI](https://en.wikipedia.org/wiki/Conventional_PCI).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy en día se continúa el desarrollo de GPU's con mayor RAM, con mayor capacidad de cómputo y mejor conectividad con la CPU. Estos avances han permitido resolver problemas con mayor exactitud que los resueltos con las CPU's, por ejemplo en el terreno de *deep learning* en reconocimiento de imágenes. Ver [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), [2012: A Breakthrough Year for Deep Learning](https://medium.com/limitlessai/2012-a-breakthrough-year-for-deep-learning-2a31a6796e73).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Para más avances ver [NVIDIA Turing Architecture In-Depth](https://devblogs.nvidia.com/nvidia-turing-architecture-in-depth/), [samsung-amd-rdna-gpu-2021](https://wccftech.com/samsung-amd-rdna-gpu-2021/), [playstation-5-specifications-revealed-but-design-is-still-a-mystery](https://www.theguardian.com/games/2020/mar/19/playstation-5-specifications-revealed-but-design-is-still-a-mystery), [xbox-series-x-tech](https://news.xbox.com/en-us/2020/03/16/xbox-series-x-tech/) y recientemente [IBM Supercomputer Summit Attacks Coronavirus…](https://www.ibm.com/blogs/nordic-msp/ibm-supercomputer-summit-attacks-coronavirus/). \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Recuérdese la [taxonomía de Flynn](https://en.wikipedia.org/wiki/Flynn%27s_taxonomy).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura en la que podemos ubicar a las GPU's es en la de un sistema MIMD y SIMD. De hecho es [SIMT: Simple Instruction Multiple Thread](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads) en un modelo de sistema de memoria compartida pues \"los *threads* en un *warp* leen la misma instrucción para ser ejecutada\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Definición\n",
    "\n",
    "Un *warp* en el contexto de GPU *programming* es un conjunto de *threads*. Equivale a $32$ *threads*.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Diferencia con la CPU multicore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/k11qub01w4nvksi/CPU_multicore.png?dl=0\" heigth=\"500\" width=\"500\">\n",
    "\n",
    "**GPU**\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/lw9kia12qhwp95r/GPU.png?dl=0\" heigth=\"500\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Obsérvese en el dibujo anterior la diferencia en tamaño del caché en la CPU y GPU. También la unidad de control es más pequeña en la GPU.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Una máquina *quad core* soporta cuatro threads en cada *core*.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de una máquina *multicore* o multi CPU's con la habilidad de lanzar en un instante de tiempo unos cuantos *threads*, la GPU puede lanzar cientos o miles de threads en un instante siendo cada core *heavily multithreaded*. Sí hay restricciones en el número de threads que se pueden lanzar en un instante pues las tarjetas gráficas tienen diferentes características (modelo) y arquitecturas, pero la diferencia con la CPU es grande. Por ejemplo, la serie **GT 200** (2009) en un instante puede lanzar 30,720 threads con sus 240 *cores*. Ver [GeForce_200_series](https://en.wikipedia.org/wiki/GeForce_200_series), [List of NVIDIA GPU's](https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver [How Graphics Cards Work](https://computer.howstuffworks.com/graphics-card1.htm) y [How Microprocessors Work](https://computer.howstuffworks.com/microprocessor.htm) para más información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Otras compañías producen tarjetas gráficas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, ver por ejemplo la lista de GPU's de [Advanced Micro Devices](https://en.wikipedia.org/wiki/List_of_AMD_graphics_processing_units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Si tengo una tarjeta gráfica de AMD puedo correr un programa de CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es posible pero algunas alternativas son:\n",
    "\n",
    "* [OpenCl](https://www.khronos.org/opencl/)\n",
    "\n",
    "* [OpenACC](https://www.openacc.org/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Si tengo una tarjeta gráfica de NVIDIA un poco antigua puedo correr un programa de CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las GPU's producidas por NVIDIA desde 2006 son capaces de correr programas basados en ***CUDA C***. La cuestión sería revisar qué *compute capability* tiene tu tarjeta. Ver [Compute Capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) para las características que tienen las tarjetas más actuales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué es *CUDA C*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una extensión al lenguaje *C* de programación en el que se utiliza una nueva sintaxis para procesamiento en la GPU. Contiene también una librería *runtime* que define funciones que se ejecutan desde el ***host*** por ejemplo para alojar y desalojar memoria en el ***device***, transferir datos entre la memoria *host* y la memoria *device* o manejar múltiples *devices*. La librería *runtime* está hecha encima de una API de *C* de bajo nivel llamada [NVIDIA CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html) la cual es accesible desde el código. Para información de la API de la librería runtime ver [NVIDIA CUDA Runtime API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "La transferencia de datos entre la memoria del *host* a *device* o viceversa constituye un *bottleneck* fuerte.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿A qué se refiere la terminología de *host* y *device*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Host* es la máquina *multicore* CPU y *device* es la GPU. Una máquina puede tener múltiples GPU's por lo que tendrá múltiples *devices*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tengo una tarjeta NVIDIA CUDA *capable* ¿qué debo realizar primero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar instalaciones dependiendo de tu sistema operativo. Ver [instalación](https://github.com/palmoreck/programming-languages/tree/master/C/extensiones_a_C/CUDA/instalacion) donde además se encontrará información para instalación de [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalé lo necesario y al ejecutar en la terminal `nvcc -V` obtengo la versión... ¿cómo puedo probar mi instalación?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Obteniendo información del *NVIDIA driver* ejecutando en la terminal el comando `nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 10 23:31:35 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla K80    On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Mon May 10 23:31:35 2021\n",
      "Driver Version                            : 465.19.01\n",
      "CUDA Version                              : 11.3\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:00:1E.0\n",
      "    Product Name                          : NVIDIA Tesla K80\n",
      "    Product Brand                         : Tesla\n",
      "    Display Mode                          : Disabled\n",
      "    Display Active                        : Disabled\n",
      "    Persistence Mode                      : Enabled\n",
      "    MIG Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Accounting Mode                       : Disabled\n",
      "    Accounting Mode Buffer Size           : 4000\n",
      "    Driver Model\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Serial Number                         : 0325116067357\n",
      "    GPU UUID                              : GPU-30cfb81b-c816-3139-4205-7e6a686b4699\n",
      "    Minor Number                          : 0\n",
      "    VBIOS Version                         : 80.21.1F.00.02\n",
      "    MultiGPU Board                        : No\n",
      "    Board ID                              : 0x1e\n",
      "    GPU Part Number                       : 900-22080-0000-000\n",
      "    Inforom Version\n",
      "        Image Version                     : 2080.0200.00.04\n",
      "        OEM Object                        : 1.1\n",
      "        ECC Object                        : 3.0\n",
      "        Power Management Object           : N/A\n",
      "    GPU Operation Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    GPU Virtualization Mode\n",
      "        Virtualization Mode               : Pass-Through\n",
      "        Host VGPU Mode                    : N/A\n",
      "    IBMNPU\n",
      "        Relaxed Ordering Mode             : N/A\n",
      "    PCI\n",
      "        Bus                               : 0x00\n",
      "        Device                            : 0x1E\n",
      "        Domain                            : 0x0000\n",
      "        Device Id                         : 0x102D10DE\n",
      "        Bus Id                            : 00000000:00:1E.0\n",
      "        Sub System Id                     : 0x106C10DE\n",
      "        GPU Link Info\n",
      "            PCIe Generation\n",
      "                Max                       : 3\n",
      "                Current                   : 1\n",
      "            Link Width\n",
      "                Max                       : 16x\n",
      "                Current                   : 16x\n",
      "        Bridge Chip\n",
      "            Type                          : N/A\n",
      "            Firmware                      : N/A\n",
      "        Replays Since Reset               : 0\n",
      "        Replay Number Rollovers           : 0\n",
      "        Tx Throughput                     : N/A\n",
      "        Rx Throughput                     : N/A\n",
      "    Fan Speed                             : N/A\n",
      "    Performance State                     : P8\n",
      "    Clocks Throttle Reasons\n",
      "        Idle                              : Active\n",
      "        Applications Clocks Setting       : Not Active\n",
      "        SW Power Cap                      : Not Active\n",
      "        HW Slowdown                       : Not Active\n",
      "            HW Thermal Slowdown           : N/A\n",
      "            HW Power Brake Slowdown       : N/A\n",
      "        Sync Boost                        : Not Active\n",
      "        SW Thermal Slowdown               : Not Active\n",
      "        Display Clock Setting             : Not Active\n",
      "    FB Memory Usage\n",
      "        Total                             : 11441 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 11441 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 16384 MiB\n",
      "        Used                              : 2 MiB\n",
      "        Free                              : 16382 MiB\n",
      "    Compute Mode                          : Default\n",
      "    Utilization\n",
      "        Gpu                               : 0 %\n",
      "        Memory                            : 0 %\n",
      "        Encoder                           : 0 %\n",
      "        Decoder                           : 0 %\n",
      "    Encoder Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    FBC Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    Ecc Mode\n",
      "        Current                           : Enabled\n",
      "        Pending                           : Enabled\n",
      "    ECC Errors\n",
      "        Volatile\n",
      "            Single Bit            \n",
      "                Device Memory             : 0\n",
      "                Register File             : 0\n",
      "                L1 Cache                  : 0\n",
      "                L2 Cache                  : 0\n",
      "                Texture Memory            : 0\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : 0\n",
      "            Double Bit            \n",
      "                Device Memory             : 0\n",
      "                Register File             : 0\n",
      "                L1 Cache                  : 0\n",
      "                L2 Cache                  : 0\n",
      "                Texture Memory            : 0\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : 0\n",
      "        Aggregate\n",
      "            Single Bit            \n",
      "                Device Memory             : 0\n",
      "                Register File             : 0\n",
      "                L1 Cache                  : 0\n",
      "                L2 Cache                  : 0\n",
      "                Texture Memory            : 0\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : 0\n",
      "            Double Bit            \n",
      "                Device Memory             : 0\n",
      "                Register File             : 0\n",
      "                L1 Cache                  : 0\n",
      "                L2 Cache                  : 0\n",
      "                Texture Memory            : 0\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : 0\n",
      "    Retired Pages\n",
      "        Single Bit ECC                    : 0\n",
      "        Double Bit ECC                    : 0\n",
      "        Pending Page Blacklist            : No\n",
      "    Remapped Rows                         : N/A\n",
      "    Temperature\n",
      "        GPU Current Temp                  : 34 C\n",
      "        GPU Shutdown Temp                 : 110 C\n",
      "        GPU Slowdown Temp                 : 88 C\n",
      "        GPU Max Operating Temp            : N/A\n",
      "        GPU Target Temperature            : N/A\n",
      "        Memory Current Temp               : N/A\n",
      "        Memory Max Operating Temp         : N/A\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 30.91 W\n",
      "        Power Limit                       : 149.00 W\n",
      "        Default Power Limit               : 149.00 W\n",
      "        Enforced Power Limit              : 149.00 W\n",
      "        Min Power Limit                   : 100.00 W\n",
      "        Max Power Limit                   : 175.00 W\n",
      "    Clocks\n",
      "        Graphics                          : 324 MHz\n",
      "        SM                                : 324 MHz\n",
      "        Memory                            : 324 MHz\n",
      "        Video                             : 405 MHz\n",
      "    Applications Clocks\n",
      "        Graphics                          : 562 MHz\n",
      "        Memory                            : 2505 MHz\n",
      "    Default Applications Clocks\n",
      "        Graphics                          : 562 MHz\n",
      "        Memory                            : 2505 MHz\n",
      "    Max Clocks\n",
      "        Graphics                          : 875 MHz\n",
      "        SM                                : 875 MHz\n",
      "        Memory                            : 2505 MHz\n",
      "        Video                             : 540 MHz\n",
      "    Max Customer Boost Clocks\n",
      "        Graphics                          : N/A\n",
      "    Clock Policy\n",
      "        Auto Boost                        : On\n",
      "        Auto Boost Default                : On\n",
      "    Processes                             : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi -a #a for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para más información del comando `nvidia-smi` ver [results-for-the-nvidia-smi-command-in-a-terminal](https://askubuntu.com/questions/1220144/can-somebody-explain-the-results-for-the-nvidia-smi-command-in-a-terminal) y [nvidia-smi-367.38](https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Comentarios\n",
    "\n",
    "* Ejecutando `nvidia-smi -l 1` nos da información cada segundo. Otra opción es `watch -n 3 nvidia-smi --query-gpu=<queries> --format=csv`. Por ejemplo:\n",
    "\n",
    "```bash\n",
    "watch -n 3 nvidia-smi --query-gpu=index,gpu_name,memory.total,memory.used,memory.free,temperature.gpu,pstate,utilization.gpu,utilization.memory --format=csv\n",
    "\n",
    "```\n",
    "\n",
    "Ver [useful-nvidia-smi-queries](https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries)\n",
    "\n",
    "* Una herramienta que nos ayuda al monitoreo de uso de la(s) GPU(s) es [nvtop](https://github.com/Syllo/nvtop).\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Compilando y ejecutando el siguiente programa de *CUDA C*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world.cu\n",
    "\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world! del bloque %d del thread %d\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    func<<<2,3>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hello world! del cpu thread\\n\");\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "La sintaxis `<<<2,3>>>` refiere que serán lanzados 2 bloques de 3 *threads* cada uno.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos con `nvcc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 hello_world.cu -o hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* `nvcc` es un *wrapper* para el compilador de programas escritos en *C*.\n",
    "\n",
    "* En ocasiones para tener funcionalidad de un determinado [compute capability](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) se especifica la *flag* de `-arch=sm_11` en la línea de `nvcc`. En este caso se le indica al compilador que compile el programa para un *compute capability* de $1.1$. Ver [run a kernel using the larger grid size support offered](https://stackoverflow.com/questions/16954931/cuda-5-0-cudagetdeviceproperties-strange-grid-size-or-a-bug-in-my-code).\n",
    "\n",
    "* Para la versión 11 de CUDA se requiere explícitamente indicar la arquitectura y código para la compilación. Ver [cuda-11-kernel-doesnt-run](https://stackoverflow.com/questions/63675040/cuda-11-kernel-doesnt-run), [cuda-how-to-use-arch-and-code-and-sm-vs-compute](https://stackoverflow.com/questions/35656294/cuda-how-to-use-arch-and-code-and-sm-vs-compute/35657430#35657430), [cuda-compute-capability-requirements](https://stackoverflow.com/questions/28932864/cuda-compute-capability-requirements/28933055#28933055), [what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api](https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! del bloque 0 del thread 0\n",
      "Hello world! del bloque 0 del thread 1\n",
      "Hello world! del bloque 0 del thread 2\n",
      "Hello world! del bloque 1 del thread 0\n",
      "Hello world! del bloque 1 del thread 1\n",
      "Hello world! del bloque 1 del thread 2\n",
      "Hello world! del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)Haciendo un query a la GPU para ver qué características tiene (lo siguiente es posible ejecutar sólo si se instaló el [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n",
      "/usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery Starting...\n",
      "\n",
      " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
      "\n",
      "Detected 1 CUDA Capable device(s)\n",
      "\n",
      "Device 0: \"NVIDIA Tesla K80\"\n",
      "  CUDA Driver Version / Runtime Version          11.3 / 11.3\n",
      "  CUDA Capability Major/Minor version number:    3.7\n",
      "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
      "  (013) Multiprocessors, (192) CUDA Cores/MP:    2496 CUDA Cores\n",
      "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
      "  Memory Clock rate:                             2505 Mhz\n",
      "  Memory Bus Width:                              384-bit\n",
      "  L2 Cache Size:                                 1572864 bytes\n",
      "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
      "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
      "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
      "  Total amount of constant memory:               65536 bytes\n",
      "  Total amount of shared memory per block:       49152 bytes\n",
      "  Total shared memory per multiprocessor:        114688 bytes\n",
      "  Total number of registers available per block: 65536\n",
      "  Warp size:                                     32\n",
      "  Maximum number of threads per multiprocessor:  2048\n",
      "  Maximum number of threads per block:           1024\n",
      "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
      "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
      "  Maximum memory pitch:                          2147483647 bytes\n",
      "  Texture alignment:                             512 bytes\n",
      "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
      "  Run time limit on kernels:                     No\n",
      "  Integrated GPU sharing Host Memory:            No\n",
      "  Support host page-locked memory mapping:       Yes\n",
      "  Alignment requirement for Surfaces:            Yes\n",
      "  Device has ECC support:                        Enabled\n",
      "  Device supports Unified Addressing (UVA):      Yes\n",
      "  Device supports Managed Memory:                Yes\n",
      "  Device supports Compute Preemption:            No\n",
      "  Supports Cooperative Kernel Launch:            No\n",
      "  Supports MultiDevice Co-op Kernel Launch:      No\n",
      "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 30\n",
      "  Compute Mode:\n",
      "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
      "\n",
      "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.3, CUDA Runtime Version = 11.3, NumDevs = 1\n",
      "Result = PASS\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /usr/local/cuda/samples/1_Utilities/deviceQuery/ && sudo make\n",
    "/usr/local/cuda/samples/1_Utilities/deviceQuery/deviceQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Por qué usar CUDA y *CUDA-C* o más general cómputo en la GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NVIDIA como se mencionó al inicio de la nota fue de las primeras compañías en utilizar la GPU para tareas no relacionadas con el área de gráficos, ha colaborado en el avance del conocimiento de las GPU's y desarrollo de algoritmos y tarjetas gráficas. Otra compañía es [Khronos_Group](https://en.wikipedia.org/wiki/Khronos_Group) por ejemplo, quien actualmente desarrolla [OpenCl](https://www.khronos.org/opencl/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "*Deep learning* se ha utilizado para resolver problemas en *machine learning* típicos. Ejemplos de esto son la clasificación de imágenes, de sonidos o análisis de textos. Ver por ejemplo [Practical text analysis using deep learning](https://medium.com/@michael.fire/practical-text-analysis-using-deep-learning-5fb0744efdf9).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El cómputo en la GPU constituye hoy en día una alternativa fuerte a la implementación de modelos de *machine learning* ampliamente utilizada por la comunidad científica, también para cómputo matricial y *deep learning*.\n",
    "\n",
    "* Sí hay publicaciones científicas para la implementación de *deep learning* en las CPU's, ver por ejemplo el *paper* reciente de [SLIDE](https://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf) cuyo repo de *github* es [HashingDeepLearning](https://github.com/keroro824/HashingDeepLearning). Tal *paper* plantea una discusión a realizar con la frase:\n",
    "\n",
    "*...change in the state-of-the-art algorithms can render specialized hardware less effective in the future*. \n",
    "\n",
    "Ver por ejemplo [Tensor Cores](https://developer.nvidia.com/tensor-cores), [NVIDIA TENSOR CORES, The Next Generation of Deep Learning](https://www.nvidia.com/en-us/data-center/tensorcore/), [The most powerful computers on the planet: SUMMIT](https://www.ibm.com/thought-leadership/summit-supercomputer/) como ejemplos de hardware especializado para aprendizaje con *Tensorflow*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "*Summit powered by 9,126 IBM Power9 CPUs and over 27,000 NVIDIA V100 Tensor Core GPUS, is able to do 200 quadrillion calculations per second...* [IBM Supercomputer Summit Attacks Coronavirus…](https://www.ibm.com/blogs/nordic-msp/ibm-supercomputer-summit-attacks-coronavirus/).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, por falta de implementaciones algorítmicas en la *CPU* se han adoptado implementaciones de *deep learning* utilizando GPU's:\n",
    "\n",
    "\n",
    "*...However, for the case of DL, this investment is justified due to the lack of significant progressin the algorithmic alternatives for years.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentario\n",
    "\n",
    "Revisar también las entradas [An algorithm could make CPUs a cheap way to train AI](https://www.engadget.com/2020/03/03/rice-university-slide-cpu-gpu-machine-learning/) y [Deep learning rethink overcomes major obstacle in AI industry](https://www.sciencedaily.com/releases/2020/03/200305135041.htm). \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CUDA-C](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiste en extensiones al lenguaje C y en una *runtime library*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Kernel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En *CUDA C* se define una función que se ejecuta en el ***device*** y que se le nombra ***kernel***. El *kernel* inicia con la sintaxis:\n",
    "\n",
    "```C\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "* Siempre es tipo `void` (no hay `return`).\n",
    "\n",
    "* El llamado al *kernel* se realiza desde el ***host*** y con una sintaxis en la que se define el número de *threads*, nombrados ***CUDA threads*** (que son distintos a los *CPU threads*), y bloques, nombrados ***CUDA blocks***, que serán utilizados para la ejecución del *kernel*. La sintaxis que se utiliza es `<<< >>>` y en la primera entrada se coloca el número de *CUDA blocks* y en la segunda entrada el número de *CUDA threads*. Por ejemplo para lanzar N bloques de 5 *threads*.\n",
    "\n",
    "\n",
    "```C\n",
    "__global__ void mifun(int param){\n",
    "...\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    int par=0;\n",
    "    mifun<<<N,5>>>(par); \n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_simple.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_simple.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "}\n",
    "int main(void){\n",
    "    func<<<1,1>>>();\n",
    "    printf(\"Hello world!\\n\");\n",
    "return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall hello_world_simple.cu -o hello_world_simple.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_simple.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Comentarios\n",
    "\n",
    "* La función `main` se ejecuta en la CPU.\n",
    "\n",
    "* `func` es un *kernel* y es ejecutada por los *CUDA threads* en el *device*. Obsérvese que tal función inicia con la sintaxis `__global__`. En este caso el *CUDA thread* que fue lanzado no realiza ninguna acción pues el cuerpo del kernel está vacío.\n",
    "\n",
    "* El *kernel* sólo puede tener un `return` tipo *void*: `__global__ void func` por lo que el *kernel* debe regresar sus resultados a través de sus argumentos.\n",
    " \n",
    "* La extensión del archivo debe ser `.cu` aunque esto puede modificarse al compilar con `nvcc`: \n",
    "\n",
    "```bash\n",
    "\n",
    "nvcc -x cu hello_world.c -o hello_world.out\n",
    "\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Bloques de threads? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los *CUDA threads* son divididos en *CUDA blocks* y éstos se encuentran en un *grid*. En el lanzamiento del *kernel* se debe especificar al hardware cuántos *CUDA blocks* tendrá nuestro *grid* y cuántos *CUDA threads* estarán en cada bloque. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`func<<<2,3>>>();` representa 2 bloques de 3 *threads* cada uno.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_2.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world! del bloque %d del thread %d\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "int main(void){\n",
    "    func<<<2,3>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall hello_world_2.cu -o hello_world_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! del bloque 0 del thread 0\n",
      "Hello world! del bloque 0 del thread 1\n",
      "Hello world! del bloque 0 del thread 2\n",
      "Hello world! del bloque 1 del thread 0\n",
      "Hello world! del bloque 1 del thread 1\n",
      "Hello world! del bloque 1 del thread 2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En lo que continúa de la nota el nombre *thread* hará referencia a *CUDA thread* y el nombre bloque a *CUDA block*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* El llamado a la ejecución del *kernel* se realizó en el *host* y se lanzaron $2$ bloques cada uno con $3$ *threads*.\n",
    "\n",
    "* Se utiliza la función [cudaDeviceSynchronize](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d) para que el *cpu-thread* espere la finalización de la ejecución del *kernel*.\n",
    "\n",
    "* En el ejemplo anterior, las variables `blockIdx` y `threadIdx` hacen referencia a los **id**'s que tienen los bloques y los *threads*. El *id* del bloque dentro del *grid* y el *id* del thread dentro del bloque. La parte `.x` de las variables: `blockIdx.x` y `threadIdx.x` refieren a la **primera coordenada** del bloque en el *grid* y a la **primera coordenada** del *thread* en en el bloque. \n",
    "\n",
    "* La elección del número de bloques en un *grid* o el número de *threads* en un bloque no corresponde a alguna disposición del *hardware*. Esto es, si se lanza un *kernel* con `<<< 1, 3 >>>` no implica que la GPU tenga en su *hardware* un bloque o 3 *threads*. Asimismo, las coordenadas que se obtienen vía `blockIdx` o `threadIdx` son meras abstracciones, no corresponden a algún ordenamiento en el hardware de la GPU.\n",
    "\n",
    "* Todos los *threads* de un bloque  ejecutan el *kernel* por lo que se tienen tantas copias del kernel como número de bloques sean lanzados. Esto es una muestra la GPU sigue el modelo  *Single Instruction Multiple Threads [(SIMT)](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)*.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Grid's y bloques 3-dimensionales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el *device* podemos definir el *grid* de bloques y el bloque de *threads* utilizando el tipo de dato `dim3` el cual también es parte de *CUDA C*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`dim3 dimGrid(1,2,1);` representa 2 bloques en el *grid*.\n",
    "\n",
    "`dim3 dimBlock(1,1,3);` representa 3 *threads* por bloque.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello_world_3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file hello_world_3.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"Hello world! del bloque %d del thread %d\\n\", blockIdx.y, threadIdx.z);\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(1,2,1);\n",
    "    dim3 dimBlock(1,1,3);\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    printf(\"Hello world! del cpu thread\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall hello_world_3.cu -o hello_world_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! del bloque 0 del thread 0\n",
      "Hello world! del bloque 0 del thread 1\n",
      "Hello world! del bloque 0 del thread 2\n",
      "Hello world! del bloque 1 del thread 0\n",
      "Hello world! del bloque 1 del thread 1\n",
      "Hello world! del bloque 1 del thread 2\n",
      "Hello world! del cpu thread\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./hello_world_3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`dim3 dimGrid(1,1,1);` representa 1 bloque en el *grid*.\n",
    "\n",
    "`dim3 dimBlock(1,3,1);` representa 3 *threads* por bloque.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing thread_idxs.cu\n"
     ]
    }
   ],
   "source": [
    "%%file thread_idxs.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    if(threadIdx.x==0 && threadIdx.y==0 && threadIdx.z==0){\n",
    "        printf(\"blockIdx.x:%d\\n\",blockIdx.x);\n",
    "    }\n",
    "    printf(\"thread idx.x:%d\\n\",threadIdx.x);\n",
    "    printf(\"thread idx.y:%d\\n\",threadIdx.y);\n",
    "    printf(\"thread idx.z:%d\\n\",threadIdx.z);\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(1,1,1);\n",
    "    dim3 dimBlock(1,3,1);\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 thread_idxs.cu -o thread_idxs.out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockIdx.x:0\n",
      "thread idx.x:0\n",
      "thread idx.x:0\n",
      "thread idx.x:0\n",
      "thread idx.y:0\n",
      "thread idx.y:1\n",
      "thread idx.y:2\n",
      "thread idx.z:0\n",
      "thread idx.z:0\n",
      "thread idx.z:0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./thread_idxs.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`dim3 dimGrid(1,2,2);` representa 4 bloques en el *grid*.\n",
    "\n",
    "`dim3 dimBlock(1,1,1);` representa 1 *thread* por bloque.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing block_idxs.cu\n"
     ]
    }
   ],
   "source": [
    "%%file block_idxs.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    printf(\"blockIdx.x:%d\\n\",blockIdx.x);\n",
    "    printf(\"blockIdx.y:%d\\n\",blockIdx.y);\n",
    "    printf(\"blockIdx.z:%d\\n\",blockIdx.z);\n",
    "\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(1,2,2); \n",
    "    dim3 dimBlock(1,1,1); \n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall block_idxs.cu -o block_idxs.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockIdx.x:0\n",
      "blockIdx.x:0\n",
      "blockIdx.x:0\n",
      "blockIdx.x:0\n",
      "blockIdx.y:1\n",
      "blockIdx.y:0\n",
      "blockIdx.y:1\n",
      "blockIdx.y:0\n",
      "blockIdx.z:1\n",
      "blockIdx.z:0\n",
      "blockIdx.z:0\n",
      "blockIdx.z:1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./block_idxs.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar la variable `blockDim` para cada coordenada `x, y` o `z` y obtener la dimensión de los bloques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`dim3 dimGrid(2,2,2);` representa 8 bloques en el *grid*.\n",
    "\n",
    "`dim3 dimBlock(3,1,2);` representa 6 *threads* por bloque.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing block_dims.cu\n"
     ]
    }
   ],
   "source": [
    "%%file block_dims.cu\n",
    "#include<stdio.h>\n",
    "__global__ void func(void){\n",
    "    if(threadIdx.x==0 && threadIdx.y==0 && threadIdx.z==0 && blockIdx.z==1){\n",
    "    printf(\"blockDim.x:%d\\n\",blockDim.x);\n",
    "    printf(\"blockDim.y:%d\\n\",blockDim.y);\n",
    "    printf(\"blockDim.z:%d\\n\",blockDim.z);\n",
    "    }\n",
    "\n",
    "}\n",
    "int main(void){\n",
    "    dim3 dimGrid(2,2,2);\n",
    "    dim3 dimBlock(3,1,2);\n",
    "    func<<<dimGrid,dimBlock>>>(); \n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall block_dims.cu -o block_dims.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockDim.x:3\n",
      "blockDim.x:3\n",
      "blockDim.x:3\n",
      "blockDim.x:3\n",
      "blockDim.y:1\n",
      "blockDim.y:1\n",
      "blockDim.y:1\n",
      "blockDim.y:1\n",
      "blockDim.z:2\n",
      "blockDim.z:2\n",
      "blockDim.z:2\n",
      "blockDim.z:2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./block_dims.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alojamiento de memoria en el *device*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para alojar memoria en el *device* se utiliza el llamado a [cudaMalloc](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g37d37965bfb4803b6d4e59ff26856356) y para transferir datos del *host* al *device* o viceversa se llama a la función [cudaMemcpy](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gc263dbe6574220cc776b45438fc351e8) con respectivos parámetros como `cudaMemcpyHostToDevice` o `cudaMemcpyDeviceToHost`. \n",
    "\n",
    "Para desalojar memoria del *device* se utiliza el llamado a [cudaFree](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N bloques de 1 thread**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`dim3 dimGrid(N,1,1);` representa N bloques en el *grid*.\n",
    "\n",
    "`dim3 dimBlock(1,1,1);` representa 1 *thread* por bloque.\n",
    "\n",
    "`<<<dimGrid,dimBlock>>>` N bloques de 1 *thread*.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vector_sum.cu\n"
     ]
    }
   ],
   "source": [
    "%%file vector_sum.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void vect_sum(int *a, int *b, int *c){\n",
    "    int block_id_x = blockIdx.x;\n",
    "    if(block_id_x<N) //we assume N is less than maximum number of blocks\n",
    "                     //that can be launched\n",
    "        c[block_id_x] = a[block_id_x]+b[block_id_x];\n",
    "}\n",
    "int main(void){\n",
    "    int a[N], b[N],c[N];\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    dim3 dimGrid(N,1,1);\n",
    "    dim3 dimBlock(1,1,1);\n",
    "    //allocation in device\n",
    "    cudaMalloc((void **)&device_a, sizeof(int)*N); \n",
    "    cudaMalloc((void **)&device_b, sizeof(int)*N);\n",
    "    cudaMalloc((void **)&device_c, sizeof(int)*N);\n",
    "    //dummy data\n",
    "    for(i=0;i<N;i++){\n",
    "        a[i]=i;\n",
    "        b[i]=i*i;\n",
    "    }\n",
    "    //making copies of a, b arrays to GPU\n",
    "    cudaMemcpy(device_a,a,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(device_b,b,N*sizeof(int), cudaMemcpyHostToDevice);\n",
    "    vect_sum<<<dimGrid,dimBlock>>>(device_a,device_b,device_c);\n",
    "    cudaDeviceSynchronize();\n",
    "    //copy result to c array\n",
    "    cudaMemcpy(c,device_c,N*sizeof(int),cudaMemcpyDeviceToHost);\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",a[i],b[i],c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall vector_sum.cu -o vector_sum.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./vector_sum.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Comentarios\n",
    "\n",
    "* El *statement*:\n",
    "\n",
    "```C\n",
    "    int *device_a, *device_b, *device_c;\n",
    "```\n",
    "\n",
    "en sintaxis de *C* definen apuntadores que refieren a una dirección de memoria. En el contexto de la *GPU programming* estos apuntadores no apuntan a una dirección de memoria en el *device*. Aunque NVIDIA añadió el *feature* de [Unified Memory](https://devblogs.nvidia.com/unified-memory-cuda-beginners/) (un espacio de memoria accesible para el *host* y el *device*) aquí no se está usando tal *feature*. Más bien se están utilizando los apuntadores anteriores para apuntar a un [struct](https://en.wikipedia.org/wiki/Struct_(C_programming_language)) de *C* en el que uno de sus tipos de datos es una dirección de memoria en el *device*.\n",
    "\n",
    "* El uso de `(void **)` en el *statement*  `cudaMalloc((void **)&device_a, sizeof(int)*N);` es por la definición de la función `cudaMalloc`.\n",
    "\n",
    "* En el programa anterior se coloca en comentario que se asume que $N$ el número de datos en el arreglo es menor al número de bloques que es posible lanzar. Esto como veremos más adelante es importante considerar pues aunque en un *device* se pueden lanzar muchos bloques y muchos *threads*, se tienen límites en el número de éstos que es posible lanzar.\n",
    "\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Perfilamiento en CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al instalar el *CUDA toolkit* en sus máquinas se instala la línea de comando [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) para perfilamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==17783== NVPROF is profiling process 17783, command: ./vector_sum.out\n",
      "==17783== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==17783== Profiling application: ./vector_sum.out\n",
      "==17783== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    46.15  5.38e-06         1  5.38e-06  5.38e-06  5.38e-06  vect_sum(int*, int*, int*)\n",
      "                    31.59  3.68e-06         2  1.84e-06  1.57e-06  2.11e-06  [CUDA memcpy HtoD]\n",
      "                    22.25  2.59e-06         1  2.59e-06  2.59e-06  2.59e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.59  0.264985         3  0.088328  4.07e-06  0.264975  cudaMalloc\n",
      "                     0.19  5.17e-04         1  5.17e-04  5.17e-04  5.17e-04  cuDeviceTotalMem\n",
      "                     0.10  2.59e-04       101  2.57e-06  7.33e-07  8.57e-05  cuDeviceGetAttribute\n",
      "                     0.06  1.58e-04         3  5.25e-05  4.71e-06  1.44e-04  cudaFree\n",
      "                     0.02  6.43e-05         3  2.14e-05  1.17e-05  2.74e-05  cudaMemcpy\n",
      "                     0.01  3.55e-05         1  3.55e-05  3.55e-05  3.55e-05  cudaLaunchKernel\n",
      "                     0.01  2.60e-05         1  2.60e-05  2.60e-05  2.60e-05  cuDeviceGetName\n",
      "                     0.00  1.05e-05         1  1.05e-05  1.05e-05  1.05e-05  cudaDeviceSynchronize\n",
      "                     0.00  9.46e-06         1  9.46e-06  9.46e-06  9.46e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  4.01e-06         3  1.34e-06  8.12e-07  2.04e-06  cuDeviceGetCount\n",
      "                     0.00  2.58e-06         2  1.29e-06  7.99e-07  1.78e-06  cuDeviceGet\n",
      "                     0.00  8.94e-07         1  8.94e-07  8.94e-07  8.94e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./vector_sum.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Las unidades en las que se reporta son s: second, ms: millisecond, us: microsecond, ns: nanosecond.\n",
    "\n",
    "* En la documentación de NVIDIA se menciona que `nvprof` será reemplazada próximamente por [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) y [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior se lanzaron $N$ bloques con $1$ *thread* cada uno y a continuación se lanza $1$ bloque con $N$ *threads*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "`dim3 dimGrid(1,1,1);` representa 1 bloque en el *grid*.\n",
    "\n",
    "`dim3 dimBlock(N,1,1);` representa N *threads* por bloque.\n",
    "\n",
    "`<<<dimGrid,dimBlock>>>` 1 bloque con N *threads*.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vector_sum_2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file vector_sum_2.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void vect_sum(int *a, int *b, int *c){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    if(thread_id_x<N) \n",
    "        c[thread_id_x] = a[thread_id_x]+b[thread_id_x];\n",
    "}\n",
    "int main(void){\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    dim3 dimGrid(1,1,1);\n",
    "    dim3 dimBlock(N,1,1);\n",
    "    //allocation in device with Unified Memory\n",
    "    cudaMallocManaged(&device_a, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_b, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_c, sizeof(int)*N);\n",
    "    //dummy data\n",
    "    for(i=0;i<N;i++){\n",
    "        device_a[i]=i;\n",
    "        device_b[i]=i*i;\n",
    "    }\n",
    "    vect_sum<<<dimGrid,dimBlock>>>(device_a,device_b,device_c); \n",
    "    cudaDeviceSynchronize();\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",device_a[i],device_b[i],device_c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall vector_sum_2.cu -o vector_sum_2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==17829== NVPROF is profiling process 17829, command: ./vector_sum_2.out\n",
      "==17829== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==17829== Profiling application: ./vector_sum_2.out\n",
      "==17829== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  5.89e-06         1  5.89e-06  5.89e-06  5.89e-06  vect_sum(int*, int*, int*)\n",
      "      API calls:    99.57  0.294598         3  0.098199  5.80e-06  0.294578  cudaMallocManaged\n",
      "                     0.18  5.23e-04         1  5.23e-04  5.23e-04  5.23e-04  cuDeviceTotalMem\n",
      "                     0.09  2.68e-04       101  2.65e-06  7.37e-07  9.11e-05  cuDeviceGetAttribute\n",
      "                     0.09  2.62e-04         1  2.62e-04  2.62e-04  2.62e-04  cudaLaunchKernel\n",
      "                     0.05  1.35e-04         3  4.50e-05  9.83e-06  1.01e-04  cudaFree\n",
      "                     0.01  3.49e-05         1  3.49e-05  3.49e-05  3.49e-05  cuDeviceGetName\n",
      "                     0.01  1.92e-05         1  1.92e-05  1.92e-05  1.92e-05  cudaDeviceSynchronize\n",
      "                     0.00  9.34e-06         1  9.34e-06  9.34e-06  9.34e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  3.96e-06         3  1.32e-06  7.76e-07  1.99e-06  cuDeviceGetCount\n",
      "                     0.00  2.94e-06         2  1.47e-06  9.12e-07  2.02e-06  cuDeviceGet\n",
      "                     0.00  9.58e-07         1  9.58e-07  9.58e-07  9.58e-07  cuDeviceGetUuid\n",
      "\n",
      "==17829== Unified Memory profiling result:\n",
      "Device \"NVIDIA Tesla K80 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       1  8.0000KB  8.0000KB  8.0000KB  8.000000KB  3.8720e-06s  Host To Device\n",
      "       5  25.600KB  4.0000KB  60.000KB  128.0000KB  2.7422e-05s  Device To Host\n",
      "Total CPU Page faults: 2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./vector_sum_2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* El programa anterior utiliza la [Unified Memory](https://devblogs.nvidia.com/unified-memory-cuda-beginners/) con la función [cudaMallocManaged](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1gcf6b9b1019e73c5bc2b39b39fe90816e). La *Unified Memory* es un *feature* que se añadió a CUDA desde las arquitecturas de **Kepler** y **Maxwell** pero que ha ido mejorando (por ejemplo añadiendo [page faulting](https://en.wikipedia.org/wiki/Page_fault) and [migration](https://www.kernel.org/doc/html/latest/vm/page_migration.html)) en las arquitecturas siguientes a la de *Kepler*: la arquitectura Pascal y Volta. Por esto en el *output* anterior de *nvprof* aparece una sección de *page fault*. \n",
    "\n",
    "* Al igual que antes, en el programa anterior se asume que $N$ el número de datos en el arreglo es menor al número de *threads* que es posible lanzar. Esto como veremos más adelante es importante considerar pues aunque en el *device* se pueden lanzar muchos bloques y muchos *threads*, se tienen límites en el número de éstos que es posible lanzar.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Tenemos que inicializar los datos en la CPU y copiarlos hacia la GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad no tenemos que realizarlo para el ejemplo de `vector_sum_3.cu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vector_sum_3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file vector_sum_3.cu\n",
    "#include<stdio.h>\n",
    "#define N 10\n",
    "__global__ void fill_arrays(int *a, int *b){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    a[thread_id_x]=thread_id_x;\n",
    "    b[thread_id_x]=thread_id_x*thread_id_x;\n",
    "}\n",
    "\n",
    "__global__ void vect_sum(int *a, int *b, int *c){\n",
    "    int thread_id_x = threadIdx.x;\n",
    "    if(thread_id_x<N)\n",
    "        c[thread_id_x] = a[thread_id_x]+b[thread_id_x];\n",
    "}\n",
    "\n",
    "int main(void){\n",
    "    int *device_a, *device_b, *device_c;\n",
    "    int i;\n",
    "    dim3 dimGrid(1,1,1);\n",
    "    dim3 dimBlock(N,1,1);\n",
    "    //allocating using Unified Memory in device\n",
    "    cudaMallocManaged(&device_a, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_b, sizeof(int)*N);\n",
    "    cudaMallocManaged(&device_c, sizeof(int)*N);\n",
    "    fill_arrays<<<dimGrid,dimBlock>>>(device_a,device_b);\n",
    "    cudaDeviceSynchronize();\n",
    "    vect_sum<<<dimGrid,dimBlock>>>(device_a,device_b,device_c);\n",
    "    cudaDeviceSynchronize();\n",
    "    for(i=0;i<N;i++)\n",
    "        printf(\"%d+%d = %d\\n\",device_a[i],device_b[i],device_c[i]);\n",
    "    cudaFree(device_a);\n",
    "    cudaFree(device_b);\n",
    "    cudaFree(device_c);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall vector_sum_3.cu -o vector_sum_3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+0 = 0\n",
      "1+1 = 2\n",
      "2+4 = 6\n",
      "3+9 = 12\n",
      "4+16 = 20\n",
      "5+25 = 30\n",
      "6+36 = 42\n",
      "7+49 = 56\n",
      "8+64 = 72\n",
      "9+81 = 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==17875== NVPROF is profiling process 17875, command: ./vector_sum_3.out\n",
      "==17875== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==17875== Profiling application: ./vector_sum_3.out\n",
      "==17875== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    50.47  5.12e-06         1  5.12e-06  5.12e-06  5.12e-06  fill_arrays(int*, int*)\n",
      "                    49.53  5.02e-06         1  5.02e-06  5.02e-06  5.02e-06  vect_sum(int*, int*, int*)\n",
      "      API calls:    99.55  0.293179         3  0.097726  6.68e-06  0.293161  cudaMallocManaged\n",
      "                     0.18  5.16e-04         1  5.16e-04  5.16e-04  5.16e-04  cuDeviceTotalMem\n",
      "                     0.12  3.40e-04         2  1.70e-04  1.54e-04  1.86e-04  cudaLaunchKernel\n",
      "                     0.09  2.61e-04       101  2.59e-06  7.38e-07  8.75e-05  cuDeviceGetAttribute\n",
      "                     0.05  1.36e-04         3  4.53e-05  9.37e-06  1.02e-04  cudaFree\n",
      "                     0.01  3.53e-05         1  3.53e-05  3.53e-05  3.53e-05  cuDeviceGetName\n",
      "                     0.01  3.11e-05         2  1.55e-05  1.39e-05  1.71e-05  cudaDeviceSynchronize\n",
      "                     0.00  8.34e-06         1  8.34e-06  8.34e-06  8.34e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  4.03e-06         3  1.34e-06  7.84e-07  2.03e-06  cuDeviceGetCount\n",
      "                     0.00  2.78e-06         2  1.39e-06  9.01e-07  1.88e-06  cuDeviceGet\n",
      "                     0.00  8.84e-07         1  8.84e-07  8.84e-07  8.84e-07  cuDeviceGetUuid\n",
      "\n",
      "==17875== Unified Memory profiling result:\n",
      "Device \"NVIDIA Tesla K80 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "       3  21.333KB  4.0000KB  52.000KB  64.00000KB  1.4625e-05s  Device To Host\n",
      "Total CPU Page faults: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./vector_sum_3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de una GPU y límites en número de *threads* y bloques que podemos lanzar en el *kernel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *device* está compuesto por arreglos de **streaming multiprocessors SM's** (también denotados como MP's) y en cada *SM* encontramos un número (determinado por la arquitectura del device) de **streaming processors SP's** que comparten el caché y unidades de control (que están dentro de cada SM):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dl.dropboxusercontent.com/s/oxx55upoayfmliw/SMS_CUDA.png?dl=0\" heigth=\"700\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver [Hardware model: streamingmultiprocessor](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dibujo anterior se muestran las SM's en color rojo y los SP's en morado. Hay dos SM's por cada bloque anaranjado y ocho SP's por cada SM. Así, una GPU es una máquina *multicore*. Aunque cada SM ejecuta las instrucciones de forma independiente a otra SM, comparten la **memoria global**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los bloques de *threads* son **asignados a cada SM por el *CUDA runtime system***, el cual puede asignar más de un bloque a una SM pero hay un límite de bloques que pueden ser asignados a cada SM. Ver [maximum number of blocks per multiprocessor](https://stackoverflow.com/questions/22520209/programmatically-retrieve-maximum-number-of-blocks-per-multiprocessor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Por ejemplo para el modelo *GT200* el máximo número de bloques que podían asignarse a cada SM eran de $8$ bloques. Tal modelo tenía $30$ SM's lo que resultaban en $240$ bloques que en un instante podían asignarse al *device* para su ejecución simultánea (asignándose en cualquier orden en alguna SM disponible). Por supuesto que un *grid* podía contener más de $240$ bloques en este modelo y en este caso el *CUDA runtime system* lleva una lista de bloques que va asignando a cada SM y conforme cada SM terminan la ejecución, nuevos bloques son asignados a tales SM que finalizaron. Para visualizar esta situación, considérese una simplificación de lo anterior en donde se tiene un *device* con $2$ SM's y con un *kernel* se han lanzado $6$ bloques. El *CUDA runtime system* ha asignado $3$ bloques a cada SM, entonces se tiene un dibujo como el siguiente:\n",
    "\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/p0nu72ofmdjtck8/kernel_launch_example.png?dl=0\" heigth=\"600\" width=\"600\">\n",
    "\n",
    "\n",
    "* Los bloques asignados a una SM comparten recursos (por ejemplo memoria) y su ejecución es independiente entre ellos, no es posible sincronizar al bloque 1 con el bloque 0. También no es posible sincronizar a los *threads* de diferentes SM's pero sí es posible sincronizar a los *threads* dentro de un mismo bloque.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué otros límites puedo encontrar en mi(s) device(s) de mi sistema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder lo anterior se puede utilizar el siguiente programa que está basado en [how-query-device-properties-and-handle-errors-cuda-cc](https://devblogs.nvidia.com/how-query-device-properties-and-handle-errors-cuda-cc/) y [cudaDeviceProp Struct Reference](https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing device_properties.cu\n"
     ]
    }
   ],
   "source": [
    "%%file device_properties.cu\n",
    "\n",
    "#include<stdio.h>\n",
    "\n",
    "int main(void){\n",
    "    cudaDeviceProp properties;\n",
    "    int count;\n",
    "    int i;\n",
    "    cudaGetDeviceCount(&count);\n",
    "    for(i=0;i<count;i++){\n",
    "        printf(\"----------------------\\n\");\n",
    "        cudaGetDeviceProperties(&properties, i);\n",
    "        printf(\"----device %d ----\\n\",i); \n",
    "        printf(\"Device Name: %s\\n\", properties.name);\n",
    "        printf(\"Compute capability: %d.%d\\n\", properties.major, properties.minor);\n",
    "        printf(\"Clock rate: %d\\n\", properties.clockRate);\n",
    "        printf(\"Unified memory: %d\\n\", properties.unifiedAddressing);\n",
    "        printf(\" ---Memory Information for device %d (results on bytes)---\\n\", i);\n",
    "        printf(\"Total global mem: %ld\\n\", properties.totalGlobalMem); \n",
    "        printf(\"Total constant Mem: %ld\\n\", properties.totalConstMem);\n",
    "        printf(\"Shared memory per thread block: %ld\\n\", properties.sharedMemPerBlock);\n",
    "        printf(\"Shared memory per SM: %ld\\n\",properties.sharedMemPerMultiprocessor );\n",
    "        printf(\" ---MP Information for device %d ---\\n\", i);\n",
    "        printf(\"SM count: %d\\n\", properties.multiProcessorCount);\n",
    "        printf(\"Threads in warp: %d\\n\", properties.warpSize);\n",
    "        printf(\"Max threads per SM: %d\\n\", properties.maxThreadsPerMultiProcessor);\n",
    "        printf(\"Max warps per SM: %d\\n\",properties.maxThreadsPerMultiProcessor/properties.warpSize);\n",
    "        printf(\"Max threads per block: %d\\n\", properties.maxThreadsPerBlock);\n",
    "        printf(\"Max thread dimensions: (%d, %d, %d)\\n\", properties.maxThreadsDim[0], properties.maxThreadsDim[1], properties.maxThreadsDim[2]);\n",
    "        printf(\"Max grid dimensions: (%d, %d, %d)\\n\", properties.maxGridSize[0], properties.maxGridSize[1], properties.maxGridSize[2]); \n",
    "    }\n",
    "    return 0;\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc --compiler-options -Wall device_properties.cu -o device_properties.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "----device 0 ----\n",
      "Device Name: NVIDIA Tesla K80\n",
      "Compute capability: 3.7\n",
      "Clock rate: 823500\n",
      "Unified memory: 1\n",
      " ---Memory Information for device 0 (results on bytes)---\n",
      "Total global mem: 11996954624\n",
      "Total constant Mem: 65536\n",
      "Shared memory per thread block: 49152\n",
      "Shared memory per SM: 114688\n",
      " ---MP Information for device 0 ---\n",
      "SM count: 13\n",
      "Threads in warp: 32\n",
      "Max threads per SM: 2048\n",
      "Max warps per SM: 64\n",
      "Max threads per block: 1024\n",
      "Max thread dimensions: (1024, 1024, 64)\n",
      "Max grid dimensions: (2147483647, 65535, 65535)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./device_properties.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* También en la documentación oficial de NVIDIA dentro de [compute-capabilities](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) se pueden revisar los valores anteriores y muchos más.\n",
    "\n",
    "* En un *device* encontramos diferentes tipos de memoria: global, constante, *shared* y *texture*. En esta nota únicamente trabajamos con la memoria global. \n",
    "\n",
    "* Tenemos funciones en CUDA para poder comunicar/coordinar a los *threads* en un bloque por medio de la *shared memory*. Ver por ejemplo [Using Shared Memory in CUDA C/C++](https://devblogs.nvidia.com/using-shared-memory-cuda-cc/) para un pequeño *post* del $2013$ sobre *shared memory*.\n",
    "\n",
    "* Los bloques de *threads* que son asignados a una SM son divididos en ***warps*** que es la unidad de ***thread scheduling*** que tiene el *CUDA run time system*. El *output* anterior indica que son divisiones de $32$ *threads*.\n",
    "\n",
    "* El *thread scheduling* se puede pensar a la funcionalidad que tiene el *hardware* del *device* para seleccionar una instrucción del programa y asginar su ejecución por los *threads* en un *warp* ([SIMT](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)). Otro ejemplo es tener una instrucción que indica que se debe realizar lectura o escritura, entonces el *hardware* del *device* utiliza un *warp* de threads para tal operación mientras selecciona un *warp* de *threads* distinto para seleccionar otra instrucción diferente a la de I/O.\n",
    "\n",
    "* El número máximo de *threads* que pueden iniciarse de forma simultánea o en un instante por SM es de $2048$ o bien $2048/32 = 64$ warps.\n",
    "\n",
    "* El *output* anterior muestra los límites para número de bloques en las tres dimensiones de un *grid* y el número de *threads* en las tres dimensiones en un bloque.\n",
    "\n",
    "* Un bloque puede tener como máximo $1024$ *threads* en cualquier configuración: por ejemplo $(1024,1,1), (32,1,32), (4,4,64)$.\n",
    "\n",
    "* Por los puntos anteriores si lanzamos bloques de $1024$ *threads* entonces sólo $2$ bloques pueden residir en una SM en un instante. Con esta configuración alcanzaríamos $1024/32=32$ *warps* por cada bloque y como lanzamos $2$ bloques alcanzaríamos $64$ *warps* (que es el máximo de *warps* por SM que podemos tener en un instante). Otra configuración para alcanzar el máximo número de *warps* en un instante, es considerar $4$ bloques de $512$ *threads* pues tendríamos $512/32=16$ *warps* por bloque y en total serían $16*4$ (*warps* $\\times$ bloques) $=64$ *warps*. Entre los datos que hay que elegir en los programas de *CUDA C* se encuentran las configuraciones en el número de *threads* y el número de bloques a lanzar. La idea es alcanzar o rebasar el máximo número de *warps* en cada SM que soporta nuestro *device* en un instante.\n",
    "\n",
    "* Por ejemplo para el dibujo en el que se asumió que el *CUDA runtime system* había asignado $3$ bloques a cada SM, se tendría una división de cada bloque en un *warp* de $32$ *threads* como sigue:\n",
    "\n",
    "\n",
    "<img src=\"https://dl.dropboxusercontent.com/s/yngq4r66i2nk5mg/warp_division.png?dl=0\" heigth=\"600\" width=\"600\">\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Grid Configuration Choices*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los programas de *CUDA C* tienen la opción de elegir el número de *threads* y de *bloques* a ser lanzados. En la referencia *Parallel Computing for Data Science. With Examples in R, C++ and CUDA* de N. Matloff se enlistan algunas consideraciones para elegir tales parámetros:\n",
    "\n",
    "* *Given that scheduling is done on a warp basis, block size should be a multiple of the warp size (32).*\n",
    "\n",
    "* *One wants to utilize all the SMs. If one sets the block size too large, not all will be used, as a block cannot be split across SM's.*\n",
    "\n",
    "* *..., barrier synchronization can be done effectively only at the block level. The larger the block, the more the barrier delay, so one might want smaller blocks.*\n",
    "\n",
    "* *On the other hand, if one is using shared memory, this can only be done at the block level, and efficient use may indicate using a larger block.*\n",
    "\n",
    "* *Two threads doing unrelated work, or the same work but with many if/elses, would cause a lot of thread divergence if they were in the same block. In some cases, it may be known in advance which threads will do the \"ifs\" and which will do the \"elses\", in which case they should be placed in different blocks if possible.*\n",
    "\n",
    "* *A commonly-cited rule of thumb is to have between $128$ and $256$ *threads* per block.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo regla compuesta del rectángulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el uso de CUDA se recomienda que:\n",
    "\n",
    "* *Users* escriban código de *CUDA C* simple.\n",
    "\n",
    "* Utilicen las librerías ya hechas por NVIDIA o terceros para mantener simplicidad y eficiencia en el código.\n",
    "\n",
    "Lo anterior para disminuir el tiempo y la cantidad de código que *users* tengan que hacer (o rehacer) y puesto que dominar la programación de *CUDA C* requiere una buena inversión de tiempo.\n",
    "\n",
    "Así, tenemos a [Thrust](https://docs.nvidia.com/cuda/thrust/index.html) una *template library* basada en la [Standard Template Library (STL)](https://en.wikipedia.org/wiki/Standard_Template_Library) de C++ construída por NVIDIA que de acuerdo a su documentación: \n",
    "\n",
    "*Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically. As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.* \n",
    "\n",
    "\n",
    "*Thrust* tiene la opción de utilizarse con [OpenMP](https://www.openmp.org/), [Thread Building Blocks (TBB)](https://www.threadingbuildingblocks.org/intel-tbb-tutorial) y con *CUDA C++*. Ver por ejemplo [Device Backends](https://github.com/thrust/thrust/wiki/Device-Backends) para conocer cómo cambiar entre *OpenMP* y *CUDA C++*, lo cual se realiza en la compilación y **¡sin hacer cambios en el código!**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Comentarios\n",
    "\n",
    "* Al *software* que aprovecha el *feature* anterior de los sistemas computacionales (por ejemplo cambiar entre *OpenMP* y *CUDA C++*) se les nombra [Heterogeneous computing](https://en.wikipedia.org/wiki/Heterogeneous_computing).\n",
    "\n",
    "* Si se instala el *CUDA toolkit*, los *headers* en la librería template de `Thrust` estarán disponibles para su uso.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo de la regla del rectángulo compuesta se utiliza:\n",
    "\n",
    " * [Reductions](https://docs.nvidia.com/cuda/thrust/index.html#reductions)\n",
    " \n",
    " * Los *headers*: \n",
    " \n",
    "    * [thrust/execution_policy](https://thrust.github.io/doc/structthrust_1_1device__execution__policy.html),\n",
    "    \n",
    "    * [thhrust/reduce](https://thrust.github.io/doc/group__reductions_ga43eea9a000f912716189687306884fc7.html#ga43eea9a000f912716189687306884fc7).\n",
    " \n",
    "\n",
    "Se hace explícito el uso de la política de ejecucion [thrust::device](https://thrust.github.io/doc/group__execution__policies_ga78249cb3aa4239b64e65aaf6e82ac2f8.html).\n",
    "\n",
    "Referencias para el programa siguiente se encuentran en [thrust inside user written kernels](https://stackoverflow.com/questions/5510715/thrust-inside-user-written-kernels) y [cuda how to sum all elements of an array into one number within the gpu](https://stackoverflow.com/questions/42525713/cuda-how-to-sum-all-elements-of-an-array-into-one-number-within-the-gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primero utilicemos $n=10^3$ subintervalos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    if(threadIdx.x<=n-1){\n",
    "        x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "        data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n=1e3; \n",
    "    double obj=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<1,n>>>(d_data, a,h_hat,n,d_sum);\n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf.cu -o Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
      "Error relativo de la solución: 4.104931878976858e-08\n",
      "Tiempo de cálculo en la gpu 0.00014\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241634690490e-01\n",
      "Error relativo de la solución: 4.104931878976858e-08\n",
      "Tiempo de cálculo en la gpu 0.00019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==18211== NVPROF is profiling process 18211, command: ./Rcf.out\n",
      "==18211== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==18211== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
      "==18211== Profiling application: ./Rcf.out\n",
      "==18211== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    97.53  1.01e-04         1  1.01e-04  1.01e-04  1.01e-04  Rcf(double*, double, double, int, double*)\n",
      "                     2.47  2.56e-06         1  2.56e-06  2.56e-06  2.56e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.58  0.285237         2  0.142618  6.00e-06  0.285231  cudaMalloc\n",
      "                     0.18  5.22e-04         1  5.22e-04  5.22e-04  5.22e-04  cuDeviceTotalMem\n",
      "                     0.10  2.84e-04       101  2.81e-06  7.33e-07  9.81e-05  cuDeviceGetAttribute\n",
      "                     0.05  1.48e-04         2  7.41e-05  9.64e-06  1.38e-04  cudaFree\n",
      "                     0.04  1.08e-04         1  1.08e-04  1.08e-04  1.08e-04  cudaDeviceSynchronize\n",
      "                     0.02  6.53e-05         1  6.53e-05  6.53e-05  6.53e-05  cudaLaunchKernel\n",
      "                     0.01  3.28e-05         1  3.28e-05  3.28e-05  3.28e-05  cudaMemcpy\n",
      "                     0.01  3.16e-05         1  3.16e-05  3.16e-05  3.16e-05  cuDeviceGetName\n",
      "                     0.00  9.56e-06         1  9.56e-06  9.56e-06  9.56e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  4.41e-06         3  1.47e-06  7.78e-07  2.30e-06  cuDeviceGetCount\n",
      "                     0.00  2.58e-06         2  1.29e-06  8.15e-07  1.76e-06  cuDeviceGet\n",
      "                     0.00  9.06e-07         1  9.06e-07  9.06e-07  9.06e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./Rcf.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incrementemos a $n=1025$ subintervalos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf2.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf2.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    if(threadIdx.x<=n-1){\n",
    "        x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "        data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n=1025; \n",
    "    double obj=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<1,n>>>(d_data, a,h_hat,n,d_sum);\n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf2.cu -o Rcf2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 0.000000000000000e+00\n",
      "Error relativo de la solución: 1.000000000000000e+00\n",
      "Tiempo de cálculo en la gpu 0.00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf2.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Obsérvese error relativo de $100\\%$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo lo arreglamos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf3.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf3.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int stride=0;\n",
    "    if(threadIdx.x<=n-1){\n",
    "        x=a+(threadIdx.x+1/2.0)*h_hat;\n",
    "        data[threadIdx.x]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        stride=blockDim.x;\n",
    "        x=a+(threadIdx.x+stride+1/2.0)*h_hat;\n",
    "        data[threadIdx.x+stride]=std::exp(-std::pow(x,2));\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_blocks=2;\n",
    "    int n=1025;\n",
    "    double obj=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<n_blocks,n_threads_per_block>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf3.cu -o Rcf3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241619918411e-01\n",
      "Error relativo de la solución: 3.907133247860604e-08\n",
      "Tiempo de cálculo en la gpu 0.00015\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero en la propuesta anterior lanzamos $2*1024$ (bloques $\\times$ número de *threads*) $=2048$ *threads* y sólo ocupamos $1025$ *threads*. Entonces podemos cambiar el código anterior para aprovechar los $2048$ *threads* como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf4.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf4.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int stride=0;\n",
    "    int i;\n",
    "    stride=blockDim.x;\n",
    "    for(i=threadIdx.x;i<=n-1;i+=stride){\n",
    "        if(i<=n-1){\n",
    "            x=a+(i+1/2.0)*h_hat;\n",
    "            data[i]=std::exp(-std::pow(x,2));\n",
    "        }\n",
    "    }\n",
    "    if(threadIdx.x==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_blocks=2;\n",
    "    int n=n_threads_per_block*n_blocks;\n",
    "    double obj=0.7468241328124271;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin=clock();\n",
    "    Rcf<<<n_blocks,n_threads_per_block>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end=clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf4.cu -o Rcf4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241401215338e-01\n",
      "Error relativo de la solución: 9.786918140590463e-09\n",
      "Tiempo de cálculo en la gpu 0.00024\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf4.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y podemos no utilizar el ciclo *for***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Para una visualización sobre la construcción del índice en el kernel utilizando `blockDim.x*blockIdx.x + threadIdx.x` ver [An Even Easier Introduction to CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda/).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf5.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf5.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(idx==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_blocks=2;\n",
    "    double obj=0.7468241328124271;\n",
    "    int n=n_blocks*n_threads_per_block;//number of subintervals\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<n_blocks,n_threads_per_block>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf5.cu -o Rcf5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241401215338e-01\n",
      "Error relativo de la solución: 9.786918140590463e-09\n",
      "Tiempo de cálculo en la gpu 0.00024\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf5.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de 0.000000 a 1.000000 = 7.468241401215338e-01\n",
      "Error relativo de la solución: 9.786918140590463e-09\n",
      "Tiempo de cálculo en la gpu 0.00028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==18401== NVPROF is profiling process 18401, command: ./Rcf5.out\n",
      "==18401== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==18401== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
      "==18401== Profiling application: ./Rcf5.out\n",
      "==18401== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:    98.70  1.95e-04         1  1.95e-04  1.95e-04  1.95e-04  Rcf(double*, double, double, int, double*)\n",
      "                     1.30  2.56e-06         1  2.56e-06  2.56e-06  2.56e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    99.51  0.258055         2  0.129027  4.82e-06  0.258050  cudaMalloc\n",
      "                     0.20  5.14e-04         1  5.14e-04  5.14e-04  5.14e-04  cuDeviceTotalMem\n",
      "                     0.10  2.58e-04       101  2.56e-06  7.36e-07  8.51e-05  cuDeviceGetAttribute\n",
      "                     0.08  2.00e-04         1  2.00e-04  2.00e-04  2.00e-04  cudaDeviceSynchronize\n",
      "                     0.06  1.49e-04         2  7.45e-05  9.64e-06  1.39e-04  cudaFree\n",
      "                     0.03  6.99e-05         1  6.99e-05  6.99e-05  6.99e-05  cudaLaunchKernel\n",
      "                     0.01  3.26e-05         1  3.26e-05  3.26e-05  3.26e-05  cudaMemcpy\n",
      "                     0.01  2.71e-05         1  2.71e-05  2.71e-05  2.71e-05  cuDeviceGetName\n",
      "                     0.00  9.56e-06         1  9.56e-06  9.56e-06  9.56e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  4.23e-06         3  1.41e-06  9.18e-07  2.18e-06  cuDeviceGetCount\n",
      "                     0.00  2.52e-06         2  1.26e-06  8.01e-07  1.72e-06  cuDeviceGet\n",
      "                     0.00  9.07e-07         1  9.07e-07  9.07e-07  9.07e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./Rcf5.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilicemos más nodos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el siguiente código, incrementamos el número de bloques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf6.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf6.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(idx==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    int n_blocks=0;\n",
    "    double obj=0.7468241328124271;\n",
    "    int n=0;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;\n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    //we choose a multiple of the number of SMs.\n",
    "    n_blocks = 256 * properties.multiProcessorCount;\n",
    "    n = n_blocks*n_threads_per_block;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<n_blocks,n_threads_per_block>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Número de subintervalos: %d\\n\", n);\n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf6.cu -o Rcf6.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Mientras se ejecuta la siguiente celda se sugiere en la terminal ejecutar en la línea de comando `nvtop`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subintervalos: 3407872\n",
      "Integral de 0.000000 a 1.000000 = 7.468241328124654e-01\n",
      "Error relativo de la solución: 5.128743524305478e-14\n",
      "Tiempo de cálculo en la gpu 0.42854\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf6.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subintervalos: 3407872\n",
      "Integral de 0.000000 a 1.000000 = 7.468241328124654e-01\n",
      "Error relativo de la solución: 5.128743524305478e-14\n",
      "Tiempo de cálculo en la gpu 0.38611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==18515== NVPROF is profiling process 18515, command: ./Rcf6.out\n",
      "==18515== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==18515== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
      "==18515== Profiling application: ./Rcf6.out\n",
      "==18515== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  0.386719         1  0.386719  0.386719  0.386719  Rcf(double*, double, double, int, double*)\n",
      "                     0.00  3.23e-06         1  3.23e-06  3.23e-06  3.23e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    58.94  0.386743         1  0.386743  0.386743  0.386743  cudaDeviceSynchronize\n",
      "                    40.45  0.265420         2  0.132710  1.41e-04  0.265279  cudaMalloc\n",
      "                     0.43  2.85e-03         2  1.43e-03  2.51e-04  2.60e-03  cudaFree\n",
      "                     0.08  5.46e-04         1  5.46e-04  5.46e-04  5.46e-04  cuDeviceTotalMem\n",
      "                     0.04  2.65e-04       101  2.62e-06  7.41e-07  8.77e-05  cuDeviceGetAttribute\n",
      "                     0.03  1.72e-04         1  1.72e-04  1.72e-04  1.72e-04  cudaGetDeviceProperties\n",
      "                     0.01  7.08e-05         1  7.08e-05  7.08e-05  7.08e-05  cudaMemcpy\n",
      "                     0.01  6.56e-05         1  6.56e-05  6.56e-05  6.56e-05  cudaLaunchKernel\n",
      "                     0.00  2.80e-05         1  2.80e-05  2.80e-05  2.80e-05  cuDeviceGetName\n",
      "                     0.00  8.45e-06         1  8.45e-06  8.45e-06  8.45e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  4.07e-06         3  1.36e-06  7.75e-07  2.08e-06  cuDeviceGetCount\n",
      "                     0.00  2.56e-06         2  1.28e-06  8.00e-07  1.76e-06  cuDeviceGet\n",
      "                     0.00  9.30e-07         1  9.30e-07  9.30e-07  9.30e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./Rcf6.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incrementamos el número de subintervalos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf7.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf7.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if(idx<=n-1){\n",
    "        x=a+(idx+1/2.0)*h_hat;\n",
    "        data[idx]=std::exp(-std::pow(x,2));\n",
    "    }\n",
    "    if(idx==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=512; \n",
    "    int n_blocks=0;\n",
    "    double obj=0.7468241328124271;\n",
    "    int n=0;\n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;\n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_blocks = 1500 * properties.multiProcessorCount;\n",
    "    n = n_blocks*n_threads_per_block;\n",
    "    cudaMalloc((void **)&d_data,sizeof(double)*n);\n",
    "    cudaMalloc((void**)&d_sum,sizeof(double));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<n_blocks,n_threads_per_block>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost);\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data) ;\n",
    "    cudaFree(d_sum) ;\n",
    "    printf(\"Número de subintervalos: %d\\n\", n);    \n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf7.cu -o Rcf7.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Mientras se ejecuta la siguiente celda se sugiere en la terminal ejecutar en la línea de comando `nvtop`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subintervalos: 9984000\n",
      "Integral de 0.000000 a 1.000000 = 7.468241328124303e-01\n",
      "Error relativo de la solución: 4.311117745068373e-15\n",
      "Tiempo de cálculo en la gpu 1.10993\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf7.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subintervalos: 9984000\n",
      "Integral de 0.000000 a 1.000000 = 7.468241328124303e-01\n",
      "Error relativo de la solución: 4.311117745068373e-15\n",
      "Tiempo de cálculo en la gpu 1.12291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==18755== NVPROF is profiling process 18755, command: ./Rcf7.out\n",
      "==18755== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
      "==18755== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
      "==18755== Profiling application: ./Rcf7.out\n",
      "==18755== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      "                        %         s                   s         s         s\n",
      " GPU activities:   100.00  1.121829         1  1.121829  1.121829  1.121829  Rcf(double*, double, double, int, double*)\n",
      "                     0.00  2.24e-06         1  2.24e-06  2.24e-06  2.24e-06  [CUDA memcpy DtoH]\n",
      "      API calls:    80.22  1.121856         1  1.121856  1.121856  1.121856  cudaDeviceSynchronize\n",
      "                    19.21  0.268620         2  0.134310  1.46e-04  0.268474  cudaMalloc\n",
      "                     0.49  6.86e-03         2  3.43e-03  2.98e-04  6.56e-03  cudaFree\n",
      "                     0.04  5.16e-04         1  5.16e-04  5.16e-04  5.16e-04  cuDeviceTotalMem\n",
      "                     0.02  2.75e-04       101  2.72e-06  7.35e-07  9.35e-05  cuDeviceGetAttribute\n",
      "                     0.01  1.89e-04         1  1.89e-04  1.89e-04  1.89e-04  cudaGetDeviceProperties\n",
      "                     0.01  8.55e-05         1  8.55e-05  8.55e-05  8.55e-05  cudaLaunchKernel\n",
      "                     0.00  6.15e-05         1  6.15e-05  6.15e-05  6.15e-05  cudaMemcpy\n",
      "                     0.00  3.98e-05         1  3.98e-05  3.98e-05  3.98e-05  cuDeviceGetName\n",
      "                     0.00  9.34e-06         1  9.34e-06  9.34e-06  9.34e-06  cuDeviceGetPCIBusId\n",
      "                     0.00  3.96e-06         3  1.32e-06  7.68e-07  1.83e-06  cuDeviceGetCount\n",
      "                     0.00  2.41e-06         2  1.20e-06  7.43e-07  1.67e-06  cuDeviceGet\n",
      "                     0.00  9.00e-07         1  9.00e-07  9.00e-07  9.00e-07  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvprof --normalized-time-unit s ./Rcf7.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incrementamos el número de subintervalos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(RCF8CU)="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rcf8.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf8.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf8.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, long int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    int num_threads=gridDim.x * blockDim.x;\n",
    "    int stride = num_threads;\n",
    "    int i;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    for(i=idx; i<=n-1; i+=stride){\n",
    "        if(idx<=n-1){\n",
    "            x=a+(idx+1/2.0)*h_hat;\n",
    "            data[idx]=std::exp(-std::pow(x,2));\n",
    "        }\n",
    "}\n",
    "    \n",
    "    if(idx==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "cudaError_t check_error(cudaError_t result) {\n",
    "    if (result != cudaSuccess) {\n",
    "        fprintf(stderr, \"Error: %s\\n\", cudaGetErrorString(result));\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    long int n_blocks=0; \n",
    "    double obj=0.7468241328124271;\n",
    "    long int n=0; \n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;    \n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_blocks = 100000 * properties.multiProcessorCount;\n",
    "    n = n_blocks*n_threads_per_block;\n",
    "    dim3 dimGrid(n_blocks,1,1);\n",
    "    dim3 dimBlock(n_threads_per_block,1,1);\n",
    "    check_error(cudaMalloc((void **)&d_data,sizeof(double)*n));\n",
    "    check_error(cudaMalloc((void**)&d_sum,sizeof(double)));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<dimGrid,dimBlock>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    check_error(cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost));\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_sum);\n",
    "    printf(\"Número de subintervalos: %ld\\n\", n);    \n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf8.cu -o Rcf8.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subintervalos: 1331200000\n",
      "Integral de 0.000000 a 1.000000 = 7.468241328124491e-01\n",
      "Error relativo de la solución: 2.943452805253579e-14\n",
      "Tiempo de cálculo en la gpu 130.57445\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf8.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "En la programación con CUDA-C es importante checar posibles errores de alojamiento de memoria. Una forma es con los tipos [cudaError_t](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gf599e5b8b829ce7db0f5216928f6ecb6) y `cudaSuccess` . Ver [why-do-i-have-insufficient-buffer-space-when-i-put-allocation-code-in-a-functi](https://stackoverflow.com/questions/58902166/why-do-i-have-insufficient-buffer-space-when-i-put-allocation-code-in-a-functi).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incrementamos el número de subintervalos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Rcf9.cu\n"
     ]
    }
   ],
   "source": [
    "%%file Rcf9.cu\n",
    "#include<stdio.h>\n",
    "#include <thrust/reduce.h>\n",
    "#include <thrust/execution_policy.h>\n",
    "\n",
    "__global__ void Rcf(double *data, double a, double h_hat, long int n, double *sum_res ) {\n",
    "    /*\n",
    "    Compute numerical approximation using rectangle or mid-point method in \n",
    "    an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "        data (double): array that will hold values evaluated in function\n",
    "        a (int): left point of interval\n",
    "        h_hat (double): width of subinterval    \n",
    "        n (int): number of subintervals\n",
    "        sum_res (double): pointer to result    \n",
    "    Returns:\n",
    "        sum_res (double): pointer to result\n",
    "    */\n",
    "    double x=0.0;\n",
    "    int idx;\n",
    "    int num_threads=gridDim.x * blockDim.x;\n",
    "    int stride = num_threads;\n",
    "    int i;\n",
    "    idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    for(i=idx; i<=n-1; i+=stride){\n",
    "        if(idx<=n-1){\n",
    "            x=a+(idx+1/2.0)*h_hat;\n",
    "            data[idx]=std::exp(-std::pow(x,2));\n",
    "        }\n",
    "}\n",
    "    \n",
    "    if(idx==0){\n",
    "        *sum_res = thrust::reduce(thrust::device, data , data + n, (double)0, thrust::plus<double>());\n",
    "    }\n",
    "}\n",
    "\n",
    "cudaError_t check_error(cudaError_t result) {\n",
    "    if (result != cudaSuccess) {\n",
    "        fprintf(stderr, \"Error: %s\\n\", cudaGetErrorString(result));\n",
    "    }\n",
    "    return result;\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "    double sum_res=0.0;\n",
    "    double *d_data;\n",
    "    double *d_sum;\n",
    "    double a=0.0, b=1.0;\n",
    "    double h_hat;\n",
    "    int n_threads_per_block=1024; \n",
    "    long int n_blocks=0; \n",
    "    double obj=0.7468241328124271;\n",
    "    long int n=0; \n",
    "    double time_spent;\n",
    "    clock_t begin,end;\n",
    "    cudaDeviceProp properties;    \n",
    "    cudaGetDeviceProperties(&properties, 0);\n",
    "    n_blocks = 150000 * properties.multiProcessorCount;\n",
    "    n = n_blocks*n_threads_per_block;\n",
    "    dim3 dimGrid(n_blocks,1,1);\n",
    "    dim3 dimBlock(n_threads_per_block,1,1);\n",
    "    check_error(cudaMalloc((void **)&d_data,sizeof(double)*n));\n",
    "    check_error(cudaMalloc((void**)&d_sum,sizeof(double)));\n",
    "    h_hat=(b-a)/n;\n",
    "    begin = clock();\n",
    "    Rcf<<<dimGrid,dimBlock>>>(d_data, a,h_hat,n,d_sum); \n",
    "    cudaDeviceSynchronize();\n",
    "    end = clock();\n",
    "    time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
    "    check_error(cudaMemcpy(&sum_res, d_sum, sizeof(double), cudaMemcpyDeviceToHost));\n",
    "    sum_res=h_hat*sum_res;\n",
    "    cudaFree(d_data);\n",
    "    cudaFree(d_sum);\n",
    "    printf(\"Número de subintervalos: %ld\\n\", n);    \n",
    "    printf(\"Integral de %f a %f = %1.15e\\n\", a,b,sum_res);\n",
    "    printf(\"Error relativo de la solución: %1.15e\\n\", fabs(sum_res-obj)/fabs(obj));\n",
    "    printf(\"Tiempo de cálculo en la gpu %.5f\\n\", time_spent);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.profile\n",
    "nvcc -gencode arch=compute_37,code=sm_37 --compiler-options -Wall Rcf9.cu -o Rcf9.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de subintervalos: 1996800000\n",
      "Integral de 0.000000 a 1.000000 = 0.000000000000000e+00\n",
      "Error relativo de la solución: 1.000000000000000e+00\n",
      "Tiempo de cálculo en la gpu 0.06584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: out of memory\n",
      "Error: an illegal memory access was encountered\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./Rcf9.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Ejercicio\n",
    ":class: tip\n",
    "\n",
    "Implementar la regla de Simpson compuesta con *CUDA-C* en una máquina de AWS con las mismas características que la que se presenta en esta nota y medir tiempo de ejecución.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CuPy](https://github.com/cupy/cupy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NumPy-like API accelerated with CUDA. CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA. CuPy consists of the core multi-dimensional array class, cupy.ndarray, and many functions on it. It supports a subset of numpy.ndarray interface.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "\n",
    "Ver [Basics of CuPy](https://docs-cupy.chainer.org/en/stable/tutorial/basic.html).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un subconjunto de funciones del paquete *NumPy* de *Python* están implementadas en *CuPy* vía la clase [cupy.ndarray](https://docs-cupy.chainer.org/en/stable/reference/generated/cupy.ndarray.html#cupy.ndarray) la cual es compatible en la GPU con la clase [numpy.ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) que utiliza la CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Arrays*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el *array* $1$-dimensional anterior está alojado en la GPU. \n",
    "\n",
    "Podemos obtener información del *array* anterior utilizando algunos métodos y atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_gpu.ndim: 1\n",
      "x_gpu.shape: (3,)\n",
      "x_gpu.size: 3\n",
      "x_gpu.dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('x_gpu.ndim:',x_gpu.ndim)\n",
    "print('x_gpu.shape:',x_gpu.shape)\n",
    "print('x_gpu.size:',x_gpu.size)\n",
    "print('x_gpu.dtype:',x_gpu.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accedemos con corchetes a sus componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primer elemento 1\n",
      "último elemento 3\n",
      "segundo elemento 2\n",
      "penúltimo elemento 2\n",
      "del primero al 2º elemento incluyendo este último [1 2]\n",
      "del 2º al último elemento sin incluir el 2º [3]\n"
     ]
    }
   ],
   "source": [
    "print('primer elemento', x_gpu[0])\n",
    "print('último elemento', x_gpu[-1])\n",
    "print('segundo elemento', x_gpu[1])\n",
    "print('penúltimo elemento', x_gpu[-2])\n",
    "print('del primero al 2º elemento incluyendo este último', x_gpu[:2])\n",
    "print('del 2º al último elemento sin incluir el 2º', x_gpu[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferencia de *NumPy* que nos devuelve un error al ejecutar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-78c8861b3dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "print(x_cpu[[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con *CuPy* se reciclan los índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(x_gpu[[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de generar *arrays* en *NumPy* es con la función [arange](https://docs.cupy.dev/en/stable/reference/generated/cupy.arange.html) o [random](https://docs.cupy.dev/en/stable/reference/random.html) para un *array* pseudo aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(cp.arange(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59640368 0.42459851 0.15535147 0.95573683]\n"
     ]
    }
   ],
   "source": [
    "cp.random.seed(2000)\n",
    "print(cp.random.rand(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Array*'s dos dimensionales.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[1,2,3],[4,5,6]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.ndim: 2\n",
      "A.shape: (2, 3)\n",
      "A.size: 6\n",
      "A.dtype int64\n"
     ]
    }
   ],
   "source": [
    "print('A.ndim:', A.ndim)\n",
    "print('A.shape:', A.shape)\n",
    "print('A.size:', A.size)\n",
    "print('A.dtype', A.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accedemos con corchetes a sus componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elemento en la posición (0,0): 1\n",
      "elemento en la posición (1,2): 6\n",
      "elemento en la posición (0,0): 1\n",
      "elemento en la posición (1,2): 6\n"
     ]
    }
   ],
   "source": [
    "print('elemento en la posición (0,0):', A[0][0])\n",
    "print('elemento en la posición (1,2):', A[1][2])\n",
    "#also with:\n",
    "print('elemento en la posición (0,0):', A[0,0])\n",
    "print('elemento en la posición (1,2):', A[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primer columna: [1 4]\n",
      "tercer columna: [3 6]\n",
      "segundo renglón: [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print('primer columna:', A[:,0])\n",
    "print('tercer columna:', A[:,2])\n",
    "print('segundo renglón:', A[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones `arange` o `random`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[0.  0.2]\n",
      " [0.4 0.6]\n",
      " [0.8 1. ]]\n"
     ]
    }
   ],
   "source": [
    "print(cp.arange(6).reshape(2,3))\n",
    "print(cp.arange(0,1.2,.2).reshape(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59640368 0.42459851 0.15535147 0.95573683]\n",
      " [0.97054217 0.68966838 0.72790884 0.12783432]]\n"
     ]
    }
   ],
   "source": [
    "cp.random.seed(2000)\n",
    "print(cp.random.rand(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones en el álgebra lineal con CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto escalar-vector, suma y punto entre vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = cp.array([6,-3,4])\n",
    "v2 = cp.array([4,5,0])\n",
    "scalar = -1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.   1.5 -2. ]\n"
     ]
    }
   ],
   "source": [
    "print(scalar*v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(v1.dot(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  2  4]\n"
     ]
    }
   ],
   "source": [
    "print(v1+v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto matriz vector *point-wise*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5  0]\n",
      " [ 3  6  6]\n",
      " [-6  4 -1]\n",
      " [ 5  4  9]]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[2,5,0],[3,6,6],[-6,4,-1],[5,4,9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  1  4]\n"
     ]
    }
   ],
   "source": [
    "v = cp.array([-2,1,4])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -4   5   0]\n",
      " [ -6   6  24]\n",
      " [ 12   4  -4]\n",
      " [-10   4  36]]\n"
     ]
    }
   ],
   "source": [
    "print(A*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto matriz-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5  0]\n",
      " [ 3  6  6]\n",
      " [-6  4 -1]\n",
      " [ 5  4  9]]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[2,5,0],[3,6,6],[-6,4,-1],[5,4,9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Obsérvese que las clases de los objetos deben ser del mismo tipo.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  1  4]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([-2,1,4])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'b' has incorrect type (expected cupy._core.core.ndarray, got numpy.ndarray)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1b54d307edad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'b' has incorrect type (expected cupy._core.core.ndarray, got numpy.ndarray)"
     ]
    }
   ],
   "source": [
    "print(A.dot(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  1  4]\n"
     ]
    }
   ],
   "source": [
    "v = cp.array([-2,1,4])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 24 12 30]\n"
     ]
    }
   ],
   "source": [
    "print(A.dot(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 24 12 30]\n"
     ]
    }
   ],
   "source": [
    "print(A@v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  0 -3  2]\n"
     ]
    }
   ],
   "source": [
    "v = cp.array([7,0,-3,2])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42 31 21]\n"
     ]
    }
   ],
   "source": [
    "print(v@A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suma y producto matriz-matriz pointwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5  0]\n",
      " [ 3  6  6]\n",
      " [-6  4 -1]\n",
      " [ 5  4  9]]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[2,5,0],[3,6,6],[-6,4,-1],[5,4,9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -2  3]\n",
      " [ 1 -1  5]\n",
      " [ 0 -2  1]\n",
      " [ 0  0 -3]]\n"
     ]
    }
   ],
   "source": [
    "B = cp.array([[2,-2,3],[1,-1,5],[0,-2,1],[0,0,-3]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  3  3]\n",
      " [ 4  5 11]\n",
      " [-6  2  0]\n",
      " [ 5  4  6]]\n"
     ]
    }
   ],
   "source": [
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4 -10   0]\n",
      " [  3  -6  30]\n",
      " [  0  -8  -1]\n",
      " [  0   0 -27]]\n"
     ]
    }
   ],
   "source": [
    "print(A*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producto matriz-matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5  0]\n",
      " [ 3  6  6]\n",
      " [-6  4 -1]\n",
      " [ 5  4  9]]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[2,5,0],[3,6,6],[-6,4,-1],[5,4,9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -2  3]\n",
      " [ 1 -1  5]\n",
      " [ 0 -2  1]]\n"
     ]
    }
   ],
   "source": [
    "B = cp.array([[2,-2,3],[1,-1,5],[0,-2,1]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9  -9  31]\n",
      " [ 12 -24  45]\n",
      " [ -8  10   1]\n",
      " [ 14 -32  44]]\n"
     ]
    }
   ],
   "source": [
    "print(A@B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas operaciones básicas del álgebra lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norma de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "v = cp.array([1,2,3])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7416573867739413\n"
     ]
    }
   ],
   "source": [
    "print(cp.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norma de matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  5  0]\n",
      " [ 3  6  6]\n",
      " [-6  4 -1]]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[2,5,0],[3,6,6],[-6,4,-1]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.767145334803704\n"
     ]
    }
   ],
   "source": [
    "print(cp.linalg.norm(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolver sistema de ecuaciones lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[ 8 -6  2]\n",
      " [-4 11 -7]\n",
      " [ 4 -7  6]]\n",
      "b:\n",
      "[ 28 -40  33]\n"
     ]
    }
   ],
   "source": [
    "A = cp.array([[8, -6, 2], [-4, 11, -7], [4, -7, 6]])\n",
    "b = cp.array([28,-40,33])\n",
    "print('A:')\n",
    "print(A)\n",
    "print('b:')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cp.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[ 2. -1.  3.]\n"
     ]
    }
   ],
   "source": [
    "print('x:')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando resultado Ax = b\n",
      "b:\n",
      "[ 28 -40  33]\n",
      "Ax:\n",
      "[ 28. -40.  33.]\n"
     ]
    }
   ],
   "source": [
    "print('Verificando resultado Ax = b')\n",
    "print('b:')\n",
    "print(b)\n",
    "print('Ax:')\n",
    "print(A@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferencia de datos del *host* al *device* o viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3])\n",
    "x_gpu = cp.asarray(x_cpu)  # move the data to the current device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(x_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cupy._core.core.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.array([1, 2, 3])  # create an array in the current device\n",
    "x_cpu = cp.asnumpy(x_gpu)  # move the array to the host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y estas funciones pueden utilizarse para realizar operaciones dependiendo del tipo de *array*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cpu = np.array([5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-e0e7324dadac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_gpu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.__add__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.__array_ufunc__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._preprocess_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "print(x_gpu + y_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  8 10]\n"
     ]
    }
   ],
   "source": [
    "print(x_gpu + cp.asarray(y_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  8 10]\n"
     ]
    }
   ],
   "source": [
    "print(cp.asnumpy(x_gpu) + y_cpu )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función ejecutada dependiendo de que sean *array*'s de *NumPy* o *CuPy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible ejecutar una función dependiendo de sus argumentos con el módulo [get_array_module](https://docs-cupy.chainer.org/en/stable/reference/generated/cupy.get_array_module.html#cupy.get_array_module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    xp = cp.get_array_module(x)\n",
    "    return xp.exp(-x) + xp.cos(xp.sin(-abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03424619 0.74963557 1.03984615]\n"
     ]
    }
   ],
   "source": [
    "print(fun(x_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03424619 0.74963557 1.03984615]\n"
     ]
    }
   ],
   "source": [
    "print(fun(x_cpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo regla compuesta del rectángulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cp = lambda x: cp.exp(-x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rcf_cupy(f,a,b,n):\n",
    "    \"\"\"\n",
    "    Compute numerical approximation using rectangle or mid-point\n",
    "    method in an interval.\n",
    "    Nodes are generated via formula: x_i = a+(i+1/2)h_hat for\n",
    "    i=0,1,...,n-1 and h_hat=(b-a)/n\n",
    "    Args:\n",
    "    \n",
    "        f (float): function expression of integrand.\n",
    "        \n",
    "        a (float): left point of interval.\n",
    "        \n",
    "        b (float): right point of interval.\n",
    "        \n",
    "        n (int): number of subintervals.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        sum_res (float): numerical approximation to integral\n",
    "            of f in the interval a,b\n",
    "    \"\"\"\n",
    "    h_hat = (b-a)/n\n",
    "    aux_vec = cp.linspace(a, b, n+1)\n",
    "    nodes = (aux_vec[:-1]+aux_vec[1:])/2\n",
    "    return h_hat*cp.sum(f(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from pytest import approx\n",
    "from scipy.integrate import quad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**7\n",
    "a = 0\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=lambda x: math.exp(-x**2) #using math library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj, err = quad(f, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cupy = Rcf_cupy(f_cp, a, b,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(res_cupy.get() == approx(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx.time import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/cupyx/time.py:115: FutureWarning: cupyx.time.repeat is experimental. The interface can change in the future.\n",
      "  _util.experimental('cupyx.time.repeat')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rcf_cupy            :    CPU:  379.885 us   +/-64.506 (min:  328.881 / max:  543.411) us     GPU-0:21021.779 us   +/-768.554 (min:19397.568 / max:22042.303) us\n"
     ]
    }
   ],
   "source": [
    "print(repeat(Rcf_cupy, (f_cp,a,b,n), n_repeat=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver [performance](https://docs.cupy.dev/en/stable/user_guide/performance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Obsérvese que se utiliza mayor cantidad de memoria por *CuPy* que utilizando la implementación con *CUDA-C* {ref}`Rcf8.cu <RCF8CU>`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/cupyx/time.py:115: FutureWarning: cupyx.time.repeat is experimental. The interface can change in the future.\n",
      "  _util.experimental('cupyx.time.repeat')\n"
     ]
    },
    {
     "ename": "CompileException",
     "evalue": "/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(220): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(221): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(322): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(327): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(12): error: no operator \"[]\" matches these operands\n            operand types are: CArray<double, 0, true, false> [ const ptrdiff_t * ]\n\n5 errors detected in the compilation of \"/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu\".\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNVRTCError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, options, log_stream)\u001b[0m\n\u001b[1;32m    622\u001b[0m                     \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddAddNameExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompileProgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m             \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.compileProgram\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.compileProgram\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy_backends/cuda/libs/nvrtc.pyx\u001b[0m in \u001b[0;36mcupy_backends.cuda.libs.nvrtc.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNVRTCError\u001b[0m: NVRTC_ERROR_COMPILATION (6)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCompileException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-33ae1eff6e88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRcf_cupy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf_cp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupyx/time.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(func, args, kwargs, n_repeat, name, n_warmup, max_duration, devices)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`devices` should be of tuple type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     return _repeat(\n\u001b[0m\u001b[1;32m    140\u001b[0m         func, args, kwargs, n_repeat, name, n_warmup, max_duration, devices)\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupyx/time.py\u001b[0m in \u001b[0;36m_repeat\u001b[0;34m(func, args, kwargs, n_repeat, name, n_warmup, max_duration, devices)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-417a01c1f5d1>\u001b[0m in \u001b[0;36mRcf_cupy\u001b[0;34m(f, a, b, n)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m     \u001b[0mh_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0maux_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maux_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maux_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh_hat\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/_creation/ranges.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mscalar_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscalar_start\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscalar_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_linspace_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscalar_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/_creation/ranges.py\u001b[0m in \u001b[0;36m_linspace_scalar\u001b[0;34m(start, stop, num, endpoint, retstep, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Here num == div + 1 > 1 is ensured.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.__setitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_routines_indexing.pyx\u001b[0m in \u001b[0;36mcupy._core._routines_indexing._ndarray_setitem\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_routines_indexing.pyx\u001b[0m in \u001b[0;36mcupy._core._routines_indexing._scatter_op\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray.fill\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ElementwiseKernel.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ElementwiseKernel._get_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_util.pyx\u001b[0m in \u001b[0;36mcupy._util.memoize.decorator.ret\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_simple_elementwise_kernel\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.compile_with_cache\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_with_cache\u001b[0;34m(source, options, arch, cache_dir, extra_source, backend, enable_cooperative_groups, name_expressions, log_stream, jitify)\u001b[0m\n\u001b[1;32m    430\u001b[0m             name_expressions, log_stream, cache_in_memory)\n\u001b[1;32m    431\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         return _compile_with_cache_cuda(\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0menable_cooperative_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_expressions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36m_compile_with_cache_cuda\u001b[0;34m(source, options, arch, cache_dir, extra_source, backend, enable_cooperative_groups, name_expressions, log_stream, cache_in_memory, jitify)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nvrtc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mcu_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache_in_memory\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.cu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         ptx, mapping = compile_using_nvrtc(\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcu_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_expressions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             log_stream, cache_in_memory, jitify)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_using_nvrtc\u001b[0;34m(source, options, arch, filename, name_expressions, log_stream, cache_in_memory, jitify)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mcu_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             return _compile(source, options, cu_path,\n\u001b[0m\u001b[1;32m    272\u001b[0m                             name_expressions, log_stream, jitify)\n\u001b[1;32m    273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(source, options, cu_path, name_expressions, log_stream, jitify)\u001b[0m\n\u001b[1;32m    253\u001b[0m                              name_expressions=name_expressions)\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mptx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCompileException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             dump = _get_bool_env_variable(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cupy/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, options, log_stream)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNVRTCError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnvrtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetProgramLog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             raise CompileException(log, self.src, self.name, options,\n\u001b[0m\u001b[1;32m    636\u001b[0m                                    'nvrtc' if not runtime.is_hip else 'hiprtc')\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCompileException\u001b[0m: /home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(220): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(221): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(322): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/home/ubuntu/.local/lib/python3.8/site-packages/cupy/_core/include/cupy/carray.cuh(327): error: the size of an array must be greater than zero\n          detected during instantiation of class \"CArray<T, _ndim, _c_contiguous, _use_32bit_indexing> [with T=double, _ndim=0, _c_contiguous=true, _use_32bit_indexing=false]\" \n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(8): here\n\n/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu(12): error: no operator \"[]\" matches these operands\n            operand types are: CArray<double, 0, true, false> [ const ptrdiff_t * ]\n\n5 errors detected in the compilation of \"/tmp/tmpy38z_fir/80b50dce313ef4bb36a0d424596a2ca1_2.cubin.cu\".\n"
     ]
    }
   ],
   "source": [
    "print(repeat(Rcf_cupy, (f_cp,a,b,n), n_repeat=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Ejercicio\n",
    ":class: tip\n",
    "\n",
    "Implementar la regla de Simpson compuesta con *CuPy* en una máquina de AWS con las mismas características que la que se presenta en esta nota y medir tiempo de ejecución.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias de interés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para más sobre *Unified Memory* revisar:\n",
    "\n",
    "* [Even easier introduction to cuda](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "\n",
    "* [Unified memory cuda beginners](https://devblogs.nvidia.com/unified-memory-cuda-beginners/)\n",
    "\n",
    "Es importante el manejo de errores por ejemplo en el alojamiento de memoria en la GPU. En este caso es útil revisar:\n",
    "\n",
    "* [How to Query Device Properties and Handle Errors in CUDA C/C++](https://devblogs.nvidia.com/how-query-device-properties-and-handle-errors-cuda-cc/)\n",
    "\n",
    "En las siguientes preguntas encontramos a personas desarrolladoras de CUDA que las resuelven y resultan muy útiles para continuar con el aprendizaje de *CUDA C*. Por ejemplo: \n",
    "\n",
    "* [Parallel reduction over one axis](https://stackoverflow.com/questions/51526082/cuda-parallel-reduction-over-one-axis)\n",
    "\n",
    "Otros sistemas de software para el [Heterogeneous computing](https://en.wikipedia.org/wiki/Heterogeneous_computing) son:\n",
    "\n",
    "* [OpenCl](https://en.wikipedia.org/wiki/OpenCL). Ver [NVIDIA OpenCL SDK Code Samples](https://developer.nvidia.com/opencl) para ejemplos con NVIDIA GPU's.\n",
    "\n",
    "* [Rth-org/Rth](https://github.com/Rth-org/Rth) y más reciente [matloff/Rth](https://github.com/matloff/Rth). Ver también [rdrr.io matloff/Rth](https://rdrr.io/github/matloff/Rth/f/README.md).\n",
    "\n",
    "Es posible escribir *kernels* con *CuPy*. Ver por ejemplo: [User-Defined Kernels](https://docs-cupy.chainer.org/en/stable/tutorial/kernel.html).\n",
    "\n",
    "\n",
    "Otro paquete para uso de Python+GPU para cómputo matricial es:\n",
    "\n",
    "* [PyCUDA](https://github.com/inducer/pycuda/) y ver [PyCUDA en el repo de la clase](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/tree/master/Python/PyCUDA) para más información.\n",
    "\n",
    "Un paquete para uso de pandas+GPU:\n",
    "\n",
    "* [Rapids](https://github.com/rapidsai), [cudf](https://github.com/rapidsai/cudf)\n",
    "\n",
    "Ver [optional-libraries](https://docs-cupy.chainer.org/en/stable/install.html#optional-libraries) para librerías que pueden ser utilizadas con CuPy.\n",
    "\n",
    "Un paquete de *R* para uso de GPU: [gputools: cran](https://rdrr.io/cran/gputools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Ejercicios\n",
    ":class: tip\n",
    "\n",
    "1.Resuelve los ejercicios y preguntas de la nota.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas de comprehensión:**\n",
    "\n",
    "1)¿Qué factores han determinado un mejor *performance* de una GPU vs una CPU? (contrasta los diseños de una CPU vs una GPU).\n",
    "\n",
    "2)¿Dentro de qué modelo de arquitectura de máquinas se ubica a la GPU dentro de la taxonomía de Flynn? (tip: tal modelo se le puede comparar con el modelo **Single Program Multiple Data (SPMD)**)\n",
    "\n",
    "3)¿Qué significan las siglas CUDA y detalla qué es CUDA?.\n",
    "\n",
    "4)¿Qué es y en qué consiste CUDA C?\n",
    "    \n",
    "5)¿Qué es un *kernel*?\n",
    "\n",
    "6)¿Qué pieza de CUDA se encarga de asignar los bloques de *cuda-threads* a las SM’s?\n",
    "\n",
    "7)¿Qué características (recursos compartidos, dimensiones, forma de agendar la ejecución en *threads*) tienen los bloques que se asignan a una SM al lanzarse y ejecutarse un *kernel*?\n",
    "\n",
    "8)¿Qué es un *warp*?\n",
    "\n",
    "9)Menciona los tipos de memorias que existen en las GPU’s.\n",
    "\n",
    "10)Supón que tienes una tarjeta GT200 cuyas características son:\n",
    "\n",
    "* Máximo número de *threads* que soporta una SM en un mismo instante en el tiempo: 1024\n",
    "* Máximo número de *threads* en un bloque: 512\n",
    "* Máximo número de bloques por SM: 8\n",
    "* Número de SM’s que tiene esta GPU: 30\n",
    "\n",
    "Responde:\n",
    "\n",
    "a)¿Cuál es la máxima cantidad de *threads* que puede soportar esta GPU en un mismo instante en el tiempo?\n",
    "\n",
    "b)¿Cuál es la máxima cantidad de *warps* por SM que puede soportar esta GPU en un mismo instante en el tiempo?\n",
    "\n",
    "c)¿Cuáles configuraciones de bloques y *threads* siguientes aprovechan la máxima cantidad de *warps* en una SM de esta GPU para un mismo instante en el tiempo?\n",
    "    \n",
    "1.Una configuración del tipo: bloques de 64 *threads* y 16 bloques.\n",
    "\n",
    "2.Una configuración del tipo: bloques de 1024 *threads* y 1 bloque.\n",
    "\n",
    "3.Una configuración del tipo: bloques de 256 *threads* y 4 bloques.\n",
    "\n",
    "4.Una configuración del tipo: bloques de 512 *threads* y 8 bloques.\n",
    "\n",
    "\\*Debes considerar las restricciones/características de la GPU dadas para responder pues algunas configuraciones infringen las mismas. No estamos considerando *registers* o *shared memory*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Referencias:**\n",
    "\n",
    "1. N. Matloff, Parallel Computing for Data Science. With Examples in R, C++ and CUDA, 2014.\n",
    "\n",
    "2. D. B. Kirk, W. W. Hwu, Programming Massively Parallel Processors: A Hands-on Approach, Morgan Kaufmann, 2010.\n",
    "\n",
    "3. NVIDIA,CUDA Programming Guide, NVIDIA Corporation, 2007.\n",
    "\n",
    "4. B. W. Kernighan, D. M. Ritchie, The C Programming Language, Prentice Hall Software Series, 1988\n",
    "\n",
    "5. [C/extensiones_a_C/CUDA/](https://github.com/palmoreck/programming-languages/tree/master/C/extensiones_a_C/CUDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
