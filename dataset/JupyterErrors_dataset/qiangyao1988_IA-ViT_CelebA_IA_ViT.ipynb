{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7b0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ml_collections\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "from os.path import join as pjoin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
    "from utils.data_utils import get_loader\n",
    "from utils.dist_util import get_world_size\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bc5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Required parameters\n",
    "parser.add_argument(\"--name\", type=str, default=\"cifar10-100_500\",\n",
    "                    help=\"Name of this run. Used for monitoring.\")\n",
    "parser.add_argument(\"--dataset\", choices=[\"cifar10\", \"cifar100\"], default=\"cifar10\",\n",
    "                    help=\"Which downstream task.\")\n",
    "parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n",
    "                                             \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],\n",
    "                    default=\"ViT-B_16\",\n",
    "                    help=\"Which variant to use.\")\n",
    "parser.add_argument(\"--pretrained_dir\", type=str, default=\"checkpoint/ViT-B_16.npz\",\n",
    "                    help=\"Where to search for pretrained ViT models.\")\n",
    "parser.add_argument(\"--output_dir\", default=\"output\", type=str,\n",
    "                    help=\"The output directory where checkpoints will be written.\")\n",
    "\n",
    "parser.add_argument(\"--img_size\", default=224, type=int,\n",
    "                    help=\"Resolution size\")\n",
    "parser.add_argument(\"--train_batch_size\", default=64, type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\", default=64, type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--eval_every\", default=100, type=int,\n",
    "                    help=\"Run prediction on validation set every so many steps.\"\n",
    "                         \"Will always run one evaluation at the end of training.\")\n",
    "\n",
    "parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
    "                    help=\"The initial learning rate for SGD.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
    "                    help=\"Weight deay if we apply some.\")\n",
    "parser.add_argument(\"--num_steps\", default=10000, type=int,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
    "                    help=\"How to decay the learning rate.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
    "                    help=\"Step of training to perform learning rate warmup for.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                    help=\"Max gradient norm.\")\n",
    "\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "parser.add_argument('--fp16', action='store_true',\n",
    "                    help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
    "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                         \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "parser.add_argument('--loss_scale', type=float, default=0,\n",
    "                    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "                         \"0 (default value): dynamic loss scaling.\\n\"\n",
    "                         \"Positive power of 2: static loss scaling value.\\n\")\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d498246",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Setup CUDA, GPU & distributed training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m      3\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl',\n",
    "                                         timeout=timedelta(minutes=60))\n",
    "    args.n_gpu = 1\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9122b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(args, model):\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_checkpoint = os.path.join(args.output_dir, \"%s_checkpoint_ivit_ali_mmd_celeba.bin\" % args.name)\n",
    "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
    "    logger.info(\"Saved model checkpoint to [DIR: %s]\", args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5288dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
    "    config.hidden_size = 768\n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    config.classifier = 'token'\n",
    "    config.representation_size = None\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_b16_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\"\n",
    "\n",
    "def np2th(weights, conv=False):\n",
    "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95531194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        if config.patches.get(\"grid\") is not None:\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
    "                                         width_factor=config.resnet.width_factor)\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
    "                                       out_channels=config.hidden_size,\n",
    "                                       kernel_size=patch_size,\n",
    "                                       stride=patch_size)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.hybrid:\n",
    "            x = self.hybrid_model(x)\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        x = self.attention_norm(x)\n",
    "        x, weights = self.attn(x)\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = self.ffn(x)\n",
    "        x = x + h\n",
    "        return x, weights\n",
    "\n",
    "    def load_from(self, weights, n_block):\n",
    "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
    "        with torch.no_grad():\n",
    "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
    "\n",
    "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
    "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
    "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
    "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
    "\n",
    "            self.attn.query.weight.copy_(query_weight)\n",
    "            self.attn.key.weight.copy_(key_weight)\n",
    "            self.attn.value.weight.copy_(value_weight)\n",
    "            self.attn.out.weight.copy_(out_weight)\n",
    "            self.attn.query.bias.copy_(query_bias)\n",
    "            self.attn.key.bias.copy_(key_bias)\n",
    "            self.attn.value.bias.copy_(value_bias)\n",
    "            self.attn.out.bias.copy_(out_bias)\n",
    "\n",
    "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
    "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
    "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
    "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
    "\n",
    "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
    "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
    "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
    "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
    "\n",
    "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
    "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
    "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
    "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87645fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)\n",
    "        return encoded, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7952f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.head = Linear(config.hidden_size, num_classes)\n",
    "        \n",
    "        \n",
    "        self.query = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = Linear(config.hidden_size,config.hidden_size)\n",
    "        \n",
    "        self.fc1 = Linear(config.hidden_size, 1)\n",
    "        self.fc2 = Linear(196, num_classes)\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x, atts = self.transformer(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "\n",
    "        h = x[:, 1:]\n",
    "        \n",
    "        # new_h = (new_atts.unsqueeze(dim = 2) * h)\n",
    "        mixed_query_layer = self.query(h)\n",
    "        mixed_key_layer = self.key(h)\n",
    "        mixed_value_layer = self.value(h)\n",
    "        \n",
    "        attention_scores = torch.matmul(mixed_query_layer, mixed_key_layer.transpose(-1, -2))\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, mixed_value_layer)\n",
    "        new_atts = attention_probs\n",
    "        \n",
    "        inter_logits = self.fc2(self.fc1(context_layer).squeeze(dim = 2))\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits, inter_logits, atts, new_atts\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.weight)\n",
    "                nn.init.zeros_(self.head.bias)\n",
    "            else:\n",
    "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
    "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
    "\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
    "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
    "                ntok_new = posemb_new.size(1)\n",
    "\n",
    "                if self.classifier == \"token\":\n",
    "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                    ntok_new -= 1\n",
    "                else:\n",
    "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
    "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=bname, n_unit=uname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db209d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes, vis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac36ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_from(np.load(args.pretrained_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592937b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(args.device)\n",
    "num_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bafc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss_fct = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hinton_loss(outputs, t_outputs,  kd_temp=3):\n",
    "\n",
    "    soft_label = F.softmax(t_outputs / kd_temp, dim=1)\n",
    "    kd_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs / kd_temp, dim=1),\n",
    "                                                  soft_label) * (kd_temp * kd_temp)\n",
    "    return kd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42400a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3056dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(args, model, writer, test_loader, global_step):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    logger.info(\"***** Running Validation *****\")\n",
    "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True,\n",
    "                          disable=args.local_rank not in [-1, 0])\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "\n",
    "    logger.info(\"\\n\")\n",
    "    logger.info(\"Validation Results\")\n",
    "    logger.info(\"Global Steps: %d\" % global_step)\n",
    "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
    "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
    "\n",
    "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f66cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(feature1, feature2):\n",
    "    return (feature1 - feature2).pow(2).mean()\n",
    "\n",
    "def at(x):\n",
    "    return F.normalize(x.pow(2).view(x.size(0), -1))\n",
    "\n",
    "def at_loss(x, y):\n",
    "    return (at(x) - at(y)).pow(2).mean()\n",
    "\n",
    "def compute_at_loss(feature1, feature2):\n",
    "    attention_loss = (1 / 2) * (at_loss(feature1, feature2))\n",
    "    return attention_loss\n",
    "\n",
    "def compute_kl_loss(feature1, feature2, kd_temp=3):\n",
    "    \n",
    "    kd_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(feature1 / kd_temp, dim=1),\n",
    "                                                  feature2) * (kd_temp * kd_temp)\n",
    "\n",
    "    return kd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, sigma, kernel):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def forward(self, old_atts, new_atts,):\n",
    "\n",
    "        mmd_loss = self.pdist(old_atts, new_atts)[0].mean()\n",
    "\n",
    "        return mmd_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def pdist(e1, e2, eps=1e-12, kernel='rbf', sigma_base=1.0, sigma_avg=None):\n",
    "        if len(e1) == 0 or len(e2) == 0:\n",
    "            res = torch.zeros(1)\n",
    "        else:\n",
    "            if kernel == 'rbf':\n",
    "                e1_square = e1.pow(2).sum(dim=1)\n",
    "                e2_square = e2.pow(2).sum(dim=1)\n",
    "                prod = e1 @ e2.t()\n",
    "                res = (e1_square.unsqueeze(1) + e2_square.unsqueeze(0) - 2 * prod).clamp(min=eps)\n",
    "                res = res.clone()\n",
    "\n",
    "                sigma_avg = res.mean().detach() if sigma_avg is None else sigma_avg\n",
    "                res = torch.exp(-res / (2*(sigma_base**2)*sigma_avg))\n",
    "            elif kernel == 'poly':\n",
    "                res = torch.matmul(e1, e2.t()).pow(2)\n",
    "                \n",
    "        return res, sigma_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80175c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd = MMDLoss(sigma=1.0, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc78eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class celebADataset(Dataset):\n",
    "    def __init__(self, split, transform):\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "        self.split = split\n",
    "        self.dataset_name = 'celeba'\n",
    "        self.root_dir = 'datasets'\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.dataset_name)\n",
    "        self.metadata_df = pd.read_csv(os.path.join(self.dataset_dir, 'celebA_split.csv'))\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['split']==self.split_dict[self.split]]\n",
    "\n",
    "        self.y_array = self.metadata_df['Gray_Hair'].values\n",
    "        self.s_array = self.metadata_df['Male'].values\n",
    "        self.filename_array = self.metadata_df['image_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        s = self.s_array[idx]\n",
    "        img_filename = os.path.join(\n",
    "            self.dataset_dir,\n",
    "            'img_align_celeba',\n",
    "            'img_align_celeba',\n",
    "            self.filename_array[idx])\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a97bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "trainset = celebADataset(split = \"train\", transform = transform_train)\n",
    "valset = celebADataset(split = \"val\", transform = transform_train)\n",
    "testset = celebADataset(split = \"test\", transform = transform_test)\n",
    "\n",
    "train_sampler = RandomSampler(trainset) \n",
    "val_sampler = SequentialSampler(valset)\n",
    "test_sampler = SequentialSampler(testset)\n",
    "\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(trainset,\n",
    "                          sampler=train_sampler,\n",
    "                          batch_size=train_batch_size,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "val_loader = DataLoader(valset,\n",
    "                          sampler=val_sampler,\n",
    "                          batch_size=eval_batch_size,\n",
    "                          num_workers=8,\n",
    "                          pin_memory=True,\n",
    "                          drop_last = True)\n",
    "\n",
    "test_loader = DataLoader(testset,\n",
    "                         sampler=test_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c711db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, test_loader):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=os.path.join(\"logs\", args.name))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    # Prepare optimizer and scheduler\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=args.learning_rate,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    t_total = args.num_steps\n",
    "    if args.decay_type == \"cosine\":\n",
    "        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    else:\n",
    "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(models=model,\n",
    "                                          optimizers=optimizer,\n",
    "                                          opt_level=args.fp16_opt_level)\n",
    "        amp._amp_state.loss_scalers[0]._loss_scale = 2**20\n",
    "\n",
    "    # Distributed training\n",
    "    if args.local_rank != -1:\n",
    "        model = DDP(model, message_size=250000000, gradient_predivide_factor=get_world_size())\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Total optimization steps = %d\", args.num_steps)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
    "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "\n",
    "    model.zero_grad()\n",
    "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
    "    ce_losses1 = AverageMeter()\n",
    "    ce_losses2 = AverageMeter()\n",
    "    kd_losses = AverageMeter()\n",
    "    reg_losses = AverageMeter()\n",
    "    global_step, best_acc = 0, 0\n",
    "    while True:\n",
    "        model.train()\n",
    "        epoch_iterator = tqdm(train_loader,\n",
    "                              desc=\"Training (X / X Steps) (loss=X.X)\",\n",
    "                              bar_format=\"{l_bar}{r_bar}\",\n",
    "                              dynamic_ncols=True,\n",
    "                              disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            x, y = batch\n",
    "        \n",
    "            logits, inter_logits, atts, new_atts = model(x)\n",
    "            ce_loss1 = ce_loss_fct(logits, y)\n",
    "            # ce_loss2 = ce_loss_fct(inter_logits, y)\n",
    "            kd_loss = compute_hinton_loss(inter_logits, logits)\n",
    "            \n",
    "            # get attention weights\n",
    "            attn_weights_dic = {}\n",
    "            for l in range(len(atts)):\n",
    "                atten_select = l\n",
    "                atten_layer = atts[atten_select].mean(dim=1)\n",
    "                attn_weights_dic[l] = atten_layer\n",
    "\n",
    "            old_atts = sum(attn_weights_dic.values())\n",
    "            \n",
    "            old_atts = old_atts[:,1:,1:]\n",
    "            # old_atts = old_atts[:, 0, 1:]\n",
    "            old_atts = old_atts.mean(dim = 1)\n",
    "            new_atts = new_atts.mean(dim = 1)\n",
    "            \n",
    "            # mse_loss = (1 / 2) * (mse(old_atts, new_atts))\n",
    "            # kl_loss = (1 / 2) * (compute_kl_loss(old_atts, new_atts))\n",
    "            # at_loss = (1 / 2) * (compute_at_loss(old_atts, new_atts))\n",
    "            mmd_loss = mmd(old_atts, new_atts)\n",
    "            \n",
    "            # loss = ce_loss1 + ce_loss2 + kd_loss\n",
    "            loss = ce_loss1 + kd_loss + mmd_loss\n",
    "            \n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                \n",
    "                ce_losses1.update(ce_loss1.item() * args.gradient_accumulation_steps)\n",
    "                # ce_losses2.update(ce_loss2.item() * gradient_accumulation_steps)\n",
    "                kd_losses.update(kd_loss.item() * args.gradient_accumulation_steps)\n",
    "                reg_losses.update(mmd_loss.item() * args.gradient_accumulation_steps)\n",
    "                \n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                scheduler.step()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                 \n",
    "                epoch_iterator.set_description(\n",
    "                    \"Training (%d / %d Steps) (ce_loss1=%2.5f, kd_loss=%2.5f, reg_loss=%2.5f,)\" \n",
    "                    % (global_step, t_total, ce_losses1.val, kd_losses.val, reg_losses.val)\n",
    "                ) \n",
    "                \n",
    "                # if args.local_rank in [-1, 0]:\n",
    "                    # writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
    "                    # writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
    "                if global_step % args.eval_every == 0 and args.local_rank in [-1, 0]:\n",
    "                    accuracy = valid(args, model, writer, test_loader, global_step)\n",
    "                    if best_acc < accuracy:\n",
    "                        save_model(args, model)\n",
    "                        best_acc = accuracy\n",
    "                    model.train()\n",
    "\n",
    "                if global_step % t_total == 0:\n",
    "                    break\n",
    "        ce_losses1.reset()\n",
    "        # ce_losses2.reset()\n",
    "        kd_losses.reset()\n",
    "        reg_losses.reset()\n",
    "        \n",
    "        if global_step % t_total == 0:\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        writer.close()\n",
    "    logger.info(\"Best Accuracy: \\t%f\" % best_acc)\n",
    "    logger.info(\"End Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\" %\n",
    "               (args.local_rank, args.device, args.n_gpu, bool(args.local_rank != -1), args.fp16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f45c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40180c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train(args, model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = os.path.join(args.output_dir, \"%s_checkpoint_ivit_ali_mmd_celeba.bin\" % args.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df79ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(model_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04732e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_valid(args, model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True,\n",
    "                          disable=args.local_rank not in [-1, 0])\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec086876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = new_valid(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3586af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val accuracy: %2.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_valid(args, model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    epoch_iterator = tqdm(test_loader,\n",
    "                          desc=\"Validating... (loss=X.X)\",\n",
    "                          bar_format=\"{l_bar}{r_bar}\",\n",
    "                          dynamic_ncols=True,\n",
    "                          disable=args.local_rank not in [-1, 0])\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        x, y = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[1]\n",
    "\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f92a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = inter_valid(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val accuracy: %2.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \n",
    "        \"\"\"\n",
    "        tensor = tensor.clone()\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "to_PIL = transforms.ToPILImage()\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "test_data = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488368a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "im = to_PIL(unorm(test_data[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e992ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "model.eval()\n",
    "\n",
    "logits, inter_logits, att_mat, new_att_mat = model(test_data[0][i].unsqueeze(0))\n",
    "\n",
    "att_mat = torch.stack(att_mat).squeeze(1)\n",
    "\n",
    "# Average the attention weights across all heads.\n",
    "att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "# To account for residual connections, we add an identity matrix to the\n",
    "# attention matrix and re-normalize the weights.\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    \n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[-1]\n",
    "\n",
    "\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "result = (mask * im).astype(\"uint8\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e90926",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_att_mat = new_att_mat.squeeze(0)\n",
    "avg_new_att_mat = torch.mean(new_att_mat, dim = 0)\n",
    "new_mask = avg_new_att_mat.reshape(grid_size, grid_size).detach().numpy()\n",
    "new_mask = cv2.resize(new_mask / new_mask.max(), im.size)[..., np.newaxis]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(new_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be36f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heatmap from mask on image\n",
    "def show_cam_on_image(img, mask):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * new_mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) # / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "vis = show_cam_on_image(im, mask)\n",
    "vis =  np.uint8(255 * vis)\n",
    "vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "_ = ax2.imshow(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class celebADataset(Dataset):\n",
    "    def __init__(self, split, sensitive, transform):\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "        self.split = split\n",
    "        self.sensitive = sensitive\n",
    "        self.dataset_name = 'celeba'\n",
    "        self.root_dir = 'datasets'\n",
    "        self.dataset_dir = os.path.join(self.root_dir, self.dataset_name)\n",
    "        # if not os.path.exists(self.dataset_dir):\n",
    "            # raise ValueError(f'{self.dataset_dir} does not exist yet. Please generate the dataset first.')\n",
    "        self.metadata_df = pd.read_csv(os.path.join(self.dataset_dir, 'celebA_split.csv'))\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['split']==self.split_dict[self.split]]\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['Male']==self.sensitive]\n",
    "\n",
    "        self.y_array = self.metadata_df['Gray_Hair'].values\n",
    "        self.s_array = self.metadata_df['Male'].values\n",
    "        self.filename_array = self.metadata_df['image_id'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filename_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        s = self.s_array[idx]\n",
    "        img_filename = os.path.join(\n",
    "            self.dataset_dir,\n",
    "            'img_align_celeba',\n",
    "            'img_align_celeba',\n",
    "            self.filename_array[idx])\n",
    "        img = Image.open(img_filename).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "\n",
    "        return img, y, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "\n",
    "testset_0 = celebADataset(split = \"test\", sensitive = 0, transform = transform_test)\n",
    "\n",
    "test_0_sampler = SequentialSampler(testset_0)\n",
    "\n",
    "eval_batch_size = 32\n",
    "\n",
    "test_0_loader = DataLoader(testset_0,\n",
    "                         sampler=test_0_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)\n",
    "\n",
    "testset_1 = celebADataset(split = \"test\", sensitive = 1, transform = transform_test)\n",
    "\n",
    "test_1_sampler = SequentialSampler(testset_1)\n",
    "\n",
    "eval_batch_size = 32\n",
    "\n",
    "test_1_loader = DataLoader(testset_1,\n",
    "                         sampler=test_1_sampler,\n",
    "                         batch_size=eval_batch_size,\n",
    "                         num_workers=8,\n",
    "                         pin_memory=True,\n",
    "                         drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, test_loader):\n",
    "    # Validation!\n",
    "    eval_losses = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_label = [], []\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for step, batch in enumerate(test_loader):\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, s = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x)[0]\n",
    "            eval_loss = loss_fct(logits, y)\n",
    "            eval_losses.update(eval_loss.item())\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if len(all_preds) == 0:\n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_label.append(y.detach().cpu().numpy())\n",
    "        else:\n",
    "            all_preds[0] = np.append(\n",
    "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "            all_label[0] = np.append(\n",
    "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "    all_preds, all_label = all_preds[0], all_label[0]\n",
    "    \n",
    "    accuracy = simple_accuracy(all_preds, all_label)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_preds, all_label).ravel()\n",
    "\n",
    "    return accuracy, tn, fp, fn, tp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, test_loader):\n",
    "    model.eval().cuda()\n",
    "    acc, tn, fp, fn, tp = valid(model, test_loader)\n",
    "    print(\"Accuracy: %2.5f\" % acc)\n",
    "    \n",
    "    print(\"Confusion matrix: \\n\",\n",
    "    \"True Positive: %s \\t False Positive: %s \\n\" %(tp, fp),\n",
    "    \"False Negative: %s \\t True Negative: %s\" %(fn, tn))\n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    tnr = tn / (tn + fp)\n",
    "    print(\"True Positive Rate: %2.5f\" % tpr)\n",
    "    print(\"True Negative Rate: %2.5f\" % tnr)\n",
    "\n",
    "    fpr = fp / (tn + fp)\n",
    "    fnr = fn / (tp + fn)\n",
    "    \n",
    "    print(\"False Positive Rate: %2.5f\" % fpr)\n",
    "    print(\"False Negative Rate: %2.5f\" % fnr)\n",
    "    \n",
    "    return acc, tpr, tnr, fpr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc0, tpr0, tnr0, fpr0, fnr0 = get_results(model, test_0_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1, tpr1, tnr1, fpr1, fnr1 = get_results(model, test_1_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.5 * (acc0 + acc1 )\n",
    "print(\"Accuracy: %2.5f\" % acc)\n",
    "\n",
    "dp_acc = abs(acc0 - acc1)\n",
    "print(\"DP_Acc: %2.5f\" % dp_acc)\n",
    "\n",
    "dp = abs(tpr1 - tpr0)\n",
    "print(\"DP: %2.5f\" % dp)\n",
    "\n",
    "ba = 0.25*(tpr0 + tnr0 + tpr1 + tnr1)\n",
    "print(\"BA: %2.5f\" % ba)\n",
    "\n",
    "eqodds = 0.5 * abs(tpr1 - tpr0) + 0.5 * abs(fpr1 - fpr0)\n",
    "print(\"Equal Odds: %2.5f\" % eqodds)\n",
    "\n",
    "diff_ba = 0.5*(tpr0 + tnr0) - 0.5*(tpr1 + tnr1)\n",
    "print(\"Difference BA: %2.5f\" % abs(diff_ba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68704b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalMetric():\n",
    "\n",
    "    def __init__(self, model, mode, step, substrate_fn):\n",
    "        r\"\"\"Create deletion/insertion metric instance.\n",
    "        Args:\n",
    "            model (nn.Module): Black-box model being explained.\n",
    "            mode (str): 'del' or 'ins'.\n",
    "            step (int): number of pixels modified per one iteration.\n",
    "            substrate_fn (func): a mapping from old pixels to new pixels.\n",
    "        \"\"\"\n",
    "        assert mode in ['del', 'ins']\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        self.substrate_fn = substrate_fn\n",
    "        self.sm = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def single_run(self, img_tensor, label, explanation):\n",
    "        r\"\"\"Run metric on one image-saliency pair.\n",
    "        Args:\n",
    "            img_tensor (Tensor): normalized image tensor.\n",
    "            explanation (np.ndarray): saliency map.\n",
    "            verbose (int): in [0, 1, 2].\n",
    "                0 - return list of scores.\n",
    "                1 - also plot final step.\n",
    "                2 - also plot every step and print 2 top classes.\n",
    "            save_to (str): directory to save every step plots to.\n",
    "        Return:\n",
    "            scores (nd.array): Array containing scores at every step.\n",
    "        \"\"\"\n",
    "        n_steps = (HW + self.step - 1) // self.step\n",
    "\n",
    "        if self.mode == 'del':\n",
    "            title = 'Deletion game'\n",
    "            ylabel = 'Pixels deleted'\n",
    "            start = img_tensor.clone()\n",
    "            finish = self.substrate_fn(img_tensor)\n",
    "            finish = finish.cpu().numpy()\n",
    "            \n",
    "        elif self.mode == 'ins':\n",
    "            title = 'Insertion game'\n",
    "            ylabel = 'Pixels inserted'\n",
    "            start = self.substrate_fn(img_tensor).cuda()\n",
    "            finish = img_tensor.clone()\n",
    "            finish = finish.cpu().numpy()\n",
    "            \n",
    "        scores = np.empty(n_steps + 1)\n",
    "        # Coordinates of pixels in order of decreasing saliency\n",
    "        salient_order = np.flip(np.argsort(explanation.reshape(-1, HW), axis=1), axis=-1)\n",
    "        for i in range(n_steps+1):\n",
    "            pred, _, _ , _ = self.model(start.unsqueeze(0))\n",
    "            scores[i] = self.sm(pred)[0, label]\n",
    "            \n",
    "            if i < n_steps:\n",
    "                start = start.cpu().numpy()\n",
    "                coords = salient_order[:, self.step * i:self.step * (i + 1)]\n",
    "                start.reshape(1, 3, HW)[0, :, coords] = finish.reshape(1, 3, HW)[0, :, coords]\n",
    "                start = torch.from_numpy(start).cuda()\n",
    "                \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91531e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model.eval()\n",
    "scores = {'del': [], 'ins': []}\n",
    "deletion = CausalMetric(model, 'del', 224, substrate_fn=torch.zeros_like)\n",
    "insertion = CausalMetric(model, 'ins', 224, substrate_fn=torch.zeros_like)\n",
    "HW = 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(tqdm(test_loader)):\n",
    "        print(\"step:{}\".format(step))\n",
    "        x, y = batch\n",
    "        for image,label in zip(x, y):\n",
    "            image = image.cuda()\n",
    "            label = label.numpy()\n",
    "            logits, _,  _, new_att_mat = model(image.unsqueeze(0))\n",
    "\n",
    "            new_att_mat = new_att_mat.squeeze(0)\n",
    "            avg_new_att_mat = torch.mean(new_att_mat, dim = 0)\n",
    "            new_mask = avg_new_att_mat.reshape(grid_size, grid_size).cpu().detach().numpy()\n",
    "            new_mask = cv2.resize(new_mask / new_mask.max(), im.size)[..., np.newaxis]\n",
    "\n",
    "            del_score = deletion.single_run(image, label, new_mask)\n",
    "            print(\"del_score:{}\".format(del_score.mean()))\n",
    "            scores['del'].append(del_score.mean())\n",
    "\n",
    "            ins_score = insertion.single_run(image, label, new_mask)\n",
    "            print(\"ins_score:{}\".format(ins_score.mean()))\n",
    "            scores['ins'].append(ins_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad600b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "curDelection = sum(scores['del'])/len(scores['del'])\n",
    "curInsertion = sum(scores['ins'])/len(scores['ins'])\n",
    "\n",
    "print(\"current deletion score: {}\".format(curDelection))\n",
    "print(\"current insertion score: {}\".format(curInsertion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b857e3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
