{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z6Wag04Hk5jA"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb transformers google-cloud-aiplatform google-auth google-search-results > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS']='/content/generativeaitrial-trialLC.json'"
      ],
      "metadata": {
        "id": "veeMOmfFn3W8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import VertexAI\n",
        "from langchain.chat_models import ChatVertexAI"
      ],
      "metadata": {
        "id": "nEJ7I3hFlqX0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import AnalyzeDocumentChain\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "llm = VertexAI(temperature=0)\n",
        "summary_chain = load_summarize_chain(llm, \n",
        "                                     chain_type=\"map_reduce\")"
      ],
      "metadata": {
        "id": "NUEmaHJ1lqP4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)"
      ],
      "metadata": {
        "id": "eeXjZhwnlqKS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip lc_documentdb.zip"
      ],
      "metadata": {
        "id": "saRKisZYpD5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embeddings = VertexAIEmbeddings()\n",
        "\n",
        "persist_directory = '/content/content/lc_documentdb'\n",
        "\n",
        "vectordb = Chroma(persist_directory=persist_directory, \n",
        "                  embedding_function=embeddings)\n",
        "\n",
        "db_retriever = vectordb.as_retriever()\n",
        "\n",
        "\n",
        "langchain_qa = RetrievalQA.from_chain_type(llm=VertexAI(), \n",
        "                                 chain_type=\"stuff\", \n",
        "                                 retriever=db_retriever)"
      ],
      "metadata": {
        "id": "kbEayoqgoe_w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_output(topic):\n",
        "  relevant_data = langchain_qa(topic)\n",
        "  print(relevant_data)\n",
        "  summary = summarize_document_chain.run(relevant_data['result'])\n",
        "  return summary"
      ],
      "metadata": {
        "id": "cvXyvXk0lqHA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'List the langchain Agents'"
      ],
      "metadata": {
        "id": "AbJyajTNqawS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_output(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "RVH50Ty8qYQW",
        "outputId": "fbf4d881-40b5-4d8a-9fe4-51712500acb1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Action agents act without planning, while plan-and-execute agents plan before acting.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n"
      ],
      "metadata": {
        "id": "NlNx3jrjrytY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = 'What are the concepts in Langchain'"
      ],
      "metadata": {
        "id": "sKnIhay1tkGX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db_retriever.get_relevant_documents(query1)\n",
        "docs"
      ],
      "metadata": {
        "id": "1dcLNylstIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "for t in docs:\n",
        "  text = text + t.page_content"
      ],
      "metadata": {
        "id": "KZFuZA41tP4t"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain_route = qa_document_chain.run(input_document=text, \n",
        "                      question=query1)"
      ],
      "metadata": {
        "id": "d0EniG69ryne"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain_route"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "pQ3srBkpryjz",
        "outputId": "06b63d61-476a-4f46-e079-33eca565ed3a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chain of Thought, Action Plan Generation, ReAct, Self-ask, Prompt Chaining, Memetic Proxy, Self Consistency, Inception, MemPrompt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_output(query1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "DcB6V29wryg2",
        "outputId": "8c6a2baa-cedc-4d50-9c3d-8d36f8e6bfae"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'What are the concepts in Langchain', 'result': 'The concepts in Langchain are Chain of Thought, Action Plan Generation, ReAct, Self-ask, Prompt Chaining, Memetic Proxy, Self Consistency, Inception, and MemPrompt.'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Langchain is a language model that can generate text, translate languages, write creative content, and answer your questions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward-Looking Active REtrieval augmented generation (FLARE).\n",
        "\n",
        "The idea is:\n",
        "\n",
        "Start answering a question\n",
        "\n",
        "If you start generating tokens the model is uncertain about, look up relevant documents\n",
        "\n",
        "Use those documents to continue generating\n",
        "\n",
        "Repeat until finished"
      ],
      "metadata": {
        "id": "ZpprZIshuzQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Need three things:\n",
        "\n",
        "An LLM to generate the answer\n",
        "\n",
        "An LLM to generate hypothetical questions to use in retrieval\n",
        "\n",
        "A retriever to use to look up answers for"
      ],
      "metadata": {
        "id": "vPe5RYPEu4tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SERPER_API_KEY\"] = \"a5029817748ec7119fba1bff69445b00d3b9c617a88b8eaf7dbb4470af9a7d1a\"\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from langchain.schema import BaseRetriever\n",
        "from langchain.utilities import GoogleSerperAPIWrapper\n",
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "Qf4UEy5Yrybm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SerperSearchRetriever(BaseRetriever):\n",
        "    def __init__(self, search):\n",
        "        self.search = search\n",
        "    \n",
        "    def get_relevant_documents(self, query: str):\n",
        "        return [Document(page_content=self.search.run(query))]\n",
        "    \n",
        "    async def aget_relevant_documents(self, query: str):\n",
        "        raise NotImplemented\n",
        "        \n",
        "        \n",
        "retriever = SerperSearchRetriever(GoogleSerperAPIWrapper())"
      ],
      "metadata": {
        "id": "0dq1beOwvMPv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import FlareChain\n",
        "\n",
        "flare = FlareChain.from_llm(\n",
        "    ChatVertexAI(temperature=0), \n",
        "    retriever=retriever,\n",
        "    max_generation_len=164,\n",
        "    min_prob=.3,\n",
        ")"
      ],
      "metadata": {
        "id": "-zqxi5DcvSC6",
        "outputId": "e09739ab-4b7b-4427-a380-e5e6dcec9587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-e967ad6d4f1e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlareChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m flare = FlareChain.from_llm(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mChatVertexAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/flare/base.py\u001b[0m in \u001b[0;36mfrom_llm\u001b[0;34m(cls, llm, max_generation_len, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     ) -> FlareChain:\n\u001b[1;32m    217\u001b[0m         \u001b[0mquestion_gen_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuestionGeneratorChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         response_llm = OpenAI(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_generation_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"logprobs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ]
    }
  ]
}