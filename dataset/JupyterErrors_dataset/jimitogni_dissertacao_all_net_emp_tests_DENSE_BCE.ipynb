{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hatmap\n",
    "- Melhorar o plot de imagens\n",
    "- Propor algo novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy \n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import shutil, sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "#model_name = \"densenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "#num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs to train for\n",
    "\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "#num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Create training and validation dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                    batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# trans = ['train','val','test']\n",
    "# categories = ['train','val','test']\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files per classes\n",
      "----------------------------------------\n",
      "normal :  4023\n",
      "pneumonia :  4035\n",
      "covid :  4105\n",
      "--------------------\n",
      "Train, test, validation\n",
      "--------------------\n",
      "len_train_dir :  9632\n",
      "len_test_dir :  31\n",
      "len_val_dir :  2409\n"
     ]
    }
   ],
   "source": [
    "# Path to data\n",
    "data_dir = '/home/jimi/dissertacao/covid19/datasets/80-20/'\n",
    "train_dir = data_dir+'train/'\n",
    "test_dir = data_dir+'test/'\n",
    "val_dir = data_dir+'val/'\n",
    "\n",
    "normal_dir = data_dir+'normal/'\n",
    "pneumonia_dir = data_dir+'pneumonia/'\n",
    "covid_dir = data_dir+'covid/'\n",
    "\n",
    "len_covid = len([iq for iq in os.scandir(normal_dir)])\n",
    "len_normal = len([iq for iq in os.scandir(pneumonia_dir)])\n",
    "len_pneumonia = len([iq for iq in os.scandir(covid_dir)])\n",
    "\n",
    "len_train_dir = len([iq for iq in os.scandir(train_dir+'covid/')]) + len([iq for iq in os.scandir(train_dir+'normal/')]) + len([iq for iq in os.scandir(train_dir+'pneumonia/')])\n",
    "len_test_dir = len([iq for iq in os.scandir(test_dir+'covid/')]) + len([iq for iq in os.scandir(test_dir+'normal/')]) + len([iq for iq in os.scandir(test_dir+'pneumonia/')])\n",
    "len_val_dir = len([iq for iq in os.scandir(val_dir+'covid/')]) + len([iq for iq in os.scandir(val_dir+'normal/')]) + len([iq for iq in os.scandir(val_dir+'pneumonia/')])\n",
    "\n",
    "print('Files per classes')\n",
    "print(\"----\"*10)\n",
    "print(\"normal : \", len_covid)\n",
    "\n",
    "print(\"pneumonia : \", len_normal)\n",
    "\n",
    "print(\"covid : \", len_pneumonia)\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print('Train, test, validation')\n",
    "\n",
    "print(\"-\"*20)\n",
    "\n",
    "print(\"len_train_dir : \", len_train_dir)\n",
    "\n",
    "print(\"len_test_dir : \", len_test_dir)\n",
    "\n",
    "print(\"len_val_dir : \", len_val_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs, \n",
    "                model_name, lr, batch_size, opt_name, crt_name):\n",
    "    since = time.time()\n",
    "    is_inception = False\n",
    "    \n",
    "    #tensorboard\n",
    "    writer = SummaryWriter(f'runs/dg_{model_name}_lr={lr}_epoch={num_epochs}_batch_size={batch_size}')\n",
    "    step = 0\n",
    "\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        print('-' * 10)\n",
    "        \n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "\n",
    "            writer.add_scalar('training loss', loss, global_step=step)\n",
    "            writer.add_scalar('training accuracy', epoch_acc, global_step=step)\n",
    "            step += 1\n",
    "            \n",
    "            #only to plot the graph\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print('#'*30)\n",
    "    print('------ Summary ------')\n",
    "    print(f'model -> {_model}')\n",
    "    print(f'epochs -> {_epochs}')\n",
    "    print(f'lr -> {_lrs}')\n",
    "    print(f'batch size -> {_batch}')\n",
    "    print(f'optimizer -> {opt_name}'), \n",
    "    print(f'criteriun -> {crt_name}')\n",
    "    print()\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('#'*30)\n",
    "    \n",
    "    plt.figure(figsize=(13, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.plot(train_acc_history, label=\"Validation Loss\")\n",
    "    plt.title('Accuracy and Loss in Validation Dataset')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_loss_history, label=\"val_loss_history\")\n",
    "    plt.plot(train_loss_history, label=\"train_loss_history\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hist_'+_model+'_opt_'+opt_name+'_crt_'+crt_name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "\n",
    "    print('==== END ====')\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=4):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 3, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('cm_'+title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "==== INITIALIZING WITH PARAMETERS: ====\n",
      "model -> squeezenet\n",
      "epochs -> 300\n",
      "lr -> 0.0001\n",
      "batch size -> 8\n",
      "optimizer -> 1\n",
      "criteriun -> 2\n",
      "\n",
      "--------------------\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\n",
      "--------------------\n",
      "\n",
      "== Epochs ==\n",
      "Epoch 0/299\n",
      "----------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target and input must have the same number of elements. target nelement (16) != input nelement (48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-698ad7d53895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m                         model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft,\n\u001b[1;32m     82\u001b[0m                                                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_lrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                                                 batch_size=_batch, opt_name=opt_name, crt_name=crt_name)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-aad31950d9cf>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, model_name, lr, batch_size, opt_name, crt_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[0;32m-> 2372\u001b[0;31m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[1;32m   2373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (16) != input nelement (48)"
     ]
    }
   ],
   "source": [
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "num_classes = 3\n",
    "\n",
    "_models = ['squeezenet', 'densenet', 'resnet', 'alexnet', 'vgg']\n",
    "lrs = [1e-4]\n",
    "_epoch = [300]\n",
    "batch_sizes = [8]\n",
    "opt = [1, 2, 3]\n",
    "crt = [2]\n",
    "\n",
    "for _model in _models:\n",
    "    for _epochs in _epoch:\n",
    "        for _lrs in lrs:\n",
    "            for _batch in batch_sizes:\n",
    "                for _opt in opt:\n",
    "                    for _crt in crt:\n",
    "                               \n",
    "                        print()\n",
    "                        print('='*60)\n",
    "                        print('==== INITIALIZING WITH PARAMETERS: ====')\n",
    "                        print(f'model -> {_model}')\n",
    "                        print(f'epochs -> {_epochs}')\n",
    "                        print(f'lr -> {_lrs}')\n",
    "                        print(f'batch size -> {_batch}')\n",
    "                        print(f'optimizer -> {_opt}')\n",
    "                        print(f'criteriun -> {_crt}')\n",
    "                        print()\n",
    "\n",
    "                        feature_extract = True\n",
    "\n",
    "                        model_ft, input_size = initialize_model(_model, num_classes, \n",
    "                                                                feature_extract, use_pretrained=True)\n",
    "\n",
    "                        # Send the model to GPU\n",
    "                        model_ft = model_ft.to(device)\n",
    "\n",
    "                        print('-'*20)\n",
    "                        params_to_update = model_ft.parameters()\n",
    "                        print(\"Params to learn:\")\n",
    "                        if feature_extract:\n",
    "                            params_to_update = []\n",
    "                            for name,param in model_ft.named_parameters():\n",
    "                                if param.requires_grad == True:\n",
    "                                    params_to_update.append(param)\n",
    "                                    print(\"\\t\",name)\n",
    "\n",
    "                        else:\n",
    "                            for name,param in model_ft.named_parameters():\n",
    "                                if param.requires_grad == True:\n",
    "                                    print(\"\\t\",name)\n",
    "\n",
    "\n",
    "                        print()\n",
    "                        print('-'*20)\n",
    "                        print()\n",
    "                        print('== Epochs ==')\n",
    "\n",
    "                        if _opt == 1:\n",
    "                            optimizer_ft = optim.SGD(params_to_update, _lrs, momentum=0.9)\n",
    "                            opt_name = 'SGD'\n",
    "\n",
    "                        if _opt == 2:\n",
    "                            optimizer_ft = optim.Adam(params_to_update, _lrs)\n",
    "                            opt_name = 'ADAM'\n",
    "                            \n",
    "                        if _opt == 3:\n",
    "                            optimizer_ft = optim.RMSprop(params_to_update, _lrs)\n",
    "                            opt_name = 'RMSprop'\n",
    "\n",
    "\n",
    "                        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "                        #tray nn.NLLLoss\n",
    "                        if _crt == 1:\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            crt_name = 'CrossEntropyLoss'\n",
    "                        if _crt == 2:\n",
    "                            criterion = nn.BCELoss()\n",
    "                            crt_name = 'BCELoss'\n",
    "\n",
    "                        model_ft, hist = train_model(model_ft, dataloaders, criterion, optimizer_ft,\n",
    "                                                num_epochs=_epochs, model_name=_model, lr=_lrs,\n",
    "                                                batch_size=_batch, opt_name=opt_name, crt_name=crt_name)\n",
    "\n",
    "                        from sklearn.metrics import confusion_matrix\n",
    "\n",
    "                        nb_classes = 3\n",
    "\n",
    "                        # Initialize the prediction and label lists(tensors)\n",
    "                        predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "                        lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "                                inputs = inputs.to(device) #labels atuais\n",
    "                                classes = classes.to(device) #classes\n",
    "                                outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "                                _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "                                # Append batch prediction results\n",
    "                                predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "                                lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "                        # Confusion matrix\n",
    "                        conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                        print(conf_mat)\n",
    "                        print()\n",
    "\n",
    "                        from sklearn import metrics\n",
    "\n",
    "                        #analise dos resultados do modelo\n",
    "                        print('Sensitivity or recall total')\n",
    "                        print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "                        print()\n",
    "                        print('Sensitivity or recall per classes')\n",
    "                        print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        print()\n",
    "                        print('Precision')\n",
    "                        print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        print()\n",
    "                        print('F1 Score')\n",
    "                        print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "                        cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "                        np.set_printoptions(precision=2)\n",
    "\n",
    "                        plt.figure()\n",
    "\n",
    "                        plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], \n",
    "                        title='Confusion matrix model'+_model+'_opt_'+opt_name+'_criteriun_'+crt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Os resultados na matriz de confuzão e scores estão ruins, provavelmente pelo param.requires_grad = True \n",
    "\n",
    "Testar depois com False\n",
    "\n",
    "**Peguei o código original**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"------ RESULTADOS ------\")\n",
    "print()\n",
    "plt.figure(figsize=(13, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label=\"AUC Treino\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"AUC VALIDAÇÃO\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label=\"loss validação\")\n",
    "plt.plot(history.history['val_loss'], label=\"AUC validação\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hook the feature extractor\n",
    "# features_blobs = []\n",
    "# def hook_feature(module, input, output):\n",
    "#     features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "# model_ft._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# # get the softmax weight\n",
    "# params = list(model_ft.parameters())\n",
    "# weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "# def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "#     # generate the class activation maps upsample to 256x256\n",
    "#     size_upsample = (256, 256)\n",
    "#     bz, nc, h, w = feature_conv.shape\n",
    "#     output_cam = []\n",
    "#     for idx in class_idx:\n",
    "#         cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "#         cam = cam.reshape(h, w)\n",
    "#         cam = cam - np.min(cam)\n",
    "#         cam_img = cam / np.max(cam)\n",
    "#         cam_img = np.uint8(255 * cam_img)\n",
    "#         output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "#     return output_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Normalize(\n",
    "#    mean=[0.485, 0.456, 0.406],\n",
    "#    std=[0.229, 0.224, 0.225]\n",
    "# )\n",
    "# preprocess = transforms.Compose([\n",
    "#    transforms.Resize((224,224)),\n",
    "#    transforms.ToTensor(),\n",
    "#    normalize\n",
    "# ])\n",
    "\n",
    "# response = requests.get(IMG_URL)\n",
    "# img_pil = Image.open(io.BytesIO(response.content))\n",
    "# img_pil.save('test.jpg')\n",
    "\n",
    "# img_tensor = preprocess(img_pil)\n",
    "# img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "# logit = model_ft(img_variable)\n",
    "\n",
    "# # download the imagenet category list\n",
    "# # classes = {int(key):value for (key, value)\n",
    "# #           in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "# # h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "# # probs, idx = h_x.sort(0, True)\n",
    "# # probs = probs.numpy()\n",
    "# # idx = idx.numpy()\n",
    "\n",
    "# # # output the prediction\n",
    "# # for i in range(0, 5):\n",
    "# #     print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# # generate class activation mapping for the top1 prediction\n",
    "# CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# # render the CAM and output\n",
    "# # print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "# img = cv2.imread('test.jpg')\n",
    "# height, width, _ = img.shape\n",
    "# heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "# result = heatmap * 0.3 + img * 0.5\n",
    "# cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython.display as display\n",
    "# from PIL import Image\n",
    "# image_path = 'CAM.jpg'\n",
    "# display.display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 1 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    #mean = np.array([0.485, 0.456, 0.405])\n",
    "    #std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    #plt.pause(1)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=10):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['test']):\n",
    "        #inputs, labels = data\n",
    "\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.figure(figsize=(20,20))\n",
    "            ax = plt.subplot(5, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('{}'.format(class_names[predlist[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "#print(dir(model))\n",
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savePath = \"test_model.pth\"\n",
    "# torch.save(model_ft.state_dict(), savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY CAM\n",
    "#do it using RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import PIL\n",
    "import scipy.ndimage as nd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "#transforms.RandomRotation(degrees=(-5, 5)),\n",
    "#transforms.ColorJitter(brightness=.02),\n",
    "    \n",
    "transformers = {\n",
    "    'train_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'valid_transforms': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "trans = ['train_transforms','valid_transforms','test_transforms']\n",
    "\n",
    "path = \"/home/jimi/dissertacao/covid19/datasets/80-20/\"\n",
    "categories = ['train','val','test']\n",
    "dset = {x : torchvision.datasets.ImageFolder(path+x,\n",
    "                                             transform=transformers[y]) for x,y in zip(categories, trans)}\n",
    "\n",
    "dataset_sizes = ['train']\n",
    "\n",
    "\n",
    "num_threads = 4 \n",
    "dataloaders =  {x : torch.utils.data.DataLoader(dset[x], batch_size=16, shuffle=True, num_workers=num_threads)\n",
    "               for x in categories}\n",
    "\n",
    "dataset_sizes = {x : len(dset[x]) for x in ['train','val','test']}\n",
    "\n",
    "class_names = dset['train'].classes\n",
    "\n",
    "#class_names = image_datasets['train'].classes\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/300\n",
      "----------\n",
      "train loss:  0.2861  acc: 0.8961\n",
      "val loss:  0.1513  acc: 0.9481\n",
      "epoch 2/300\n",
      "----------\n",
      "train loss:  0.1862  acc: 0.9335\n",
      "val loss:  0.1124  acc: 0.9606\n",
      "epoch 3/300\n",
      "----------\n",
      "train loss:  0.1733  acc: 0.9355\n",
      "val loss:  0.1183  acc: 0.9597\n",
      "epoch 4/300\n",
      "----------\n",
      "train loss:  0.1353  acc: 0.9514\n",
      "val loss:  0.1139  acc: 0.9589\n",
      "epoch 5/300\n",
      "----------\n",
      "train loss:  0.1371  acc: 0.9514\n",
      "val loss:  0.1076  acc: 0.9618\n",
      "epoch 6/300\n",
      "----------\n",
      "train loss:  0.1363  acc: 0.9499\n",
      "val loss:  0.1058  acc: 0.9626\n",
      "epoch 7/300\n",
      "----------\n",
      "train loss:  0.1365  acc: 0.9512\n",
      "val loss:  0.1081  acc: 0.9606\n",
      "epoch 8/300\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "##### RESNET\n",
    "##Build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = torchvision.models.resnet152(pretrained=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features,3),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        for params in self.model.parameters():\n",
    "            params.requires_grad = True\n",
    "        self.model.fc = self.classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def fit(self, dataloaders, num_epochs):\n",
    "        train_on_gpu = torch.cuda.is_available()\n",
    "        optimizer = optim.Adam(self.model.fc.parameters())\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 4)\n",
    "        criterion = nn.NLLLoss()\n",
    "        since = time.time()\n",
    "        \n",
    "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            self.model = self.model.cuda()\n",
    "            \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            print(\"epoch {}/{}\".format(epoch, num_epochs))\n",
    "            print(\"-\" * 10)\n",
    "            \n",
    "            for phase in ['train','val']:\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    self.model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0.0\n",
    "                \n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    if train_on_gpu:\n",
    "                        inputs = inputs.cuda()\n",
    "                        labels = labels.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = self.model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                print(\"{} loss:  {:.4f}  acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(self.model.state_dict())\n",
    "        \n",
    "        time_elapsed = time.time() - since\n",
    "        print('time completed: {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 600))\n",
    "        print(\"best val acc: {:.4f}\".format(best_acc))\n",
    "        \n",
    "        self.model.load_state_dict(best_model_wts)\n",
    "        return self.model\n",
    "    \n",
    "model = Model()\n",
    "model_ft = model.fit(dataloaders,300)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "#analise dos resultados do modelo\n",
    "print('Sensitivity or recall total')\n",
    "print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average='micro'))\n",
    "\n",
    "print()\n",
    "print('Sensitivity or recall per classes')\n",
    "print (metrics.recall_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "print()\n",
    "print('Precision')\n",
    "print (metrics.precision_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "print()\n",
    "print('F1 Score')\n",
    "print (metrics.f1_score(lbllist.numpy(), predlist.numpy(), average=None))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "# Initialize the prediction and label lists(tensors)\n",
    "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device) #labels atuais\n",
    "        classes = classes.to(device) #classes\n",
    "        outputs = model_ft(inputs) #valores preditos = Passa o label atual e retorna o que o modelo predice\n",
    "        _, preds = torch.max(outputs, 1) #pega o maior valor das predições\n",
    "\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "print(conf_mat)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm, classes=['norm', 'covid', 'pnemo'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_test = \"/home/jimi/dissertacao/covid19/datasets/80-20/test/\"\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      #                     [0.229, 0.224, 0.225])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(num):\n",
    "    data = datasets.ImageFolder(data_dir_test, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num]\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    loader = torch.utils.data.DataLoader(data,sampler=sampler, batch_size=num)\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = dataiter.next()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "to_pil = transforms.ToPILImage()\n",
    "images, labels = get_random_images(30)\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "for ii in range(len(images)):\n",
    "    image = to_pil(images[ii])\n",
    "    index = predict_image(image)\n",
    "    \n",
    "    data = datasets.ImageFolder(data_dir_test, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    \n",
    "    #print (f'index: {index}')\n",
    "    #print (f'image: {image}')\n",
    "    #print (f'labes: {labels}')\n",
    "    #print (f'classes index :{classes[index]}')\n",
    "    #print (f'classes 2:{classes}')\n",
    "    \n",
    "    sub = fig.add_subplot(8, 4, ii+1)\n",
    "    \n",
    "    #print()\n",
    "    res = int(labels[ii]) == 1\n",
    "    #print(f'int(labels[ii]): {int(labels[ii])}')\n",
    "    #print(f'index: {index}')\n",
    "    #print(f'res = int(labels[ii]) == index: {res}')\n",
    "    #print()\n",
    "    \n",
    "    #print (f'res : {res}')\n",
    "    \n",
    "    sub.set_title(str(classes[1]) + \":\" + str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])\n",
    "def image_loader(image_name):\n",
    "    image = PIL.Image.open(image_name).convert(\"RGB\")\n",
    "    image = loader(image).float()\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=[]\n",
    "    def __init__(self,model):\n",
    "        self.hooks = []\n",
    "        self.hooks.append(model.layer4.register_forward_hook(self.hook_fn))\n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features.append(output)\n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/1785.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/860.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/val/normal/2480.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001.png'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001-9-a.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/jimi/dissertacao/covid19/datasets/80-20/test/covid/000001-10.jpg'\n",
    "img = image_loader(image_path).cuda()\n",
    "\n",
    "acts = LayerActivations(model_ft)\n",
    "\n",
    "logps = model_ft(img).cuda()\n",
    "ps = torch.exp(logps)\n",
    "\n",
    "out_features = acts.features[0]\n",
    "out_features = torch.squeeze(out_features, dim=0)\n",
    "out_features = out_features.cpu().detach().numpy()\n",
    "out_features = np.transpose(out_features,axes=(1,2,0))\n",
    "\n",
    "W = model_ft.fc[0].weight\n",
    "top_probs, top_classes = torch.topk(ps, k=3)\n",
    "ps = ps.cpu().detach().numpy()\n",
    "pred = np.argmax(ps)\n",
    "w = W[pred,:]\n",
    "\n",
    "w = w.cpu().detach().numpy()\n",
    "cam = np.dot(out_features, w)\n",
    "#type(w)\n",
    "\n",
    "class_activation = nd.zoom(cam, zoom=(32,32),order=1)\n",
    "\n",
    "img = torch.squeeze(img,0)\n",
    "img = img.cpu().numpy()\n",
    "img = np.transpose(img,(1,2,0))\n",
    "mean = np.array([0.5,0.5,0.5])\n",
    "std =  np.array([0.5,0.5,0.5])\n",
    "#img = img.cpu().numpy()\n",
    "img = (img + mean) * std\n",
    "img = np.clip(img, a_max=1, a_min=0)\n",
    "\n",
    "plt.imshow(class_activation, cmap='jet',alpha=1)\n",
    "plt.imshow(img, alpha=0.55)\n",
    "plt.title(dset['val'].classes[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 2 FIIMMMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref - https://github.com/ironWolf1990/pytorch-covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref- https://github.com/ironWolf1990/pytorch-covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 ref- https://github.com/ironWolf1990/pytorch-covid19\n",
    "########## CAM try 3 ref\n",
    "########## CAM try 3 ref\n",
    "########## CAM try 3 ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, gridspec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        cmap = cm.get_cmap(\"rainbow\")\n",
    "        c = cmap(int(255 * s / 9))\n",
    "        plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max())\n",
    "    plt.ylim(Y.min(), Y.max())\n",
    "    plt.show()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "def data_viz(layer, label):\n",
    "    # https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents-notebooks/401_CNN.ipynb\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init=\"pca\", n_iter=5000)\n",
    "    plot_only = 500\n",
    "    low_dim_embs = tsne.fit_transform(layer.data.numpy()[:plot_only, :])\n",
    "    labels = label.numpy()[:plot_only]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "\n",
    "def plot_test_image_result(img, ps, le, cam=None):\n",
    "\n",
    "    _ = plt.figure(figsize=(8, 6))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "    ax1, ax2 = plt.subplot(gs[0]), plt.subplot(gs[1])\n",
    "\n",
    "    if cam is not None:\n",
    "        ax1.imshow(cam, alpha=0.6)\n",
    "        ax1.imshow(img, alpha=0.4)\n",
    "    else:\n",
    "        ax1.imshow(img)\n",
    "\n",
    "    ax2.barh(np.arange(len(ps)), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(len(ps)))\n",
    "\n",
    "    for i, v in enumerate(ps):\n",
    "        ax2.text(\n",
    "            .01,\n",
    "            i-0.1,\n",
    "            f'{v:.3f}',\n",
    "            color='blue',\n",
    "            fontweight='bold')\n",
    "\n",
    "    if le is None:\n",
    "        ax2.set_yticklabels(np.arange(len(ps)))\n",
    "    else:\n",
    "        ax2.set_yticklabels(le.inverse_transform(np.arange(len(ps))))\n",
    "\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makedataset.py\n",
    "\n",
    "from os.path import isfile, join\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import tee\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    path = \"./data\"\n",
    "    sample_per_category = 500\n",
    "    seed = 24\n",
    "    split_frac = 0.20\n",
    "\n",
    "    df_raw = None\n",
    "\n",
    "    genFiles = (\n",
    "        (dirpath, dirnames, filenames) for (dirpath, dirnames, filenames) in walk(path)\n",
    "    )\n",
    "\n",
    "    files, genFiles = tee(genFiles)\n",
    "    file_count = sum(len(f) for _, _, f in files)\n",
    "\n",
    "    df_raw = pd.DataFrame(\n",
    "        data=np.nan, index=np.arange(0, file_count - 1), columns=[\"LABEL\", \"FILE\"]\n",
    "    )\n",
    "\n",
    "    files, genFiles = tee(genFiles)\n",
    "    idx = 0\n",
    "    for r, _, f in files:\n",
    "        for _f in f:\n",
    "            if isfile(join(r, _f)) and _f.endswith(\n",
    "                (\".jpeg\", \".png\", \"jpg\", \".JPEG\", \".PNG\", \"JPG\")\n",
    "            ):\n",
    "                path = \"/\".join((r, _f))\n",
    "                *_, label = r.split(\"/\")\n",
    "                df_raw.iloc[idx] = [label, path]\n",
    "                idx += 1\n",
    "\n",
    "    df_raw.to_csv(\"./data/raw.csv\", index=False)\n",
    "\n",
    "    # # 3-class\n",
    "    df_main = pd.DataFrame(\n",
    "        data=np.nan,\n",
    "        index=np.arange(0, sample_per_category * 3),\n",
    "        columns=[\"FILE\", \"LABEL\"],\n",
    "    )\n",
    "\n",
    "    df_main = df_raw.groupby(\"LABEL\").apply(\n",
    "        lambda s: s.sample(n=min(len(s), sample_per_category), random_state=seed)\n",
    "    )\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_main, random_state=seed, test_size=split_frac, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(\"./data/3_class_train_df.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/3_class_test_df.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n3_class_train_df:\\n{train_df['LABEL'].value_counts()}\")\n",
    "    print(f\"3_class_test_df:\\n{test_df['LABEL'].value_counts()}\")\n",
    "\n",
    "    # 2-class\n",
    "    df_main = pd.DataFrame(\n",
    "        data=np.nan,\n",
    "        index=np.arange(0, sample_per_category * 2),\n",
    "        columns=[\"FILE\", \"LABEL\"],\n",
    "    )\n",
    "\n",
    "    index = df_raw[df_raw[\"LABEL\"] == \"pneumonia\"].index\n",
    "    df_raw.drop(index, inplace=True)\n",
    "\n",
    "    df_main = df_raw.groupby(\"LABEL\").apply(\n",
    "        lambda s: s.sample(n=min(len(s), sample_per_category), random_state=seed)\n",
    "    )\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_main, random_state=seed, test_size=split_frac, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(\"./data/2_class_train_df.csv\", index=False)\n",
    "    test_df.to_csv(\"./data/2_class_test_df.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n2_class_train_df:\\n{train_df['LABEL'].value_counts()}\")\n",
    "    print(f\"2_class_test_df:\\n{test_df['LABEL'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architectures.py\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Rn50(nn.Module):\n",
    "    def __init__(self, device, train_base=False, classes=2):\n",
    "        super(Rn50, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.net_back = resnet50(pretrained=True).to(self.device)\n",
    "        self._trainable(train_base)\n",
    "\n",
    "        fc_size = self.net_back.fc.in_features\n",
    "        self.net_back.fc = Identity()\n",
    "\n",
    "        self.net_head = nn.Sequential(\n",
    "            nn.Linear(in_features=fc_size, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=512, out_features=classes),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        ).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_back(x.to(self.device))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net_head(x.to(self.device))\n",
    "\n",
    "    def _trainable(self, flag):\n",
    "        for param in self.net_back.parameters():\n",
    "            param.requires_grad = flag\n",
    "\n",
    "\n",
    "# old way\n",
    "# modules = list(resnet50(pretrained=True).children())[:-1]\n",
    "# self.net_back = nn.Sequential(*modules).to(self.device)\n",
    "# fc_size = list(self.net_back.parameters())[-1].size(0)\n",
    "# self.net_head = nn.Sequential(...).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activationmap.py\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class FeatureBuffer():\n",
    "\n",
    "    features=None\n",
    "\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        # self.features = ((output.cpu()).data).numpy()\n",
    "        self.features = output\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "# def GradCam(model, input_image_tensor):\n",
    "\n",
    "#     https://github.com/tyui592/class_activation_map/blob/master/cam.py\n",
    "#     https://github.com/daixiangzi/Grad_Cam-pytorch-resnet50/blob/578db29d13b0e7d17aa53d9bac116674771618ec/test_grad_cam.py#L19\n",
    "#     https://snappishproductions.com/blog/2018/01/03/class-activation-mapping-in-pytorch.html.html\n",
    "#     https://github.com/MarcoCBA/Class-Activation-Maps-PyTorch/blob/master/class_activation_maps.ipynb\n",
    "\n",
    "#     print(model)\n",
    "\n",
    "#     final_conv_layer = model.net_back._modules.get('layer4')\n",
    "#     fc_layer = model.net_head._modules.get('0')\n",
    "#     fb = FeatureBuffer(final_conv_layer)\n",
    "\n",
    "#     model = model.eval()\n",
    "#     out = model(input_image_tensor)\n",
    "\n",
    "#      # based on model caluculate output!!!\n",
    "#     probabilities = torch.exp(out)\n",
    "#     _, predicted = torch.max(probabilities, 1)\n",
    "#     feature_maps = fb.features\n",
    "\n",
    "#     print(\"Output's shape: \", out.shape)\n",
    "#     print(\"Feature maps's shape: \", feature_maps.shape)\n",
    "\n",
    "#     weights_and_biases = list(fc_layer.parameters())\n",
    "#     class_weights = weights_and_biases[0][predicted]\n",
    "#     print(\"Weights's shape: \", weights_and_biases[0].shape)\n",
    "#     print(\"Biases's shape: \", weights_and_biases[1].shape)\n",
    "#     print(\"Class weights's shape :\", class_weights.shape)\n",
    "\n",
    "#     class_weights = class_weights.reshape((-1, 1, 1))\n",
    "#     feature_maps = feature_maps.flatten(start_dim=0, end_dim=1)\n",
    "#     print(\"Class weights's shape :\", class_weights.shape)\n",
    "#     print(\"Feature maps's shape: \", feature_maps.shape)\n",
    "\n",
    "#     class_activation_maps = np.array(torch.sum(feature_maps * class_weights, dim=0).detach(), dtype=np.float32)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(class_activation_maps)\n",
    "#     plt.show()\n",
    "\n",
    "#     resized_cam = cv2.resize(class_activation_maps, dsize=(224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(resized_cam)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import math\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "except:\n",
    "    MODELSUMMARY = False\n",
    "else:\n",
    "    MODELSUMMARY = True\n",
    "\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "except:\n",
    "    VIZTSNE = False\n",
    "else:\n",
    "    VIZTSNE = True\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    device,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    dataloader_len,\n",
    "    input_shape,\n",
    "    scheduler=None,\n",
    "    num_epochs=50,\n",
    "):\n",
    "\n",
    "    if MODELSUMMARY:\n",
    "        summary(model, input_data=input_shape)\n",
    "\n",
    "    start = time()\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t_epoch = time()\n",
    "        print(f\"epoch: {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for idx, (labels, inputs) in enumerate(dataloaders[phase]):\n",
    "                iter_batch = math.ceil(\n",
    "                    dataloader_len[phase] / dataloaders[phase].batch_size\n",
    "                )\n",
    "                print(f\"[phase: {phase}] batch: {idx+1}/{iter_batch}\", end=\"\\r\")\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataloader_len[phase]\n",
    "            epoch_acc = running_corrects.double() / dataloader_len[phase]\n",
    "            print(f\"[phase: {phase}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                print(f\"[saving model] epoch: {epoch+1} Acc: {epoch_acc:.4f}\")\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "        t_elapsed = time() - t_epoch\n",
    "        print(f\"epoch training complete in {t_elapsed//60:.0f}m {t_elapsed%60:.0f}s\")\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time() - start\n",
    "    print(f\"training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s\")\n",
    "    print(f\"best val Acc: {best_acc:4f}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"input_shape\": input_shape,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, \"./models/checkpoint.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scripts.activationmap import FeatureBuffer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#from .utils import plot_test_image_result\n",
    "\n",
    "\n",
    "def test_model(model, testloader, device, encoder=None):\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true_list = list()\n",
    "    pred_list = list()\n",
    "\n",
    "    for idx, (labels, inputs) in enumerate(testloader):\n",
    "        iter_batch = math.ceil(len(testloader.dataset)/testloader.batch_size)\n",
    "        print(f'[phase: test] batch: {idx+1}/{iter_batch}', end='\\r')\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.exp(outputs)\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "\n",
    "            total = idx + 1\n",
    "            correct += torch.sum(predicted == labels.data)\n",
    "            true_list.append((labels.data.cpu()).numpy().item())\n",
    "            pred_list.append((predicted.cpu()).numpy().item())\n",
    "\n",
    "    acc = 100*(correct.item()/total)\n",
    "    print(f\"[phase: test] total: {total}, correct: {correct}, acc: {acc:.3f}\")\n",
    "\n",
    "    print(classification_report(tuple(true_list), tuple(pred_list)))\n",
    "\n",
    "    y_true = pd.Series(true_list, name='Actual')\n",
    "    y_pred = pd.Series(pred_list, name='Predicted')\n",
    "    cm = pd.crosstab(y_true, y_pred,  margins=True)\n",
    "\n",
    "    print(\"confusion matrix\")\n",
    "    if encoder is not None:\n",
    "        print({i : encoder.classes_[i] for i in range(0, len(encoder.classes_))})\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "\n",
    "def test_image(model, image, in_shape, transform, device, labelencoder=None, cam=None):\n",
    "    \"\"\"\n",
    "    GradCam\n",
    "    \"\"\"\n",
    "\n",
    "    if cam is not None:\n",
    "        final_conv_layer = model.net_back._modules.get('layer4')\n",
    "        fc_layer = model.net_head._modules.get('0')\n",
    "        fb = FeatureBuffer(final_conv_layer)\n",
    "\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    inputs = input_tensor.to(device)\n",
    "\n",
    "    model = model.eval()\n",
    "    outputs = model(inputs)\n",
    "    probabilities = torch.exp(outputs)\n",
    "    prob = (probabilities.cpu()).detach().numpy().flatten()\n",
    "\n",
    "    if cam is not None:\n",
    "        _, predicted = torch.max(probabilities, 1)\n",
    "        feature_maps = fb.features\n",
    "\n",
    "        weights_and_biases = list(fc_layer.parameters())\n",
    "        class_weights = weights_and_biases[0][predicted]\n",
    "\n",
    "        class_weights = class_weights.reshape((-1, 1, 1))\n",
    "        feature_maps = feature_maps.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "        class_activation_maps = np.array(\n",
    "            torch.sum(feature_maps * class_weights, dim=0).cpu().detach(),\n",
    "            dtype=np.float32)\n",
    "\n",
    "        cam_map = cv2.resize(\n",
    "            class_activation_maps,\n",
    "            dsize=in_shape,\n",
    "            interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    if cam is not None:\n",
    "        plot_test_image_result(image.resize(in_shape), prob, labelencoder, cam_map)\n",
    "    else:\n",
    "        plot_test_image_result(image, prob, labelencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.py \n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from scripts.datagen import Datagen\n",
    "from scripts.architectures import Rn50\n",
    "from scripts.test import test_model, test_image\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "test_file = \"data/3_class_test_df.csv\"\n",
    "image_file = \"data/raw/covid/covid_001.jpg\"\n",
    "num_workers = 2\n",
    "batch_size = 1\n",
    "input_shape = (256, 256)\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv(test_file)\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(input_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_set = Datagen(df, l_encoder=le, transforms=test_transforms)\n",
    "label_enc = test_set.get_le()\n",
    "device = get_device()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, num_workers=num_workers,\n",
    ")\n",
    "\n",
    "model = Rn50(device=device, classes=3)\n",
    "model.load_state_dict(torch.load(\"./models/checkpoint.pth\")[\"state_dict\"])\n",
    "\n",
    "test_model(\n",
    "    model=model,\n",
    "    testloader=test_loader,\n",
    "    device=device,\n",
    "    encoder=label_enc)\n",
    "\n",
    "input_image = Image.open(image_file).convert(\"RGB\")\n",
    "test_image(\n",
    "    model=model,\n",
    "    image=input_image,\n",
    "    in_shape=input_shape,\n",
    "    transform=test_transforms,\n",
    "    device=device,\n",
    "    labelencoder=label_enc,\n",
    "    cam=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Datagen(Dataset):\n",
    "    def __init__(self, dataframe, transforms=None, l_encoder=None):\n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        self.encoder = l_encoder\n",
    "\n",
    "        if self.encoder is not None:\n",
    "            self.df[\"LABEL\"] = self.encoder.fit_transform(self.df[\"LABEL\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        label = self.df.iloc[idx, 0]\n",
    "        image_file = self.df.iloc[idx, 1]\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # print(image.shape)\n",
    "        return (label, image)\n",
    "\n",
    "    def get_le(self):\n",
    "        return self.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scripts.datagen import Datagen\n",
    "from scripts.architectures import Rn50\n",
    "from scripts.train import train_model\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "train_file = \"data/3_class_train_df.csv\"\n",
    "num_workers = 2\n",
    "val_split = 0.2\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "input_shape = (3, 256, 256)\n",
    "le = LabelEncoder()\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = Datagen(df, l_encoder=le, transforms=train_transforms)\n",
    "validation_set = Datagen(df, l_encoder=le, transforms=validation_transforms)\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(df))), test_size=val_split)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    # shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validation_set,\n",
    "    # shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    sampler=valid_sampler,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "device = get_device()\n",
    "net = Rn50(device=device, classes=3)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"val\": valid_loader}\n",
    "dataloader_len = {\"train\": len(train_idx), \"val\": len(val_idx)}\n",
    "\n",
    "criteration = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(\n",
    "    model=net,\n",
    "    device=device,\n",
    "    criterion=criteration,\n",
    "    optimizer=optimizer,\n",
    "    dataloaders=dataloaders,\n",
    "    dataloader_len=dataloader_len,\n",
    "    input_shape=input_shape,\n",
    "    num_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM\n",
    "########## CAM try 3 FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CAM functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of CAM in PyTorch for the networks such as ResNet, DenseNet, SqueezeNet, Inception\n",
    "\n",
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pdb\n",
    "\n",
    "# input image\n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "IMG_URL = 'https://diariodonordeste.verdesmares.com.br/image/contentid/policy:1.2966908:1594933666/ferramenta-sesa.jpg'\n",
    "\n",
    "# networks such as googlenet, resnet, densenet already use global average pooling at the end, so CAM could be used directly.\n",
    "model_id = 1\n",
    "if model_id == 1:\n",
    "    net = models.squeezenet1_1(pretrained=True)\n",
    "    finalconv_name = 'features' # this is the last conv layer of the network\n",
    "elif model_id == 2:\n",
    "    net = models.resnet18(pretrained=True)\n",
    "    finalconv_name = 'layer4'\n",
    "elif model_id == 3:\n",
    "    net = models.densenet161(pretrained=True)\n",
    "    finalconv_name = 'features'\n",
    "\n",
    "#net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "net._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "response = requests.get(IMG_URL)\n",
    "img_pil = Image.open(io.BytesIO(response.content))\n",
    "img_pil.save('test.jpg')\n",
    "\n",
    "img_tensor = preprocess(img_pil)\n",
    "img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "logit = net(img_variable) ## aqui tem algo\n",
    "\n",
    "# # download the imagenet category list\n",
    "classes = {int(key):value for (key, value)\n",
    "          in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "probs = probs.numpy()\n",
    "idx = idx.numpy()\n",
    "\n",
    "# # output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# generate class activation mapping for the top1 prediction\n",
    "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "# render the CAM and output\n",
    "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
    "img = cv2.imread('test.jpg')\n",
    "height, width, _ = img.shape\n",
    "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "result = heatmap * 0.3 + img * 0.5\n",
    "cv2.imwrite('CAM.jpg', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "image_path = 'CAM.jpg'\n",
    "display.display(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
