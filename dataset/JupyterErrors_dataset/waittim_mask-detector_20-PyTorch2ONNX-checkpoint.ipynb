{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.google_utils import *\n",
    "from utils.layers import *\n",
    "from utils.parse_config import *\n",
    "\n",
    "ONNX_EXPORT = False\n",
    "\n",
    "\n",
    "def create_modules(module_defs, img_size, cfg):\n",
    "    # Constructs module list of layer blocks from module configuration in module_defs\n",
    "\n",
    "    img_size = [img_size] * 2 if isinstance(img_size, int) else img_size  # expand if necessary\n",
    "    _ = module_defs.pop(0)  # cfg training hyperparams (unused)\n",
    "    output_filters = [3]  # input channels\n",
    "    module_list = nn.ModuleList()\n",
    "    routs = []  # list of layers which rout to deeper layers\n",
    "    yolo_index = -1\n",
    "\n",
    "    for i, mdef in enumerate(module_defs):\n",
    "        modules = nn.Sequential()\n",
    "\n",
    "        if mdef['type'] == 'convolutional':\n",
    "            bn = mdef['batch_normalize']\n",
    "            filters = mdef['filters']\n",
    "            k = mdef['size']  # kernel size\n",
    "            stride = mdef['stride'] if 'stride' in mdef else (mdef['stride_y'], mdef['stride_x'])\n",
    "            if isinstance(k, int):  # single-size conv\n",
    "                modules.add_module('Conv2d', nn.Conv2d(in_channels=output_filters[-1],\n",
    "                                                       out_channels=filters,\n",
    "                                                       kernel_size=k,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=k // 2 if mdef['pad'] else 0,\n",
    "                                                       groups=mdef['groups'] if 'groups' in mdef else 1,\n",
    "                                                       bias=not bn))\n",
    "            else:  # multiple-size conv\n",
    "                modules.add_module('MixConv2d', MixConv2d(in_ch=output_filters[-1],\n",
    "                                                          out_ch=filters,\n",
    "                                                          k=k,\n",
    "                                                          stride=stride,\n",
    "                                                          bias=not bn))\n",
    "\n",
    "            if bn:\n",
    "                modules.add_module('BatchNorm2d', nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4))\n",
    "            else:\n",
    "                routs.append(i)  # detection output (goes into yolo layer)\n",
    "\n",
    "            if mdef['activation'] == 'leaky':  # activation study https://github.com/ultralytics/yolov3/issues/441\n",
    "                modules.add_module('activation', nn.LeakyReLU(0.1, inplace=True))\n",
    "            elif mdef['activation'] == 'swish':\n",
    "                modules.add_module('activation', Swish())\n",
    "            elif mdef['activation'] == 'mish':\n",
    "                modules.add_module('activation', Mish())\n",
    "            elif mdef['activation'] == 'relu':\n",
    "                modules.add_module('activation', nn.ReLU())\n",
    "\n",
    "        elif mdef['type'] == 'BatchNorm2d':\n",
    "            filters = output_filters[-1]\n",
    "            modules = nn.BatchNorm2d(filters, momentum=0.03, eps=1E-4)\n",
    "            if i == 0 and filters == 3:  # normalize RGB image\n",
    "                # imagenet mean and var https://pytorch.org/docs/stable/torchvision/models.html#classification\n",
    "                modules.running_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "                modules.running_var = torch.tensor([0.0524, 0.0502, 0.0506])\n",
    "\n",
    "        elif mdef['type'] == 'maxpool':\n",
    "            k = mdef['size']  # kernel size\n",
    "            stride = mdef['stride']\n",
    "            maxpool = nn.MaxPool2d(kernel_size=k, stride=stride, padding=(k - 1) // 2)\n",
    "            if k == 2 and stride == 1:  # yolov3-tiny\n",
    "                modules.add_module('ZeroPad2d', nn.ZeroPad2d((0, 1, 0, 1)))\n",
    "                modules.add_module('MaxPool2d', maxpool)\n",
    "            else:\n",
    "                modules = maxpool\n",
    "\n",
    "        elif mdef['type'] == 'upsample':\n",
    "            if ONNX_EXPORT:  # explicitly state size, avoid scale_factor\n",
    "                g = (yolo_index + 1) * 2 / 32  # gain\n",
    "                modules = nn.Upsample(size=tuple(int(x * g) for x in img_size))  # img_size = (320, 192)\n",
    "            else:\n",
    "                modules = nn.Upsample(scale_factor=mdef['stride'])\n",
    "\n",
    "        elif mdef['type'] == 'route':  # nn.Sequential() placeholder for 'route' layer\n",
    "            layers = mdef['layers']\n",
    "            filters = sum([output_filters[l + 1 if l > 0 else l] for l in layers])\n",
    "            routs.extend([i + l if l < 0 else l for l in layers])\n",
    "            modules = FeatureConcat(layers=layers)\n",
    "\n",
    "        elif mdef['type'] == 'shortcut':  # nn.Sequential() placeholder for 'shortcut' layer\n",
    "            layers = mdef['from']\n",
    "            filters = output_filters[-1]\n",
    "            routs.extend([i + l if l < 0 else l for l in layers])\n",
    "            modules = WeightedFeatureFusion(layers=layers, weight='weights_type' in mdef)\n",
    "\n",
    "        elif mdef['type'] == 'reorg3d':  # yolov3-spp-pan-scale\n",
    "            pass\n",
    "\n",
    "        elif mdef['type'] == 'yolo':\n",
    "            yolo_index += 1\n",
    "            stride = [32, 16, 8]  # P5, P4, P3 strides\n",
    "            if any(x in cfg for x in ['panet', 'yolov4', 'cd53']):  # stride order reversed\n",
    "                stride = list(reversed(stride))\n",
    "            layers = mdef['from'] if 'from' in mdef else []\n",
    "            modules = YOLOLayer(anchors=mdef['anchors'][mdef['mask']],  # anchor list\n",
    "                                nc=mdef['classes'],  # number of classes\n",
    "                                img_size=img_size,  # (416, 416)\n",
    "                                yolo_index=yolo_index,  # 0, 1, 2...\n",
    "                                layers=layers,  # output layers\n",
    "                                stride=stride[yolo_index])\n",
    "\n",
    "            # Initialize preceding Conv2d() bias (https://arxiv.org/pdf/1708.02002.pdf section 3.3)\n",
    "            try:\n",
    "                j = layers[yolo_index] if 'from' in mdef else -1\n",
    "                # If previous layer is a dropout layer, get the one before\n",
    "                if module_list[j].__class__.__name__ == 'Dropout':\n",
    "                    j -= 1\n",
    "                bias_ = module_list[j][0].bias  # shape(255,)\n",
    "                bias = bias_[:modules.no * modules.na].view(modules.na, -1)  # shape(3,85)\n",
    "                bias[:, 4] += -4.5  # obj\n",
    "                bias[:, 5:] += math.log(0.6 / (modules.nc - 0.99))  # cls (sigmoid(p) = 1/nc)\n",
    "                module_list[j][0].bias = torch.nn.Parameter(bias_, requires_grad=bias_.requires_grad)\n",
    "            except:\n",
    "                print('WARNING: smart bias initialization failure.')\n",
    "\n",
    "        elif mdef['type'] == 'dropout':\n",
    "            perc = float(mdef['probability'])\n",
    "            modules = nn.Dropout(p=perc)\n",
    "        else:\n",
    "            print('Warning: Unrecognized Layer Type: ' + mdef['type'])\n",
    "\n",
    "        # Register module list and number of output filters\n",
    "        module_list.append(modules)\n",
    "        output_filters.append(filters)\n",
    "\n",
    "    routs_binary = [False] * (i + 1)\n",
    "    for i in routs:\n",
    "        routs_binary[i] = True\n",
    "    return module_list, routs_binary\n",
    "\n",
    "\n",
    "class YOLOLayer(nn.Module):\n",
    "    def __init__(self, anchors, nc, img_size, yolo_index, layers, stride):\n",
    "        super(YOLOLayer, self).__init__()\n",
    "        self.anchors = torch.Tensor(anchors)\n",
    "        self.index = yolo_index  # index of this layer in layers\n",
    "        self.layers = layers  # model output layer indices\n",
    "        self.stride = stride  # layer stride\n",
    "        self.nl = len(layers)  # number of output layers (3)\n",
    "        self.na = len(anchors)  # number of anchors (3)\n",
    "        self.nc = nc  # number of classes (80)\n",
    "        self.no = nc + 5  # number of outputs (85)\n",
    "        self.nx, self.ny, self.ng = 0, 0, 0  # initialize number of x, y gridpoints\n",
    "        self.anchor_vec = self.anchors / self.stride\n",
    "        self.anchor_wh = self.anchor_vec.view(1, self.na, 1, 1, 2)\n",
    "\n",
    "        if ONNX_EXPORT:\n",
    "            self.training = False\n",
    "            self.create_grids((img_size[1] // stride, img_size[0] // stride))  # number x, y grid points\n",
    "\n",
    "    def create_grids(self, ng=(13, 13), device='cpu'):\n",
    "        self.nx, self.ny = ng  # x and y grid size\n",
    "        self.ng = torch.tensor(ng, dtype=torch.float)\n",
    "\n",
    "        # build xy offsets\n",
    "        if not self.training:\n",
    "            yv, xv = torch.meshgrid([torch.arange(self.ny, device=device), torch.arange(self.nx, device=device)])\n",
    "            self.grid = torch.stack((xv, yv), 2).view((1, 1, self.ny, self.nx, 2)).float()\n",
    "\n",
    "        if self.anchor_vec.device != device:\n",
    "            self.anchor_vec = self.anchor_vec.to(device)\n",
    "            self.anchor_wh = self.anchor_wh.to(device)\n",
    "\n",
    "    def forward(self, p, out):\n",
    "        ASFF = False  # https://arxiv.org/abs/1911.09516\n",
    "        if ASFF:\n",
    "            i, n = self.index, self.nl  # index in layers, number of layers\n",
    "            p = out[self.layers[i]]\n",
    "            bs, _, ny, nx = p.shape  # bs, 255, 13, 13\n",
    "            if (self.nx, self.ny) != (nx, ny):\n",
    "                self.create_grids((nx, ny), p.device)\n",
    "\n",
    "            # outputs and weights\n",
    "            # w = F.softmax(p[:, -n:], 1)  # normalized weights\n",
    "            w = torch.sigmoid(p[:, -n:]) * (2 / n)  # sigmoid weights (faster)\n",
    "            # w = w / w.sum(1).unsqueeze(1)  # normalize across layer dimension\n",
    "\n",
    "            # weighted ASFF sum\n",
    "            p = out[self.layers[i]][:, :-n] * w[:, i:i + 1]\n",
    "            for j in range(n):\n",
    "                if j != i:\n",
    "                    p += w[:, j:j + 1] * \\\n",
    "                         F.interpolate(out[self.layers[j]][:, :-n], size=[ny, nx], mode='bilinear', align_corners=False)\n",
    "\n",
    "        elif ONNX_EXPORT:\n",
    "            bs = 1  # batch size\n",
    "        else:\n",
    "            bs, _, ny, nx = p.shape  # bs, 255, 13, 13\n",
    "            if (self.nx, self.ny) != (nx, ny):\n",
    "                self.create_grids((nx, ny), p.device)\n",
    "\n",
    "        # p.view(bs, 255, 13, 13) -- > (bs, 3, 13, 13, 85)  # (bs, anchors, grid, grid, classes + xywh)\n",
    "        p = p.view(bs, self.na, self.no, self.ny, self.nx).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n",
    "\n",
    "        if self.training:\n",
    "            return p\n",
    "\n",
    "        elif ONNX_EXPORT:\n",
    "            # Avoid broadcasting for ANE operations\n",
    "            m = self.na * self.nx * self.ny\n",
    "            ng = 1. / self.ng.repeat(m, 1)\n",
    "            grid = self.grid.repeat(1, self.na, 1, 1, 1).view(m, 2)\n",
    "            anchor_wh = self.anchor_wh.repeat(1, 1, self.nx, self.ny, 1).view(m, 2) * ng\n",
    "\n",
    "            p = p.view(m, self.no)\n",
    "            xy = torch.sigmoid(p[:, 0:2]) + grid  # x, y\n",
    "            wh = torch.exp(p[:, 2:4]) * anchor_wh  # width, height\n",
    "            p_cls = torch.sigmoid(p[:, 4:5]) if self.nc == 1 else \\\n",
    "                torch.sigmoid(p[:, 5:self.no]) * torch.sigmoid(p[:, 4:5])  # conf\n",
    "            return p_cls, xy * ng, wh\n",
    "\n",
    "        else:  # inference\n",
    "            io = p.clone()  # inference output\n",
    "            io[..., :2] = torch.sigmoid(io[..., :2]) + self.grid  # xy\n",
    "            io[..., 2:4] = torch.exp(io[..., 2:4]) * self.anchor_wh  # wh yolo method\n",
    "            io[..., :4] *= self.stride\n",
    "            torch.sigmoid_(io[..., 4:])\n",
    "            return io.view(bs, -1, self.no), p  # view [1, 3, 13, 13, 85] as [1, 507, 85]\n",
    "\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    # YOLOv3 object detection model\n",
    "\n",
    "    def __init__(self, cfg, img_size=(416, 416), verbose=False):\n",
    "        super(Darknet, self).__init__()\n",
    "\n",
    "        self.module_defs = parse_model_cfg(cfg)\n",
    "        self.module_list, self.routs = create_modules(self.module_defs, img_size, cfg)\n",
    "        self.yolo_layers = get_yolo_layers(self)\n",
    "        # torch_utils.initialize_weights(self)\n",
    "\n",
    "        # Darknet Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346\n",
    "        self.version = np.array([0, 2, 5], dtype=np.int32)  # (int32) version info: major, minor, revision\n",
    "        self.seen = np.array([0], dtype=np.int64)  # (int64) number of images seen during training\n",
    "        self.info(verbose) if not ONNX_EXPORT else None  # print model description\n",
    "\n",
    "    def forward(self, x, augment=False, verbose=False):\n",
    "\n",
    "        if not augment:\n",
    "            return self.forward_once(x)\n",
    "        else:  # Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\n",
    "            img_size = x.shape[-2:]  # height, width\n",
    "            s = [0.83, 0.67]  # scales\n",
    "            y = []\n",
    "            for i, xi in enumerate((x,\n",
    "                                    torch_utils.scale_img(x.flip(3), s[0], same_shape=False),  # flip-lr and scale\n",
    "                                    torch_utils.scale_img(x, s[1], same_shape=False),  # scale\n",
    "                                    )):\n",
    "                # cv2.imwrite('img%g.jpg' % i, 255 * xi[0].numpy().transpose((1, 2, 0))[:, :, ::-1])\n",
    "                y.append(self.forward_once(xi)[0])\n",
    "\n",
    "            y[1][..., :4] /= s[0]  # scale\n",
    "            y[1][..., 0] = img_size[1] - y[1][..., 0]  # flip lr\n",
    "            y[2][..., :4] /= s[1]  # scale\n",
    "\n",
    "            # for i, yi in enumerate(y):  # coco small, medium, large = < 32**2 < 96**2 <\n",
    "            #     area = yi[..., 2:4].prod(2)[:, :, None]\n",
    "            #     if i == 1:\n",
    "            #         yi *= (area < 96. ** 2).float()\n",
    "            #     elif i == 2:\n",
    "            #         yi *= (area > 32. ** 2).float()\n",
    "            #     y[i] = yi\n",
    "\n",
    "            y = torch.cat(y, 1)\n",
    "            return y, None\n",
    "\n",
    "    def forward_once(self, x, augment=False, verbose=False):\n",
    "        img_size = x.shape[-2:]  # height, width\n",
    "        yolo_out, out = [], []\n",
    "        if verbose:\n",
    "            print('0', x.shape)\n",
    "            str = ''\n",
    "\n",
    "        # Augment images (inference and test only)\n",
    "        if augment:  # https://github.com/ultralytics/yolov3/issues/931\n",
    "            nb = x.shape[0]  # batch size\n",
    "            s = [0.83, 0.67]  # scales\n",
    "            x = torch.cat((x,\n",
    "                           torch_utils.scale_img(x.flip(3), s[0]),  # flip-lr and scale\n",
    "                           torch_utils.scale_img(x, s[1]),  # scale\n",
    "                           ), 0)\n",
    "\n",
    "        for i, module in enumerate(self.module_list):\n",
    "            name = module.__class__.__name__\n",
    "            if name in ['WeightedFeatureFusion', 'FeatureConcat']:  # sum, concat\n",
    "                if verbose:\n",
    "                    l = [i - 1] + module.layers  # layers\n",
    "                    sh = [list(x.shape)] + [list(out[i].shape) for i in module.layers]  # shapes\n",
    "                    str = ' >> ' + ' + '.join(['layer %g %s' % x for x in zip(l, sh)])\n",
    "                x = module(x, out)  # WeightedFeatureFusion(), FeatureConcat()\n",
    "            elif name == 'YOLOLayer':\n",
    "                yolo_out.append(module(x, out))\n",
    "            else:  # run module directly, i.e. mtype = 'convolutional', 'upsample', 'maxpool', 'batchnorm2d' etc.\n",
    "                x = module(x)\n",
    "\n",
    "            out.append(x if self.routs[i] else [])\n",
    "            if verbose:\n",
    "                print('%g/%g %s -' % (i, len(self.module_list), name), list(x.shape), str)\n",
    "                str = ''\n",
    "\n",
    "        if self.training:  # train\n",
    "            return yolo_out\n",
    "        elif ONNX_EXPORT:  # export\n",
    "            x = [torch.cat(x, 0) for x in zip(*yolo_out)]\n",
    "            return x[0], torch.cat(x[1:3], 1)  # scores, boxes: 3780x80, 3780x4\n",
    "        else:  # inference or test\n",
    "            x, p = zip(*yolo_out)  # inference output, training output\n",
    "            x = torch.cat(x, 1)  # cat yolo outputs\n",
    "            if augment:  # de-augment results\n",
    "                x = torch.split(x, nb, dim=0)\n",
    "                x[1][..., :4] /= s[0]  # scale\n",
    "                x[1][..., 0] = img_size[1] - x[1][..., 0]  # flip lr\n",
    "                x[2][..., :4] /= s[1]  # scale\n",
    "                x = torch.cat(x, 1)\n",
    "            return x, p\n",
    "\n",
    "    def fuse(self):\n",
    "        # Fuse Conv2d + BatchNorm2d layers throughout model\n",
    "        print('Fusing layers...')\n",
    "        fused_list = nn.ModuleList()\n",
    "        for a in list(self.children())[0]:\n",
    "            if isinstance(a, nn.Sequential):\n",
    "                for i, b in enumerate(a):\n",
    "                    if isinstance(b, nn.modules.batchnorm.BatchNorm2d):\n",
    "                        # fuse this bn layer with the previous conv2d layer\n",
    "                        conv = a[i - 1]\n",
    "                        fused = torch_utils.fuse_conv_and_bn(conv, b)\n",
    "                        a = nn.Sequential(fused, *list(a.children())[i + 1:])\n",
    "                        break\n",
    "            fused_list.append(a)\n",
    "        self.module_list = fused_list\n",
    "        self.info() if not ONNX_EXPORT else None  # yolov3-spp reduced from 225 to 152 layers\n",
    "\n",
    "    def info(self, verbose=False):\n",
    "        torch_utils.model_info(self, verbose)\n",
    "\n",
    "\n",
    "def get_yolo_layers(model):\n",
    "    return [i for i, m in enumerate(model.module_list) if m.__class__.__name__ == 'YOLOLayer']  # [89, 101, 113]\n",
    "\n",
    "\n",
    "def load_darknet_weights(self, weights, cutoff=-1):\n",
    "    # Parses and loads the weights stored in 'weights'\n",
    "\n",
    "    # Establish cutoffs (load layers between 0 and cutoff. if cutoff = -1 all are loaded)\n",
    "    file = Path(weights).name\n",
    "    if file == 'darknet53.conv.74':\n",
    "        cutoff = 75\n",
    "    elif file == 'yolov3-tiny.conv.15':\n",
    "        cutoff = 15\n",
    "\n",
    "    # Read weights file\n",
    "    with open(weights, 'rb') as f:\n",
    "        # Read Header https://github.com/AlexeyAB/darknet/issues/2914#issuecomment-496675346\n",
    "        self.version = np.fromfile(f, dtype=np.int32, count=3)  # (int32) version info: major, minor, revision\n",
    "        self.seen = np.fromfile(f, dtype=np.int64, count=1)  # (int64) number of images seen during training\n",
    "\n",
    "        weights = np.fromfile(f, dtype=np.float32)  # the rest are weights\n",
    "\n",
    "    ptr = 0\n",
    "    for i, (mdef, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
    "        if mdef['type'] == 'convolutional':\n",
    "            conv = module[0]\n",
    "            if mdef['batch_normalize']:\n",
    "                # Load BN bias, weights, running mean and running variance\n",
    "                bn = module[1]\n",
    "                nb = bn.bias.numel()  # number of biases\n",
    "                # Bias\n",
    "                bn.bias.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.bias))\n",
    "                ptr += nb\n",
    "                # Weight\n",
    "                bn.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.weight))\n",
    "                ptr += nb\n",
    "                # Running Mean\n",
    "                bn.running_mean.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.running_mean))\n",
    "                ptr += nb\n",
    "                # Running Var\n",
    "                bn.running_var.data.copy_(torch.from_numpy(weights[ptr:ptr + nb]).view_as(bn.running_var))\n",
    "                ptr += nb\n",
    "            else:\n",
    "                # Load conv. bias\n",
    "                nb = conv.bias.numel()\n",
    "                conv_b = torch.from_numpy(weights[ptr:ptr + nb]).view_as(conv.bias)\n",
    "                conv.bias.data.copy_(conv_b)\n",
    "                ptr += nb\n",
    "            # Load conv. weights\n",
    "            nw = conv.weight.numel()  # number of weights\n",
    "            conv.weight.data.copy_(torch.from_numpy(weights[ptr:ptr + nw]).view_as(conv.weight))\n",
    "            ptr += nw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n"
     ]
    }
   ],
   "source": [
    "imgsz = (320, 192)\n",
    "cfg = 'cfg/yolov3-spp.cfg'\n",
    "weights = 'weights/yolov3-spp.weights'\n",
    "device = ''\n",
    "\n",
    "# Initialize model\n",
    "model = Darknet(cfg, imgsz)\n",
    "\n",
    "# Load weights\n",
    "if weights.endswith('.pt'):  # pytorch format\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "else:  # darknet format\n",
    "    load_darknet_weights(model, weights)\n",
    "\n",
    "model.eval()\n",
    "batch_size = 1\n",
    "# Input to the model\n",
    "x = torch.randn(batch_size, 3, 320, 192, requires_grad=True)\n",
    "torch_out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3780, 85])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsz = (320, 192)\n",
    "imgsz = 320\n",
    "\n",
    "cfg = 'cfg/yolo-fastest.cfg'\n",
    "weights = 'weights/best.weights'\n",
    "device = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: 253 layers, 290066 parameters, 290066 gradients\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = Darknet(cfg, imgsz)\n",
    "\n",
    "# Load weights\n",
    "if weights.endswith('.pt'):  # pytorch format\n",
    "    model.load_state_dict(torch.load(weights, map_location=device)['model'])\n",
    "else:  # darknet format\n",
    "    load_darknet_weights(model, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Darknet(\n",
       "  (module_list): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (Conv2d): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (Conv2d): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (Conv2d): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (Conv2d): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(4, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (Conv2d): Conv2d(4, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (Conv2d): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (Conv2d): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(4, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): Dropout(p=0.15, inplace=False)\n",
       "    (8): WeightedFeatureFusion()\n",
       "    (9): Sequential(\n",
       "      (Conv2d): Conv2d(4, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (Conv2d): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (Conv2d): Conv2d(24, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (Conv2d): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (Conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (14): Sequential(\n",
       "      (Conv2d): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (15): Dropout(p=0.15, inplace=False)\n",
       "    (16): WeightedFeatureFusion()\n",
       "    (17): Sequential(\n",
       "      (Conv2d): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (18): Sequential(\n",
       "      (Conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (19): Sequential(\n",
       "      (Conv2d): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (20): Dropout(p=0.15, inplace=False)\n",
       "    (21): WeightedFeatureFusion()\n",
       "    (22): Sequential(\n",
       "      (Conv2d): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (23): Sequential(\n",
       "      (Conv2d): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(32, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (24): Sequential(\n",
       "      (Conv2d): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (25): Sequential(\n",
       "      (Conv2d): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (26): Sequential(\n",
       "      (Conv2d): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (27): Sequential(\n",
       "      (Conv2d): Conv2d(48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (28): Dropout(p=0.15, inplace=False)\n",
       "    (29): WeightedFeatureFusion()\n",
       "    (30): Sequential(\n",
       "      (Conv2d): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (31): Sequential(\n",
       "      (Conv2d): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (32): Sequential(\n",
       "      (Conv2d): Conv2d(48, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(8, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (33): Dropout(p=0.15, inplace=False)\n",
       "    (34): WeightedFeatureFusion()\n",
       "    (35): Sequential(\n",
       "      (Conv2d): Conv2d(8, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (36): Sequential(\n",
       "      (Conv2d): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (37): Sequential(\n",
       "      (Conv2d): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(16, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (38): Sequential(\n",
       "      (Conv2d): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (39): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (40): Sequential(\n",
       "      (Conv2d): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(16, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (41): Dropout(p=0.15, inplace=False)\n",
       "    (42): WeightedFeatureFusion()\n",
       "    (43): Sequential(\n",
       "      (Conv2d): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (44): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (45): Sequential(\n",
       "      (Conv2d): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(16, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (46): Dropout(p=0.15, inplace=False)\n",
       "    (47): WeightedFeatureFusion()\n",
       "    (48): Sequential(\n",
       "      (Conv2d): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (49): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (50): Sequential(\n",
       "      (Conv2d): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(16, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (51): Dropout(p=0.15, inplace=False)\n",
       "    (52): WeightedFeatureFusion()\n",
       "    (53): Sequential(\n",
       "      (Conv2d): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (54): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (55): Sequential(\n",
       "      (Conv2d): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(16, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (56): Dropout(p=0.15, inplace=False)\n",
       "    (57): WeightedFeatureFusion()\n",
       "    (58): Sequential(\n",
       "      (Conv2d): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (59): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (60): Sequential(\n",
       "      (Conv2d): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (61): Sequential(\n",
       "      (Conv2d): Conv2d(24, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (62): Sequential(\n",
       "      (Conv2d): Conv2d(136, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=136, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (63): Sequential(\n",
       "      (Conv2d): Conv2d(136, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (64): Dropout(p=0.15, inplace=False)\n",
       "    (65): WeightedFeatureFusion()\n",
       "    (66): Sequential(\n",
       "      (Conv2d): Conv2d(24, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (67): Sequential(\n",
       "      (Conv2d): Conv2d(136, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=136, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (68): Sequential(\n",
       "      (Conv2d): Conv2d(136, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (69): Dropout(p=0.15, inplace=False)\n",
       "    (70): WeightedFeatureFusion()\n",
       "    (71): Sequential(\n",
       "      (Conv2d): Conv2d(24, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (72): Sequential(\n",
       "      (Conv2d): Conv2d(136, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=136, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (73): Sequential(\n",
       "      (Conv2d): Conv2d(136, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (74): Dropout(p=0.15, inplace=False)\n",
       "    (75): WeightedFeatureFusion()\n",
       "    (76): Sequential(\n",
       "      (Conv2d): Conv2d(24, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (77): Sequential(\n",
       "      (Conv2d): Conv2d(136, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=136, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (78): Sequential(\n",
       "      (Conv2d): Conv2d(136, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(24, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (79): Dropout(p=0.15, inplace=False)\n",
       "    (80): WeightedFeatureFusion()\n",
       "    (81): Sequential(\n",
       "      (Conv2d): Conv2d(24, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (82): Sequential(\n",
       "      (Conv2d): Conv2d(136, 136, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=136, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(136, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (83): Sequential(\n",
       "      (Conv2d): Conv2d(136, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (84): Sequential(\n",
       "      (Conv2d): Conv2d(48, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (85): Sequential(\n",
       "      (Conv2d): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (86): Sequential(\n",
       "      (Conv2d): Conv2d(224, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (87): Dropout(p=0.15, inplace=False)\n",
       "    (88): WeightedFeatureFusion()\n",
       "    (89): Sequential(\n",
       "      (Conv2d): Conv2d(48, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (90): Sequential(\n",
       "      (Conv2d): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (91): Sequential(\n",
       "      (Conv2d): Conv2d(224, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (92): Dropout(p=0.15, inplace=False)\n",
       "    (93): WeightedFeatureFusion()\n",
       "    (94): Sequential(\n",
       "      (Conv2d): Conv2d(48, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (95): Sequential(\n",
       "      (Conv2d): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (96): Sequential(\n",
       "      (Conv2d): Conv2d(224, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (97): Dropout(p=0.15, inplace=False)\n",
       "    (98): WeightedFeatureFusion()\n",
       "    (99): Sequential(\n",
       "      (Conv2d): Conv2d(48, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (100): Sequential(\n",
       "      (Conv2d): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (101): Sequential(\n",
       "      (Conv2d): Conv2d(224, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (102): Dropout(p=0.15, inplace=False)\n",
       "    (103): WeightedFeatureFusion()\n",
       "    (104): Sequential(\n",
       "      (Conv2d): Conv2d(48, 224, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (105): Sequential(\n",
       "      (Conv2d): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(224, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (106): Sequential(\n",
       "      (Conv2d): Conv2d(224, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(48, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (107): Dropout(p=0.15, inplace=False)\n",
       "    (108): WeightedFeatureFusion()\n",
       "    (109): Sequential(\n",
       "      (Conv2d): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (110): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (111): Sequential(\n",
       "      (Conv2d): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (112): Sequential(\n",
       "      (Conv2d): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=128, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (113): Sequential(\n",
       "      (Conv2d): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(128, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (114): Sequential(\n",
       "      (Conv2d): Conv2d(128, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (115): YOLOLayer()\n",
       "    (116): FeatureConcat()\n",
       "    (117): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (118): FeatureConcat()\n",
       "    (119): Sequential(\n",
       "      (Conv2d): Conv2d(232, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (120): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (121): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (122): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "      (activation): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (123): Sequential(\n",
       "      (Conv2d): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (BatchNorm2d): BatchNorm2d(96, eps=0.0001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (124): Sequential(\n",
       "      (Conv2d): Conv2d(96, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (125): YOLOLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waittim/Documents/Python/Personal/mask-detector/modeling/utils/layers.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if nx == na:  # same shape\n",
      "/Users/waittim/Documents/Python/Personal/mask-detector/modeling/models.py:195: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (self.nx, self.ny) != (nx, ny):\n",
      "/Users/waittim/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py:955: UserWarning: Provided key input for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n",
      "/Users/waittim/anaconda3/lib/python3.7/site-packages/torch/onnx/utils.py:955: UserWarning: Provided key output for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "# Input to the model\n",
    "x = torch.randn(batch_size, 3, 320, 512, requires_grad=True)\n",
    "torch_out = model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"cfg/yolo-fastest.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['images'],   # the model's input names\n",
    "                  output_names = ['classes','boxes'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                'output' : {0 : 'batch_size'}}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"yolo-fastest.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %images[FLOAT, 1x3x320x192]\n",
      ") initializers (\n",
      "  %1536[INT64, 1]\n",
      "  %1537[INT64, 1]\n",
      "  %1538[INT64, 1]\n",
      "  %1539[INT64, 1]\n",
      "  %1540[INT64, 1]\n",
      "  %1541[INT64, 1]\n",
      "  %1542[INT64, 1]\n",
      "  %1543[INT64, 1]\n",
      "  %1544[INT64, 1]\n",
      "  %1549[FLOAT, 4]\n",
      "  %1550[INT64, 1]\n",
      "  %1551[INT64, 1]\n",
      "  %1552[INT64, 1]\n",
      "  %1553[INT64, 1]\n",
      "  %1554[INT64, 1]\n",
      "  %1555[INT64, 1]\n",
      "  %1556[INT64, 1]\n",
      "  %1557[INT64, 1]\n",
      "  %1558[INT64, 1]\n",
      "  %module_list.0.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.0.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.0.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.0.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.0.Conv2d.weight[FLOAT, 8x3x3x3]\n",
      "  %module_list.1.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.1.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.1.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.1.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.1.Conv2d.weight[FLOAT, 8x8x1x1]\n",
      "  %module_list.10.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.10.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.10.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.10.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.10.Conv2d.weight[FLOAT, 24x1x3x3]\n",
      "  %module_list.100.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.100.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.100.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.100.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.100.Conv2d.weight[FLOAT, 224x1x3x3]\n",
      "  %module_list.101.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.101.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.101.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.101.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.101.Conv2d.weight[FLOAT, 48x224x1x1]\n",
      "  %module_list.104.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.104.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.104.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.104.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.104.Conv2d.weight[FLOAT, 224x48x1x1]\n",
      "  %module_list.105.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.105.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.105.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.105.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.105.Conv2d.weight[FLOAT, 224x1x3x3]\n",
      "  %module_list.106.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.106.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.106.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.106.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.106.Conv2d.weight[FLOAT, 48x224x1x1]\n",
      "  %module_list.109.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.109.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.109.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.109.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.109.Conv2d.weight[FLOAT, 96x48x1x1]\n",
      "  %module_list.11.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.11.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.11.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.11.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.11.Conv2d.weight[FLOAT, 8x24x1x1]\n",
      "  %module_list.110.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.110.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.110.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.110.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.110.Conv2d.weight[FLOAT, 96x1x5x5]\n",
      "  %module_list.111.BatchNorm2d.bias[FLOAT, 128]\n",
      "  %module_list.111.BatchNorm2d.running_mean[FLOAT, 128]\n",
      "  %module_list.111.BatchNorm2d.running_var[FLOAT, 128]\n",
      "  %module_list.111.BatchNorm2d.weight[FLOAT, 128]\n",
      "  %module_list.111.Conv2d.weight[FLOAT, 128x96x1x1]\n",
      "  %module_list.112.BatchNorm2d.bias[FLOAT, 128]\n",
      "  %module_list.112.BatchNorm2d.running_mean[FLOAT, 128]\n",
      "  %module_list.112.BatchNorm2d.running_var[FLOAT, 128]\n",
      "  %module_list.112.BatchNorm2d.weight[FLOAT, 128]\n",
      "  %module_list.112.Conv2d.weight[FLOAT, 128x1x5x5]\n",
      "  %module_list.113.BatchNorm2d.bias[FLOAT, 128]\n",
      "  %module_list.113.BatchNorm2d.running_mean[FLOAT, 128]\n",
      "  %module_list.113.BatchNorm2d.running_var[FLOAT, 128]\n",
      "  %module_list.113.BatchNorm2d.weight[FLOAT, 128]\n",
      "  %module_list.113.Conv2d.weight[FLOAT, 128x128x1x1]\n",
      "  %module_list.114.Conv2d.bias[FLOAT, 21]\n",
      "  %module_list.114.Conv2d.weight[FLOAT, 21x128x1x1]\n",
      "  %module_list.119.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.119.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.119.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.119.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.119.Conv2d.weight[FLOAT, 96x232x1x1]\n",
      "  %module_list.12.BatchNorm2d.bias[FLOAT, 32]\n",
      "  %module_list.12.BatchNorm2d.running_mean[FLOAT, 32]\n",
      "  %module_list.12.BatchNorm2d.running_var[FLOAT, 32]\n",
      "  %module_list.12.BatchNorm2d.weight[FLOAT, 32]\n",
      "  %module_list.12.Conv2d.weight[FLOAT, 32x8x1x1]\n",
      "  %module_list.120.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.120.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.120.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.120.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.120.Conv2d.weight[FLOAT, 96x1x5x5]\n",
      "  %module_list.121.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.121.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.121.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.121.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.121.Conv2d.weight[FLOAT, 96x96x1x1]\n",
      "  %module_list.122.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.122.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.122.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.122.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.122.Conv2d.weight[FLOAT, 96x1x5x5]\n",
      "  %module_list.123.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.123.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.123.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.123.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.123.Conv2d.weight[FLOAT, 96x96x1x1]\n",
      "  %module_list.124.Conv2d.bias[FLOAT, 21]\n",
      "  %module_list.124.Conv2d.weight[FLOAT, 21x96x1x1]\n",
      "  %module_list.13.BatchNorm2d.bias[FLOAT, 32]\n",
      "  %module_list.13.BatchNorm2d.running_mean[FLOAT, 32]\n",
      "  %module_list.13.BatchNorm2d.running_var[FLOAT, 32]\n",
      "  %module_list.13.BatchNorm2d.weight[FLOAT, 32]\n",
      "  %module_list.13.Conv2d.weight[FLOAT, 32x1x3x3]\n",
      "  %module_list.14.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.14.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.14.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.14.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.14.Conv2d.weight[FLOAT, 8x32x1x1]\n",
      "  %module_list.17.BatchNorm2d.bias[FLOAT, 32]\n",
      "  %module_list.17.BatchNorm2d.running_mean[FLOAT, 32]\n",
      "  %module_list.17.BatchNorm2d.running_var[FLOAT, 32]\n",
      "  %module_list.17.BatchNorm2d.weight[FLOAT, 32]\n",
      "  %module_list.17.Conv2d.weight[FLOAT, 32x8x1x1]\n",
      "  %module_list.18.BatchNorm2d.bias[FLOAT, 32]\n",
      "  %module_list.18.BatchNorm2d.running_mean[FLOAT, 32]\n",
      "  %module_list.18.BatchNorm2d.running_var[FLOAT, 32]\n",
      "  %module_list.18.BatchNorm2d.weight[FLOAT, 32]\n",
      "  %module_list.18.Conv2d.weight[FLOAT, 32x1x3x3]\n",
      "  %module_list.19.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.19.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.19.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.19.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.19.Conv2d.weight[FLOAT, 8x32x1x1]\n",
      "  %module_list.2.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.2.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.2.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.2.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.2.Conv2d.weight[FLOAT, 8x1x3x3]\n",
      "  %module_list.22.BatchNorm2d.bias[FLOAT, 32]\n",
      "  %module_list.22.BatchNorm2d.running_mean[FLOAT, 32]\n",
      "  %module_list.22.BatchNorm2d.running_var[FLOAT, 32]\n",
      "  %module_list.22.BatchNorm2d.weight[FLOAT, 32]\n",
      "  %module_list.22.Conv2d.weight[FLOAT, 32x8x1x1]\n",
      "  %module_list.23.BatchNorm2d.bias[FLOAT, 32]\n",
      "  %module_list.23.BatchNorm2d.running_mean[FLOAT, 32]\n",
      "  %module_list.23.BatchNorm2d.running_var[FLOAT, 32]\n",
      "  %module_list.23.BatchNorm2d.weight[FLOAT, 32]\n",
      "  %module_list.23.Conv2d.weight[FLOAT, 32x1x3x3]\n",
      "  %module_list.24.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.24.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.24.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.24.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.24.Conv2d.weight[FLOAT, 8x32x1x1]\n",
      "  %module_list.25.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.25.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.25.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.25.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.25.Conv2d.weight[FLOAT, 48x8x1x1]\n",
      "  %module_list.26.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.26.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.26.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.26.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.26.Conv2d.weight[FLOAT, 48x1x3x3]\n",
      "  %module_list.27.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.27.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.27.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.27.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.27.Conv2d.weight[FLOAT, 8x48x1x1]\n",
      "  %module_list.3.BatchNorm2d.bias[FLOAT, 4]\n",
      "  %module_list.3.BatchNorm2d.running_mean[FLOAT, 4]\n",
      "  %module_list.3.BatchNorm2d.running_var[FLOAT, 4]\n",
      "  %module_list.3.BatchNorm2d.weight[FLOAT, 4]\n",
      "  %module_list.3.Conv2d.weight[FLOAT, 4x8x1x1]\n",
      "  %module_list.30.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.30.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.30.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.30.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.30.Conv2d.weight[FLOAT, 48x8x1x1]\n",
      "  %module_list.31.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.31.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.31.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.31.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.31.Conv2d.weight[FLOAT, 48x1x3x3]\n",
      "  %module_list.32.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.32.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.32.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.32.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.32.Conv2d.weight[FLOAT, 8x48x1x1]\n",
      "  %module_list.35.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.35.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.35.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.35.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.35.Conv2d.weight[FLOAT, 48x8x1x1]\n",
      "  %module_list.36.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.36.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.36.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.36.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.36.Conv2d.weight[FLOAT, 48x1x3x3]\n",
      "  %module_list.37.BatchNorm2d.bias[FLOAT, 16]\n",
      "  %module_list.37.BatchNorm2d.running_mean[FLOAT, 16]\n",
      "  %module_list.37.BatchNorm2d.running_var[FLOAT, 16]\n",
      "  %module_list.37.BatchNorm2d.weight[FLOAT, 16]\n",
      "  %module_list.37.Conv2d.weight[FLOAT, 16x48x1x1]\n",
      "  %module_list.38.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.38.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.38.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.38.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.38.Conv2d.weight[FLOAT, 96x16x1x1]\n",
      "  %module_list.39.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.39.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.39.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.39.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.39.Conv2d.weight[FLOAT, 96x1x3x3]\n",
      "  %module_list.4.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.4.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.4.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.4.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.4.Conv2d.weight[FLOAT, 8x4x1x1]\n",
      "  %module_list.40.BatchNorm2d.bias[FLOAT, 16]\n",
      "  %module_list.40.BatchNorm2d.running_mean[FLOAT, 16]\n",
      "  %module_list.40.BatchNorm2d.running_var[FLOAT, 16]\n",
      "  %module_list.40.BatchNorm2d.weight[FLOAT, 16]\n",
      "  %module_list.40.Conv2d.weight[FLOAT, 16x96x1x1]\n",
      "  %module_list.43.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.43.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.43.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.43.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.43.Conv2d.weight[FLOAT, 96x16x1x1]\n",
      "  %module_list.44.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.44.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.44.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.44.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.44.Conv2d.weight[FLOAT, 96x1x3x3]\n",
      "  %module_list.45.BatchNorm2d.bias[FLOAT, 16]\n",
      "  %module_list.45.BatchNorm2d.running_mean[FLOAT, 16]\n",
      "  %module_list.45.BatchNorm2d.running_var[FLOAT, 16]\n",
      "  %module_list.45.BatchNorm2d.weight[FLOAT, 16]\n",
      "  %module_list.45.Conv2d.weight[FLOAT, 16x96x1x1]\n",
      "  %module_list.48.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.48.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.48.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.48.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.48.Conv2d.weight[FLOAT, 96x16x1x1]\n",
      "  %module_list.49.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.49.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.49.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.49.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.49.Conv2d.weight[FLOAT, 96x1x3x3]\n",
      "  %module_list.5.BatchNorm2d.bias[FLOAT, 8]\n",
      "  %module_list.5.BatchNorm2d.running_mean[FLOAT, 8]\n",
      "  %module_list.5.BatchNorm2d.running_var[FLOAT, 8]\n",
      "  %module_list.5.BatchNorm2d.weight[FLOAT, 8]\n",
      "  %module_list.5.Conv2d.weight[FLOAT, 8x1x3x3]\n",
      "  %module_list.50.BatchNorm2d.bias[FLOAT, 16]\n",
      "  %module_list.50.BatchNorm2d.running_mean[FLOAT, 16]\n",
      "  %module_list.50.BatchNorm2d.running_var[FLOAT, 16]\n",
      "  %module_list.50.BatchNorm2d.weight[FLOAT, 16]\n",
      "  %module_list.50.Conv2d.weight[FLOAT, 16x96x1x1]\n",
      "  %module_list.53.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.53.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.53.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.53.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.53.Conv2d.weight[FLOAT, 96x16x1x1]\n",
      "  %module_list.54.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.54.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.54.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.54.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.54.Conv2d.weight[FLOAT, 96x1x3x3]\n",
      "  %module_list.55.BatchNorm2d.bias[FLOAT, 16]\n",
      "  %module_list.55.BatchNorm2d.running_mean[FLOAT, 16]\n",
      "  %module_list.55.BatchNorm2d.running_var[FLOAT, 16]\n",
      "  %module_list.55.BatchNorm2d.weight[FLOAT, 16]\n",
      "  %module_list.55.Conv2d.weight[FLOAT, 16x96x1x1]\n",
      "  %module_list.58.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.58.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.58.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.58.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.58.Conv2d.weight[FLOAT, 96x16x1x1]\n",
      "  %module_list.59.BatchNorm2d.bias[FLOAT, 96]\n",
      "  %module_list.59.BatchNorm2d.running_mean[FLOAT, 96]\n",
      "  %module_list.59.BatchNorm2d.running_var[FLOAT, 96]\n",
      "  %module_list.59.BatchNorm2d.weight[FLOAT, 96]\n",
      "  %module_list.59.Conv2d.weight[FLOAT, 96x1x3x3]\n",
      "  %module_list.6.BatchNorm2d.bias[FLOAT, 4]\n",
      "  %module_list.6.BatchNorm2d.running_mean[FLOAT, 4]\n",
      "  %module_list.6.BatchNorm2d.running_var[FLOAT, 4]\n",
      "  %module_list.6.BatchNorm2d.weight[FLOAT, 4]\n",
      "  %module_list.6.Conv2d.weight[FLOAT, 4x8x1x1]\n",
      "  %module_list.60.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.60.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.60.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.60.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.60.Conv2d.weight[FLOAT, 24x96x1x1]\n",
      "  %module_list.61.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.61.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.61.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.61.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.61.Conv2d.weight[FLOAT, 136x24x1x1]\n",
      "  %module_list.62.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.62.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.62.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.62.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.62.Conv2d.weight[FLOAT, 136x1x3x3]\n",
      "  %module_list.63.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.63.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.63.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.63.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.63.Conv2d.weight[FLOAT, 24x136x1x1]\n",
      "  %module_list.66.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.66.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.66.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.66.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.66.Conv2d.weight[FLOAT, 136x24x1x1]\n",
      "  %module_list.67.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.67.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.67.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.67.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.67.Conv2d.weight[FLOAT, 136x1x3x3]\n",
      "  %module_list.68.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.68.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.68.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.68.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.68.Conv2d.weight[FLOAT, 24x136x1x1]\n",
      "  %module_list.71.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.71.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.71.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.71.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.71.Conv2d.weight[FLOAT, 136x24x1x1]\n",
      "  %module_list.72.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.72.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.72.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.72.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.72.Conv2d.weight[FLOAT, 136x1x3x3]\n",
      "  %module_list.73.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.73.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.73.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.73.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.73.Conv2d.weight[FLOAT, 24x136x1x1]\n",
      "  %module_list.76.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.76.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.76.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.76.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.76.Conv2d.weight[FLOAT, 136x24x1x1]\n",
      "  %module_list.77.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.77.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.77.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.77.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.77.Conv2d.weight[FLOAT, 136x1x3x3]\n",
      "  %module_list.78.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.78.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.78.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.78.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.78.Conv2d.weight[FLOAT, 24x136x1x1]\n",
      "  %module_list.81.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.81.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.81.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.81.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.81.Conv2d.weight[FLOAT, 136x24x1x1]\n",
      "  %module_list.82.BatchNorm2d.bias[FLOAT, 136]\n",
      "  %module_list.82.BatchNorm2d.running_mean[FLOAT, 136]\n",
      "  %module_list.82.BatchNorm2d.running_var[FLOAT, 136]\n",
      "  %module_list.82.BatchNorm2d.weight[FLOAT, 136]\n",
      "  %module_list.82.Conv2d.weight[FLOAT, 136x1x3x3]\n",
      "  %module_list.83.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.83.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.83.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.83.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.83.Conv2d.weight[FLOAT, 48x136x1x1]\n",
      "  %module_list.84.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.84.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.84.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.84.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.84.Conv2d.weight[FLOAT, 224x48x1x1]\n",
      "  %module_list.85.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.85.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.85.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.85.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.85.Conv2d.weight[FLOAT, 224x1x3x3]\n",
      "  %module_list.86.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.86.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.86.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.86.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.86.Conv2d.weight[FLOAT, 48x224x1x1]\n",
      "  %module_list.89.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.89.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.89.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.89.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.89.Conv2d.weight[FLOAT, 224x48x1x1]\n",
      "  %module_list.9.BatchNorm2d.bias[FLOAT, 24]\n",
      "  %module_list.9.BatchNorm2d.running_mean[FLOAT, 24]\n",
      "  %module_list.9.BatchNorm2d.running_var[FLOAT, 24]\n",
      "  %module_list.9.BatchNorm2d.weight[FLOAT, 24]\n",
      "  %module_list.9.Conv2d.weight[FLOAT, 24x4x1x1]\n",
      "  %module_list.90.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.90.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.90.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.90.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.90.Conv2d.weight[FLOAT, 224x1x3x3]\n",
      "  %module_list.91.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.91.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.91.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.91.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.91.Conv2d.weight[FLOAT, 48x224x1x1]\n",
      "  %module_list.94.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.94.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.94.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.94.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.94.Conv2d.weight[FLOAT, 224x48x1x1]\n",
      "  %module_list.95.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.95.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.95.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.95.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.95.Conv2d.weight[FLOAT, 224x1x3x3]\n",
      "  %module_list.96.BatchNorm2d.bias[FLOAT, 48]\n",
      "  %module_list.96.BatchNorm2d.running_mean[FLOAT, 48]\n",
      "  %module_list.96.BatchNorm2d.running_var[FLOAT, 48]\n",
      "  %module_list.96.BatchNorm2d.weight[FLOAT, 48]\n",
      "  %module_list.96.Conv2d.weight[FLOAT, 48x224x1x1]\n",
      "  %module_list.99.BatchNorm2d.bias[FLOAT, 224]\n",
      "  %module_list.99.BatchNorm2d.running_mean[FLOAT, 224]\n",
      "  %module_list.99.BatchNorm2d.running_var[FLOAT, 224]\n",
      "  %module_list.99.BatchNorm2d.weight[FLOAT, 224]\n",
      "  %module_list.99.Conv2d.weight[FLOAT, 224x48x1x1]\n",
      ") {\n",
      "  %503 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%images, %module_list.0.Conv2d.weight)\n",
      "  %504 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%503, %module_list.0.BatchNorm2d.weight, %module_list.0.BatchNorm2d.bias, %module_list.0.BatchNorm2d.running_mean, %module_list.0.BatchNorm2d.running_var)\n",
      "  %505 = LeakyRelu[alpha = 0.100000001490116](%504)\n",
      "  %506 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%505, %module_list.1.Conv2d.weight)\n",
      "  %507 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%506, %module_list.1.BatchNorm2d.weight, %module_list.1.BatchNorm2d.bias, %module_list.1.BatchNorm2d.running_mean, %module_list.1.BatchNorm2d.running_var)\n",
      "  %508 = LeakyRelu[alpha = 0.100000001490116](%507)\n",
      "  %509 = Conv[dilations = [1, 1], group = 8, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%508, %module_list.2.Conv2d.weight)\n",
      "  %510 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%509, %module_list.2.BatchNorm2d.weight, %module_list.2.BatchNorm2d.bias, %module_list.2.BatchNorm2d.running_mean, %module_list.2.BatchNorm2d.running_var)\n",
      "  %511 = LeakyRelu[alpha = 0.100000001490116](%510)\n",
      "  %512 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%511, %module_list.3.Conv2d.weight)\n",
      "  %513 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%512, %module_list.3.BatchNorm2d.weight, %module_list.3.BatchNorm2d.bias, %module_list.3.BatchNorm2d.running_mean, %module_list.3.BatchNorm2d.running_var)\n",
      "  %514 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%513, %module_list.4.Conv2d.weight)\n",
      "  %515 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%514, %module_list.4.BatchNorm2d.weight, %module_list.4.BatchNorm2d.bias, %module_list.4.BatchNorm2d.running_mean, %module_list.4.BatchNorm2d.running_var)\n",
      "  %516 = LeakyRelu[alpha = 0.100000001490116](%515)\n",
      "  %517 = Conv[dilations = [1, 1], group = 8, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%516, %module_list.5.Conv2d.weight)\n",
      "  %518 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%517, %module_list.5.BatchNorm2d.weight, %module_list.5.BatchNorm2d.bias, %module_list.5.BatchNorm2d.running_mean, %module_list.5.BatchNorm2d.running_var)\n",
      "  %519 = LeakyRelu[alpha = 0.100000001490116](%518)\n",
      "  %520 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%519, %module_list.6.Conv2d.weight)\n",
      "  %521 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%520, %module_list.6.BatchNorm2d.weight, %module_list.6.BatchNorm2d.bias, %module_list.6.BatchNorm2d.running_mean, %module_list.6.BatchNorm2d.running_var)\n",
      "  %522 = Add(%521, %513)\n",
      "  %523 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%522, %module_list.9.Conv2d.weight)\n",
      "  %524 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%523, %module_list.9.BatchNorm2d.weight, %module_list.9.BatchNorm2d.bias, %module_list.9.BatchNorm2d.running_mean, %module_list.9.BatchNorm2d.running_var)\n",
      "  %525 = LeakyRelu[alpha = 0.100000001490116](%524)\n",
      "  %526 = Conv[dilations = [1, 1], group = 24, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%525, %module_list.10.Conv2d.weight)\n",
      "  %527 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%526, %module_list.10.BatchNorm2d.weight, %module_list.10.BatchNorm2d.bias, %module_list.10.BatchNorm2d.running_mean, %module_list.10.BatchNorm2d.running_var)\n",
      "  %528 = LeakyRelu[alpha = 0.100000001490116](%527)\n",
      "  %529 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%528, %module_list.11.Conv2d.weight)\n",
      "  %530 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%529, %module_list.11.BatchNorm2d.weight, %module_list.11.BatchNorm2d.bias, %module_list.11.BatchNorm2d.running_mean, %module_list.11.BatchNorm2d.running_var)\n",
      "  %531 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%530, %module_list.12.Conv2d.weight)\n",
      "  %532 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%531, %module_list.12.BatchNorm2d.weight, %module_list.12.BatchNorm2d.bias, %module_list.12.BatchNorm2d.running_mean, %module_list.12.BatchNorm2d.running_var)\n",
      "  %533 = LeakyRelu[alpha = 0.100000001490116](%532)\n",
      "  %534 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%533, %module_list.13.Conv2d.weight)\n",
      "  %535 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%534, %module_list.13.BatchNorm2d.weight, %module_list.13.BatchNorm2d.bias, %module_list.13.BatchNorm2d.running_mean, %module_list.13.BatchNorm2d.running_var)\n",
      "  %536 = LeakyRelu[alpha = 0.100000001490116](%535)\n",
      "  %537 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%536, %module_list.14.Conv2d.weight)\n",
      "  %538 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%537, %module_list.14.BatchNorm2d.weight, %module_list.14.BatchNorm2d.bias, %module_list.14.BatchNorm2d.running_mean, %module_list.14.BatchNorm2d.running_var)\n",
      "  %539 = Add(%538, %530)\n",
      "  %540 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%539, %module_list.17.Conv2d.weight)\n",
      "  %541 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%540, %module_list.17.BatchNorm2d.weight, %module_list.17.BatchNorm2d.bias, %module_list.17.BatchNorm2d.running_mean, %module_list.17.BatchNorm2d.running_var)\n",
      "  %542 = LeakyRelu[alpha = 0.100000001490116](%541)\n",
      "  %543 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%542, %module_list.18.Conv2d.weight)\n",
      "  %544 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%543, %module_list.18.BatchNorm2d.weight, %module_list.18.BatchNorm2d.bias, %module_list.18.BatchNorm2d.running_mean, %module_list.18.BatchNorm2d.running_var)\n",
      "  %545 = LeakyRelu[alpha = 0.100000001490116](%544)\n",
      "  %546 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%545, %module_list.19.Conv2d.weight)\n",
      "  %547 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%546, %module_list.19.BatchNorm2d.weight, %module_list.19.BatchNorm2d.bias, %module_list.19.BatchNorm2d.running_mean, %module_list.19.BatchNorm2d.running_var)\n",
      "  %548 = Add(%547, %539)\n",
      "  %549 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%548, %module_list.22.Conv2d.weight)\n",
      "  %550 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%549, %module_list.22.BatchNorm2d.weight, %module_list.22.BatchNorm2d.bias, %module_list.22.BatchNorm2d.running_mean, %module_list.22.BatchNorm2d.running_var)\n",
      "  %551 = LeakyRelu[alpha = 0.100000001490116](%550)\n",
      "  %552 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%551, %module_list.23.Conv2d.weight)\n",
      "  %553 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%552, %module_list.23.BatchNorm2d.weight, %module_list.23.BatchNorm2d.bias, %module_list.23.BatchNorm2d.running_mean, %module_list.23.BatchNorm2d.running_var)\n",
      "  %554 = LeakyRelu[alpha = 0.100000001490116](%553)\n",
      "  %555 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%554, %module_list.24.Conv2d.weight)\n",
      "  %556 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%555, %module_list.24.BatchNorm2d.weight, %module_list.24.BatchNorm2d.bias, %module_list.24.BatchNorm2d.running_mean, %module_list.24.BatchNorm2d.running_var)\n",
      "  %557 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%556, %module_list.25.Conv2d.weight)\n",
      "  %558 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%557, %module_list.25.BatchNorm2d.weight, %module_list.25.BatchNorm2d.bias, %module_list.25.BatchNorm2d.running_mean, %module_list.25.BatchNorm2d.running_var)\n",
      "  %559 = LeakyRelu[alpha = 0.100000001490116](%558)\n",
      "  %560 = Conv[dilations = [1, 1], group = 48, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%559, %module_list.26.Conv2d.weight)\n",
      "  %561 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%560, %module_list.26.BatchNorm2d.weight, %module_list.26.BatchNorm2d.bias, %module_list.26.BatchNorm2d.running_mean, %module_list.26.BatchNorm2d.running_var)\n",
      "  %562 = LeakyRelu[alpha = 0.100000001490116](%561)\n",
      "  %563 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%562, %module_list.27.Conv2d.weight)\n",
      "  %564 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%563, %module_list.27.BatchNorm2d.weight, %module_list.27.BatchNorm2d.bias, %module_list.27.BatchNorm2d.running_mean, %module_list.27.BatchNorm2d.running_var)\n",
      "  %565 = Add(%564, %556)\n",
      "  %566 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%565, %module_list.30.Conv2d.weight)\n",
      "  %567 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%566, %module_list.30.BatchNorm2d.weight, %module_list.30.BatchNorm2d.bias, %module_list.30.BatchNorm2d.running_mean, %module_list.30.BatchNorm2d.running_var)\n",
      "  %568 = LeakyRelu[alpha = 0.100000001490116](%567)\n",
      "  %569 = Conv[dilations = [1, 1], group = 48, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%568, %module_list.31.Conv2d.weight)\n",
      "  %570 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%569, %module_list.31.BatchNorm2d.weight, %module_list.31.BatchNorm2d.bias, %module_list.31.BatchNorm2d.running_mean, %module_list.31.BatchNorm2d.running_var)\n",
      "  %571 = LeakyRelu[alpha = 0.100000001490116](%570)\n",
      "  %572 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%571, %module_list.32.Conv2d.weight)\n",
      "  %573 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%572, %module_list.32.BatchNorm2d.weight, %module_list.32.BatchNorm2d.bias, %module_list.32.BatchNorm2d.running_mean, %module_list.32.BatchNorm2d.running_var)\n",
      "  %574 = Add(%573, %565)\n",
      "  %575 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%574, %module_list.35.Conv2d.weight)\n",
      "  %576 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%575, %module_list.35.BatchNorm2d.weight, %module_list.35.BatchNorm2d.bias, %module_list.35.BatchNorm2d.running_mean, %module_list.35.BatchNorm2d.running_var)\n",
      "  %577 = LeakyRelu[alpha = 0.100000001490116](%576)\n",
      "  %578 = Conv[dilations = [1, 1], group = 48, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%577, %module_list.36.Conv2d.weight)\n",
      "  %579 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%578, %module_list.36.BatchNorm2d.weight, %module_list.36.BatchNorm2d.bias, %module_list.36.BatchNorm2d.running_mean, %module_list.36.BatchNorm2d.running_var)\n",
      "  %580 = LeakyRelu[alpha = 0.100000001490116](%579)\n",
      "  %581 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%580, %module_list.37.Conv2d.weight)\n",
      "  %582 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%581, %module_list.37.BatchNorm2d.weight, %module_list.37.BatchNorm2d.bias, %module_list.37.BatchNorm2d.running_mean, %module_list.37.BatchNorm2d.running_var)\n",
      "  %583 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%582, %module_list.38.Conv2d.weight)\n",
      "  %584 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%583, %module_list.38.BatchNorm2d.weight, %module_list.38.BatchNorm2d.bias, %module_list.38.BatchNorm2d.running_mean, %module_list.38.BatchNorm2d.running_var)\n",
      "  %585 = LeakyRelu[alpha = 0.100000001490116](%584)\n",
      "  %586 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%585, %module_list.39.Conv2d.weight)\n",
      "  %587 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%586, %module_list.39.BatchNorm2d.weight, %module_list.39.BatchNorm2d.bias, %module_list.39.BatchNorm2d.running_mean, %module_list.39.BatchNorm2d.running_var)\n",
      "  %588 = LeakyRelu[alpha = 0.100000001490116](%587)\n",
      "  %589 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%588, %module_list.40.Conv2d.weight)\n",
      "  %590 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%589, %module_list.40.BatchNorm2d.weight, %module_list.40.BatchNorm2d.bias, %module_list.40.BatchNorm2d.running_mean, %module_list.40.BatchNorm2d.running_var)\n",
      "  %591 = Add(%590, %582)\n",
      "  %592 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%591, %module_list.43.Conv2d.weight)\n",
      "  %593 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%592, %module_list.43.BatchNorm2d.weight, %module_list.43.BatchNorm2d.bias, %module_list.43.BatchNorm2d.running_mean, %module_list.43.BatchNorm2d.running_var)\n",
      "  %594 = LeakyRelu[alpha = 0.100000001490116](%593)\n",
      "  %595 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%594, %module_list.44.Conv2d.weight)\n",
      "  %596 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%595, %module_list.44.BatchNorm2d.weight, %module_list.44.BatchNorm2d.bias, %module_list.44.BatchNorm2d.running_mean, %module_list.44.BatchNorm2d.running_var)\n",
      "  %597 = LeakyRelu[alpha = 0.100000001490116](%596)\n",
      "  %598 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%597, %module_list.45.Conv2d.weight)\n",
      "  %599 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%598, %module_list.45.BatchNorm2d.weight, %module_list.45.BatchNorm2d.bias, %module_list.45.BatchNorm2d.running_mean, %module_list.45.BatchNorm2d.running_var)\n",
      "  %600 = Add(%599, %591)\n",
      "  %601 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%600, %module_list.48.Conv2d.weight)\n",
      "  %602 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%601, %module_list.48.BatchNorm2d.weight, %module_list.48.BatchNorm2d.bias, %module_list.48.BatchNorm2d.running_mean, %module_list.48.BatchNorm2d.running_var)\n",
      "  %603 = LeakyRelu[alpha = 0.100000001490116](%602)\n",
      "  %604 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%603, %module_list.49.Conv2d.weight)\n",
      "  %605 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%604, %module_list.49.BatchNorm2d.weight, %module_list.49.BatchNorm2d.bias, %module_list.49.BatchNorm2d.running_mean, %module_list.49.BatchNorm2d.running_var)\n",
      "  %606 = LeakyRelu[alpha = 0.100000001490116](%605)\n",
      "  %607 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%606, %module_list.50.Conv2d.weight)\n",
      "  %608 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%607, %module_list.50.BatchNorm2d.weight, %module_list.50.BatchNorm2d.bias, %module_list.50.BatchNorm2d.running_mean, %module_list.50.BatchNorm2d.running_var)\n",
      "  %609 = Add(%608, %600)\n",
      "  %610 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%609, %module_list.53.Conv2d.weight)\n",
      "  %611 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%610, %module_list.53.BatchNorm2d.weight, %module_list.53.BatchNorm2d.bias, %module_list.53.BatchNorm2d.running_mean, %module_list.53.BatchNorm2d.running_var)\n",
      "  %612 = LeakyRelu[alpha = 0.100000001490116](%611)\n",
      "  %613 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%612, %module_list.54.Conv2d.weight)\n",
      "  %614 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%613, %module_list.54.BatchNorm2d.weight, %module_list.54.BatchNorm2d.bias, %module_list.54.BatchNorm2d.running_mean, %module_list.54.BatchNorm2d.running_var)\n",
      "  %615 = LeakyRelu[alpha = 0.100000001490116](%614)\n",
      "  %616 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%615, %module_list.55.Conv2d.weight)\n",
      "  %617 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%616, %module_list.55.BatchNorm2d.weight, %module_list.55.BatchNorm2d.bias, %module_list.55.BatchNorm2d.running_mean, %module_list.55.BatchNorm2d.running_var)\n",
      "  %618 = Add(%617, %609)\n",
      "  %619 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%618, %module_list.58.Conv2d.weight)\n",
      "  %620 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%619, %module_list.58.BatchNorm2d.weight, %module_list.58.BatchNorm2d.bias, %module_list.58.BatchNorm2d.running_mean, %module_list.58.BatchNorm2d.running_var)\n",
      "  %621 = LeakyRelu[alpha = 0.100000001490116](%620)\n",
      "  %622 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%621, %module_list.59.Conv2d.weight)\n",
      "  %623 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%622, %module_list.59.BatchNorm2d.weight, %module_list.59.BatchNorm2d.bias, %module_list.59.BatchNorm2d.running_mean, %module_list.59.BatchNorm2d.running_var)\n",
      "  %624 = LeakyRelu[alpha = 0.100000001490116](%623)\n",
      "  %625 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%624, %module_list.60.Conv2d.weight)\n",
      "  %626 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%625, %module_list.60.BatchNorm2d.weight, %module_list.60.BatchNorm2d.bias, %module_list.60.BatchNorm2d.running_mean, %module_list.60.BatchNorm2d.running_var)\n",
      "  %627 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%626, %module_list.61.Conv2d.weight)\n",
      "  %628 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%627, %module_list.61.BatchNorm2d.weight, %module_list.61.BatchNorm2d.bias, %module_list.61.BatchNorm2d.running_mean, %module_list.61.BatchNorm2d.running_var)\n",
      "  %629 = LeakyRelu[alpha = 0.100000001490116](%628)\n",
      "  %630 = Conv[dilations = [1, 1], group = 136, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%629, %module_list.62.Conv2d.weight)\n",
      "  %631 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%630, %module_list.62.BatchNorm2d.weight, %module_list.62.BatchNorm2d.bias, %module_list.62.BatchNorm2d.running_mean, %module_list.62.BatchNorm2d.running_var)\n",
      "  %632 = LeakyRelu[alpha = 0.100000001490116](%631)\n",
      "  %633 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%632, %module_list.63.Conv2d.weight)\n",
      "  %634 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%633, %module_list.63.BatchNorm2d.weight, %module_list.63.BatchNorm2d.bias, %module_list.63.BatchNorm2d.running_mean, %module_list.63.BatchNorm2d.running_var)\n",
      "  %635 = Add(%634, %626)\n",
      "  %636 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%635, %module_list.66.Conv2d.weight)\n",
      "  %637 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%636, %module_list.66.BatchNorm2d.weight, %module_list.66.BatchNorm2d.bias, %module_list.66.BatchNorm2d.running_mean, %module_list.66.BatchNorm2d.running_var)\n",
      "  %638 = LeakyRelu[alpha = 0.100000001490116](%637)\n",
      "  %639 = Conv[dilations = [1, 1], group = 136, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%638, %module_list.67.Conv2d.weight)\n",
      "  %640 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%639, %module_list.67.BatchNorm2d.weight, %module_list.67.BatchNorm2d.bias, %module_list.67.BatchNorm2d.running_mean, %module_list.67.BatchNorm2d.running_var)\n",
      "  %641 = LeakyRelu[alpha = 0.100000001490116](%640)\n",
      "  %642 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%641, %module_list.68.Conv2d.weight)\n",
      "  %643 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%642, %module_list.68.BatchNorm2d.weight, %module_list.68.BatchNorm2d.bias, %module_list.68.BatchNorm2d.running_mean, %module_list.68.BatchNorm2d.running_var)\n",
      "  %644 = Add(%643, %635)\n",
      "  %645 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%644, %module_list.71.Conv2d.weight)\n",
      "  %646 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%645, %module_list.71.BatchNorm2d.weight, %module_list.71.BatchNorm2d.bias, %module_list.71.BatchNorm2d.running_mean, %module_list.71.BatchNorm2d.running_var)\n",
      "  %647 = LeakyRelu[alpha = 0.100000001490116](%646)\n",
      "  %648 = Conv[dilations = [1, 1], group = 136, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%647, %module_list.72.Conv2d.weight)\n",
      "  %649 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%648, %module_list.72.BatchNorm2d.weight, %module_list.72.BatchNorm2d.bias, %module_list.72.BatchNorm2d.running_mean, %module_list.72.BatchNorm2d.running_var)\n",
      "  %650 = LeakyRelu[alpha = 0.100000001490116](%649)\n",
      "  %651 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%650, %module_list.73.Conv2d.weight)\n",
      "  %652 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%651, %module_list.73.BatchNorm2d.weight, %module_list.73.BatchNorm2d.bias, %module_list.73.BatchNorm2d.running_mean, %module_list.73.BatchNorm2d.running_var)\n",
      "  %653 = Add(%652, %644)\n",
      "  %654 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%653, %module_list.76.Conv2d.weight)\n",
      "  %655 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%654, %module_list.76.BatchNorm2d.weight, %module_list.76.BatchNorm2d.bias, %module_list.76.BatchNorm2d.running_mean, %module_list.76.BatchNorm2d.running_var)\n",
      "  %656 = LeakyRelu[alpha = 0.100000001490116](%655)\n",
      "  %657 = Conv[dilations = [1, 1], group = 136, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%656, %module_list.77.Conv2d.weight)\n",
      "  %658 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%657, %module_list.77.BatchNorm2d.weight, %module_list.77.BatchNorm2d.bias, %module_list.77.BatchNorm2d.running_mean, %module_list.77.BatchNorm2d.running_var)\n",
      "  %659 = LeakyRelu[alpha = 0.100000001490116](%658)\n",
      "  %660 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%659, %module_list.78.Conv2d.weight)\n",
      "  %661 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%660, %module_list.78.BatchNorm2d.weight, %module_list.78.BatchNorm2d.bias, %module_list.78.BatchNorm2d.running_mean, %module_list.78.BatchNorm2d.running_var)\n",
      "  %662 = Add(%661, %653)\n",
      "  %663 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%662, %module_list.81.Conv2d.weight)\n",
      "  %664 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%663, %module_list.81.BatchNorm2d.weight, %module_list.81.BatchNorm2d.bias, %module_list.81.BatchNorm2d.running_mean, %module_list.81.BatchNorm2d.running_var)\n",
      "  %665 = LeakyRelu[alpha = 0.100000001490116](%664)\n",
      "  %666 = Conv[dilations = [1, 1], group = 136, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%665, %module_list.82.Conv2d.weight)\n",
      "  %667 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%666, %module_list.82.BatchNorm2d.weight, %module_list.82.BatchNorm2d.bias, %module_list.82.BatchNorm2d.running_mean, %module_list.82.BatchNorm2d.running_var)\n",
      "  %668 = LeakyRelu[alpha = 0.100000001490116](%667)\n",
      "  %669 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%668, %module_list.83.Conv2d.weight)\n",
      "  %670 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%669, %module_list.83.BatchNorm2d.weight, %module_list.83.BatchNorm2d.bias, %module_list.83.BatchNorm2d.running_mean, %module_list.83.BatchNorm2d.running_var)\n",
      "  %671 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%670, %module_list.84.Conv2d.weight)\n",
      "  %672 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%671, %module_list.84.BatchNorm2d.weight, %module_list.84.BatchNorm2d.bias, %module_list.84.BatchNorm2d.running_mean, %module_list.84.BatchNorm2d.running_var)\n",
      "  %673 = LeakyRelu[alpha = 0.100000001490116](%672)\n",
      "  %674 = Conv[dilations = [1, 1], group = 224, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%673, %module_list.85.Conv2d.weight)\n",
      "  %675 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%674, %module_list.85.BatchNorm2d.weight, %module_list.85.BatchNorm2d.bias, %module_list.85.BatchNorm2d.running_mean, %module_list.85.BatchNorm2d.running_var)\n",
      "  %676 = LeakyRelu[alpha = 0.100000001490116](%675)\n",
      "  %677 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%676, %module_list.86.Conv2d.weight)\n",
      "  %678 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%677, %module_list.86.BatchNorm2d.weight, %module_list.86.BatchNorm2d.bias, %module_list.86.BatchNorm2d.running_mean, %module_list.86.BatchNorm2d.running_var)\n",
      "  %679 = Add(%678, %670)\n",
      "  %680 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%679, %module_list.89.Conv2d.weight)\n",
      "  %681 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%680, %module_list.89.BatchNorm2d.weight, %module_list.89.BatchNorm2d.bias, %module_list.89.BatchNorm2d.running_mean, %module_list.89.BatchNorm2d.running_var)\n",
      "  %682 = LeakyRelu[alpha = 0.100000001490116](%681)\n",
      "  %683 = Conv[dilations = [1, 1], group = 224, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%682, %module_list.90.Conv2d.weight)\n",
      "  %684 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%683, %module_list.90.BatchNorm2d.weight, %module_list.90.BatchNorm2d.bias, %module_list.90.BatchNorm2d.running_mean, %module_list.90.BatchNorm2d.running_var)\n",
      "  %685 = LeakyRelu[alpha = 0.100000001490116](%684)\n",
      "  %686 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%685, %module_list.91.Conv2d.weight)\n",
      "  %687 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%686, %module_list.91.BatchNorm2d.weight, %module_list.91.BatchNorm2d.bias, %module_list.91.BatchNorm2d.running_mean, %module_list.91.BatchNorm2d.running_var)\n",
      "  %688 = Add(%687, %679)\n",
      "  %689 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%688, %module_list.94.Conv2d.weight)\n",
      "  %690 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%689, %module_list.94.BatchNorm2d.weight, %module_list.94.BatchNorm2d.bias, %module_list.94.BatchNorm2d.running_mean, %module_list.94.BatchNorm2d.running_var)\n",
      "  %691 = LeakyRelu[alpha = 0.100000001490116](%690)\n",
      "  %692 = Conv[dilations = [1, 1], group = 224, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%691, %module_list.95.Conv2d.weight)\n",
      "  %693 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%692, %module_list.95.BatchNorm2d.weight, %module_list.95.BatchNorm2d.bias, %module_list.95.BatchNorm2d.running_mean, %module_list.95.BatchNorm2d.running_var)\n",
      "  %694 = LeakyRelu[alpha = 0.100000001490116](%693)\n",
      "  %695 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%694, %module_list.96.Conv2d.weight)\n",
      "  %696 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%695, %module_list.96.BatchNorm2d.weight, %module_list.96.BatchNorm2d.bias, %module_list.96.BatchNorm2d.running_mean, %module_list.96.BatchNorm2d.running_var)\n",
      "  %697 = Add(%696, %688)\n",
      "  %698 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%697, %module_list.99.Conv2d.weight)\n",
      "  %699 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%698, %module_list.99.BatchNorm2d.weight, %module_list.99.BatchNorm2d.bias, %module_list.99.BatchNorm2d.running_mean, %module_list.99.BatchNorm2d.running_var)\n",
      "  %700 = LeakyRelu[alpha = 0.100000001490116](%699)\n",
      "  %701 = Conv[dilations = [1, 1], group = 224, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%700, %module_list.100.Conv2d.weight)\n",
      "  %702 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%701, %module_list.100.BatchNorm2d.weight, %module_list.100.BatchNorm2d.bias, %module_list.100.BatchNorm2d.running_mean, %module_list.100.BatchNorm2d.running_var)\n",
      "  %703 = LeakyRelu[alpha = 0.100000001490116](%702)\n",
      "  %704 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%703, %module_list.101.Conv2d.weight)\n",
      "  %705 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%704, %module_list.101.BatchNorm2d.weight, %module_list.101.BatchNorm2d.bias, %module_list.101.BatchNorm2d.running_mean, %module_list.101.BatchNorm2d.running_var)\n",
      "  %706 = Add(%705, %697)\n",
      "  %707 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%706, %module_list.104.Conv2d.weight)\n",
      "  %708 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%707, %module_list.104.BatchNorm2d.weight, %module_list.104.BatchNorm2d.bias, %module_list.104.BatchNorm2d.running_mean, %module_list.104.BatchNorm2d.running_var)\n",
      "  %709 = LeakyRelu[alpha = 0.100000001490116](%708)\n",
      "  %710 = Conv[dilations = [1, 1], group = 224, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%709, %module_list.105.Conv2d.weight)\n",
      "  %711 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%710, %module_list.105.BatchNorm2d.weight, %module_list.105.BatchNorm2d.bias, %module_list.105.BatchNorm2d.running_mean, %module_list.105.BatchNorm2d.running_var)\n",
      "  %712 = LeakyRelu[alpha = 0.100000001490116](%711)\n",
      "  %713 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%712, %module_list.106.Conv2d.weight)\n",
      "  %714 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%713, %module_list.106.BatchNorm2d.weight, %module_list.106.BatchNorm2d.bias, %module_list.106.BatchNorm2d.running_mean, %module_list.106.BatchNorm2d.running_var)\n",
      "  %715 = Add(%714, %706)\n",
      "  %716 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%715, %module_list.109.Conv2d.weight)\n",
      "  %717 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%716, %module_list.109.BatchNorm2d.weight, %module_list.109.BatchNorm2d.bias, %module_list.109.BatchNorm2d.running_mean, %module_list.109.BatchNorm2d.running_var)\n",
      "  %718 = LeakyRelu[alpha = 0.100000001490116](%717)\n",
      "  %719 = Conv[dilations = [1, 1], group = 96, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%718, %module_list.110.Conv2d.weight)\n",
      "  %720 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%719, %module_list.110.BatchNorm2d.weight, %module_list.110.BatchNorm2d.bias, %module_list.110.BatchNorm2d.running_mean, %module_list.110.BatchNorm2d.running_var)\n",
      "  %721 = LeakyRelu[alpha = 0.100000001490116](%720)\n",
      "  %722 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%721, %module_list.111.Conv2d.weight)\n",
      "  %723 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%722, %module_list.111.BatchNorm2d.weight, %module_list.111.BatchNorm2d.bias, %module_list.111.BatchNorm2d.running_mean, %module_list.111.BatchNorm2d.running_var)\n",
      "  %724 = Conv[dilations = [1, 1], group = 128, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%723, %module_list.112.Conv2d.weight)\n",
      "  %725 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%724, %module_list.112.BatchNorm2d.weight, %module_list.112.BatchNorm2d.bias, %module_list.112.BatchNorm2d.running_mean, %module_list.112.BatchNorm2d.running_var)\n",
      "  %726 = LeakyRelu[alpha = 0.100000001490116](%725)\n",
      "  %727 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%726, %module_list.113.Conv2d.weight)\n",
      "  %728 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%727, %module_list.113.BatchNorm2d.weight, %module_list.113.BatchNorm2d.bias, %module_list.113.BatchNorm2d.running_mean, %module_list.113.BatchNorm2d.running_var)\n",
      "  %729 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%728, %module_list.114.Conv2d.weight, %module_list.114.Conv2d.bias)\n",
      "  %730 = Shape(%729)\n",
      "  %731 = Constant[value = <Scalar Tensor []>]()\n",
      "  %732 = Gather[axis = 0](%730, %731)\n",
      "  %737 = Unsqueeze[axes = [0]](%732)\n",
      "  %742 = Concat[axis = 0](%737, %1536, %1537, %1538, %1539)\n",
      "  %743 = Reshape(%729, %742)\n",
      "  %boxes = Transpose[perm = [0, 1, 3, 4, 2]](%743)\n",
      "  %745 = Constant[value = <Tensor>]()\n",
      "  %746 = Constant[value = <Tensor>]()\n",
      "  %747 = Constant[value = <Tensor>]()\n",
      "  %748 = Constant[value = <Tensor>]()\n",
      "  %749 = Slice(%boxes, %746, %747, %745, %748)\n",
      "  %750 = Sigmoid(%749)\n",
      "  %751 = Constant[value = <Tensor>]()\n",
      "  %752 = Add(%750, %751)\n",
      "  %753 = Constant[value = <Tensor>]()\n",
      "  %754 = Reshape(%752, %753)\n",
      "  %755 = Constant[value = <Tensor>]()\n",
      "  %757 = ConstantOfShape[value = <Tensor>](%1540)\n",
      "  %758 = Constant[value = <Scalar Tensor []>]()\n",
      "  %759 = Mul(%757, %758)\n",
      "  %760 = Constant[value = <Tensor>]()\n",
      "  %761 = Equal(%760, %759)\n",
      "  %762 = Where(%761, %757, %755)\n",
      "  %763 = Expand(%754, %762)\n",
      "  %764 = Shape(%boxes)\n",
      "  %765 = Constant[value = <Scalar Tensor []>]()\n",
      "  %766 = Gather[axis = 0](%764, %765)\n",
      "  %767 = Cast[to = 7](%766)\n",
      "  %768 = Constant[value = <Scalar Tensor []>]()\n",
      "  %769 = Constant[value = <Scalar Tensor []>]()\n",
      "  %770 = Range(%768, %767, %769)\n",
      "  %771 = Shape(%boxes)\n",
      "  %772 = Constant[value = <Scalar Tensor []>]()\n",
      "  %773 = Gather[axis = 0](%771, %772)\n",
      "  %774 = Cast[to = 7](%773)\n",
      "  %775 = Constant[value = <Scalar Tensor []>]()\n",
      "  %776 = Constant[value = <Scalar Tensor []>]()\n",
      "  %777 = Range(%775, %774, %776)\n",
      "  %778 = Shape(%boxes)\n",
      "  %779 = Constant[value = <Scalar Tensor []>]()\n",
      "  %780 = Gather[axis = 0](%778, %779)\n",
      "  %781 = Cast[to = 7](%780)\n",
      "  %782 = Constant[value = <Scalar Tensor []>]()\n",
      "  %783 = Constant[value = <Scalar Tensor []>]()\n",
      "  %784 = Range(%782, %781, %783)\n",
      "  %785 = Shape(%boxes)\n",
      "  %786 = Constant[value = <Scalar Tensor []>]()\n",
      "  %787 = Gather[axis = 0](%785, %786)\n",
      "  %788 = Cast[to = 7](%787)\n",
      "  %789 = Constant[value = <Scalar Tensor []>]()\n",
      "  %790 = Constant[value = <Scalar Tensor []>]()\n",
      "  %791 = Range(%789, %788, %790)\n",
      "  %792 = Shape(%boxes)\n",
      "  %793 = Constant[value = <Scalar Tensor []>]()\n",
      "  %794 = Gather[axis = 0](%792, %793)\n",
      "  %795 = Cast[to = 7](%794)\n",
      "  %796 = Constant[value = <Scalar Tensor []>]()\n",
      "  %797 = Constant[value = <Scalar Tensor []>]()\n",
      "  %798 = Range(%796, %795, %797)\n",
      "  %799 = Constant[value = <Tensor>]()\n",
      "  %800 = Constant[value = <Tensor>]()\n",
      "  %801 = Constant[value = <Tensor>]()\n",
      "  %802 = Constant[value = <Tensor>]()\n",
      "  %803 = Slice(%798, %800, %801, %799, %802)\n",
      "  %804 = Constant[value = <Tensor>]()\n",
      "  %805 = Reshape(%770, %804)\n",
      "  %806 = Constant[value = <Tensor>]()\n",
      "  %807 = Reshape(%777, %806)\n",
      "  %808 = Constant[value = <Tensor>]()\n",
      "  %809 = Reshape(%784, %808)\n",
      "  %810 = Constant[value = <Tensor>]()\n",
      "  %811 = Reshape(%791, %810)\n",
      "  %812 = Constant[value = <Tensor>]()\n",
      "  %813 = Reshape(%803, %812)\n",
      "  %814 = Add(%805, %807)\n",
      "  %815 = Add(%814, %809)\n",
      "  %816 = Add(%815, %811)\n",
      "  %817 = Add(%816, %813)\n",
      "  %818 = Shape(%817)\n",
      "  %819 = Shape(%818)\n",
      "  %820 = ConstantOfShape[value = <Tensor>](%819)\n",
      "  %821 = Constant[value = <Scalar Tensor []>]()\n",
      "  %822 = Mul(%820, %821)\n",
      "  %823 = Equal(%818, %822)\n",
      "  %824 = Where(%823, %820, %818)\n",
      "  %825 = Expand(%805, %824)\n",
      "  %826 = Unsqueeze[axes = [-1]](%825)\n",
      "  %827 = Shape(%818)\n",
      "  %828 = ConstantOfShape[value = <Tensor>](%827)\n",
      "  %829 = Constant[value = <Scalar Tensor []>]()\n",
      "  %830 = Mul(%828, %829)\n",
      "  %831 = Equal(%818, %830)\n",
      "  %832 = Where(%831, %828, %818)\n",
      "  %833 = Expand(%807, %832)\n",
      "  %834 = Unsqueeze[axes = [-1]](%833)\n",
      "  %835 = Shape(%818)\n",
      "  %836 = ConstantOfShape[value = <Tensor>](%835)\n",
      "  %837 = Constant[value = <Scalar Tensor []>]()\n",
      "  %838 = Mul(%836, %837)\n",
      "  %839 = Equal(%818, %838)\n",
      "  %840 = Where(%839, %836, %818)\n",
      "  %841 = Expand(%809, %840)\n",
      "  %842 = Unsqueeze[axes = [-1]](%841)\n",
      "  %843 = Shape(%818)\n",
      "  %844 = ConstantOfShape[value = <Tensor>](%843)\n",
      "  %845 = Constant[value = <Scalar Tensor []>]()\n",
      "  %846 = Mul(%844, %845)\n",
      "  %847 = Equal(%818, %846)\n",
      "  %848 = Where(%847, %844, %818)\n",
      "  %849 = Expand(%811, %848)\n",
      "  %850 = Unsqueeze[axes = [-1]](%849)\n",
      "  %851 = Shape(%818)\n",
      "  %852 = ConstantOfShape[value = <Tensor>](%851)\n",
      "  %853 = Constant[value = <Scalar Tensor []>]()\n",
      "  %854 = Mul(%852, %853)\n",
      "  %855 = Equal(%818, %854)\n",
      "  %856 = Where(%855, %852, %818)\n",
      "  %857 = Expand(%813, %856)\n",
      "  %858 = Unsqueeze[axes = [-1]](%857)\n",
      "  %859 = Concat[axis = -1](%826, %834, %842, %850, %858)\n",
      "  %860 = Shape(%boxes)\n",
      "  %861 = Constant[value = <Tensor>]()\n",
      "  %862 = Constant[value = <Tensor>]()\n",
      "  %863 = Constant[value = <Tensor>]()\n",
      "  %864 = Slice(%860, %862, %863, %861)\n",
      "  %865 = Concat[axis = 0](%818, %864)\n",
      "  %866 = Reshape(%763, %865)\n",
      "  %867 = ScatterND(%boxes, %859, %866)\n",
      "  %868 = Constant[value = <Tensor>]()\n",
      "  %869 = Constant[value = <Tensor>]()\n",
      "  %870 = Constant[value = <Tensor>]()\n",
      "  %871 = Constant[value = <Tensor>]()\n",
      "  %872 = Slice(%867, %869, %870, %868, %871)\n",
      "  %873 = Exp(%872)\n",
      "  %874 = Constant[value = <Tensor>]()\n",
      "  %875 = Mul(%873, %874)\n",
      "  %876 = Constant[value = <Tensor>]()\n",
      "  %877 = Reshape(%875, %876)\n",
      "  %878 = Constant[value = <Tensor>]()\n",
      "  %880 = ConstantOfShape[value = <Tensor>](%1541)\n",
      "  %881 = Constant[value = <Scalar Tensor []>]()\n",
      "  %882 = Mul(%880, %881)\n",
      "  %883 = Constant[value = <Tensor>]()\n",
      "  %884 = Equal(%883, %882)\n",
      "  %885 = Where(%884, %880, %878)\n",
      "  %886 = Expand(%877, %885)\n",
      "  %887 = Shape(%867)\n",
      "  %888 = Constant[value = <Scalar Tensor []>]()\n",
      "  %889 = Gather[axis = 0](%887, %888)\n",
      "  %890 = Cast[to = 7](%889)\n",
      "  %891 = Constant[value = <Scalar Tensor []>]()\n",
      "  %892 = Constant[value = <Scalar Tensor []>]()\n",
      "  %893 = Range(%891, %890, %892)\n",
      "  %894 = Shape(%867)\n",
      "  %895 = Constant[value = <Scalar Tensor []>]()\n",
      "  %896 = Gather[axis = 0](%894, %895)\n",
      "  %897 = Cast[to = 7](%896)\n",
      "  %898 = Constant[value = <Scalar Tensor []>]()\n",
      "  %899 = Constant[value = <Scalar Tensor []>]()\n",
      "  %900 = Range(%898, %897, %899)\n",
      "  %901 = Shape(%867)\n",
      "  %902 = Constant[value = <Scalar Tensor []>]()\n",
      "  %903 = Gather[axis = 0](%901, %902)\n",
      "  %904 = Cast[to = 7](%903)\n",
      "  %905 = Constant[value = <Scalar Tensor []>]()\n",
      "  %906 = Constant[value = <Scalar Tensor []>]()\n",
      "  %907 = Range(%905, %904, %906)\n",
      "  %908 = Shape(%867)\n",
      "  %909 = Constant[value = <Scalar Tensor []>]()\n",
      "  %910 = Gather[axis = 0](%908, %909)\n",
      "  %911 = Cast[to = 7](%910)\n",
      "  %912 = Constant[value = <Scalar Tensor []>]()\n",
      "  %913 = Constant[value = <Scalar Tensor []>]()\n",
      "  %914 = Range(%912, %911, %913)\n",
      "  %915 = Shape(%867)\n",
      "  %916 = Constant[value = <Scalar Tensor []>]()\n",
      "  %917 = Gather[axis = 0](%915, %916)\n",
      "  %918 = Cast[to = 7](%917)\n",
      "  %919 = Constant[value = <Scalar Tensor []>]()\n",
      "  %920 = Constant[value = <Scalar Tensor []>]()\n",
      "  %921 = Range(%919, %918, %920)\n",
      "  %922 = Constant[value = <Tensor>]()\n",
      "  %923 = Constant[value = <Tensor>]()\n",
      "  %924 = Constant[value = <Tensor>]()\n",
      "  %925 = Constant[value = <Tensor>]()\n",
      "  %926 = Slice(%921, %923, %924, %922, %925)\n",
      "  %927 = Constant[value = <Tensor>]()\n",
      "  %928 = Reshape(%893, %927)\n",
      "  %929 = Constant[value = <Tensor>]()\n",
      "  %930 = Reshape(%900, %929)\n",
      "  %931 = Constant[value = <Tensor>]()\n",
      "  %932 = Reshape(%907, %931)\n",
      "  %933 = Constant[value = <Tensor>]()\n",
      "  %934 = Reshape(%914, %933)\n",
      "  %935 = Constant[value = <Tensor>]()\n",
      "  %936 = Reshape(%926, %935)\n",
      "  %937 = Add(%928, %930)\n",
      "  %938 = Add(%937, %932)\n",
      "  %939 = Add(%938, %934)\n",
      "  %940 = Add(%939, %936)\n",
      "  %941 = Shape(%940)\n",
      "  %942 = Shape(%941)\n",
      "  %943 = ConstantOfShape[value = <Tensor>](%942)\n",
      "  %944 = Constant[value = <Scalar Tensor []>]()\n",
      "  %945 = Mul(%943, %944)\n",
      "  %946 = Equal(%941, %945)\n",
      "  %947 = Where(%946, %943, %941)\n",
      "  %948 = Expand(%928, %947)\n",
      "  %949 = Unsqueeze[axes = [-1]](%948)\n",
      "  %950 = Shape(%941)\n",
      "  %951 = ConstantOfShape[value = <Tensor>](%950)\n",
      "  %952 = Constant[value = <Scalar Tensor []>]()\n",
      "  %953 = Mul(%951, %952)\n",
      "  %954 = Equal(%941, %953)\n",
      "  %955 = Where(%954, %951, %941)\n",
      "  %956 = Expand(%930, %955)\n",
      "  %957 = Unsqueeze[axes = [-1]](%956)\n",
      "  %958 = Shape(%941)\n",
      "  %959 = ConstantOfShape[value = <Tensor>](%958)\n",
      "  %960 = Constant[value = <Scalar Tensor []>]()\n",
      "  %961 = Mul(%959, %960)\n",
      "  %962 = Equal(%941, %961)\n",
      "  %963 = Where(%962, %959, %941)\n",
      "  %964 = Expand(%932, %963)\n",
      "  %965 = Unsqueeze[axes = [-1]](%964)\n",
      "  %966 = Shape(%941)\n",
      "  %967 = ConstantOfShape[value = <Tensor>](%966)\n",
      "  %968 = Constant[value = <Scalar Tensor []>]()\n",
      "  %969 = Mul(%967, %968)\n",
      "  %970 = Equal(%941, %969)\n",
      "  %971 = Where(%970, %967, %941)\n",
      "  %972 = Expand(%934, %971)\n",
      "  %973 = Unsqueeze[axes = [-1]](%972)\n",
      "  %974 = Shape(%941)\n",
      "  %975 = ConstantOfShape[value = <Tensor>](%974)\n",
      "  %976 = Constant[value = <Scalar Tensor []>]()\n",
      "  %977 = Mul(%975, %976)\n",
      "  %978 = Equal(%941, %977)\n",
      "  %979 = Where(%978, %975, %941)\n",
      "  %980 = Expand(%936, %979)\n",
      "  %981 = Unsqueeze[axes = [-1]](%980)\n",
      "  %982 = Concat[axis = -1](%949, %957, %965, %973, %981)\n",
      "  %983 = Shape(%867)\n",
      "  %984 = Constant[value = <Tensor>]()\n",
      "  %985 = Constant[value = <Tensor>]()\n",
      "  %986 = Constant[value = <Tensor>]()\n",
      "  %987 = Slice(%983, %985, %986, %984)\n",
      "  %988 = Concat[axis = 0](%941, %987)\n",
      "  %989 = Reshape(%886, %988)\n",
      "  %990 = ScatterND(%867, %982, %989)\n",
      "  %991 = Constant[value = <Tensor>]()\n",
      "  %992 = Constant[value = <Tensor>]()\n",
      "  %993 = Constant[value = <Tensor>]()\n",
      "  %994 = Constant[value = <Tensor>]()\n",
      "  %995 = Slice(%990, %992, %993, %991, %994)\n",
      "  %996 = Constant[value = <Scalar Tensor []>]()\n",
      "  %997 = Mul(%995, %996)\n",
      "  %998 = Constant[value = <Tensor>]()\n",
      "  %999 = Reshape(%997, %998)\n",
      "  %1000 = Constant[value = <Tensor>]()\n",
      "  %1002 = ConstantOfShape[value = <Tensor>](%1542)\n",
      "  %1003 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1004 = Mul(%1002, %1003)\n",
      "  %1005 = Constant[value = <Tensor>]()\n",
      "  %1006 = Equal(%1005, %1004)\n",
      "  %1007 = Where(%1006, %1002, %1000)\n",
      "  %1008 = Expand(%999, %1007)\n",
      "  %1009 = Shape(%990)\n",
      "  %1010 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1011 = Gather[axis = 0](%1009, %1010)\n",
      "  %1012 = Cast[to = 7](%1011)\n",
      "  %1013 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1014 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1015 = Range(%1013, %1012, %1014)\n",
      "  %1016 = Shape(%990)\n",
      "  %1017 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1018 = Gather[axis = 0](%1016, %1017)\n",
      "  %1019 = Cast[to = 7](%1018)\n",
      "  %1020 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1021 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1022 = Range(%1020, %1019, %1021)\n",
      "  %1023 = Shape(%990)\n",
      "  %1024 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1025 = Gather[axis = 0](%1023, %1024)\n",
      "  %1026 = Cast[to = 7](%1025)\n",
      "  %1027 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1028 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1029 = Range(%1027, %1026, %1028)\n",
      "  %1030 = Shape(%990)\n",
      "  %1031 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1032 = Gather[axis = 0](%1030, %1031)\n",
      "  %1033 = Cast[to = 7](%1032)\n",
      "  %1034 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1035 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1036 = Range(%1034, %1033, %1035)\n",
      "  %1037 = Shape(%990)\n",
      "  %1038 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1039 = Gather[axis = 0](%1037, %1038)\n",
      "  %1040 = Cast[to = 7](%1039)\n",
      "  %1041 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1042 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1043 = Range(%1041, %1040, %1042)\n",
      "  %1044 = Constant[value = <Tensor>]()\n",
      "  %1045 = Constant[value = <Tensor>]()\n",
      "  %1046 = Constant[value = <Tensor>]()\n",
      "  %1047 = Constant[value = <Tensor>]()\n",
      "  %1048 = Slice(%1043, %1045, %1046, %1044, %1047)\n",
      "  %1049 = Constant[value = <Tensor>]()\n",
      "  %1050 = Reshape(%1015, %1049)\n",
      "  %1051 = Constant[value = <Tensor>]()\n",
      "  %1052 = Reshape(%1022, %1051)\n",
      "  %1053 = Constant[value = <Tensor>]()\n",
      "  %1054 = Reshape(%1029, %1053)\n",
      "  %1055 = Constant[value = <Tensor>]()\n",
      "  %1056 = Reshape(%1036, %1055)\n",
      "  %1057 = Constant[value = <Tensor>]()\n",
      "  %1058 = Reshape(%1048, %1057)\n",
      "  %1059 = Add(%1050, %1052)\n",
      "  %1060 = Add(%1059, %1054)\n",
      "  %1061 = Add(%1060, %1056)\n",
      "  %1062 = Add(%1061, %1058)\n",
      "  %1063 = Shape(%1062)\n",
      "  %1064 = Shape(%1063)\n",
      "  %1065 = ConstantOfShape[value = <Tensor>](%1064)\n",
      "  %1066 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1067 = Mul(%1065, %1066)\n",
      "  %1068 = Equal(%1063, %1067)\n",
      "  %1069 = Where(%1068, %1065, %1063)\n",
      "  %1070 = Expand(%1050, %1069)\n",
      "  %1071 = Unsqueeze[axes = [-1]](%1070)\n",
      "  %1072 = Shape(%1063)\n",
      "  %1073 = ConstantOfShape[value = <Tensor>](%1072)\n",
      "  %1074 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1075 = Mul(%1073, %1074)\n",
      "  %1076 = Equal(%1063, %1075)\n",
      "  %1077 = Where(%1076, %1073, %1063)\n",
      "  %1078 = Expand(%1052, %1077)\n",
      "  %1079 = Unsqueeze[axes = [-1]](%1078)\n",
      "  %1080 = Shape(%1063)\n",
      "  %1081 = ConstantOfShape[value = <Tensor>](%1080)\n",
      "  %1082 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1083 = Mul(%1081, %1082)\n",
      "  %1084 = Equal(%1063, %1083)\n",
      "  %1085 = Where(%1084, %1081, %1063)\n",
      "  %1086 = Expand(%1054, %1085)\n",
      "  %1087 = Unsqueeze[axes = [-1]](%1086)\n",
      "  %1088 = Shape(%1063)\n",
      "  %1089 = ConstantOfShape[value = <Tensor>](%1088)\n",
      "  %1090 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1091 = Mul(%1089, %1090)\n",
      "  %1092 = Equal(%1063, %1091)\n",
      "  %1093 = Where(%1092, %1089, %1063)\n",
      "  %1094 = Expand(%1056, %1093)\n",
      "  %1095 = Unsqueeze[axes = [-1]](%1094)\n",
      "  %1096 = Shape(%1063)\n",
      "  %1097 = ConstantOfShape[value = <Tensor>](%1096)\n",
      "  %1098 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1099 = Mul(%1097, %1098)\n",
      "  %1100 = Equal(%1063, %1099)\n",
      "  %1101 = Where(%1100, %1097, %1063)\n",
      "  %1102 = Expand(%1058, %1101)\n",
      "  %1103 = Unsqueeze[axes = [-1]](%1102)\n",
      "  %1104 = Concat[axis = -1](%1071, %1079, %1087, %1095, %1103)\n",
      "  %1105 = Shape(%990)\n",
      "  %1106 = Constant[value = <Tensor>]()\n",
      "  %1107 = Constant[value = <Tensor>]()\n",
      "  %1108 = Constant[value = <Tensor>]()\n",
      "  %1109 = Slice(%1105, %1107, %1108, %1106)\n",
      "  %1110 = Concat[axis = 0](%1063, %1109)\n",
      "  %1111 = Reshape(%1008, %1110)\n",
      "  %1112 = ScatterND(%990, %1104, %1111)\n",
      "  %1115 = Unsqueeze[axes = [0]](%732)\n",
      "  %1118 = Concat[axis = 0](%1115, %1543, %1544)\n",
      "  %1119 = Reshape(%1112, %1118)\n",
      "  %1128 = Constant[value = <Tensor>]()\n",
      "  %1129 = Resize[coordinate_transformation_mode = 'asymmetric', cubic_coeff_a = -0.75, mode = 'nearest', nearest_mode = 'floor'](%718, %1128, %1549)\n",
      "  %1130 = Concat[axis = 1](%1129, %665)\n",
      "  %1131 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%1130, %module_list.119.Conv2d.weight)\n",
      "  %1132 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%1131, %module_list.119.BatchNorm2d.weight, %module_list.119.BatchNorm2d.bias, %module_list.119.BatchNorm2d.running_mean, %module_list.119.BatchNorm2d.running_var)\n",
      "  %1133 = LeakyRelu[alpha = 0.100000001490116](%1132)\n",
      "  %1134 = Conv[dilations = [1, 1], group = 96, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%1133, %module_list.120.Conv2d.weight)\n",
      "  %1135 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%1134, %module_list.120.BatchNorm2d.weight, %module_list.120.BatchNorm2d.bias, %module_list.120.BatchNorm2d.running_mean, %module_list.120.BatchNorm2d.running_var)\n",
      "  %1136 = LeakyRelu[alpha = 0.100000001490116](%1135)\n",
      "  %1137 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%1136, %module_list.121.Conv2d.weight)\n",
      "  %1138 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%1137, %module_list.121.BatchNorm2d.weight, %module_list.121.BatchNorm2d.bias, %module_list.121.BatchNorm2d.running_mean, %module_list.121.BatchNorm2d.running_var)\n",
      "  %1139 = Conv[dilations = [1, 1], group = 96, kernel_shape = [5, 5], pads = [2, 2, 2, 2], strides = [1, 1]](%1138, %module_list.122.Conv2d.weight)\n",
      "  %1140 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%1139, %module_list.122.BatchNorm2d.weight, %module_list.122.BatchNorm2d.bias, %module_list.122.BatchNorm2d.running_mean, %module_list.122.BatchNorm2d.running_var)\n",
      "  %1141 = LeakyRelu[alpha = 0.100000001490116](%1140)\n",
      "  %1142 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%1141, %module_list.123.Conv2d.weight)\n",
      "  %1143 = BatchNormalization[epsilon = 9.99999974737875e-05, momentum = 0.970000028610229](%1142, %module_list.123.BatchNorm2d.weight, %module_list.123.BatchNorm2d.bias, %module_list.123.BatchNorm2d.running_mean, %module_list.123.BatchNorm2d.running_var)\n",
      "  %1144 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%1143, %module_list.124.Conv2d.weight, %module_list.124.Conv2d.bias)\n",
      "  %1145 = Shape(%1144)\n",
      "  %1146 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1147 = Gather[axis = 0](%1145, %1146)\n",
      "  %1152 = Unsqueeze[axes = [0]](%1147)\n",
      "  %1157 = Concat[axis = 0](%1152, %1550, %1551, %1552, %1553)\n",
      "  %1158 = Reshape(%1144, %1157)\n",
      "  %1159 = Transpose[perm = [0, 1, 3, 4, 2]](%1158)\n",
      "  %1160 = Constant[value = <Tensor>]()\n",
      "  %1161 = Constant[value = <Tensor>]()\n",
      "  %1162 = Constant[value = <Tensor>]()\n",
      "  %1163 = Constant[value = <Tensor>]()\n",
      "  %1164 = Slice(%1159, %1161, %1162, %1160, %1163)\n",
      "  %1165 = Sigmoid(%1164)\n",
      "  %1166 = Constant[value = <Tensor>]()\n",
      "  %1167 = Add(%1165, %1166)\n",
      "  %1168 = Constant[value = <Tensor>]()\n",
      "  %1169 = Reshape(%1167, %1168)\n",
      "  %1170 = Constant[value = <Tensor>]()\n",
      "  %1172 = ConstantOfShape[value = <Tensor>](%1554)\n",
      "  %1173 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1174 = Mul(%1172, %1173)\n",
      "  %1175 = Constant[value = <Tensor>]()\n",
      "  %1176 = Equal(%1175, %1174)\n",
      "  %1177 = Where(%1176, %1172, %1170)\n",
      "  %1178 = Expand(%1169, %1177)\n",
      "  %1179 = Shape(%1159)\n",
      "  %1180 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1181 = Gather[axis = 0](%1179, %1180)\n",
      "  %1182 = Cast[to = 7](%1181)\n",
      "  %1183 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1184 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1185 = Range(%1183, %1182, %1184)\n",
      "  %1186 = Shape(%1159)\n",
      "  %1187 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1188 = Gather[axis = 0](%1186, %1187)\n",
      "  %1189 = Cast[to = 7](%1188)\n",
      "  %1190 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1191 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1192 = Range(%1190, %1189, %1191)\n",
      "  %1193 = Shape(%1159)\n",
      "  %1194 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1195 = Gather[axis = 0](%1193, %1194)\n",
      "  %1196 = Cast[to = 7](%1195)\n",
      "  %1197 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1198 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1199 = Range(%1197, %1196, %1198)\n",
      "  %1200 = Shape(%1159)\n",
      "  %1201 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1202 = Gather[axis = 0](%1200, %1201)\n",
      "  %1203 = Cast[to = 7](%1202)\n",
      "  %1204 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1205 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1206 = Range(%1204, %1203, %1205)\n",
      "  %1207 = Shape(%1159)\n",
      "  %1208 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1209 = Gather[axis = 0](%1207, %1208)\n",
      "  %1210 = Cast[to = 7](%1209)\n",
      "  %1211 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1212 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1213 = Range(%1211, %1210, %1212)\n",
      "  %1214 = Constant[value = <Tensor>]()\n",
      "  %1215 = Constant[value = <Tensor>]()\n",
      "  %1216 = Constant[value = <Tensor>]()\n",
      "  %1217 = Constant[value = <Tensor>]()\n",
      "  %1218 = Slice(%1213, %1215, %1216, %1214, %1217)\n",
      "  %1219 = Constant[value = <Tensor>]()\n",
      "  %1220 = Reshape(%1185, %1219)\n",
      "  %1221 = Constant[value = <Tensor>]()\n",
      "  %1222 = Reshape(%1192, %1221)\n",
      "  %1223 = Constant[value = <Tensor>]()\n",
      "  %1224 = Reshape(%1199, %1223)\n",
      "  %1225 = Constant[value = <Tensor>]()\n",
      "  %1226 = Reshape(%1206, %1225)\n",
      "  %1227 = Constant[value = <Tensor>]()\n",
      "  %1228 = Reshape(%1218, %1227)\n",
      "  %1229 = Add(%1220, %1222)\n",
      "  %1230 = Add(%1229, %1224)\n",
      "  %1231 = Add(%1230, %1226)\n",
      "  %1232 = Add(%1231, %1228)\n",
      "  %1233 = Shape(%1232)\n",
      "  %1234 = Shape(%1233)\n",
      "  %1235 = ConstantOfShape[value = <Tensor>](%1234)\n",
      "  %1236 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1237 = Mul(%1235, %1236)\n",
      "  %1238 = Equal(%1233, %1237)\n",
      "  %1239 = Where(%1238, %1235, %1233)\n",
      "  %1240 = Expand(%1220, %1239)\n",
      "  %1241 = Unsqueeze[axes = [-1]](%1240)\n",
      "  %1242 = Shape(%1233)\n",
      "  %1243 = ConstantOfShape[value = <Tensor>](%1242)\n",
      "  %1244 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1245 = Mul(%1243, %1244)\n",
      "  %1246 = Equal(%1233, %1245)\n",
      "  %1247 = Where(%1246, %1243, %1233)\n",
      "  %1248 = Expand(%1222, %1247)\n",
      "  %1249 = Unsqueeze[axes = [-1]](%1248)\n",
      "  %1250 = Shape(%1233)\n",
      "  %1251 = ConstantOfShape[value = <Tensor>](%1250)\n",
      "  %1252 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1253 = Mul(%1251, %1252)\n",
      "  %1254 = Equal(%1233, %1253)\n",
      "  %1255 = Where(%1254, %1251, %1233)\n",
      "  %1256 = Expand(%1224, %1255)\n",
      "  %1257 = Unsqueeze[axes = [-1]](%1256)\n",
      "  %1258 = Shape(%1233)\n",
      "  %1259 = ConstantOfShape[value = <Tensor>](%1258)\n",
      "  %1260 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1261 = Mul(%1259, %1260)\n",
      "  %1262 = Equal(%1233, %1261)\n",
      "  %1263 = Where(%1262, %1259, %1233)\n",
      "  %1264 = Expand(%1226, %1263)\n",
      "  %1265 = Unsqueeze[axes = [-1]](%1264)\n",
      "  %1266 = Shape(%1233)\n",
      "  %1267 = ConstantOfShape[value = <Tensor>](%1266)\n",
      "  %1268 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1269 = Mul(%1267, %1268)\n",
      "  %1270 = Equal(%1233, %1269)\n",
      "  %1271 = Where(%1270, %1267, %1233)\n",
      "  %1272 = Expand(%1228, %1271)\n",
      "  %1273 = Unsqueeze[axes = [-1]](%1272)\n",
      "  %1274 = Concat[axis = -1](%1241, %1249, %1257, %1265, %1273)\n",
      "  %1275 = Shape(%1159)\n",
      "  %1276 = Constant[value = <Tensor>]()\n",
      "  %1277 = Constant[value = <Tensor>]()\n",
      "  %1278 = Constant[value = <Tensor>]()\n",
      "  %1279 = Slice(%1275, %1277, %1278, %1276)\n",
      "  %1280 = Concat[axis = 0](%1233, %1279)\n",
      "  %1281 = Reshape(%1178, %1280)\n",
      "  %1282 = ScatterND(%1159, %1274, %1281)\n",
      "  %1283 = Constant[value = <Tensor>]()\n",
      "  %1284 = Constant[value = <Tensor>]()\n",
      "  %1285 = Constant[value = <Tensor>]()\n",
      "  %1286 = Constant[value = <Tensor>]()\n",
      "  %1287 = Slice(%1282, %1284, %1285, %1283, %1286)\n",
      "  %1288 = Exp(%1287)\n",
      "  %1289 = Constant[value = <Tensor>]()\n",
      "  %1290 = Mul(%1288, %1289)\n",
      "  %1291 = Constant[value = <Tensor>]()\n",
      "  %1292 = Reshape(%1290, %1291)\n",
      "  %1293 = Constant[value = <Tensor>]()\n",
      "  %1295 = ConstantOfShape[value = <Tensor>](%1555)\n",
      "  %1296 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1297 = Mul(%1295, %1296)\n",
      "  %1298 = Constant[value = <Tensor>]()\n",
      "  %1299 = Equal(%1298, %1297)\n",
      "  %1300 = Where(%1299, %1295, %1293)\n",
      "  %1301 = Expand(%1292, %1300)\n",
      "  %1302 = Shape(%1282)\n",
      "  %1303 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1304 = Gather[axis = 0](%1302, %1303)\n",
      "  %1305 = Cast[to = 7](%1304)\n",
      "  %1306 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1307 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1308 = Range(%1306, %1305, %1307)\n",
      "  %1309 = Shape(%1282)\n",
      "  %1310 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1311 = Gather[axis = 0](%1309, %1310)\n",
      "  %1312 = Cast[to = 7](%1311)\n",
      "  %1313 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1314 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1315 = Range(%1313, %1312, %1314)\n",
      "  %1316 = Shape(%1282)\n",
      "  %1317 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1318 = Gather[axis = 0](%1316, %1317)\n",
      "  %1319 = Cast[to = 7](%1318)\n",
      "  %1320 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1321 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1322 = Range(%1320, %1319, %1321)\n",
      "  %1323 = Shape(%1282)\n",
      "  %1324 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1325 = Gather[axis = 0](%1323, %1324)\n",
      "  %1326 = Cast[to = 7](%1325)\n",
      "  %1327 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1328 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1329 = Range(%1327, %1326, %1328)\n",
      "  %1330 = Shape(%1282)\n",
      "  %1331 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1332 = Gather[axis = 0](%1330, %1331)\n",
      "  %1333 = Cast[to = 7](%1332)\n",
      "  %1334 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1335 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1336 = Range(%1334, %1333, %1335)\n",
      "  %1337 = Constant[value = <Tensor>]()\n",
      "  %1338 = Constant[value = <Tensor>]()\n",
      "  %1339 = Constant[value = <Tensor>]()\n",
      "  %1340 = Constant[value = <Tensor>]()\n",
      "  %1341 = Slice(%1336, %1338, %1339, %1337, %1340)\n",
      "  %1342 = Constant[value = <Tensor>]()\n",
      "  %1343 = Reshape(%1308, %1342)\n",
      "  %1344 = Constant[value = <Tensor>]()\n",
      "  %1345 = Reshape(%1315, %1344)\n",
      "  %1346 = Constant[value = <Tensor>]()\n",
      "  %1347 = Reshape(%1322, %1346)\n",
      "  %1348 = Constant[value = <Tensor>]()\n",
      "  %1349 = Reshape(%1329, %1348)\n",
      "  %1350 = Constant[value = <Tensor>]()\n",
      "  %1351 = Reshape(%1341, %1350)\n",
      "  %1352 = Add(%1343, %1345)\n",
      "  %1353 = Add(%1352, %1347)\n",
      "  %1354 = Add(%1353, %1349)\n",
      "  %1355 = Add(%1354, %1351)\n",
      "  %1356 = Shape(%1355)\n",
      "  %1357 = Shape(%1356)\n",
      "  %1358 = ConstantOfShape[value = <Tensor>](%1357)\n",
      "  %1359 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1360 = Mul(%1358, %1359)\n",
      "  %1361 = Equal(%1356, %1360)\n",
      "  %1362 = Where(%1361, %1358, %1356)\n",
      "  %1363 = Expand(%1343, %1362)\n",
      "  %1364 = Unsqueeze[axes = [-1]](%1363)\n",
      "  %1365 = Shape(%1356)\n",
      "  %1366 = ConstantOfShape[value = <Tensor>](%1365)\n",
      "  %1367 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1368 = Mul(%1366, %1367)\n",
      "  %1369 = Equal(%1356, %1368)\n",
      "  %1370 = Where(%1369, %1366, %1356)\n",
      "  %1371 = Expand(%1345, %1370)\n",
      "  %1372 = Unsqueeze[axes = [-1]](%1371)\n",
      "  %1373 = Shape(%1356)\n",
      "  %1374 = ConstantOfShape[value = <Tensor>](%1373)\n",
      "  %1375 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1376 = Mul(%1374, %1375)\n",
      "  %1377 = Equal(%1356, %1376)\n",
      "  %1378 = Where(%1377, %1374, %1356)\n",
      "  %1379 = Expand(%1347, %1378)\n",
      "  %1380 = Unsqueeze[axes = [-1]](%1379)\n",
      "  %1381 = Shape(%1356)\n",
      "  %1382 = ConstantOfShape[value = <Tensor>](%1381)\n",
      "  %1383 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1384 = Mul(%1382, %1383)\n",
      "  %1385 = Equal(%1356, %1384)\n",
      "  %1386 = Where(%1385, %1382, %1356)\n",
      "  %1387 = Expand(%1349, %1386)\n",
      "  %1388 = Unsqueeze[axes = [-1]](%1387)\n",
      "  %1389 = Shape(%1356)\n",
      "  %1390 = ConstantOfShape[value = <Tensor>](%1389)\n",
      "  %1391 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1392 = Mul(%1390, %1391)\n",
      "  %1393 = Equal(%1356, %1392)\n",
      "  %1394 = Where(%1393, %1390, %1356)\n",
      "  %1395 = Expand(%1351, %1394)\n",
      "  %1396 = Unsqueeze[axes = [-1]](%1395)\n",
      "  %1397 = Concat[axis = -1](%1364, %1372, %1380, %1388, %1396)\n",
      "  %1398 = Shape(%1282)\n",
      "  %1399 = Constant[value = <Tensor>]()\n",
      "  %1400 = Constant[value = <Tensor>]()\n",
      "  %1401 = Constant[value = <Tensor>]()\n",
      "  %1402 = Slice(%1398, %1400, %1401, %1399)\n",
      "  %1403 = Concat[axis = 0](%1356, %1402)\n",
      "  %1404 = Reshape(%1301, %1403)\n",
      "  %1405 = ScatterND(%1282, %1397, %1404)\n",
      "  %1406 = Constant[value = <Tensor>]()\n",
      "  %1407 = Constant[value = <Tensor>]()\n",
      "  %1408 = Constant[value = <Tensor>]()\n",
      "  %1409 = Constant[value = <Tensor>]()\n",
      "  %1410 = Slice(%1405, %1407, %1408, %1406, %1409)\n",
      "  %1411 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1412 = Mul(%1410, %1411)\n",
      "  %1413 = Constant[value = <Tensor>]()\n",
      "  %1414 = Reshape(%1412, %1413)\n",
      "  %1415 = Constant[value = <Tensor>]()\n",
      "  %1417 = ConstantOfShape[value = <Tensor>](%1556)\n",
      "  %1418 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1419 = Mul(%1417, %1418)\n",
      "  %1420 = Constant[value = <Tensor>]()\n",
      "  %1421 = Equal(%1420, %1419)\n",
      "  %1422 = Where(%1421, %1417, %1415)\n",
      "  %1423 = Expand(%1414, %1422)\n",
      "  %1424 = Shape(%1405)\n",
      "  %1425 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1426 = Gather[axis = 0](%1424, %1425)\n",
      "  %1427 = Cast[to = 7](%1426)\n",
      "  %1428 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1429 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1430 = Range(%1428, %1427, %1429)\n",
      "  %1431 = Shape(%1405)\n",
      "  %1432 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1433 = Gather[axis = 0](%1431, %1432)\n",
      "  %1434 = Cast[to = 7](%1433)\n",
      "  %1435 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1436 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1437 = Range(%1435, %1434, %1436)\n",
      "  %1438 = Shape(%1405)\n",
      "  %1439 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1440 = Gather[axis = 0](%1438, %1439)\n",
      "  %1441 = Cast[to = 7](%1440)\n",
      "  %1442 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1443 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1444 = Range(%1442, %1441, %1443)\n",
      "  %1445 = Shape(%1405)\n",
      "  %1446 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1447 = Gather[axis = 0](%1445, %1446)\n",
      "  %1448 = Cast[to = 7](%1447)\n",
      "  %1449 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1450 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1451 = Range(%1449, %1448, %1450)\n",
      "  %1452 = Shape(%1405)\n",
      "  %1453 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1454 = Gather[axis = 0](%1452, %1453)\n",
      "  %1455 = Cast[to = 7](%1454)\n",
      "  %1456 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1457 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1458 = Range(%1456, %1455, %1457)\n",
      "  %1459 = Constant[value = <Tensor>]()\n",
      "  %1460 = Constant[value = <Tensor>]()\n",
      "  %1461 = Constant[value = <Tensor>]()\n",
      "  %1462 = Constant[value = <Tensor>]()\n",
      "  %1463 = Slice(%1458, %1460, %1461, %1459, %1462)\n",
      "  %1464 = Constant[value = <Tensor>]()\n",
      "  %1465 = Reshape(%1430, %1464)\n",
      "  %1466 = Constant[value = <Tensor>]()\n",
      "  %1467 = Reshape(%1437, %1466)\n",
      "  %1468 = Constant[value = <Tensor>]()\n",
      "  %1469 = Reshape(%1444, %1468)\n",
      "  %1470 = Constant[value = <Tensor>]()\n",
      "  %1471 = Reshape(%1451, %1470)\n",
      "  %1472 = Constant[value = <Tensor>]()\n",
      "  %1473 = Reshape(%1463, %1472)\n",
      "  %1474 = Add(%1465, %1467)\n",
      "  %1475 = Add(%1474, %1469)\n",
      "  %1476 = Add(%1475, %1471)\n",
      "  %1477 = Add(%1476, %1473)\n",
      "  %1478 = Shape(%1477)\n",
      "  %1479 = Shape(%1478)\n",
      "  %1480 = ConstantOfShape[value = <Tensor>](%1479)\n",
      "  %1481 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1482 = Mul(%1480, %1481)\n",
      "  %1483 = Equal(%1478, %1482)\n",
      "  %1484 = Where(%1483, %1480, %1478)\n",
      "  %1485 = Expand(%1465, %1484)\n",
      "  %1486 = Unsqueeze[axes = [-1]](%1485)\n",
      "  %1487 = Shape(%1478)\n",
      "  %1488 = ConstantOfShape[value = <Tensor>](%1487)\n",
      "  %1489 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1490 = Mul(%1488, %1489)\n",
      "  %1491 = Equal(%1478, %1490)\n",
      "  %1492 = Where(%1491, %1488, %1478)\n",
      "  %1493 = Expand(%1467, %1492)\n",
      "  %1494 = Unsqueeze[axes = [-1]](%1493)\n",
      "  %1495 = Shape(%1478)\n",
      "  %1496 = ConstantOfShape[value = <Tensor>](%1495)\n",
      "  %1497 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1498 = Mul(%1496, %1497)\n",
      "  %1499 = Equal(%1478, %1498)\n",
      "  %1500 = Where(%1499, %1496, %1478)\n",
      "  %1501 = Expand(%1469, %1500)\n",
      "  %1502 = Unsqueeze[axes = [-1]](%1501)\n",
      "  %1503 = Shape(%1478)\n",
      "  %1504 = ConstantOfShape[value = <Tensor>](%1503)\n",
      "  %1505 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1506 = Mul(%1504, %1505)\n",
      "  %1507 = Equal(%1478, %1506)\n",
      "  %1508 = Where(%1507, %1504, %1478)\n",
      "  %1509 = Expand(%1471, %1508)\n",
      "  %1510 = Unsqueeze[axes = [-1]](%1509)\n",
      "  %1511 = Shape(%1478)\n",
      "  %1512 = ConstantOfShape[value = <Tensor>](%1511)\n",
      "  %1513 = Constant[value = <Scalar Tensor []>]()\n",
      "  %1514 = Mul(%1512, %1513)\n",
      "  %1515 = Equal(%1478, %1514)\n",
      "  %1516 = Where(%1515, %1512, %1478)\n",
      "  %1517 = Expand(%1473, %1516)\n",
      "  %1518 = Unsqueeze[axes = [-1]](%1517)\n",
      "  %1519 = Concat[axis = -1](%1486, %1494, %1502, %1510, %1518)\n",
      "  %1520 = Shape(%1405)\n",
      "  %1521 = Constant[value = <Tensor>]()\n",
      "  %1522 = Constant[value = <Tensor>]()\n",
      "  %1523 = Constant[value = <Tensor>]()\n",
      "  %1524 = Slice(%1520, %1522, %1523, %1521)\n",
      "  %1525 = Concat[axis = 0](%1478, %1524)\n",
      "  %1526 = Reshape(%1423, %1525)\n",
      "  %1527 = ScatterND(%1405, %1519, %1526)\n",
      "  %1530 = Unsqueeze[axes = [0]](%1147)\n",
      "  %1533 = Concat[axis = 0](%1530, %1557, %1558)\n",
      "  %1534 = Reshape(%1527, %1533)\n",
      "  %classes = Concat[axis = 1](%1119, %1534)\n",
      "  return %classes, %boxes, %1159\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "onnx.checker.check_model(onnx_model)  # Check that the IR is well formed\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))  # Print a human readable representation of the graph\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"yolo-fastest.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "x_array = to_numpy(x)\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: x_array}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2880, 7])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 2699 / 6300 (42.8%)\nMax absolute difference:      18.566\nMax relative difference:      194.19\n x: array([[[     23.695,      23.414,      58.536, ...,  3.4793e-05,    0.044989,     0.96375],\n        [      48.86,       26.48,      104.83, ...,  0.00025207,    0.018993,     0.98267],\n        [     85.665,      27.322,      158.78, ...,  0.00043596,   0.0056647,     0.99308],...\n y: array([[[     23.695,      23.414,      58.536, ...,     -10.266,     -3.0553,      3.2804],\n        [      48.86,       26.48,      104.83, ...,     -8.2856,     -3.9445,      4.0381],\n        [     85.665,      27.322,      158.78, ...,     -7.7375,     -5.1678,      4.9657],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-29f2a1871e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compare ONNX Runtime and PyTorch results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch_out_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_out_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mort_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exported model has been tested with ONNXRuntime, and the result looks good!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_allclose\u001b[0;34m(actual, desired, rtol, atol, equal_nan, err_msg, verbose)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Not equal to tolerance rtol=%g, atol=%g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\n\u001b[0;32m-> 1528\u001b[0;31m                          verbose=verbose, header=header, equal_nan=equal_nan)\n\u001b[0m\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    838\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 2699 / 6300 (42.8%)\nMax absolute difference:      18.566\nMax relative difference:      194.19\n x: array([[[     23.695,      23.414,      58.536, ...,  3.4793e-05,    0.044989,     0.96375],\n        [      48.86,       26.48,      104.83, ...,  0.00025207,    0.018993,     0.98267],\n        [     85.665,      27.322,      158.78, ...,  0.00043596,   0.0056647,     0.99308],...\n y: array([[[     23.695,      23.414,      58.536, ...,     -10.266,     -3.0553,      3.2804],\n        [      48.86,       26.48,      104.83, ...,     -8.2856,     -3.9445,      4.0381],\n        [     85.665,      27.322,      158.78, ...,     -7.7375,     -5.1678,      4.9657],..."
     ]
    }
   ],
   "source": [
    "# compare ONNX Runtime and PyTorch results\n",
    "torch_out_array = [to_numpy(torch_out[0]), to_numpy(torch_out[1][0]),to_numpy(torch_out[1][1])]\n",
    "np.testing.assert_allclose(torch_out_array[0], ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_allclose(torch_out_array[1], ort_outs[1], rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(torch_out_array[2], ort_outs[2], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
