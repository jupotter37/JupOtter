{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f69095-f2ce-4966-9544-04964e19eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import itertools\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import textgrid\n",
    "import random\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DistributedSampler, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Local\n",
    "from utils.misc import dict_to_object, plot_specgram, plot_waveform\n",
    "from supervoice.audio import spectogram, load_mono_audio\n",
    "from supervoice.model_duration import DurationPredictor\n",
    "from supervoice.tokenizer import Tokenizer\n",
    "from train_config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08046af5-d95f-4c9c-88ee-5d693d1b752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88146\n"
     ]
    }
   ],
   "source": [
    "# Load text grid files\n",
    "files = glob(\"datasets/vctk-aligned/**/*.TextGrid\")\n",
    "print(len(files))\n",
    "files = files[0:10]\n",
    "files = [textgrid.TextGrid.fromFile(f) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c07a148-f9d2-448d-b154-48a820c2c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(config)\n",
    "\n",
    "# Data extractor\n",
    "def extract_data(src):\n",
    "\n",
    "    # Prepare\n",
    "    token_duration = 0.01\n",
    "    tokens = src[1]\n",
    "    time = 0\n",
    "    output_tokens = []\n",
    "    output_durations = []\n",
    "\n",
    "    # Iterate over tokens\n",
    "    for t in tokens:\n",
    "\n",
    "        # Resolve durations\n",
    "        ends = t.maxTime\n",
    "        duration = math.floor((ends - time) / token_duration)\n",
    "        time = ends\n",
    "\n",
    "        # Resolve token\n",
    "        tok = t.mark\n",
    "        if tok == '':\n",
    "            tok = tokenizer.silence_token\n",
    "\n",
    "        # Apply\n",
    "        output_tokens.append(tok)\n",
    "        output_durations.append(duration)\n",
    "\n",
    "    # Trim start silence\n",
    "    while(output_tokens[0] == 'SIL'):\n",
    "        output_durations\n",
    "    if output_tokens[0] == 'SIL' and output_durations[0] > 1:\n",
    "        output_durations[0] = 1\n",
    "    if output_tokens[len(output_tokens) - 1] == 'SIL' and output_durations[len(output_durations) - 1] > 1:\n",
    "        output_durations[len(output_durations) - 1] = 1\n",
    "\n",
    "    # Outputs\n",
    "    return output_tokens, output_durations\n",
    "    \n",
    "class TextGridDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "    def __len__(self):\n",
    "        return len(self.files)        \n",
    "    def __getitem__(self, index):\n",
    "        tg = self.files[index]\n",
    "\n",
    "        # Load tokens/durations\n",
    "        tokens, durations = extract_data(tg)\n",
    "        tokens = tokenizer(tokens)\n",
    "        durations = torch.Tensor(durations)\n",
    "\n",
    "        # Calculate mask        \n",
    "        mask_len = random.uniform(0.3, 0.7)\n",
    "        mask_offset = random.uniform(0, 1 - mask_len)\n",
    "        mask = torch.zeros(len(durations))\n",
    "        mask_start = math.floor(mask_offset * len(durations))\n",
    "        mask_end = math.floor((mask_offset + mask_len) * len(durations))\n",
    "        mask[mask_start : mask_end] = 1\n",
    "        mask = mask.bool()\n",
    "\n",
    "        # Result\n",
    "        return tokens, durations, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e7a5b4-f033-4ed8-a0e5-c15052d002c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Dataset, model, optimizer\n",
    "device = \"cpu\"\n",
    "dataset = TextGridDataset(files)\n",
    "dataloader = DataLoader(dataset, batch_size = 1)\n",
    "model = DurationPredictor(config)\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(), 0.0002, betas=[0.8, 0.99])\n",
    "\n",
    "checkpoint = torch.load(f'./output/duration_common_n_t.pt', map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d37ab71-8350-4499-8f89-e0dd4180c291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06935125589370728\n",
      "<SIL>    ɒ    n   d̪    ə    ɡ    ɹ   aw    n   d̪    ə    w    ɛ    s    t    b    æ    ŋ    k    w    ə    z    ə    ɡ    ɛ    n   d̪    ə    f   ow    k    ə    s    ə    v    v   aj    ə    l    ə    n    s<SIL>\n",
      "    0   10    7    4    4    7    6   14    7    5    6    7    6    9    6    5    8    6    4    3    3    8    4    7    6    6    4    4   10    8    6    6    9    5    7    6   11    6    6    7    7   10    0\n",
      "   85    9    7    4    4    6    5   14    7    5    5    6    8    6    6    5    6    9    7    4    4    6    4    8    7    6    2    3   10    7    8    3    8    4    6    4   12    6    4    6   10   22    3\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    1    1    1    1    1    1    1    1    1    1    1    1    0    0\n",
      "Loss: 0.09099254757165909\n",
      "<SIL>   aj    w    ə    z    ɒ    f    ə    ɫ<SIL>\n",
      "    0    8    5    4    8    8   11    2    6    0\n",
      "   75   13    5    4    9   14   11    4   27   14\n",
      "    0    0    1    1    1    1    1    1    0    0\n",
      "Loss: 0.14222635328769684\n",
      "<SIL>   aj    w    ə    z    s    t    ɐ    n    d<SIL>\n",
      "    0   15    6    4    5    6    7    7    7    1    0\n",
      "   72   10    3    4    5   10    5    7   19    2   55\n",
      "    0    1    1    1    1    1    0    0    0    0    0\n",
      "Loss: 0.1468198001384735\n",
      "<SIL>    ɪ    t    s   d̪   iː   ow    ɲ    ʎ    i    θ    ɪ    ŋ<SIL>\n",
      "    0    4    5    8    6    8    7    5    5    5    8    6    8    0\n",
      "   83    9    4    9    4    7    4    5    4    6    9   12   16    6\n",
      "    0    0    0    0    1    1    1    1    1    1    1    1    0    0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tokens not found: ['spn']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     tokens, durations, mask \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m     predicted, loss \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m      5\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m      6\u001b[0m         durations \u001b[38;5;241m=\u001b[39m durations\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m      7\u001b[0m         mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m      8\u001b[0m         target \u001b[38;5;241m=\u001b[39m durations\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m predicted\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mTextGridDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Load tokens/durations\u001b[39;00m\n\u001b[1;32m     51\u001b[0m tokens, durations \u001b[38;5;241m=\u001b[39m extract_data(tg)\n\u001b[0;32m---> 52\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m durations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(durations)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Calculate mask        \u001b[39;00m\n",
      "File \u001b[0;32m/data/notebooks/supervoice/supervoice/tokenizer.py:22\u001b[0m, in \u001b[0;36mTokenizer.__call__\u001b[0;34m(self, tokens, force)\u001b[0m\n\u001b[1;32m     20\u001b[0m         missing\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_to_id[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens])\n",
      "\u001b[0;31mValueError\u001b[0m: Tokens not found: ['spn']"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # Predict\n",
    "    tokens, durations, mask = dataset[i]\n",
    "    predicted, loss = model(\n",
    "        tokens = tokens.unsqueeze(0).to(device), \n",
    "        durations = durations.unsqueeze(0).to(device), \n",
    "        mask = mask.unsqueeze(0).to(device), \n",
    "        target = durations.unsqueeze(0).to(device)\n",
    "    )\n",
    "    predicted = predicted.squeeze()\n",
    "\n",
    "    # Log\n",
    "    print(f'Loss: {loss.item()}')\n",
    "    print(''.join(f\"{tokenizer.tokens[num]:>5}\" for num in tokens.tolist()))\n",
    "    print(''.join(f\"{num:5}\" for num in predicted.tolist()))\n",
    "    print(''.join(f\"{int(num):5}\" for num in durations.tolist()))\n",
    "    print(''.join(f\"{int(num):5}\" for num in mask.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fbc3e-ef49-4467-a77b-613919d6d06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
