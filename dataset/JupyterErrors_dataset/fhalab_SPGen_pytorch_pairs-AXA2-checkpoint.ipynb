{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention model for generating predictions for signal peptides that end in AXA (for Zach). Information about predictions, target sp sequence, and protein sequence for 20 sps was given.\n",
    "\n",
    "Second notebook for finding at least 20 sps that end in AXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at torch/csrc/cuda/Module.cpp:267",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ace076f4264a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCharacterTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSignalTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36msynchronize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;34m\"\"\"Waits for all kernels in all streams on current device to complete.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at torch/csrc/cuda/Module.cpp:267"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from transformer import Models\n",
    "from transformer import Beam\n",
    "from transformer import Translator\n",
    "from transformer.Optim import ScheduledOptim\n",
    "\n",
    "from tools import CharacterTable\n",
    "from translator import SignalTranslator\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting the loss in the middle of the sequence\n",
    "# Different lengths for protein in training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. Don't change these. \n",
    "pad = 0\n",
    "stop = 2\n",
    "start = 1\n",
    "max_in = 107\n",
    "max_out = 72\n",
    "n_chars = 27\n",
    "\n",
    "# Hyperparameters. Do change these\n",
    "d_model = 550\n",
    "batch_size = 64\n",
    "batches = 500 # batches / epoch\n",
    "n_warmup_steps = batches * 25\n",
    "n_layers = 6\n",
    "n_head = 5\n",
    "dropout = 0.1\n",
    "d_k = 64\n",
    "epochs = 10\n",
    "lr_max = 1e-4\n",
    "decay_power = -0.03\n",
    "\n",
    "# Name for the model checkpoints\n",
    "# Change this, probably to reflect the hyperparameters chosen above\n",
    "hypers = [d_model, n_warmup_steps, batch_size, n_layers, n_head, dropout, d_k, epochs, lr_max, decay_power]\n",
    "chkpt_name = '_'.join([str(h) for h in hypers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = argparse.Namespace()\n",
    "model_opt.src_vocab_size = n_chars\n",
    "model_opt.tgt_vocab_size = n_chars\n",
    "model_opt.max_token_seq_len = max_in\n",
    "model_opt.proj_share_weight = True\n",
    "model_opt.embs_share_weight = True\n",
    "model_opt.d_k = d_k\n",
    "model_opt.d_v = d_k\n",
    "model_opt.d_model = d_model\n",
    "model_opt.d_word_vec = d_model\n",
    "model_opt.d_inner_hid = 2 * d_model\n",
    "model_opt.n_layers = n_layers\n",
    "model_opt.n_head = n_head\n",
    "model_opt.dropout = dropout\n",
    "\n",
    "##############################\n",
    "# Change this to use the GPU #\n",
    "model_opt.cuda = True        #\n",
    "##############################\n",
    "\n",
    "optim_opt = argparse.Namespace()\n",
    "optim_opt.n_warmup_steps = n_warmup_steps\n",
    "optim_opt.optim = optim.Adam\n",
    "optim_opt.lr_max = lr_max\n",
    "optim_opt.decay_power = decay_power\n",
    "optim_opt.d_model = None\n",
    "\n",
    "trans_opt = argparse.Namespace()\n",
    "trans_opt.beam_size = 1\n",
    "trans_opt.n_best = 1\n",
    "trans_opt.max_trans_length = max_out\n",
    "\n",
    "with open('../outputs/ctable_token.pkl', 'rb') as f:\n",
    "    ctable = pickle.load(f)\n",
    "    \n",
    "trans_opt.ctable = ctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See how well it does on first batch_size test sequences\n",
    "file = h5py.File('../data/test_tokens.hdf5')\n",
    "training_data = SignalTranslator.generator_from_h5(file, batch_size, shuffle=False, use_cuda=True)\n",
    "src, tgt = next(training_data) # src is prot sequence, tgt is signal pep\n",
    "#%time decoded, all_hyp, all_scores = clf.translate_batch(src) # predict signal pep from src (prot seq)\n",
    "for tg, tg2, in zip(tgt[0], src[0]):\n",
    "    actual = ctable.decode(tg.data.cpu().numpy())[:]\n",
    "    print(actual)\n",
    "    print(tg2)\n",
    "    print()\n",
    "    \n",
    "    # sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "    # means its the same SP\n",
    "    # plot signal peptide to similarity\n",
    "    # plot top three simmilar to how similar their SPs areq\n",
    "    \n",
    "    actual = ''.join(actual.split())\n",
    "    actual = actual[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = next(training_data) # src is prot sequence, tgt is signal pep\n",
    "#%time decoded, all_hyp, all_scores = clf.translate_batch(src) # predict signal pep from src (prot seq)\n",
    "for tg, tg2, in zip(tgt[0], src[0]):\n",
    "    actual = ctable.decode(tg.data.cpu().numpy())[:]\n",
    "    print(actual)\n",
    "    print(tg2)\n",
    "    print()\n",
    "    \n",
    "    # sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "    # means its the same SP\n",
    "    # plot signal peptide to similarity\n",
    "    # plot top three simmilar to how similar their SPs areq\n",
    "    \n",
    "    actual = ''.join(actual.split())\n",
    "    actual = actual[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = next(training_data) # src is prot sequence, tgt is signal pep\n",
    "#%time decoded, all_hyp, all_scores = clf.translate_batch(src) # predict signal pep from src (prot seq)\n",
    "for tg, tg2, in zip(tgt[0], src[0]):\n",
    "    actual = ctable.decode(tg.data.cpu().numpy())[:]\n",
    "    print(actual)\n",
    "    print(tg2)\n",
    "    print()\n",
    "    \n",
    "    # sum returns length of amino acids that are the same at same position? --> so full length returned\n",
    "    # means its the same SP\n",
    "    # plot signal peptide to similarity\n",
    "    # plot top three simmilar to how similar their SPs areq\n",
    "    \n",
    "    actual = ''.join(actual.split())\n",
    "    actual = actual[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
