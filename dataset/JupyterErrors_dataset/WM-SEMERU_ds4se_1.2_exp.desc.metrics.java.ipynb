{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exp.desc.metrics.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises some of the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifically in this module\n",
    "\n",
    "> - Cyclomatic complexity (CYCLO)\n",
    "> - Number of lines of code (NLOC)\n",
    "> - Lack of Cohesion of Methods 5 (LCOM5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current limitations\n",
    "\n",
    "> - Can only compute LCOM5 for Java files\n",
    "> - Can theoretically compute Cyclomatic Complexity for \n",
    "> > - C\n",
    "> > - C++ (works with C++14)\n",
    "> > - Java\n",
    "> > - C# (C Sharp)\n",
    "> > - JavaScript (With ES6 and JSX)\n",
    "> > - Objective-C\n",
    "> > - Swift\n",
    "> > - Python\n",
    "> > - Ruby\n",
    "> > - TTCN-3\n",
    "> > - PHP\n",
    "> > - Scala\n",
    "> > - GDScript\n",
    "> > - Golang\n",
    "> > - Lua\n",
    "> > - Rust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from numpy import mean, std\n",
    "from statistics import median\n",
    "from scipy.stats import sem, t\n",
    "import lizard\n",
    "import matplotlib.pyplot as plt\n",
    "from tree_sitter import Language, Parser, Node\n",
    "#Decoding files\n",
    "import chardet\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tree-sitter-java'...\n",
      "remote: Enumerating objects: 225, done.\u001b[K\n",
      "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
      "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
      "remote: Total 1510 (delta 115), reused 172 (delta 72), pack-reused 1285\u001b[K\n",
      "Receiving objects: 100% (1510/1510), 13.05 MiB | 14.91 MiB/s, done.\n",
      "Resolving deltas: 100% (935/935), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tree-sitter/tree-sitter-java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get_unicode & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "'''\n",
    "Detects file encoding and returns unicode.\n",
    "Inspired by http://reinvantveer.github.io/2017/05/19/unicode-dammit.html\n",
    "\n",
    ":param file_path: file path of file\n",
    ":returns: unicode string of the file\n",
    ":raises ValueError: empty or invalud csv file\n",
    "'''\n",
    "def get_unicode(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        detection = chardet.detect(f.read())\n",
    "        \n",
    "    enc = detection[\"encoding\"]\n",
    "    if detection[\"encoding\"] == \"ascii\":\n",
    "        with open(file_path, encoding=\"ascii\") as f:\n",
    "            data = f.read()\n",
    "    elif detection[\"encoding\"] == \"ISO-8859-9\":\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            enc = \"utf-8\"\n",
    "            data = f.read()\n",
    "    else:\n",
    "        try:\n",
    "            # Try to open as non unicode file\n",
    "            with open(file_path, encoding=detection[\"encoding\"]) as f:\n",
    "                data = f.read()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Cannot return dictionary from empty or invalid csv file {file_path} due to {e}\")\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(f\"Cannot return dictionary from empty or invalid csv file {file_path}\")\n",
    "\n",
    "    return UnicodeDammit(data).unicode_markup, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_unicode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b01c919796a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello, this is a test file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Calling function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_unicode' is not defined"
     ]
    }
   ],
   "source": [
    "# Setting up test data\n",
    "file_path = \"test_utf_8_file.txt\"\n",
    "with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(\"Hello, this is a test file\")\n",
    "# Calling function\n",
    "text, encoding = get_unicode(file_path)\n",
    "# Cleanup\n",
    "os.remove(file_path)\n",
    "# Displaying output\n",
    "print(encoding)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "'''\n",
    "Loads files from a specified folder into a pandas dataframe\n",
    "\n",
    ":param folder_path: path to folder\n",
    ":returns: corpus_data dataframe\n",
    "'''\n",
    "def simulate_getting_dataframes_from_mongo(folder_path):\n",
    "    corpus_data = {\"system\": [], \"name\": [], \"ground_truth\": [], \"contents\": [], \"encoding\": []}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not os.path.isdir(os.path.join(folder_path, file)) and file != \".DS_Store\":\n",
    "            corpus_data[\"system\"].append(None)\n",
    "            corpus_data[\"name\"].append(file)\n",
    "            corpus_data[\"ground_truth\"].append(\"src\")\n",
    "            contents, enc = get_unicode(os.path.join(folder_path, file))\n",
    "            corpus_data['encoding'].append(enc)\n",
    "            corpus_data['contents'].append(contents)\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add_mccabe_metrics & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "'''\n",
    "Adds information about function length and cyclomatic complexity for classes to a dataframe\n",
    "\n",
    ":param df: dataframe \n",
    ":param data_col: data\n",
    ":param name_col: name\n",
    ":returns: dataframe with added columns\n",
    "'''\n",
    "def add_mccabe_metrics(df, data_col, name_col):\n",
    "    num_funcs = []\n",
    "    class_ccn = []\n",
    "    avg_func_ccn = []\n",
    "    avg_func_nloc = []\n",
    "    for i in range(len(df)):\n",
    "        file_num_funcs = []\n",
    "        file_class_ccn = []\n",
    "        file_avg_func_ccn = []\n",
    "        file_avg_func_nloc = []\n",
    "        metrics = lizard.analyze_file.analyze_source_code(df[name_col][i], df[data_col][i])\n",
    "        class_dict = {}\n",
    "        for func in metrics.function_list:\n",
    "            class_name = '::'.join(func.name.split(\"::\")[:-1])\n",
    "            if class_name in class_dict:\n",
    "                class_dict[class_name].append(func)\n",
    "            else:\n",
    "                class_dict[class_name] = [func]\n",
    "        for class_key in class_dict:\n",
    "            total_class_ccn = 0\n",
    "            total_class_nloc = 0\n",
    "            for func in class_dict[class_key]:\n",
    "                total_class_ccn += func.cyclomatic_complexity\n",
    "                total_class_nloc += func.length\n",
    "            file_num_funcs.append(len(class_dict[class_key]))\n",
    "            file_class_ccn.append(total_class_ccn)\n",
    "            file_avg_func_ccn.append(total_class_ccn/len(class_dict[class_key]))\n",
    "            file_avg_func_nloc.append(total_class_nloc/len(class_dict[class_key]))\n",
    "\n",
    "        num_funcs.append(file_num_funcs)\n",
    "        class_ccn.append(file_class_ccn)\n",
    "        avg_func_ccn.append(file_avg_func_ccn)\n",
    "        avg_func_nloc.append(file_avg_func_nloc)\n",
    "\n",
    "    df[\"num_funcs\"] = num_funcs\n",
    "    df[\"class_ccn\"] = class_ccn\n",
    "    df[\"avg_func_ccn\"] = avg_func_ccn\n",
    "    df[\"avg_func_nloc\"] = avg_func_nloc\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up test data\n",
    "file_path = \"test_java_file.java\"\n",
    "folder_path = \"test_dir\"\n",
    "os.mkdir(folder_path)\n",
    "with open(os.path.join(folder_path, file_path), 'w') as f:\n",
    "    f.write(\"public int main() {}\")\n",
    "# Calling function\n",
    "pd_dataframe = simulate_getting_dataframes_from_mongo(folder_path)\n",
    "pd_dataframe = add_mccabe_metrics(pd_dataframe, \"contents\", \"name\")\n",
    "# Cleanup\n",
    "os.remove(os.path.join(folder_path,file_path))\n",
    "os.rmdir(folder_path)\n",
    "# Displaying output\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Creates a dictionary of tree-sitter parsers for select languages\n",
    "\n",
    ":param path: None\n",
    ":returns: dictionary with java and java language\n",
    "'''\n",
    "def create_parser_builds(path=None):\n",
    "    Language.build_library(\n",
    "        # Store the library in the `build` directory\n",
    "        'build/my-languages.so',\n",
    "\n",
    "        # Include one or more languages\n",
    "        [\n",
    "            'tree-sitter-java'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    JAVA_LANGUAGE = Language('build/my-languages.so', 'java')\n",
    "    \n",
    "    return {\"java\":JAVA_LANGUAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Returns language specific keywords for a parser to find\n",
    "'''\n",
    "def lang_keywords():\n",
    "    keyword_dict = {}\n",
    "    keyword_dict[\"java\"] = {\"class\": \"class_declaration\", \"method\":\"method_declaration\", \"field_dec\":\"field_declaration\", \"field_name\":\"identifier\"}\n",
    "    return keyword_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Recursively searches an AST for class nodes\n",
    "\n",
    ":param root_node: root node to begin search\n",
    ":param key: key to search\n",
    ":returns: list of class nodes\n",
    "'''\n",
    "def find_class_nodes(root_node, key):\n",
    "    node_list = []\n",
    "    def rec_class_search(node):\n",
    "        if node.type == key[\"class\"]:\n",
    "            node_list.append(node)\n",
    "        for child in node.children:\n",
    "            rec_class_search(child)\n",
    "\n",
    "    rec_class_search(root_node)\n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Recursively searches an AST for method nodes\n",
    "\n",
    ":param root_node: root node to begin search\n",
    ":param key: key to search\n",
    ":returns: list of method nodes\n",
    "'''\n",
    "def find_method_nodes(class_node, key):\n",
    "    node_list = []\n",
    "    def rec_method_search(node):\n",
    "        if node.type == key[\"method\"]:\n",
    "            node_list.append(node)\n",
    "        if node.type != key[\"class\"]:\n",
    "            for child in node.children:\n",
    "                rec_method_search(child)\n",
    "    \n",
    "    for node in class_node.children:\n",
    "        rec_method_search(node)\n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Finds the fields/attributes for a class AST\n",
    "\n",
    ":param class_node: class node to search\n",
    ":param file_bytes: list of bytes\n",
    ":param key: key to search\n",
    ":returns: list of class fields\n",
    "'''\n",
    "def find_field_names(class_node, file_bytes, key):\n",
    "    class_fields = []\n",
    "    \n",
    "    def rec_name_search(node):\n",
    "        if node.type == key[\"field_name\"]:\n",
    "            word = []\n",
    "            for i in range(node.start_byte, node.end_byte):\n",
    "                word.append(file_bytes[i])\n",
    "            class_fields.append(word)\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                rec_name_search(child)\n",
    "    \n",
    "    def rec_field_search(node):\n",
    "        if node.type == key[\"field_dec\"]:\n",
    "            rec_name_search(node)\n",
    "        if node.type != key[\"class\"]:\n",
    "            for child in node.children:\n",
    "                rec_field_search(child)\n",
    "\n",
    "    for node in class_node.children:\n",
    "        rec_field_search(node)\n",
    "\n",
    "    return class_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Counts the number of occurences of a byte-pattern array in a sample of code\n",
    "\n",
    ":param node:\n",
    ":param pattern: pattern to look for\n",
    ":param file_bytes: byte-array\n",
    ":returns: number of occurrences\n",
    ":raises IndexError: if file_bytes[i] not good\n",
    "'''\n",
    "def find_string_in_text(node, pattern, file_bytes):\n",
    "    if len(node.children) > 0:\n",
    "        count = 0\n",
    "        for i in node.children:\n",
    "            count += find_string_in_text(i, pattern, file_bytes)\n",
    "        return count\n",
    "    else:\n",
    "        word = []\n",
    "        for i in range(node.start_byte, node.end_byte):\n",
    "            num_index_fails = 0\n",
    "            try:\n",
    "                word.append(file_bytes[i])\n",
    "            except IndexError:\n",
    "                num_index_fails += 1\n",
    "        if(num_index_fails):\n",
    "            print(f\"INDEX ERROR ({num_index_fails} times)\")\n",
    "            print(\"Start byte:\", node.start_byte, \"End byte:\", node.end_byte, \"Word:\", word)\n",
    "        if word == pattern:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Recursively searches an AST for method nodes\n",
    "\n",
    ":param class_node:\n",
    ":param field_names: \n",
    ":param file_bytes: byte array\n",
    ":param key: key to search\n",
    ":returns: number of total distinct calls\n",
    "'''\n",
    "def distinct_field_calls(class_node, field_names, file_bytes, key):\n",
    "    total_distinct_calls = []\n",
    "    \n",
    "    def rec_method_search(node):\n",
    "        if node.type == key[\"method\"]:\n",
    "            distinct_method_field_calls = 0\n",
    "            for field in field_names:\n",
    "                if find_string_in_text(node, field, file_bytes):\n",
    "                    distinct_method_field_calls += 1\n",
    "            total_distinct_calls.append(distinct_method_field_calls)\n",
    "        if node.type != key[\"class\"]:\n",
    "            for child in node.children:\n",
    "                rec_method_search(child)\n",
    "    \n",
    "    for node in class_node.children:\n",
    "        rec_method_search(node)\n",
    "        \n",
    "    return len(total_distinct_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Parses the syntax tree of code to calculate the LCOM5 of its classes\n",
    "\n",
    ":param tree: syntax tree\n",
    ":param extension: file extension type\n",
    ":param file_bytes: list of bytes\n",
    ":param name: not used\n",
    ":returns: LCOM5 list\n",
    "'''\n",
    "def calculate_lcom5(tree, extension, file_bytes, name):\n",
    "    keyword_dict = lang_keywords()\n",
    "\n",
    "    if extension not in keyword_dict:\n",
    "        print(f\"Tried to get LCOM5 of file with unsupported extension '.{extension}', 0 assigned to column.\")\n",
    "        return [\"Undefined\"]\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    keywords = keyword_dict[extension]\n",
    "    class_nodes = find_class_nodes(root_node, keywords)\n",
    "    class_method_nodes = []\n",
    "    class_field_names = []\n",
    "    class_dfc = [] # Distinct field calls, as per the definition of LCOM5\n",
    "    for node in enumerate(class_nodes):\n",
    "        class_method_nodes.append(find_method_nodes(node[1], keywords))\n",
    "        class_field_names.append(find_field_names(node[1], file_bytes, keywords))\n",
    "        class_dfc.append(distinct_field_calls(node[1], class_field_names[node[0]], file_bytes, keywords))\n",
    "    lcom5_list = []\n",
    "    for j in range(len(class_nodes)):\n",
    "        num_fields = len(class_field_names[j])\n",
    "        num_meths = len(class_method_nodes[j])\n",
    "        num_dac = class_dfc[j]\n",
    "        numerator = num_dac - (num_meths*num_fields)\n",
    "        denominator = num_fields - (num_meths*num_fields)\n",
    "        if denominator == 0:\n",
    "            lcom5_list.append(\"Undefined\")\n",
    "        else:\n",
    "            lcom5_list.append(numerator/denominator)\n",
    "    return lcom5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Adds a column with the LCOM5 of each class of each file to a dataframe\n",
    "\n",
    ":param df: input dataframe\n",
    ":param col: not used\n",
    ":returns: updated dataframe\n",
    "'''\n",
    "def add_lcom5(df, col):\n",
    "    lang_builds = create_parser_builds()\n",
    "    parser = Parser()\n",
    "    class_lcom5 = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        ext = df[\"name\"][i].split('.')[-1]\n",
    "        parser.set_language(lang_builds[ext])\n",
    "        enc = df[\"encoding\"][i]\n",
    "        tree = parser.parse(bytes(df[\"contents\"][i], df[\"encoding\"][i]))\n",
    "        class_lcom5.append(calculate_lcom5(tree, ext, bytes(df[\"contents\"][i], df[\"encoding\"][i]), df[\"name\"][i]))\n",
    "    df[\"class_lcom5\"] = class_lcom5\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Takes in a list of lists and flattens it, returning a list of each entry\n",
    "\n",
    ":param list_list: list of lists to flatten\n",
    ":returns: flattened list\n",
    "'''\n",
    "def flatten_lol(list_list):\n",
    "    flattened_list = []\n",
    "    for sublist in list_list:\n",
    "        for entry in sublist:\n",
    "            flattened_list.append(entry)\n",
    "    return flattened_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "'''\n",
    "Computes statistical metrics about the entries in a dataframe column or list\n",
    "\n",
    ":param col: dataframe column or list\n",
    ":param conf: confidence level\n",
    ":param sig_figs: significant figures for rounding\n",
    ":param clean: only look at ints, floats, or complex in col\n",
    ":param verbose_clean: displays number of non-numeric entries removed\n",
    "'''\n",
    "def display_numeric_col_stats(col, conf = 0.95, sig_figs = 4, clean=True, verbose_clean=False):\n",
    "    previous_length = len(col)\n",
    "    numeric_types = [int, float, complex]\n",
    "    if clean: col = [x for x in col if type(x) in numeric_types]\n",
    "    if verbose_clean: print(f\"Cleaning removed {previous_length - len(col)} non-numeric entries\")\n",
    "\n",
    "    if len(col) < 1:\n",
    "        print(\"Error, data must contain at least one valid entry to display statistics\")\n",
    "        return\n",
    "\n",
    "    print(\"Min =\", round(min(col), sig_figs))\n",
    "    print(\"Max =\", round(max(col), sig_figs))\n",
    "    print(\"Average =\", round(mean(col), sig_figs))\n",
    "    print(\"Median =\", round(median(col), sig_figs))\n",
    "    print(\"Standard Deviation =\", round(std(col), sig_figs))\n",
    "    \n",
    "    n = len(col)\n",
    "    m = mean(col)\n",
    "    std_err = sem(col)\n",
    "    h = std_err * t.ppf((1 + conf) / 2, n - 1)\n",
    "\n",
    "    start = m - h\n",
    "    end = m + h\n",
    "    print(f\"{conf} of data points fall between {round(start, sig_figs)} and {round(end, sig_figs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''\n",
    "Displays a histogram with a customized number of bins for the data in a specified dataframe column or list\n",
    "\n",
    ":param col: df column or list to plot\n",
    ":param col_name: name of col for labeling\n",
    ":param num_bins: number of bins\n",
    ":param clean: only look at ints, floats, or complex in col\n",
    ":param verbose_clean: displays non-numeric entries removed\n",
    "'''\n",
    "def display_numeric_col_hist(col, col_name=\"Metric\", num_bins=20, clean=True, verbose_clean=False):\n",
    "    previous_length = len(col)\n",
    "    numeric_types = [int, float, complex]\n",
    "    if clean: col = [x for x in col if type(x) in numeric_types]\n",
    "    if verbose_clean: print(f\"Cleaning removed {previous_length - len(col)} non-numeric entries\")\n",
    "\n",
    "    if len(col) < 1:\n",
    "        print(\"Error, data must contain at least one valid entry to display histogram\")\n",
    "        return    \n",
    "\n",
    "    rng = max(col) - min(col)\n",
    "    num = len(col)\n",
    "    stnd_dev = std(col)\n",
    "\n",
    "    plt.hist(col, num_bins, color=\"blue\", alpha=0.5, edgecolor=\"black\", linewidth=1.0)\n",
    "    plt.title(col_name + \" Histogram\")\n",
    "    plt.ylabel(\"Value  Range  Occurrences\")\n",
    "    plt.xlabel(col_name)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
