{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having heard about the announcement about Theano from Bengio lab , as a Theano user, I am happy and sad to see the fading of the old hero, caused by many raising stars. Sad to see it is too old to compete with its industrial competitors, and happy to have so many excellent deep learning frameworks to choose from. Recently I started translating some of my old codes to Pytorch and have been really impressed by its dynamic nature and clearness. But at the very beginning, I was very confused by the ``backward()`` function when reading the tutorials and documentations. This motivated me to write this post in order for other Pytorch beginners to ease the understanding a bit. And I'll assume that you already know the [``autograd``](http://pytorch.org/docs/master/autograd.html) module and what a [``Variable``](http://pytorch.org/docs/0.1.12/_modules/torch/autograd/variable.html) is, but are a little confused by definition of ``backward()``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's recall the gradient computing under mathematical notions. For an independent variable $x$ (scalar or vector), the whatever operation on $x$ is $y = f(x)$. Then the gradient of $y$ w.r.t $x_i$s is\n",
    "$$\\begin{align}\\nabla y&=\\begin{bmatrix}\n",
    "\\frac{\\partial y}{\\partial x_1}\\\\\n",
    "\\frac{\\partial y}{\\partial x_2}\\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "\\end{align}.\n",
    "$$\n",
    "Then for a specific point of $x=[X_1, X_2, \\dots]$, we'll get the gradient of $y$ on that point as a vector. With these notions in my mind, those things are a bit confusing at the beginning\n",
    "\n",
    "1. Mathematically, we would say \"The gradients of a function w.r.t. the independent variables\", whereas the ``.grad`` is attached to the leaf ``Variable``s. In Theano and Tensorflow, the computed gradients are stored separately in a variable. But with a memont of adjustment, it is fairly easy to buy that. In Pytorch it is also possible to get the ``.grad`` for intermediate ``Variable``s with help of ``register_hook`` function\n",
    "\n",
    "2. The parameter ``grad_variables`` of the function ``torch.autograd.backward(variables, grad_variables=None, retain_graph=None, create_graph=None, retain_variables=None)`` is not straightforward for knowing its functionality. \n",
    "\n",
    "3. What is ``retain_graph`` doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplicity of using ``backward()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 0.6194\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "y Variable containing:\n",
      " 1.2388\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "z Variable containing:\n",
      " 1.9013\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define a scalar variable, set requires_grad to be true to add it to backward path for computing gradients\n",
    "\n",
    "It is actually very simple to use backward()\n",
    "\n",
    "first define the computation graph, then call backward()\n",
    "'''\n",
    "\n",
    "x = Variable(T.randn(1, 1), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "print('z', z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple operations defined a forward path $z=(2x)^3$, $z$ will be the final output ``Variable`` we would like to compute gradient: $dz=24x^2dx$, which will be passed to the parameter ``Variables`` in ``backward()`` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes, it is just as simple as this to compute gradients:\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 9.2083\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad) # note that x.grad is also a Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients of both $y$ and $z$ are None, since the function returns the gradient for the leaves, which is $x$ in this case. At the very beginning, I was assuming something like this:\n",
    "\n",
    "``x gradient None\n",
    "y gradient None\n",
    "z gradient Variable containing:\n",
    " 128.3257\n",
    "[torch.FloatTensor of size 1x1]``,\n",
    "since the gradient is for the final output $z$.\n",
    "\n",
    "With a blink of thinking, we could figure out it would be practically chaos if $x$ is a multi-dimensional vector. ``x.grad`` should be interpreted as the gradient of $z$ at $x$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we use ``grad_variables``?\n",
    "\n",
    "``grad_variables`` should be a list of torch tensors. In default case, the ``backward()`` is applied to scalar-valued function, the default value of ``grad_variables`` is thus ``torch.FloatTensor([1])``. But why is that? What if we put some other values to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      "-0.2782\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Keeping the default value of grad_variables gives\n",
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 1.8581\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(T.randn(1, 1), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "\n",
    "z.backward(T.FloatTensor([1]), retain_graph=True)\n",
    "print('Keeping the default value of grad_variables gives')\n",
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifying the default value of grad_variables to 0.1 gives\n",
      "z gradient None\n",
      "y gradient None\n",
      "x gradient Variable containing:\n",
      " 0.1858\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "z.backward(T.FloatTensor([0.1]), retain_graph=True)\n",
    "print('Modifying the default value of grad_variables to 0.1 gives')\n",
    "print('z gradient', z.grad)\n",
    "print('y gradient', y.grad)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set $x$ to be a matrix. Note that $z$ will also be a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 1.3689 -1.6859\n",
      " 1.0549 -0.9156\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "z shape: torch.Size([2, 2])\n",
      "x gradient Variable containing:\n",
      " 44.9719   0.0000\n",
      " 26.7060   0.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x gradient Variable containing:\n",
      "  0.0000  68.2102\n",
      "  0.0000  20.1196\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x gradient Variable containing:\n",
      " 44.9719  68.2102\n",
      " 26.7060  20.1196\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Try to set x to be column vector or row vector! You'll see different behaviors. \n",
    "\n",
    "'''\n",
    "\n",
    "x = Variable(T.randn(2, 2), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "\n",
    "print('z shape:', z.size())\n",
    "\n",
    "z.backward(T.FloatTensor([1, 0]), retain_graph=True)\n",
    "print('x gradient', x.grad)\n",
    "\n",
    "x.grad.data.zero_() #the gradient for x will be accumulated, it needs to be cleared.\n",
    "\n",
    "z.backward(T.FloatTensor([0, 1]), retain_graph=True)\n",
    "print('x gradient', x.grad)\n",
    "\n",
    "x.grad.data.zero_()\n",
    "\n",
    "z.backward(T.FloatTensor([1, 1]), retain_graph=True)\n",
    "print('x gradient', x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can clearly see the gradients of $z$ are computed w.r.t to each dimension of $x$, because the operations are all element-wise. ``T.FloatTensor([1, 0])`` will give the gradients for first column of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then what if we render the output one-dimensional (scalar) while $x$ is two-dimensional. This is a real simplified scenario of neural networks. \n",
    "$$f(x)=\\frac{1}{n}\\sum_i^n(2x_i)^3$$\n",
    "$$f'(x)=\\frac{1}{n}\\sum_i^n24x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 0.0192 -0.1596\n",
      "-1.1868 -0.1631\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "out Variable containing:\n",
      "-3.3603\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "x gradient Variable containing:\n",
      " 0.0022  0.1528\n",
      " 8.4515  0.1596\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "x gradient Variable containing:\n",
      " 0.0002  0.0153\n",
      " 0.8451  0.0160\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(T.randn(2, 2), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "#print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "out = z.mean()\n",
    "print('out', out)\n",
    "out.backward(T.FloatTensor([1]), retain_graph=True)\n",
    "print('x gradient', x.grad)\n",
    "x.grad.data.zero_()\n",
    "out.backward(T.FloatTensor([0.1]), retain_graph=True)\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is ``retain_graph`` doing?\n",
    "\n",
    "When training a model, the graph will be re-generated for each iteration. Therefore each iteration will consume the graph if the ``retain_graph`` is false, in order to keep the graph, we need to set it be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Variable containing:\n",
      " 0.3358 -1.0588\n",
      "-0.4989 -0.9955\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "out Variable containing:\n",
      "-4.5198\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "x gradient Variable containing:\n",
      " 0.6765  6.7261\n",
      " 1.4936  5.9466\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0ae5673f71fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x gradient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x gradient'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/function.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "x = Variable(T.randn(2, 2), requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
    "print('x', x)\n",
    "#define an operation on x\n",
    "y = 2 * x\n",
    "#print('y', y)\n",
    "#define one more operation to check the chain rule\n",
    "z = y ** 3\n",
    "out = z.mean()\n",
    "print('out', out)\n",
    "out.backward(T.FloatTensor([1]))  #without setting retain_graph to be true, this gives an error.\n",
    "print('x gradient', x.grad)\n",
    "x.grad.data.zero_()\n",
    "out.backward(T.FloatTensor([0.1]))\n",
    "print('x gradient', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up\n",
    "\n",
    "1. The ``backward()`` function made differentiation very simple\n",
    "2. For non-scalar ``Variable``s, we need to specify ``grad_variables``\n",
    "3. If you need to backward() twice on a graph or subgraph, you will need to set ``retain_graph`` to be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
