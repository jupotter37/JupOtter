{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "* Load the data\n",
    "* Build features from the data and format for model\n",
    "* Load and fit the data with the random forest model\n",
    "* Display results\n",
    "\n",
    "---\n",
    "# Code\n",
    "## Import Necessary Tools and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2020-09-28T16:17:59.570702Z",
     "iopub.status.busy": "2020-09-28T16:17:59.569958Z",
     "iopub.status.idle": "2020-09-28T16:18:01.227363Z",
     "shell.execute_reply": "2020-09-28T16:18:01.226384Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-35273d7b16df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminmax_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_cube_utilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdc_frac\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrac_coverage_classify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ.get('NOTEBOOK_ROOT'))\n",
    "\n",
    "import datacube\n",
    "import datetime \n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.data_cube_utilities.dc_display_map as dm\n",
    "import xarray as xr\n",
    "\n",
    "from folium import plugins\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from utils.data_cube_utilities.dc_frac import frac_coverage_classify\n",
    "from utils.data_cube_utilities.dc_mosaic import ls8_unpack_qa\n",
    "from utils.data_cube_utilities.dc_mosaic import create_median_mosaic\n",
    "from utils.data_cube_utilities.dc_rgb import rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We will use data from Datacube. Important features needed in the initial dataset are the following in order:\n",
    "1. red\n",
    "2. green\n",
    "3. blue\n",
    "4. nir\n",
    "5. swir1\n",
    "6. swir2\n",
    "7. pixel_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dc_data():\n",
    "    \"\"\"Loads the dataset from Datacube\"\"\"\n",
    "    dc = datacube.Datacube()\n",
    "\n",
    "    params = dict(platform = 'LANDSAT_8',\n",
    "                  product   = 'ls8_lasrc_uruguay',\n",
    "                  latitude = (-34.44988376, -34.096445),\n",
    "                  longitude = (-56.29119062, -55.24653668),\n",
    "                  time = ('2016-01-01', '2017-01-01'), \n",
    "                  measurements = ['red', 'green', 'blue', 'nir', 'swir1', 'swir2', 'pixel_qa'])  \n",
    "\n",
    "    dataset = dc.load(**params)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Median Composite\n",
    "\n",
    "A clean mask is generated using the pixel_qa values from the dataset. From the clean mask a median temporal composite is created. The pixel_qa is then dropped as the Random Forest Classifier does not explicitly use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TemporalCompositor:\n",
    "    \"\"\"A TemporalCompositor object for creating median composites over a temporal dimension.\n",
    "    \n",
    "    Attributes:\n",
    "        dataset (xarray.Dataset): The dataset used in the compositing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"Initialize object and set the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset (xarray.Dataset): The dataset used in the compositing.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def clean_mask_ls8(self):\n",
    "        \"\"\"A function to create a clean mask for compositing.\n",
    "        \n",
    "        Returns:\n",
    "            The clean mask.\n",
    "        \"\"\"\n",
    "        water_mask = ls8_unpack_qa(self.dataset.pixel_qa, cover_type = \"water\")\n",
    "        clear_mask = ls8_unpack_qa(self.dataset.pixel_qa, cover_type = \"clear\")\n",
    "        clean_mask = np.logical_or(water_mask, clear_mask)\n",
    "        return clean_mask\n",
    "\n",
    "    def create_temporal_composite(self):\n",
    "        \"\"\"A function to create the median temporal composite.\n",
    "        \n",
    "        Returns:\n",
    "            The median temporal composite.\n",
    "        \"\"\"\n",
    "        clean = self.clean_mask_ls8()\n",
    "        composite = create_median_mosaic(self.dataset, clean_mask = clean)\n",
    "        composite = composite.drop('pixel_qa')\n",
    "        return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features\n",
    "\n",
    "The methods used for creating the needed features for classification are defined below. We will use our `TemporalCompositor` from above to give us a cloud-free composite with which we will use to build our necessary features in our `build_features` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NDVI(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return (dataset.nir - dataset.red)/(dataset.nir + dataset.red).rename(\"NDVI\")\n",
    "\n",
    "def NBR(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return ((dataset.nir - dataset.swir2) / (dataset.swir2 + dataset.nir)).rename(\"NBR\")\n",
    "\n",
    "def NDWI_2(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return (dataset.green - dataset.nir)/(dataset.green + dataset.nir).rename(\"NDWI_2\")\n",
    "\n",
    "def SCI(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return ((dataset.swir1 - dataset.nir)/(dataset.swir1 + dataset.nir)).rename(\"SCI\")\n",
    "\n",
    "def PNDVI(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    \n",
    "    nir = dataset.nir\n",
    "    green = dataset.green\n",
    "    blue = dataset.blue\n",
    "    red = dataset.red\n",
    "    \n",
    "    return ((nir - (green + red + blue))/(nir + (green + red + blue))).rename(\"PNDVI\")\n",
    "\n",
    "def CVI(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return (dataset.nir * (dataset.red / (dataset.green * dataset.green))).rename(\"CVI\")\n",
    "\n",
    "def CCCI(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return ((dataset.nir - dataset.red)/(dataset.nir + dataset.red)).rename(\"CCCI\")\n",
    "\n",
    "def NBR2(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return (dataset.swir1 - dataset.swir2)/(dataset.swir1 + dataset.swir2)\n",
    "\n",
    "\n",
    "def coefficient_of_variance(da:xr.DataArray):\n",
    "    return da.std(dim = \"time\")/da.mean(dim = \"time\")   \n",
    "\n",
    "def NDVI_coeff_var(ds, mask = None):\n",
    "    ds_ndvi = NDVI(ds)    \n",
    "    masked_ndvi = ds_ndvi.where(mask)\n",
    "    return coefficient_of_variance(masked_ndvi)  \n",
    "\n",
    "def fractional_cover_2d(dataset: xr.Dataset) -> xr.DataArray:\n",
    "    return  frac_coverage_classify(dataset, clean_mask= np.ones(dataset.red.values.shape).astype(bool))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "The code to load in the model and to classify the given feature set. The order of features matters becuase we are using the model exported from the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \"\"\"A Classifier object for performing the classification on a dataset.\n",
    "    \n",
    "    Attributes:\n",
    "        rf (RandomForestClassifier): The RandomForestClassifier used in the classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_location='./classifiers/models/random_forest.model'):\n",
    "        \"\"\"Initializes the data and loads the binary model\n",
    "        \n",
    "        Args:\n",
    "            model_location (string): The location of the RandomForestClassifier's exported binary.\n",
    "        \"\"\"\n",
    "        self.rf = joblib.load(model_location)\n",
    "    \n",
    "    def classify(self, features):\n",
    "        \"\"\"A function to classify the given dataset.\n",
    "        \n",
    "        Args:\n",
    "            features (xarray.Dataset): The set of features to run the classifier with.\n",
    "        \n",
    "        Returns:\n",
    "            An Xarray Dataset conatining the given features with the classification results appended.\n",
    "        \"\"\"\n",
    "        X = features.values\n",
    "        X = np.array_split(X, 100)\n",
    "        \n",
    "        y_pred = []\n",
    "        for i in range(len(X)):\n",
    "            y_pred.append(self.rf.predict(X[i]))\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "\n",
    "        df = pd.DataFrame(y_pred, columns=['label'])\n",
    "        features['label'] = df.values\n",
    "        return features\n",
    "\n",
    "    def build_features(self, dataset):\n",
    "        \"\"\"Builds the features used in classification.\n",
    "\n",
    "        Args:\n",
    "            dataset (xarray.Dataset): The dataset to use for building the features.\n",
    "\n",
    "        Returns:\n",
    "            A Pandas DataFrame of the given features with the built features appended.\n",
    "        \"\"\"\n",
    "        features = xr.Dataset()\n",
    "        compositor = TemporalCompositor(dataset)\n",
    "        composite = compositor.create_temporal_composite()\n",
    "        features = features.merge(composite)\n",
    "\n",
    "        feature_list = (NDVI,\n",
    "                        NDVI_coeff_var,\n",
    "                        PNDVI,\n",
    "                        NBR,\n",
    "                        NBR2,\n",
    "                        NDWI_2,\n",
    "                        SCI,\n",
    "                        CVI,\n",
    "                        CCCI,\n",
    "                        fractional_cover_2d\n",
    "                       )\n",
    "\n",
    "        clean = compositor.clean_mask_ls8()\n",
    "        for i in range(len(feature_list)):\n",
    "            if(feature_list[i].__name__ == 'NDVI_coeff_var'):\n",
    "                features[feature_list[i].__name__] = feature_list[i](dataset, mask = clean)\n",
    "            elif(feature_list[i].__name__ == 'fractional_cover_2d'):\n",
    "                features = features.merge(fractional_cover_2d(composite))\n",
    "            else:\n",
    "                features[feature_list[i].__name__] = feature_list[i](composite)\n",
    "\n",
    "        features.NDVI_coeff_var.values[ np.isnan(features.NDVI_coeff_var.values)] = 0\n",
    "\n",
    "        return features.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display\n",
    "Code for displaying the results in ways that are easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeaturesDisplay:\n",
    "    \"\"\"A FeaturesDisplay object for presenting classification results.\n",
    "    \n",
    "    Attributes:\n",
    "        dataset (xarray.Dataset): The features DataFrame represented as an Xarray Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, features: xr.Dataset):\n",
    "        \"\"\"Initializes the FeaturesDisplay object.\n",
    "        \n",
    "        Args:\n",
    "            features (pandas.DataFrame): A classified features DataFrame with a label key.\n",
    "        \"\"\"\n",
    "        self.dataset = xr.Dataset.from_dataframe(features)\n",
    "        \n",
    "    def images(self):\n",
    "        \"\"\"Generates multiple images of the features based on classification results.\"\"\"\n",
    "        landuse = ('Forest',\n",
    "                   'Misc',\n",
    "                   'Naturalgrassland',\n",
    "                   'Prairie',\n",
    "                   'Summercrops'\n",
    "                   )\n",
    "\n",
    "        dataset = self.dataset\n",
    "        for i in range(len(landuse)): \n",
    "            tmp = dataset.where(dataset.label == landuse[i])\n",
    "            print(\"%s:\" % landuse[i])\n",
    "            rgb(tmp, bands= [\"swir1\",\"nir\",\"red\"], width= 20)\n",
    "    \n",
    "    def _get_canvas(self, key:str) -> np.array:\n",
    "        \n",
    "        canvas = np.zeros((len(self.dataset.latitude.values),\n",
    "                           len(self.dataset.longitude.values),\n",
    "                           4))\n",
    "        \n",
    "        paint_here = self.dataset.label == key\n",
    "        \n",
    "        canvas[paint_here]  = np.array([255, 255, 255, 179])\n",
    "        canvas[~paint_here] = np.array([0, 0, 0, 0])\n",
    "        return canvas\n",
    "        \n",
    "        \n",
    "    def map_overlay(self, key, color=None):\n",
    "        \"\"\"Maps classifications using Folium.\n",
    "\n",
    "        Args:\n",
    "            key (string): The classification to map from the following: (Forest, Misc, Naturalgrassland, Prairie, Summercrops)\n",
    "            color: Set to False to disable colorized overlay.\n",
    "        \"\"\"\n",
    "        dataset = self.dataset\n",
    "        tmp = dataset.where(dataset.label == key)\n",
    "\n",
    "        latitudes = (min(dataset.latitude.values), max(dataset.latitude.values))\n",
    "        longitudes = (min(dataset.longitude.values), max(dataset.longitude.values))\n",
    "\n",
    "        zoom_level = dm._degree_to_zoom_level(latitudes[0], latitudes[1])\n",
    "\n",
    "        # I don't know why, but this makes Folium work for these labels\n",
    "        if(key == 'Summercrops' or key == 'Naturalgrassland'):\n",
    "            mult = 255/3\n",
    "        else:\n",
    "            mult = 1\n",
    "\n",
    "        print(\"%s:\" % key)\n",
    "\n",
    "        if(color == None):\n",
    "            r = tmp.nir.values\n",
    "            g = tmp.red.values\n",
    "            b = tmp.green.values\n",
    "            \n",
    "            r[np.isnan(r)] = 0\n",
    "            g[np.isnan(g)] = 0\n",
    "            b[np.isnan(b)] = 0\n",
    "\n",
    "            minmax_scale(r, feature_range=(0,255), copy=False)\n",
    "            minmax_scale(g, feature_range=(0,255), copy=False)\n",
    "            minmax_scale(b, feature_range=(0,255), copy=False)\n",
    "\n",
    "            rgb_stack = np.dstack((r,g,b))\n",
    "\n",
    "            a = np.ones(r.shape) * 128\n",
    "            a[np.where(r == 0)] = 0\n",
    "\n",
    "            rgb_uint8 = (rgb_stack / mult).astype(np.uint8)\n",
    "            rgb_uint8 = np.dstack((rgb_uint8, a))\n",
    "            \n",
    "        else: \n",
    "            rgb_uint8 = self._get_canvas(key).astype(np.uint8)\n",
    "        \n",
    "\n",
    "        m = folium.Map(location=[np.average(latitudes),\n",
    "                                 np.average(longitudes)],\n",
    "                       zoom_start=zoom_level+1,\n",
    "                       tiles=\" http://mt1.google.com/vt/lyrs=y&z={z}&x={x}&y={y}\",\n",
    "                       attr=\"Google\")\n",
    "        m.add_child(plugins.ImageOverlay(np.flipud(rgb_uint8), \\\n",
    "                bounds =[[min(latitudes), min(longitudes)], [max(latitudes), max(longitudes)]]))\n",
    "\n",
    "        folium.LayerControl().add_to(m)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Results\n",
    "\n",
    "## Load in Data\n",
    "Load a dataset from Datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Features\n",
    "Load our Classifier object and build the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8add29597043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dd4f39778080>\u001b[0m in \u001b[0;36mbuild_features\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mcompositor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporalCompositor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mcomposite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompositor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_temporal_composite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomposite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4047074f0ef8>\u001b[0m in \u001b[0;36mcreate_temporal_composite\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m         \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_mask_ls8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mcomposite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_median_mosaic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mcomposite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomposite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pixel_qa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomposite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/data_cube_notebooks/utils/data_cube_utilities/dc_mosaic.py\u001b[0m in \u001b[0;36mcreate_median_mosaic\u001b[0;34m(dataset_in, clean_mask, no_data, intermediate_product, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mclean_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_default_clean_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mdataset_in_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_in\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mno_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdataset_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_in_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, cond, other, drop)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/ops.py\u001b[0m in \u001b[0;36mwhere_method\u001b[0;34m(self, cond, other)\u001b[0m\n\u001b[1;32m    180\u001b[0m                        \u001b[0mdataset_join\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                        \u001b[0mdask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'allowed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                        keep_attrs=True)\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/computation.py\u001b[0m in \u001b[0;36mapply_ufunc\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m                                    \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_fill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                                    \u001b[0mdataset_join\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_join\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                                    keep_attrs=keep_attrs)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         return apply_dataarray_ufunc(variables_ufunc, *args,\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/computation.py\u001b[0m in \u001b[0;36mapply_dataset_ufunc\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m     result_vars = apply_dict_of_variables_ufunc(\n\u001b[1;32m    368\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_join\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         fill_value=fill_value)\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/computation.py\u001b[0m in \u001b[0;36mapply_dict_of_variables_ufunc\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mresult_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrouped_by_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mresult_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvariable_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/computation.py\u001b[0m in \u001b[0;36mapply_variable_ufunc\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             raise ValueError('unknown setting for dask array handling in '\n\u001b[1;32m    560\u001b[0m                              'apply_ufunc: {}'.format(dask))\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0mresult_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/duck_array_ops.py\u001b[0m in \u001b[0;36mwhere_method\u001b[0;34m(data, cond, other)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fill_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/duck_array_ops.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;34m\"\"\"Three argument where() with better dtype promotion rules.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mas_shared_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datacube/datacube_env/lib/python3.5/site-packages/xarray/core/duck_array_ops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meager_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = Classifier()\n",
    "\n",
    "features = classifier.build_features(dataset)\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Set of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = classifier.classify(features)\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results\n",
    "### Image Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display = FeaturesDisplay(features)\n",
    "display.images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display.map_overlay('Forest', color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display.map_overlay('Misc', color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display.map_overlay('Naturalgrassland', color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display.map_overlay('Prairie', color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display.map_overlay('Summercrops', color=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Example Use\n",
    "## Python Module for Explicit Forest Classification\n",
    "\n",
    "To show how this proof of concept code may be used; we've created an example Python module. The module can be used to determine whether an input feature set is explicitly a forest or not a forest. This example module can be easily modified to do the same for the other classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This imports the actual classifier.\n",
    "from classifiers.forest_classifier import ForestClassifier\n",
    "\n",
    "\"\"\"\n",
    "    Load in out dataset. This is just an example. You can use a different\n",
    "    dataset as long as it contains the appropriate features and they are\n",
    "    in the order that the classifier needs them.\n",
    "\"\"\"\n",
    "dataset = load_dc_data()\n",
    "\n",
    "\"\"\"\n",
    "    Generate a clean mask using the method in this same notebook.\n",
    "    This is just an example. You can supply your own mask as long\n",
    "    as it is of boolean type or boolean-like (1 or 0).\n",
    "\"\"\"\n",
    "mask = TemporalCompositor(dataset).clean_mask_ls8()\n",
    "\n",
    "# Running the actual classifier with our example dataset and mask.\n",
    "forest_classifier = ForestClassifier('./classifiers/models/random_forest.model')\n",
    "forest = forest_classifier.classify(dataset, mask)\n",
    "forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
