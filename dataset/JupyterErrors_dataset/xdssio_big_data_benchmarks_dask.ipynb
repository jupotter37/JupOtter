{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T20:04:30.087591Z",
     "start_time": "2020-05-27T20:04:28.323364Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install -U pip dask[complete] numpy fsspec>=0.3.3 tqdm pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:27:29.030635Z",
     "start_time": "2020-05-27T21:27:28.723400Z"
    },
    "code_folding": [
     16,
     31,
     36,
     47
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We test every benchmark twice and save both results\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "print(f\"We test every benchmark twice and save both results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:27:41.524308Z",
     "start_time": "2020-05-27T21:27:30.093794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "# I found that running this line makes for better results\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:27:43.737168Z",
     "start_time": "2020-05-27T21:27:41.531161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_file took: 0.8108847141265869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8108847141265869"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(data_path, engine='pyarrow')\n",
    "\n",
    "benchmark(read_file_parquet, df=data, name='read_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:00.078754Z",
     "start_time": "2020-05-27T21:27:43.738756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count took: 7.938486099243164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.938486099243164"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(df=None):\n",
    "    # there is a dask bug - len(df) takes 20X time longer\n",
    "    return len(df.vendor_id) \n",
    "\n",
    "benchmark(count, df=data, name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:11.395255Z",
     "start_time": "2020-05-27T21:28:00.080420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean took: 5.056631803512573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.056631803512573"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "benchmark(mean, df=data, name='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:22.583181Z",
     "start_time": "2020-05-27T21:28:11.397022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation took: 5.378662586212158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.378662586212158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "benchmark(standard_deviation, df=data, name='standard deviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:36.977654Z",
     "start_time": "2020-05-27T21:28:22.584840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum columns mean took: 6.816420078277588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.816420078277588"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.trip_distance).mean().compute()\n",
    "\n",
    "benchmark(mean_of_sum, df=data, name='sum columns mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:37.560475Z",
     "start_time": "2020-05-27T21:28:36.980044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum columns took: 0.0010232925415039062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0010232925415039062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lazy evaulation - instant\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.trip_distance)\n",
    "\n",
    "benchmark(sum_columns, df=data, name='sum columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:51.518685Z",
     "start_time": "2020-05-27T21:28:37.562329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product columns mean took: 6.626446723937988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.626446723937988"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.trip_distance).mean().compute()\n",
    "\n",
    "benchmark(mean_of_product, df=data, name='product columns mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:28:52.144439Z",
     "start_time": "2020-05-27T21:28:51.520316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product columns took: 0.0011837482452392578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0011837482452392578"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lazy evaulation - instant\n",
    "def product_columns(df):\n",
    "    return df.fare_amount * df.trip_distance\n",
    "\n",
    "benchmark(product_columns, df=data, name='product columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:33:16.210086Z",
     "start_time": "2020-05-27T21:29:00.083187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lazy evaluation took: 153.01910591125488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "153.01910591125488"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lazy_mean(df):\n",
    "    df['lazy'] = df.fare_amount * df.trip_distance\n",
    "    return df['lazy'].mean().compute()\n",
    "    \n",
    "benchmark(lazy_mean, df=data, name='lazy evaluation')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart and reload the data\n",
    "* I got results 4X to 75X faster by restarting here.\n",
    "* `client.restart()` did not match the effect of retarting the kenrel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:36:15.005842Z",
     "start_time": "2020-05-27T21:36:03.197324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "benchmarks = reload()\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:36:26.329649Z",
     "start_time": "2020-05-27T21:36:15.033436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value counts took: 5.161530494689941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.161530494689941"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_counts(df):\n",
    "    return df.passenger_count.value_counts().compute()\n",
    "\n",
    "benchmark(value_counts, df=data, name='value counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:39:06.898262Z",
     "start_time": "2020-05-27T21:37:52.565642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arithmetic operation mean took: 36.519623041152954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.519623041152954"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=data, name='arithmetic operation mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:39:07.516482Z",
     "start_time": "2020-05-27T21:39:06.899736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arithmetic operation took: 0.019608259201049805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.019608259201049805"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "benchmark(complicated_arithmetic_operation, df=data, name='arithmetic operation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart\n",
    "* This restart got me a 7X faster results on \"group-by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:22:25.677178Z",
     "start_time": "2020-05-27T21:22:14.154146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "benchmarks = reload()\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T21:45:28.725243Z",
     "start_time": "2020-05-27T21:42:00.896969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby statistics took: 104.00652551651001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104.00652551651001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], \n",
    "                                               'tip_amount': ['mean', 'std']\n",
    "                                              }).compute()\n",
    "\n",
    "benchmark(groupby_statistics, df=data, name='groupby statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For join before restarting\n",
    "other = data.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], 'tip_amount': ['mean', 'std']}).compute()\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "other.to_parquet('datasets/other.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart\n",
    "* Without a restart this task crash\n",
    "* Removing this line `client = Client(threads_per_worker=1)` is paramount or you get memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Join best practice](https://docs.dask.org/en/latest/dataframe-best-practices.html#joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:16:03.825859Z",
     "start_time": "2020-05-27T22:15:19.304783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    "benchmarks = reload()\n",
    "# client = Client(threads_per_worker=1) # this gives memory errors on join\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "other = pd.read_parquet('datasets/other.parquet')\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:37:38.150121Z",
     "start_time": "2020-05-27T22:16:14.158180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join count took: 646.4160277843475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646.4160277843475"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "benchmark(join_count, data, name='join count', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:38:19.188195Z",
     "start_time": "2020-05-27T22:38:18.624049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join took: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crashes\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True)\n",
    "\n",
    "add_nan('join')\n",
    "# benchmark(join_count, data, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered data (and restart the kernel again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:38:49.294736Z",
     "start_time": "2020-05-27T22:38:36.612300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 1173057928 with 18 columns and 1174 partitions\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "          \n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.trip_distance).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.trip_distance)\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.trip_distance).mean().compute()\n",
    "\n",
    "\n",
    "def product_columns(df):\n",
    "    return df.fare_amount * df.trip_distance\n",
    "          \n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.pickup_longitude\n",
    "    phi_1 = df.pickup_latitude\n",
    "    theta_2 = df.dropoff_longitude\n",
    "    phi_2 = df.dropoff_latitude\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.passenger_count.value_counts().compute()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], \n",
    "                                               'tip_amount': ['mean', 'std']\n",
    "                                              }).compute()\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "          \n",
    "benchmarks = reload()\n",
    "client = Client(threads_per_worker=1)\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "size = len(data.vendor_id)\n",
    "other = pd.read_parquet('datasets/other.parquet')\n",
    "print(f\"size: {size} with {len(data.columns)} columns and {data.npartitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:39:50.234262Z",
     "start_time": "2020-05-27T22:39:49.545991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare filtered data and deleted 388 MB\n",
      "filter data took: 0.00021028518676757812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00021028518676757812"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Prepare filtered data and deleted {gc.collect()} MB\")\n",
    "expr_filter = (data.pickup_longitude > long_min)  & (data.pickup_longitude < long_max) & \\\n",
    "                  (data.pickup_latitude > lat_min)    & (data.pickup_latitude < lat_max) & \\\n",
    "                  (data.dropoff_longitude > long_min) & (data.dropoff_longitude < long_max) & \\\n",
    "                  (data.dropoff_latitude > lat_min)   & (data.dropoff_latitude < lat_max)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "benchmark(filter_data, data, name='filter data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T22:42:22.237665Z",
     "start_time": "2020-05-27T22:40:23.840931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned 248 mb\n"
     ]
    }
   ],
   "source": [
    "# https://docs.dask.org/en/latest/dataframe-best-practices.html\n",
    "filtered = filter_data(data)\n",
    "nb_partitions = int(data.npartitions//(len(filtered.vendor_id)/size))\n",
    "\n",
    "filtered = filtered.repartition(npartitions=nb_partitions)\n",
    "del data\n",
    "print(f\"cleaned {gc.collect()} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T00:08:45.737525Z",
     "start_time": "2020-05-27T22:50:36.772468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean took: 694.67067527771\n",
      "filtered standard deviation took: 121.11800575256348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered sum columns mean took: 625.5094294548035\n",
      "filtered sum columns took: 0.0011899471282958984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered product columns mean took: 274.0449981689453\n",
      "filtered product columns took: 0.0011372566223144531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered arithmetic operation mean took: 286.51024436950684\n",
      "filtered arithmetic operation took: 0.02183842658996582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered value counts took: 121.52165389060974\n",
      "filtered groupby statistics took: 127.68544888496399\n",
      "filtered join took: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "(\"('getitem-read-parquet-repartition-1204-9f22f52b33af43f8a4a930ad6d1cf213', 368)\", <Worker 'tcp://127.0.0.1:45821', name: 0, memory: 0, processing: 79>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f81c539b929c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0madd_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtered join'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'filtered join count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a43353a69888>\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(f, df, name, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mbenchmarks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbenchmarks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-22eedf023f97>\u001b[0m in \u001b[0;36mjoin_count\u001b[0;34m(df, other)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjoin_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3421\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3425\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         return self.reduction(\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"len\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         ).compute()\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1972\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1975\u001b[0m             )\n\u001b[1;32m   1976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             return sync(\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m             )\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhad_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1831\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKilledWorker\u001b[0m: (\"('getitem-read-parquet-repartition-1204-9f22f52b33af43f8a4a930ad6d1cf213', 368)\", <Worker 'tcp://127.0.0.1:45821', name: 0, memory: 0, processing: 79>)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "benchmark(mean, filtered, name='filtered mean')\n",
    "benchmark(standard_deviation, filtered, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, filtered, name ='filtered sum columns mean')\n",
    "benchmark(sum_columns, df=filtered, name='filtered sum columns')\n",
    "benchmark(mean_of_product, filtered, name ='filtered product columns mean')\n",
    "benchmark(product_columns, df=filtered, name='filtered product columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, filtered, name='filtered arithmetic operation mean')\n",
    "benchmark(complicated_arithmetic_operation, filtered, name='filtered arithmetic operation')\n",
    "benchmark(value_counts, filtered, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, filtered, name='filtered groupby statistics')\n",
    "other = filtered.groupby(by='passenger_count').agg({'fare_amount': ['mean', 'std'], 'tip_amount': ['mean', 'std']}).compute()\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "other.to_parquet('datasets/filtered_other.parquet')\n",
    "add_nan('filtered join')\n",
    "# benchmark(join_count, filtered, name='filtered join count', other=other) # should have removed it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rastart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T00:59:27.908174Z",
     "start_time": "2020-05-28T00:59:26.497424Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster \n",
    "\n",
    "\n",
    "instance_type = 'c5d4xlarge' # change this\n",
    "results_bucket = f\"s3://xdss-benchmarks/benchmarks\" # change this\n",
    "\n",
    "name = 'dask'\n",
    "data_path = 'datasets/taxi_parquet/'\n",
    "output_file = f'{name}_{instance_type}.csv'\n",
    "results_path = f\"results/{output_file}\"\n",
    "results_bucket = f\"{results_bucket}/{output_file}\" \n",
    "benchmarks = {\n",
    "    'run':[],\n",
    "    'duration': [],\n",
    "    'task': []   \n",
    "}\n",
    "\n",
    "long_min = -74.05\n",
    "long_max = -73.75\n",
    "lat_min = 40.58\n",
    "lat_max = 40.90\n",
    "\n",
    "# Load data\n",
    "data = dd.read_parquet(data_path, engine='pyarrow')\n",
    "\n",
    "\n",
    "def get_results(benchmarks=benchmarks):\n",
    "    return pd.DataFrame.from_dict(benchmarks, orient='index').T\n",
    "\n",
    "def persist():\n",
    "    gc.collect()\n",
    "    get_results(benchmarks).to_csv(results_path)\n",
    "    os.system(f\"aws s3 cp {results_path} {results_bucket}\")\n",
    "    \n",
    "def benchmark(f, df, name, **kwargs):    \n",
    "    for i in range(2):\n",
    "        start_time = time.time()\n",
    "        ret = f(df, **kwargs)\n",
    "        benchmarks['duration'].append(time.time()-start_time)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "          \n",
    "def add_nan(name):\n",
    "    for i in range(2):\n",
    "        benchmarks['duration'].append(np.nan)\n",
    "        benchmarks['task'].append(name)\n",
    "        benchmarks['run'].append(i+1)\n",
    "    persist()\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]}\")\n",
    "    return benchmarks['duration'][-1]\n",
    "\n",
    "def reload():\n",
    "    df = pd.read_csv(f\"results/{output_file}\").drop(['Unnamed: 0'],axis=1)\n",
    "    benchmarks = df.to_dict(orient='list')\n",
    "    return benchmarks\n",
    "          \n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "\n",
    "expr_filter = (data.pickup_longitude > long_min)  & (data.pickup_longitude < long_max) & \\\n",
    "                  (data.pickup_latitude > lat_min)    & (data.pickup_latitude < lat_max) & \\\n",
    "                  (data.dropoff_longitude > long_min) & (data.dropoff_longitude < long_max) & \\\n",
    "                  (data.dropoff_latitude > lat_min)   & (data.dropoff_latitude < lat_max)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "          \n",
    "benchmarks = reload()\n",
    "\n",
    "filtered = filter_data(data)\n",
    "# This calculation takes ages without client = Client(threads_per_worker=1)           \n",
    "# nb_partitions = int(data.npartitions//(len(filtered.vendor_id)/len(data.vendor_id)))          \n",
    "nb_partitions = 1204 \n",
    "filtered = filtered.repartition(npartitions=nb_partitions)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T01:24:55.995696Z",
     "start_time": "2020-05-28T00:59:28.138153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered join count took: 762.8446152210236\n",
      "dask\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>duration</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.771796</td>\n",
       "      <td>read_file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.810885</td>\n",
       "      <td>read_file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7.84285</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7.93849</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.54969</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5.05663</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>5.22817</td>\n",
       "      <td>standard deviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>5.37866</td>\n",
       "      <td>standard deviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>6.89718</td>\n",
       "      <td>sum columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>6.81642</td>\n",
       "      <td>sum columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00212479</td>\n",
       "      <td>sum columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00102329</td>\n",
       "      <td>sum columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>6.73536</td>\n",
       "      <td>product columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6.62645</td>\n",
       "      <td>product columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00209713</td>\n",
       "      <td>product columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00118375</td>\n",
       "      <td>product columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>101.833</td>\n",
       "      <td>lazy evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>153.019</td>\n",
       "      <td>lazy evaluation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>5.17194</td>\n",
       "      <td>value counts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>5.16153</td>\n",
       "      <td>value counts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>37.0301</td>\n",
       "      <td>arithmetic operation mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>36.5196</td>\n",
       "      <td>arithmetic operation mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0242774</td>\n",
       "      <td>arithmetic operation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0196083</td>\n",
       "      <td>arithmetic operation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>102.539</td>\n",
       "      <td>groupby statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>104.007</td>\n",
       "      <td>groupby statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>636.31</td>\n",
       "      <td>join count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>646.416</td>\n",
       "      <td>join count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000322104</td>\n",
       "      <td>filter data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000210285</td>\n",
       "      <td>filter data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>640.875</td>\n",
       "      <td>filtered mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>694.671</td>\n",
       "      <td>filtered mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>119.481</td>\n",
       "      <td>filtered standard deviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>121.118</td>\n",
       "      <td>filtered standard deviation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>521.56</td>\n",
       "      <td>filtered sum columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>625.509</td>\n",
       "      <td>filtered sum columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00242567</td>\n",
       "      <td>filtered sum columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00118995</td>\n",
       "      <td>filtered sum columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>503.485</td>\n",
       "      <td>filtered product columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>274.045</td>\n",
       "      <td>filtered product columns mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00246263</td>\n",
       "      <td>filtered product columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00113726</td>\n",
       "      <td>filtered product columns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>247.544</td>\n",
       "      <td>filtered arithmetic operation mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>286.51</td>\n",
       "      <td>filtered arithmetic operation mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0262141</td>\n",
       "      <td>filtered arithmetic operation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0218384</td>\n",
       "      <td>filtered arithmetic operation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>122.566</td>\n",
       "      <td>filtered value counts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>121.522</td>\n",
       "      <td>filtered value counts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>126.429</td>\n",
       "      <td>filtered groupby statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>127.685</td>\n",
       "      <td>filtered groupby statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>filtered join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>filtered join</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>763.506</td>\n",
       "      <td>filtered join count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>762.845</td>\n",
       "      <td>filtered join count</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run     duration                                task\n",
       "0    1     0.771796                           read_file\n",
       "1    2     0.810885                           read_file\n",
       "2    1      7.84285                               count\n",
       "3    2      7.93849                               count\n",
       "4    1      5.54969                                mean\n",
       "5    2      5.05663                                mean\n",
       "6    1      5.22817                  standard deviation\n",
       "7    2      5.37866                  standard deviation\n",
       "8    1      6.89718                    sum columns mean\n",
       "9    2      6.81642                    sum columns mean\n",
       "10   1   0.00212479                         sum columns\n",
       "11   2   0.00102329                         sum columns\n",
       "12   1      6.73536                product columns mean\n",
       "13   2      6.62645                product columns mean\n",
       "14   1   0.00209713                     product columns\n",
       "15   2   0.00118375                     product columns\n",
       "16   1      101.833                     lazy evaluation\n",
       "17   2      153.019                     lazy evaluation\n",
       "18   1      5.17194                        value counts\n",
       "19   2      5.16153                        value counts\n",
       "20   1      37.0301           arithmetic operation mean\n",
       "21   2      36.5196           arithmetic operation mean\n",
       "22   1    0.0242774                arithmetic operation\n",
       "23   2    0.0196083                arithmetic operation\n",
       "24   1      102.539                  groupby statistics\n",
       "25   2      104.007                  groupby statistics\n",
       "26   1       636.31                          join count\n",
       "27   2      646.416                          join count\n",
       "28   1          NaN                                join\n",
       "29   2          NaN                                join\n",
       "30   1  0.000322104                         filter data\n",
       "31   2  0.000210285                         filter data\n",
       "32   1      640.875                       filtered mean\n",
       "33   2      694.671                       filtered mean\n",
       "34   1      119.481         filtered standard deviation\n",
       "35   2      121.118         filtered standard deviation\n",
       "36   1       521.56           filtered sum columns mean\n",
       "37   2      625.509           filtered sum columns mean\n",
       "38   1   0.00242567                filtered sum columns\n",
       "39   2   0.00118995                filtered sum columns\n",
       "40   1      503.485       filtered product columns mean\n",
       "41   2      274.045       filtered product columns mean\n",
       "42   1   0.00246263            filtered product columns\n",
       "43   2   0.00113726            filtered product columns\n",
       "44   1      247.544  filtered arithmetic operation mean\n",
       "45   2       286.51  filtered arithmetic operation mean\n",
       "46   1    0.0262141       filtered arithmetic operation\n",
       "47   2    0.0218384       filtered arithmetic operation\n",
       "48   1      122.566               filtered value counts\n",
       "49   2      121.522               filtered value counts\n",
       "50   1      126.429         filtered groupby statistics\n",
       "51   2      127.685         filtered groupby statistics\n",
       "52   1          NaN                       filtered join\n",
       "53   2          NaN                       filtered join\n",
       "54   1      763.506                 filtered join count\n",
       "55   2      762.845                 filtered join count"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other = pd.read_parquet('datasets/filtered_other.parquet')\n",
    "benchmark(join_count, filtered, name='filtered join count', other=other)\n",
    "print(name)\n",
    "get_results(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
