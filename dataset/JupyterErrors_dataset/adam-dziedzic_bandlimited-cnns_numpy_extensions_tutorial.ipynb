{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating extensions using numpy and scipy\n",
    "=========================================\n",
    "**Author**: `Adam Paszke` [https://github.com/apaszke](https://github.com/apaszke)\n",
    "\n",
    "**Updated by**: `Adam Dziedzic` [https://github.com/adam-dziedzic](https://github.com/adam-dziedzic)\n",
    "\n",
    "In this tutorial, we shall go through two tasks:\n",
    "\n",
    "1. Create a neural network layer with no parameters.\n",
    "\n",
    "    -  This calls into **numpy** as part of it’s implementation\n",
    "\n",
    "2. Create a neural network layer that has learnable weights\n",
    "\n",
    "    -  This calls into **SciPy** as part of it’s implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter-less example\n",
    "----------------------\n",
    "\n",
    "This layer doesn’t particularly do anything useful or mathematically\n",
    "correct.\n",
    "\n",
    "It is aptly named BadFFTFunction\n",
    "\n",
    "**Layer Implementation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.fft import rfft2, irfft2\n",
    "\n",
    "class BadFFTFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, input):\n",
    "        numpy_input = input.detach().numpy()\n",
    "        result = abs(rfft2(numpy_input))\n",
    "        return input.new(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        numpy_go = grad_output.numpy()\n",
    "        result = irfft2(numpy_go)\n",
    "        return grad_output.new(result)\n",
    "\n",
    "# since this layer does not have any parameters, we can\n",
    "# simply declare this as a function, rather than as an nn.Module class\n",
    "\n",
    "def incorrect_fft(input):\n",
    "    return BadFFTFunction()(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example usage of the created layer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input to fft forward pass:  tensor([[-0.3340,  2.1187, -1.3309,  0.6148, -0.7633, -0.7805,  0.4434,\n         -0.6965],\n        [ 0.1043,  0.3583,  1.1323, -0.9970, -0.0019,  0.6787,  0.4112,\n          1.5120],\n        [-0.2142, -3.1594, -0.6444, -0.4585,  0.9635, -2.1726,  0.0333,\n         -0.8942],\n        [-0.3636, -0.0730, -0.3926,  0.6311,  0.8769,  1.0458, -1.3366,\n         -0.8047],\n        [-0.0504,  0.6493,  0.1017, -2.1825, -0.5158, -0.5393, -0.8690,\n         -1.3570],\n        [-0.9512, -1.2126,  1.6024, -0.6277, -2.2618,  0.8980,  0.0194,\n         -0.5690],\n        [-0.1116, -0.7792, -0.8842, -0.2721,  0.9937,  0.4120,  2.3265,\n         -1.5192],\n        [-0.9701,  0.0215, -0.5756,  1.4675, -1.9559, -0.5680,  0.7020,\n         -0.0408]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-90f119542fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the input to fft forward pass: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincorrect_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result of forward fft: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e2048634110d>\u001b[0m in \u001b[0;36mincorrect_fft\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mincorrect_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBadFFTFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "input = torch.randn(8, 8, requires_grad=True)\n",
    "print(\"the input to fft forward pass: \", input)\n",
    "result = incorrect_fft(input)\n",
    "print(\"result of forward fft: \", result)\n",
    "result.backward(torch.randn(result.size()))\n",
    "print(\"Gradient for the input: \", input.grad)\n",
    "print(\"size of the gradient: \", input.grad.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input to fft forward pass:  tensor([[-0.3340,  2.1187, -1.3309,  0.6148, -0.7633, -0.7805,  0.4434,\n         -0.6965],\n        [ 0.1043,  0.3583,  1.1323, -0.9970, -0.0019,  0.6787,  0.4112,\n          1.5120],\n        [-0.2142, -3.1594, -0.6444, -0.4585,  0.9635, -2.1726,  0.0333,\n         -0.8942],\n        [-0.3636, -0.0730, -0.3926,  0.6311,  0.8769,  1.0458, -1.3366,\n         -0.8047],\n        [-0.0504,  0.6493,  0.1017, -2.1825, -0.5158, -0.5393, -0.8690,\n         -1.3570],\n        [-0.9512, -1.2126,  1.6024, -0.6277, -2.2618,  0.8980,  0.0194,\n         -0.5690],\n        [-0.1116, -0.7792, -0.8842, -0.2721,  0.9937,  0.4120,  2.3265,\n         -1.5192],\n        [-0.9701,  0.0215, -0.5756,  1.4675, -1.9559, -0.5680,  0.7020,\n         -0.0408]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-90f119542fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the input to fft forward pass: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincorrect_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result of forward fft: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e2048634110d>\u001b[0m in \u001b[0;36mincorrect_fft\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mincorrect_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBadFFTFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all fulfill this condition\n",
    "\n",
    "class FFTModule(Module):\n",
    "    def __init__(self):\n",
    "        super(FFTModule, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return BadFFTFunction.apply(input)\n",
    "\n",
    "fftModule = FFTModule()\n",
    "\n",
    "input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]\n",
    "# print(\"input: \", input)\n",
    "test = gradcheck(fftModule, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Are the gradients correct: \", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrized example\n",
    "--------------------\n",
    "\n",
    "An implementation of a layer with learnable weights, where cross-correlation has a learnable kernel.\n",
    "\n",
    "In deep learning literature, it’s confusingly referred to as convolution while the actual operation is cross-correlation (the only difference is that filter is flipped for convolution, which is not the case for cross-correlation).\n",
    "\n",
    "The backward pass computes the gradient wrt the input and the gradient wrt the\n",
    "filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import flip\n",
    "import numpy as np\n",
    "from scipy.signal import correlate2d\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class ScipyConv2dFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, filter, bias):\n",
    "        # detach so we can cast to NumPy\n",
    "        input, filter, bias = input.detach(), filter.detach(), bias.detach()  \n",
    "        result = correlate2d(input.numpy(), filter.numpy(), mode='valid')\n",
    "        result += bias.numpy()\n",
    "        ctx.save_for_backward(input, filter, bias)\n",
    "        return torch.from_numpy(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input, filter, bias = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_bias = np.sum(grad_output, keepdims=True)\n",
    "        grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')\n",
    "        grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')\n",
    "        return torch.from_numpy(grad_input), torch.from_numpy(grad_filter), torch.from_numpy(grad_bias)\n",
    "\n",
    "\n",
    "class ScipyConv2d(Module):\n",
    "    def __init__(self, kh, kw):\n",
    "        super(ScipyConv2d, self).__init__()\n",
    "        self.filter = Parameter(torch.randn(kh, kw))\n",
    "        self.bias = Parameter(torch.randn(1, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return ScipyConv2dFunction.apply(input, self.filter, self.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example usage:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter and bias:  [Parameter containing:\ntensor([[ 0.9778, -2.1926, -1.5007],\n        [-0.8564, -0.1252,  0.1965],\n        [-1.0147, -1.9099, -0.3765]]), Parameter containing:\ntensor(1.00000e-02 *\n       [[ 8.8882]])]\nOutput from the convolution:  tensor([[ 3.7728, -0.5321,  1.2976,  3.0419,  0.7295,  2.7042,  0.3629,\n         -0.6140],\n        [-7.2798, -4.4747, -2.0613,  5.4178,  3.8800,  3.5487,  3.0402,\n          0.8574],\n        [ 4.3338, -4.8571, -3.9630,  0.2303,  3.9771,  0.5293, -0.2377,\n         -3.1850],\n        [-8.4965, -2.0868,  4.7418,  4.5396,  0.4477,  2.1242,  1.7706,\n          1.2795],\n        [-3.0739, -2.5370, -2.3028,  7.2233,  6.8461,  3.8279, -2.4395,\n         -5.5369],\n        [-2.3000, -0.0067,  5.2505,  2.9312,  2.7599, -0.4494,  3.1907,\n          1.4693],\n        [ 6.3484,  5.1985,  1.9591,  7.1688,  4.7119,  4.7957, -0.1767,\n         -5.4811],\n        [-2.2898,  3.0240,  1.5137,  3.9723,  1.5269, -2.4496,  2.4694,\n         -4.7401]])\nGradient for the input map:  tensor([[-0.5286,  0.5118,  2.7124, -0.2537,  2.4665, -3.8262, -3.5456,\n         -2.6171,  1.0281,  1.2815],\n        [ 1.8458, -0.9604, -6.2492, -2.0131,  1.4828, -0.1558, -2.6333,\n          3.3682, -1.6295, -2.2947],\n        [-1.2107,  1.4227,  3.9368, -1.2688, -5.8396, -3.2579, -7.7326,\n         -2.4428,  6.8131,  2.7474],\n        [-1.9957, -2.3627, -1.0864,  2.1290, -1.7753, -6.9618,  3.2805,\n          4.8536, -3.3722, -1.6082],\n        [ 4.1659, -4.0965, -5.0243, -7.0333, -3.9770,  1.9662, -3.6105,\n         -1.0917, -3.4475, -2.2777],\n        [-1.9198,  3.3376,  3.1498, -3.9072, -0.7895, -1.7712,  0.7477,\n         -0.2375,  2.3166,  1.8479],\n        [-1.7591, -5.5526, -6.1582,  1.6847, -0.9167, -0.1142, -7.6462,\n         -2.8397, -5.3348, -2.9096],\n        [ 0.7107, -0.5023,  3.7035, -4.9632, -0.0422, -3.5087,  0.1475,\n          4.2706,  2.8665,  0.3850],\n        [-0.8669, -1.1093, -1.6010,  1.2238, -1.7606, -1.0224,  0.0086,\n          0.3984, -2.2434, -0.4536],\n        [-0.4599, -0.3487, -1.2904, -3.2727, -1.1598, -1.9030,  1.0776,\n          1.6920, -0.0197, -0.0725]])\n"
     ]
    }
   ],
   "source": [
    "module = ScipyConv2d(3, 3)\n",
    "print(\"Filter and bias: \", list(module.parameters()))\n",
    "input = torch.randn(10, 10, requires_grad=True)\n",
    "output = module(input)\n",
    "print(\"Output from the convolution: \", output)\n",
    "output.backward(torch.randn(8, 8))\n",
    "print(\"Gradient for the input map: \", input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the gradients correct:  True\n"
     ]
    }
   ],
   "source": [
    "moduleConv = ScipyConv2d(3, 3)\n",
    "\n",
    "input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]\n",
    "# print(\"input: \", input)\n",
    "test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Are the gradients correct: \", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
