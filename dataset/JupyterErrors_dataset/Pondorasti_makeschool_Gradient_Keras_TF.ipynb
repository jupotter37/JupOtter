{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9. 4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "\n",
    "\n",
    "def square(x):\n",
    "    return K.square(x)\n",
    "\n",
    "a = np.array([3.0, 2.0])\n",
    "print(K.eval(square(K.constant(a))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Resource:\n",
    "\n",
    "http://laid.delanover.com/gradients-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients_96/Square_50_grad/Mul_1:0\", shape=(2,), dtype=float32)\n",
      "[array([6., 4.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "i = K.placeholder(shape=(2,), name=\"input\")\n",
    "s = square(i)\n",
    "grads = K.gradients(s, i)[0]\n",
    "print(grads)\n",
    "# iterate = K.function([input_img], [loss, grads])\n",
    "f = K.function([i], [grads])\n",
    "print(f([a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([9., 4.], dtype=float32), array([6., 4.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "m = K.placeholder(shape=(2,), name=\"input\")\n",
    "square_f = K.square(m)\n",
    "grad = K.gradients([square_f], [m])[0]\n",
    "f = K.function([m], [square_f, grad])\n",
    "ival = np.array([3.0, 2.0])\n",
    "print(f([ival]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1.]), 17, 1)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as so\n",
    "def f(x):\n",
    "    return (x[0]*x[1]-1)**2+1, [(x[0]*x[1]-1)*x[1], (x[0]*x[1]-1)*x[0]]\n",
    "g = np.array([0.1,0.1])\n",
    "b=[(-10,10),(-10,10)]\n",
    "so.fmin_tnc(f,g,bounds=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000005, 1.00000009]),\n",
       " 1.441767747301186e-15,\n",
       " {'grad': array([ 1.02331202e-07, -2.59299369e-08]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 17,\n",
       "  'nit': 16,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as so\n",
    "\n",
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "\n",
    "# Derivative of f\n",
    "def fprime(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "\n",
    "so.fmin_l_bfgs_b(f, [2, 2], fprime=fprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 4.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(shape=(2,), dtype = tf.float64)\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "def grad(x):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        out = f(x)\n",
    "    return t.gradient(out, x)\n",
    "\n",
    "a = np.array([3.0, 2.0])\n",
    "\n",
    "# tf 2.0\n",
    "# x = tf.convert_to_tensor(a)\n",
    "# grad(x).numpy()\n",
    "\n",
    "# tf 1.x\n",
    "sess = tf.Session()\n",
    "print(sess.run(grad(x), feed_dict={x: a}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value y.\n",
    "dz_dy = t.gradient(z, y)\n",
    "sess = tf.Session()\n",
    "print(sess.run(dz_dy))\n",
    "# assert dz_dy.numpy() == 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8. 8.]\n",
      " [8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = tf.reduce_sum(x)\n",
    "    z = tf.multiply(y, y)\n",
    "\n",
    "# Use the tape to compute the derivative of z with respect to the\n",
    "# intermediate value y.\n",
    "dz_dx = t.gradient(z, x)\n",
    "sess = tf.Session()\n",
    "print(sess.run(dz_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is what I like :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000006 1.00000004]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x =  tf.placeholder(dtype=tf.float32, shape=(2,))\n",
    "\n",
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "#     return .5*(1 - tf.gather(x, 0))**2 + (tf.gather(x, 1) - tf.gather(x, 0)**2)**2\n",
    "\n",
    "# Derivative of f\n",
    "def fprime(x):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        out = f(x)\n",
    "    return t.gradient(out, x)\n",
    "\n",
    "a = np.array([2.0, 2.0])\n",
    "grad = fprime(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(2000):\n",
    "        grad_evaluated = sess.run(grad, feed_dict={x: a})\n",
    "        a = a - 0.1*grad_evaluated\n",
    "print(a)\n",
    "#     optimizer.apply_gradients([(grads, init_image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "\n",
    "# x = tf.constant([2.0, 3.0])\n",
    "x = tf.Variable([2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "\n",
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "#     return .5*(1 - tf.gather(x, 0))**2 + (tf.gather(x, 1) - tf.gather(x, 0)**2)**2\n",
    "\n",
    "\n",
    "# Derivative of f\n",
    "def fprime(x):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        out = f(x)\n",
    "    return t.gradient(out, x)\n",
    "\n",
    "\n",
    "a = np.array([2.0, 2.0])\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(fprime(x), feed_dict={x: a}))\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "for i in range(10):\n",
    "#     print(\"Iteration: {}\".format(i))\n",
    "#     grads = sess.run(fprime(x), feed_dict={x: a})\n",
    "    grads = fprime(x)\n",
    "    updates = optimizer.apply_gradients([(grads, x)])\n",
    "    print(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-cf218caed8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfprime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# trainables = tf.trainable_variables()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make sure repeat iteration works.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0mconverted_grads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables provided."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "\n",
    "# x = tf.constant([2.0, 3.0])\n",
    "x = tf.Variable([2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "\n",
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "#     return .5*(1 - tf.gather(x, 0))**2 + (tf.gather(x, 1) - tf.gather(x, 0)**2)**2\n",
    "\n",
    "\n",
    "# Derivative of f\n",
    "def fprime(x):\n",
    "    with tf.GradientTape() as t:\n",
    "        t.watch(x)\n",
    "        out = f(x)\n",
    "    return t.gradient(out, x)\n",
    "\n",
    "\n",
    "a = np.array([2.0, 2.0])\n",
    "\n",
    "# print(sess.run(fprime(x), feed_dict={x: a}))\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "grads = fprime(x)\n",
    "trainables = tf.trainable_variables()\n",
    "updates = optimizer.apply_gradients(zip(grads, trainables))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        print(sess.run(updates, feed_dict={x: a}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0509328 1.1144927]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "\n",
    "\n",
    "x = tf.Variable([2.0, 2.0], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# def f(x):   # The rosenbrock function\n",
    "#     return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "#     return .5*(1 - tf.gather(x, 0))**2 + (tf.gather(x, 1) - tf.gather(x, 0)**2)**2\n",
    "\n",
    "\n",
    "# Derivative of f\n",
    "# def fprime(x):\n",
    "#     with tf.GradientTape() as t:\n",
    "#         t.watch(x)\n",
    "#         out = f(x)\n",
    "#     return t.gradient(out, x)\n",
    "\n",
    "\n",
    "# a = np.array([2.0, 2.0])\n",
    "\n",
    "f = .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(f)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for _ in range(100):\n",
    "        sess.run([optimizer])\n",
    "    print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0509328 1.1144927]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "\n",
    "\n",
    "x = tf.Variable([2.0, 2.0], dtype=tf.float32)\n",
    "\n",
    "\n",
    "# def f(x):   # The rosenbrock function\n",
    "#     return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "\n",
    "\n",
    "# # Derivative of f\n",
    "# def fprime(x):\n",
    "#     with tf.GradientTape() as t:\n",
    "#         t.watch(x)\n",
    "#         out = f(x)\n",
    "#     return t.gradient(out, x)\n",
    "\n",
    "f = .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "grads = optimizer.compute_gradients(f, var_list=tf.trainable_variables())\n",
    "train_step = optimizer.apply_gradients(grads)\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for _ in range(100):\n",
    "        sess.run([train_step])\n",
    "    print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 154.0]\n",
      "[6.5, 56.0]\n",
      "[4.3999996, 20.719995]\n",
      "[3.1399999, 8.019199]\n",
      "[2.3839998, 3.4469109]\n",
      "[1.9303999, 1.8008881]\n",
      "[1.65824, 1.2083197]\n",
      "[1.494944, 0.9949951]\n",
      "[1.3969663, 0.9181981]\n",
      "[1.3381798, 0.89055157]\n",
      "[1.302908, 0.88059855]\n",
      "[1.2817447, 0.8770151]\n",
      "[1.2690468, 0.8757255]\n",
      "[1.2614281, 0.87526155]\n",
      "[1.2568569, 0.87509394]\n",
      "[1.2541142, 0.87503386]\n",
      "[1.2524685, 0.87501216]\n",
      "[1.251481, 0.8750043]\n",
      "[1.2508886, 0.87500143]\n",
      "[1.2505331, 0.8750005]\n",
      "[1.2503198, 0.875]\n",
      "[1.2501919, 0.87500024]\n",
      "[1.2501152, 0.87499976]\n",
      "[1.2500691, 0.875]\n",
      "[1.2500415, 0.875]\n",
      "[1.2500249, 0.87500024]\n",
      "[1.2500149, 0.87500024]\n",
      "[1.250009, 0.875]\n",
      "[1.2500054, 0.87500024]\n",
      "[1.2500032, 0.875]\n",
      "[1.2500019, 0.875]\n",
      "[1.2500012, 0.87500024]\n",
      "[1.2500007, 0.87499976]\n",
      "[1.2500005, 0.875]\n",
      "[1.2500002, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n",
      "[1.2500001, 0.87500024]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.Variable(10.0, trainable=True)\n",
    "f_x = 2 * x* x - 5 *x + 4\n",
    "\n",
    "loss = f_x\n",
    "opt = tf.train.GradientDescentOptimizer(0.1).minimize(f_x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        print(sess.run([x,loss]))\n",
    "        sess.run(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
