{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Request Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Limitations for Reporting on Several Repos\n",
    "The visualizations in this notebook are, like most, able to coherently display information for between 1 and 8 different repositories simultaneously. \n",
    "\n",
    "## Alternatives for Reporting on Repo Groups, Comprising Many Repos\n",
    "The included queries could be rewritten to show an entire repository group's characteristics of that is your primary aim. Specifically, any query could replace this line: \n",
    "```\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "```\n",
    "\n",
    "with this line to accomplish the goal of comparing different groups of repositories: \n",
    "```\n",
    "                            WHERE repogroups.repo_group_id = {repo_id}\n",
    "```\n",
    "\n",
    "Simply replace the set of id's in the **Pull Request Filter** section with a list of repo_group_id numbers as well, to accomplish this view. \n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare all repo ids you would like to produce charts for\n",
    "repo_set = {25686, 25687}\n",
    "\n",
    "#can be set as 'competitors' or 'repo'\n",
    "#'competitors' will group graphs by type, so it is easy to compare across repos\n",
    "# 'repo' will group graphs by repo so it is easy to look at all the contributor data for each repo\n",
    "display_grouping = 'repo'\n",
    "\n",
    "#if display_grouping is set to 'competitors', enter the repo ids you do no want to alias, if 'display_grouping' is set to repo the list will not effect anything\n",
    "not_aliased_repos = [25440, 25448]\n",
    "\n",
    "begin_date = '2019-10-01'\n",
    "end_date = '2020-10-31'\n",
    "\n",
    "#specify number of outliers for removal in scatter plot\n",
    "scatter_plot_outliers_removed = 5\n",
    "save_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the Longest Running Pull Requests\n",
    "\n",
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>head_ref_force_pushed_count</th>\n",
       "      <th>merged_count</th>\n",
       "      <th>milestoned_count</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25686</td>\n",
       "      <td>408720448</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020-04-24 19:14:09</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25686</td>\n",
       "      <td>836140005</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-31 13:58:29</td>\n",
       "      <td>2022-01-31 13:28:11</td>\n",
       "      <td>2022-01-31 13:58:29</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25686</td>\n",
       "      <td>836134506</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-31 15:33:47</td>\n",
       "      <td>2022-01-31 13:22:46</td>\n",
       "      <td>2022-01-31 15:33:47</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25686</td>\n",
       "      <td>830405906</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-24 13:59:28</td>\n",
       "      <td>2022-01-24 13:29:09</td>\n",
       "      <td>2022-01-24 13:59:28</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25686</td>\n",
       "      <td>830400350</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-24 15:29:42</td>\n",
       "      <td>2022-01-24 13:23:47</td>\n",
       "      <td>2022-01-24 15:29:42</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   repo_id  pr_src_id     repo_name pr_src_author_association repo_group  \\\n",
       "0    25686  408720448  llvm-project                      NONE     dotnet   \n",
       "1    25686  836140005  llvm-project                      NONE     dotnet   \n",
       "2    25686  836134506  llvm-project                      NONE     dotnet   \n",
       "3    25686  830405906  llvm-project                      NONE     dotnet   \n",
       "4    25686  830400350  llvm-project                      NONE     dotnet   \n",
       "\n",
       "  pr_src_state        pr_merged_at       pr_created_at        pr_closed_at  \\\n",
       "0       closed 2020-04-24 20:32:13 2020-04-24 19:14:09 2020-04-24 20:32:13   \n",
       "1       closed 2022-01-31 13:58:29 2022-01-31 13:28:11 2022-01-31 13:58:29   \n",
       "2       closed 2022-01-31 15:33:47 2022-01-31 13:22:46 2022-01-31 15:33:47   \n",
       "3       closed 2022-01-24 13:59:28 2022-01-24 13:29:09 2022-01-24 13:59:28   \n",
       "4       closed 2022-01-24 15:29:42 2022-01-24 13:23:47 2022-01-24 15:29:42   \n",
       "\n",
       "   created_year  ...  head_ref_force_pushed_count  merged_count  \\\n",
       "0        2020.0  ...                          NaN           NaN   \n",
       "1        2022.0  ...                          NaN           NaN   \n",
       "2        2022.0  ...                          NaN           NaN   \n",
       "3        2022.0  ...                          NaN           NaN   \n",
       "4        2022.0  ...                          NaN           NaN   \n",
       "\n",
       "   milestoned_count unlabeled_count head_ref_deleted_count  comment_count  \\\n",
       "0               NaN             NaN                    NaN            NaN   \n",
       "1               NaN             NaN                    NaN            NaN   \n",
       "2               NaN             NaN                    NaN            NaN   \n",
       "3               NaN             NaN                    NaN            NaN   \n",
       "4               NaN             NaN                    NaN            NaN   \n",
       "\n",
       "   lines_added  lines_removed  commit_count  file_count  \n",
       "0         None           None           1.0        None  \n",
       "1         None           None           1.0        None  \n",
       "2         None           None           1.0        None  \n",
       "3         None           None           1.0        None  \n",
       "4         None           None           1.0        None  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "repo_id                                     int64\n",
       "pr_src_id                                   int64\n",
       "repo_name                                  object\n",
       "pr_src_author_association                  object\n",
       "repo_group                                 object\n",
       "pr_src_state                               object\n",
       "pr_merged_at                       datetime64[ns]\n",
       "pr_created_at                      datetime64[ns]\n",
       "pr_closed_at                       datetime64[ns]\n",
       "created_year                              float64\n",
       "created_month                             float64\n",
       "closed_year                               float64\n",
       "closed_month                              float64\n",
       "pr_src_meta_label                          object\n",
       "pr_head_or_base                            object\n",
       "hours_to_close                            float64\n",
       "days_to_close                             float64\n",
       "hours_to_first_response                   float64\n",
       "days_to_first_response                    float64\n",
       "hours_to_last_response                    float64\n",
       "days_to_last_response                     float64\n",
       "first_response_time                datetime64[ns]\n",
       "last_response_time                 datetime64[ns]\n",
       "average_time_between_responses    timedelta64[ns]\n",
       "assigned_count                            float64\n",
       "review_requested_count                    float64\n",
       "labeled_count                             float64\n",
       "subscribed_count                          float64\n",
       "mentioned_count                           float64\n",
       "referenced_count                          float64\n",
       "closed_count                              float64\n",
       "head_ref_force_pushed_count               float64\n",
       "merged_count                              float64\n",
       "milestoned_count                          float64\n",
       "unlabeled_count                           float64\n",
       "head_ref_deleted_count                    float64\n",
       "comment_count                             float64\n",
       "lines_added                                object\n",
       "lines_removed                              object\n",
       "commit_count                              float64\n",
       "file_count                                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "\n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = salc.sql.text(f\"\"\"\n",
    "                    SELECT\n",
    "                        repo.repo_id AS repo_id,\n",
    "                        pull_requests.pr_src_id AS pr_src_id,\n",
    "                        repo.repo_name AS repo_name,\n",
    "                        pr_src_author_association,\n",
    "                        repo_groups.rg_name AS repo_group,\n",
    "                        pull_requests.pr_src_state,\n",
    "                        pull_requests.pr_merged_at,\n",
    "                        pull_requests.pr_created_at AS pr_created_at,\n",
    "                        pull_requests.pr_closed_at AS pr_closed_at,\n",
    "                        date_part( 'year', pr_created_at :: DATE ) AS CREATED_YEAR,\n",
    "                        date_part( 'month', pr_created_at :: DATE ) AS CREATED_MONTH,\n",
    "                        date_part( 'year', pr_closed_at :: DATE ) AS CLOSED_YEAR,\n",
    "                        date_part( 'month', pr_closed_at :: DATE ) AS CLOSED_MONTH,\n",
    "                        pr_src_meta_label,\n",
    "                        pr_head_or_base,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_close,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_close, \n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_first_response,\n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_first_response, \n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_last_response,\n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_last_response, \n",
    "                        first_response_time,\n",
    "                        last_response_time,\n",
    "                        average_time_between_responses,\n",
    "                        assigned_count,\n",
    "                        review_requested_count,\n",
    "                        labeled_count,\n",
    "                        subscribed_count,\n",
    "                        mentioned_count,\n",
    "                        referenced_count,\n",
    "                        closed_count,\n",
    "                        head_ref_force_pushed_count,\n",
    "                        merged_count,\n",
    "                        milestoned_count,\n",
    "                        unlabeled_count,\n",
    "                        head_ref_deleted_count,\n",
    "                        comment_count,\n",
    "                        lines_added, \n",
    "                        lines_removed,\n",
    "                        commit_count, \n",
    "                        file_count\n",
    "                    FROM\n",
    "                        repo,\n",
    "                        repo_groups,\n",
    "                        pull_requests LEFT OUTER JOIN ( \n",
    "                            SELECT pull_requests.pull_request_id,\n",
    "                            count(*) FILTER (WHERE action = 'assigned') AS assigned_count,\n",
    "                            count(*) FILTER (WHERE action = 'review_requested') AS review_requested_count,\n",
    "                            count(*) FILTER (WHERE action = 'labeled') AS labeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'unlabeled') AS unlabeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'subscribed') AS subscribed_count,\n",
    "                            count(*) FILTER (WHERE action = 'mentioned') AS mentioned_count,\n",
    "                            count(*) FILTER (WHERE action = 'referenced') AS referenced_count,\n",
    "                            count(*) FILTER (WHERE action = 'closed') AS closed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_force_pushed') AS head_ref_force_pushed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_deleted') AS head_ref_deleted_count,\n",
    "                            count(*) FILTER (WHERE action = 'milestoned') AS milestoned_count,\n",
    "                            count(*) FILTER (WHERE action = 'merged') AS merged_count,\n",
    "                            MIN(message.msg_timestamp) AS first_response_time,\n",
    "                            COUNT(DISTINCT message.msg_timestamp) AS comment_count,\n",
    "                            MAX(message.msg_timestamp) AS last_response_time,\n",
    "                            (MAX(message.msg_timestamp) - MIN(message.msg_timestamp)) / COUNT(DISTINCT message.msg_timestamp) AS average_time_between_responses\n",
    "                            FROM pull_request_events, pull_requests, repo, pull_request_message_ref, message\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "                            AND repo.repo_id = pull_requests.repo_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_events.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_message_ref.pull_request_id\n",
    "                            AND pull_request_message_ref.msg_id = message.msg_id\n",
    "                            GROUP BY pull_requests.pull_request_id\n",
    "                        ) response_times\n",
    "                        ON pull_requests.pull_request_id = response_times.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT pull_request_commits.pull_request_id, count(DISTINCT pr_cmt_sha) AS commit_count                                FROM pull_request_commits, pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_cmt_sha <> pull_requests.pr_merge_commit_sha\n",
    "                            AND pr_cmt_sha <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) all_commit_counts\n",
    "                        ON pull_requests.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT MAX(pr_repo_meta_id), pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                            FROM pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_head_or_base = 'base'\n",
    "                            GROUP BY pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                        ) base_labels\n",
    "                        ON base_labels.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT sum(cmt_added) AS lines_added, sum(cmt_removed) AS lines_removed, pull_request_commits.pull_request_id, count(DISTINCT cmt_filename) AS file_count\n",
    "                            FROM pull_request_commits, commits, pull_requests, pull_request_meta\n",
    "                            WHERE cmt_commit_hash = pr_cmt_sha\n",
    "                            AND pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND commits.repo_id = pull_requests.repo_id\n",
    "                            AND commits.cmt_commit_hash <> pull_requests.pr_merge_commit_sha\n",
    "                            AND commits.cmt_commit_hash <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) master_merged_counts \n",
    "                        ON base_labels.pull_request_id = master_merged_counts.pull_request_id                    \n",
    "                    WHERE \n",
    "                        repo.repo_group_id = repo_groups.repo_group_id \n",
    "                        AND repo.repo_id = pull_requests.repo_id \n",
    "                        AND repo.repo_id = {repo_id} \n",
    "                    ORDER BY\n",
    "                       merged_count DESC\n",
    "        \"\"\")\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "display(pr_all.head())\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin data pre-processing and adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo_id                                     int64\n",
       "pr_src_id                                   int64\n",
       "repo_name                                  object\n",
       "pr_src_author_association                  object\n",
       "repo_group                                 object\n",
       "pr_src_state                               object\n",
       "pr_merged_at                       datetime64[ns]\n",
       "pr_created_at                      datetime64[ns]\n",
       "pr_closed_at                       datetime64[ns]\n",
       "created_year                               object\n",
       "created_month                             float64\n",
       "closed_year                                object\n",
       "closed_month                              float64\n",
       "pr_src_meta_label                          object\n",
       "pr_head_or_base                            object\n",
       "hours_to_close                            float64\n",
       "days_to_close                             float64\n",
       "hours_to_first_response                   float64\n",
       "days_to_first_response                    float64\n",
       "hours_to_last_response                    float64\n",
       "days_to_last_response                     float64\n",
       "first_response_time                datetime64[ns]\n",
       "last_response_time                 datetime64[ns]\n",
       "average_time_between_responses    timedelta64[ns]\n",
       "assigned_count                            float64\n",
       "review_requested_count                    float64\n",
       "labeled_count                             float64\n",
       "subscribed_count                          float64\n",
       "mentioned_count                           float64\n",
       "referenced_count                          float64\n",
       "closed_count                              float64\n",
       "head_ref_force_pushed_count               float64\n",
       "merged_count                              float64\n",
       "milestoned_count                          float64\n",
       "unlabeled_count                           float64\n",
       "head_ref_deleted_count                    float64\n",
       "comment_count                             float64\n",
       "lines_added                               float64\n",
       "lines_removed                             float64\n",
       "commit_count                              float64\n",
       "file_count                                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change count columns from float datatype to integer\n",
    "pr_all[['assigned_count',\n",
    "          'review_requested_count',\n",
    "          'labeled_count',\n",
    "          'subscribed_count',\n",
    "          'mentioned_count',\n",
    "          'referenced_count',\n",
    "          'closed_count',\n",
    "          'head_ref_force_pushed_count',\n",
    "          'merged_count',\n",
    "          'milestoned_count',\n",
    "          'unlabeled_count',\n",
    "          'head_ref_deleted_count',\n",
    "          'comment_count',\n",
    "        'commit_count',\n",
    "        'file_count',\n",
    "        'lines_added',\n",
    "        'lines_removed'\n",
    "       ]] = pr_all[['assigned_count',\n",
    "                                      'review_requested_count',\n",
    "                                      'labeled_count',\n",
    "                                      'subscribed_count',\n",
    "                                      'mentioned_count',\n",
    "                                      'referenced_count',\n",
    "                                      'closed_count',\n",
    "                                        'head_ref_force_pushed_count',\n",
    "                                    'merged_count',\n",
    "                                      'milestoned_count',          \n",
    "                                      'unlabeled_count',\n",
    "                                      'head_ref_deleted_count',\n",
    "                                      'comment_count',\n",
    "                                        'commit_count',\n",
    "                                        'file_count',\n",
    "                                        'lines_added',\n",
    "                                        'lines_removed'\n",
    "                   ]].astype(float)\n",
    "# Change years to int so that doesn't display as 2019.0 for example\n",
    "pr_all[[\n",
    "            'created_year',\n",
    "           'closed_year']] = pr_all[['created_year',\n",
    "                                       'closed_year']].fillna(-1).astype(int).astype(str)\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llvm-project' 'interactive']\n"
     ]
    }
   ],
   "source": [
    "print(pr_all['repo_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `average_days_between_responses` and `average_hours_between_responses` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>milestoned_count</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25686</td>\n",
       "      <td>408720448</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020-04-24 19:14:09</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25686</td>\n",
       "      <td>836140005</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-31 13:58:29</td>\n",
       "      <td>2022-01-31 13:28:11</td>\n",
       "      <td>2022-01-31 13:58:29</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25686</td>\n",
       "      <td>836134506</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-31 15:33:47</td>\n",
       "      <td>2022-01-31 13:22:46</td>\n",
       "      <td>2022-01-31 15:33:47</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25686</td>\n",
       "      <td>830405906</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-24 13:59:28</td>\n",
       "      <td>2022-01-24 13:29:09</td>\n",
       "      <td>2022-01-24 13:59:28</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25686</td>\n",
       "      <td>830400350</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2022-01-24 15:29:42</td>\n",
       "      <td>2022-01-24 13:23:47</td>\n",
       "      <td>2022-01-24 15:29:42</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   repo_id  pr_src_id     repo_name pr_src_author_association repo_group  \\\n",
       "0    25686  408720448  llvm-project                      NONE     dotnet   \n",
       "1    25686  836140005  llvm-project                      NONE     dotnet   \n",
       "2    25686  836134506  llvm-project                      NONE     dotnet   \n",
       "3    25686  830405906  llvm-project                      NONE     dotnet   \n",
       "4    25686  830400350  llvm-project                      NONE     dotnet   \n",
       "\n",
       "  pr_src_state        pr_merged_at       pr_created_at        pr_closed_at  \\\n",
       "0       closed 2020-04-24 20:32:13 2020-04-24 19:14:09 2020-04-24 20:32:13   \n",
       "1       closed 2022-01-31 13:58:29 2022-01-31 13:28:11 2022-01-31 13:58:29   \n",
       "2       closed 2022-01-31 15:33:47 2022-01-31 13:22:46 2022-01-31 15:33:47   \n",
       "3       closed 2022-01-24 13:59:28 2022-01-24 13:29:09 2022-01-24 13:59:28   \n",
       "4       closed 2022-01-24 15:29:42 2022-01-24 13:23:47 2022-01-24 15:29:42   \n",
       "\n",
       "  created_year  ...  milestoned_count unlabeled_count  head_ref_deleted_count  \\\n",
       "0         2020  ...               NaN             NaN                     NaN   \n",
       "1         2022  ...               NaN             NaN                     NaN   \n",
       "2         2022  ...               NaN             NaN                     NaN   \n",
       "3         2022  ...               NaN             NaN                     NaN   \n",
       "4         2022  ...               NaN             NaN                     NaN   \n",
       "\n",
       "  comment_count lines_added  lines_removed  commit_count  file_count  \\\n",
       "0           NaN         NaN            NaN           1.0         NaN   \n",
       "1           NaN         NaN            NaN           1.0         NaN   \n",
       "2           NaN         NaN            NaN           1.0         NaN   \n",
       "3           NaN         NaN            NaN           1.0         NaN   \n",
       "4           NaN         NaN            NaN           1.0         NaN   \n",
       "\n",
       "   average_days_between_responses  average_hours_between_responses  \n",
       "0                             NaN                              NaN  \n",
       "1                             NaN                              NaN  \n",
       "2                             NaN                              NaN  \n",
       "3                             NaN                              NaN  \n",
       "4                             NaN                              NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get days for average_time_between_responses time delta\n",
    "\n",
    "pr_all['average_days_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days).astype(float)\n",
    "pr_all['average_hours_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days * 24).astype(float)\n",
    "\n",
    "pr_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date filtering entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25686</td>\n",
       "      <td>408720448</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020-04-24 19:14:09</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   repo_id  pr_src_id     repo_name pr_src_author_association repo_group  \\\n",
       "0    25686  408720448  llvm-project                      NONE     dotnet   \n",
       "\n",
       "  pr_src_state        pr_merged_at       pr_created_at        pr_closed_at  \\\n",
       "0       closed 2020-04-24 20:32:13 2020-04-24 19:14:09 2020-04-24 20:32:13   \n",
       "\n",
       "   created_year  ... unlabeled_count head_ref_deleted_count  comment_count  \\\n",
       "0          2020  ...             NaN                    NaN            NaN   \n",
       "\n",
       "  lines_added lines_removed  commit_count  file_count  \\\n",
       "0         NaN           NaN           1.0         NaN   \n",
       "\n",
       "   average_days_between_responses  average_hours_between_responses  \\\n",
       "0                             NaN                              NaN   \n",
       "\n",
       "   created_yearmonth  \n",
       "0         2020-04-01  \n",
       "\n",
       "[1 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = pd.to_datetime(begin_date)\n",
    "# end_date = pd.to_datetime('2020-02-01 09:00:00')\n",
    "end_date = pd.to_datetime(end_date)\n",
    "pr_all = pr_all[(pr_all['pr_created_at'] > start_date) & (pr_all['pr_closed_at'] < end_date)]\n",
    "\n",
    "pr_all['created_year'] = pr_all['created_year'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(lambda x: '{0:0>2}'.format(x))\n",
    "pr_all['created_yearmonth'] = pd.to_datetime(pr_all['created_year'].map(str) + '-' + pr_all['created_month'].map(str) + '-01')\n",
    "pr_all.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `days_to_close` column for pull requests that are still open (closed pull requests already have this column filled from the query)\n",
    "\n",
    "Note: there will be no pull requests that are still open in the dataframe if you filtered by an end date in the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo_id, pr_src_id, repo_name, pr_src_author_association, repo_group, pr_src_state, pr_merged_at, pr_created_at, pr_closed_at, created_year, created_month, closed_year, closed_month, pr_src_meta_label, pr_head_or_base, hours_to_close, days_to_close, hours_to_first_response, days_to_first_response, hours_to_last_response, days_to_last_response, first_response_time, last_response_time, average_time_between_responses, assigned_count, review_requested_count, labeled_count, subscribed_count, mentioned_count, referenced_count, closed_count, head_ref_force_pushed_count, merged_count, milestoned_count, unlabeled_count, head_ref_deleted_count, comment_count, lines_added, lines_removed, commit_count, file_count, average_days_between_responses, average_hours_between_responses, created_yearmonth]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 44 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "# getting the number of days of (today - created at) for the PRs that are still open\n",
    "# and putting this in the days_to_close column\n",
    "\n",
    "# get timedeltas of creation time to todays date/time\n",
    "days_to_close_open_pr = datetime.datetime.now() - pr_all.loc[pr_all['pr_src_state'] == 'open']['pr_created_at']\n",
    "\n",
    "# get num days from above timedelta\n",
    "days_to_close_open_pr = days_to_close_open_pr.apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# for only OPEN pr's, set the days_to_close column equal to above dataframe\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'] = pr_all.loc[pr_all['pr_src_state'] == 'open'].assign(days_to_close=days_to_close_open_pr)\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `closed_yearmonth` column for only CLOSED pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "      <th>closed_yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25686</td>\n",
       "      <td>408720448</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>NONE</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020-04-24 19:14:09</td>\n",
       "      <td>2020-04-24 20:32:13</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25686</td>\n",
       "      <td>360600670</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-01-08 18:42:57</td>\n",
       "      <td>2020-01-08 18:42:48</td>\n",
       "      <td>2020-01-08 18:42:57</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25686</td>\n",
       "      <td>360209550</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-01-07 23:00:35</td>\n",
       "      <td>2020-01-07 22:57:47</td>\n",
       "      <td>2020-01-07 23:00:35</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25686</td>\n",
       "      <td>360191689</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-01-07 22:06:16</td>\n",
       "      <td>2020-01-07 22:00:18</td>\n",
       "      <td>2020-01-07 22:06:16</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25686</td>\n",
       "      <td>360154732</td>\n",
       "      <td>llvm-project</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-01-07 20:16:41</td>\n",
       "      <td>2020-01-07 20:16:32</td>\n",
       "      <td>2020-01-07 20:16:41</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>25687</td>\n",
       "      <td>408265106</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-05-07 08:09:42</td>\n",
       "      <td>2020-04-24 00:23:44</td>\n",
       "      <td>2020-05-07 08:09:42</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>25687</td>\n",
       "      <td>408261765</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-05-02 00:55:10</td>\n",
       "      <td>2020-04-24 00:10:57</td>\n",
       "      <td>2020-05-02 00:55:10</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18504.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>25687</td>\n",
       "      <td>369810971</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-02-11 14:49:53</td>\n",
       "      <td>2020-01-31 23:21:54</td>\n",
       "      <td>2020-02-11 14:49:53</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>25687</td>\n",
       "      <td>451540760</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-07-22 15:12:24</td>\n",
       "      <td>2020-07-17 23:18:38</td>\n",
       "      <td>2020-07-22 15:12:24</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>25687</td>\n",
       "      <td>422050578</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-05-22 18:50:34</td>\n",
       "      <td>2020-05-22 17:07:14</td>\n",
       "      <td>2020-05-22 18:50:34</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      repo_id  pr_src_id     repo_name pr_src_author_association repo_group  \\\n",
       "0       25686  408720448  llvm-project                      NONE     dotnet   \n",
       "20      25686  360600670  llvm-project                    MEMBER     dotnet   \n",
       "21      25686  360209550  llvm-project                    MEMBER     dotnet   \n",
       "22      25686  360191689  llvm-project                    MEMBER     dotnet   \n",
       "23      25686  360154732  llvm-project                    MEMBER     dotnet   \n",
       "...       ...        ...           ...                       ...        ...   \n",
       "1028    25687  408265106   interactive                    MEMBER     dotnet   \n",
       "1029    25687  408261765   interactive                    MEMBER     dotnet   \n",
       "1030    25687  369810971   interactive               CONTRIBUTOR     dotnet   \n",
       "1031    25687  451540760   interactive               CONTRIBUTOR     dotnet   \n",
       "1032    25687  422050578   interactive                    MEMBER     dotnet   \n",
       "\n",
       "     pr_src_state        pr_merged_at       pr_created_at        pr_closed_at  \\\n",
       "0          closed 2020-04-24 20:32:13 2020-04-24 19:14:09 2020-04-24 20:32:13   \n",
       "20         closed 2020-01-08 18:42:57 2020-01-08 18:42:48 2020-01-08 18:42:57   \n",
       "21         closed 2020-01-07 23:00:35 2020-01-07 22:57:47 2020-01-07 23:00:35   \n",
       "22         closed 2020-01-07 22:06:16 2020-01-07 22:00:18 2020-01-07 22:06:16   \n",
       "23         closed 2020-01-07 20:16:41 2020-01-07 20:16:32 2020-01-07 20:16:41   \n",
       "...           ...                 ...                 ...                 ...   \n",
       "1028       closed 2020-05-07 08:09:42 2020-04-24 00:23:44 2020-05-07 08:09:42   \n",
       "1029       closed 2020-05-02 00:55:10 2020-04-24 00:10:57 2020-05-02 00:55:10   \n",
       "1030       closed 2020-02-11 14:49:53 2020-01-31 23:21:54 2020-02-11 14:49:53   \n",
       "1031       closed 2020-07-22 15:12:24 2020-07-17 23:18:38 2020-07-22 15:12:24   \n",
       "1032       closed 2020-05-22 18:50:34 2020-05-22 17:07:14 2020-05-22 18:50:34   \n",
       "\n",
       "      created_year  ... head_ref_deleted_count comment_count  lines_added  \\\n",
       "0             2020  ...                    NaN           NaN          NaN   \n",
       "20            2020  ...                    NaN           NaN          NaN   \n",
       "21            2020  ...                    NaN           NaN          NaN   \n",
       "22            2020  ...                    NaN           NaN          NaN   \n",
       "23            2020  ...                    NaN           NaN          NaN   \n",
       "...            ...  ...                    ...           ...          ...   \n",
       "1028          2020  ...                    4.0           4.0          NaN   \n",
       "1029          2020  ...                    0.0          14.0      18504.0   \n",
       "1030          2020  ...                    0.0          29.0          NaN   \n",
       "1031          2020  ...                    8.0           8.0          NaN   \n",
       "1032          2020  ...                    2.0           2.0          NaN   \n",
       "\n",
       "     lines_removed commit_count  file_count  average_days_between_responses  \\\n",
       "0              NaN          1.0         NaN                             NaN   \n",
       "20             NaN          1.0         NaN                             NaN   \n",
       "21             NaN          1.0         NaN                             NaN   \n",
       "22             NaN          1.0         NaN                             NaN   \n",
       "23             NaN          1.0         NaN                             NaN   \n",
       "...            ...          ...         ...                             ...   \n",
       "1028           NaN         11.0         NaN                             0.0   \n",
       "1029        3613.0         50.0        64.0                             0.0   \n",
       "1030           NaN         18.0         NaN                             0.0   \n",
       "1031           NaN          3.0         NaN                             0.0   \n",
       "1032           NaN          2.0         NaN                             0.0   \n",
       "\n",
       "      average_hours_between_responses  created_yearmonth  closed_yearmonth  \n",
       "0                                 NaN         2020-04-01        2020-04-01  \n",
       "20                                NaN         2020-01-01        2020-01-01  \n",
       "21                                NaN         2020-01-01        2020-01-01  \n",
       "22                                NaN         2020-01-01        2020-01-01  \n",
       "23                                NaN         2020-01-01        2020-01-01  \n",
       "...                               ...                ...               ...  \n",
       "1028                              0.0         2020-04-01        2020-05-01  \n",
       "1029                              0.0         2020-04-01        2020-05-01  \n",
       "1030                              0.0         2020-01-01        2020-02-01  \n",
       "1031                              0.0         2020-07-01        2020-07-01  \n",
       "1032                              0.0         2020-05-01        2020-05-01  \n",
       "\n",
       "[582 rows x 45 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate column by setting all null datetimes\n",
    "pr_all['closed_yearmonth'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# Fill column with prettified string of year/month closed that looks like: 2019-07-01\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed'] = pr_all.loc[pr_all['pr_src_state'] == 'closed'].assign(\n",
    "    closed_yearmonth = pd.to_datetime(pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_year'].astype(int\n",
    "        ).map(str) + '-' + pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_month'].astype(int).map(str) + '-01'))\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `merged_flag` column which is just prettified strings based off of if the `pr_merged_at` column is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Merged / Accepted\n",
       "20      Merged / Accepted\n",
       "21      Merged / Accepted\n",
       "22      Merged / Accepted\n",
       "23      Merged / Accepted\n",
       "              ...        \n",
       "1028    Merged / Accepted\n",
       "1029    Merged / Accepted\n",
       "1030    Merged / Accepted\n",
       "1031    Merged / Accepted\n",
       "1032    Merged / Accepted\n",
       "Name: merged_flag, Length: 582, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Merged flag \"\"\"\n",
    "if 'pr_merged_at' in pr_all.columns.values:\n",
    "    pr_all['pr_merged_at'] = pr_all['pr_merged_at'].fillna(0)\n",
    "    pr_all['merged_flag'] = 'Not Merged / Rejected'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_merged_at'] != 0] = 'Merged / Accepted'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_src_state'] == 'open'] = 'Still Open'\n",
    "    del pr_all['pr_merged_at']\n",
    "pr_all['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into different dataframes\n",
    "### All, open, closed, and slowest 20% of these 3 categories (6 dataframes total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Merged / Accepted\n",
       "20      Merged / Accepted\n",
       "21      Merged / Accepted\n",
       "22      Merged / Accepted\n",
       "23      Merged / Accepted\n",
       "              ...        \n",
       "1028    Merged / Accepted\n",
       "1029    Merged / Accepted\n",
       "1030    Merged / Accepted\n",
       "1031    Merged / Accepted\n",
       "1032    Merged / Accepted\n",
       "Name: merged_flag, Length: 582, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the different state PRs for now\n",
    "pr_open = pr_all.loc[pr_all['pr_src_state'] == 'open']\n",
    "pr_closed = pr_all.loc[pr_all['pr_src_state'] == 'closed']\n",
    "pr_merged = pr_all.loc[pr_all['merged_flag'] == 'Merged / Accepted']\n",
    "pr_not_merged = pr_all.loc[pr_all['merged_flag'] == 'Not Merged / Rejected']\n",
    "pr_closed['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes that contain the slowest 20% pull requests of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>created_month</th>\n",
       "      <th>...</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "      <th>closed_yearmonth</th>\n",
       "      <th>merged_flag</th>\n",
       "      <th>percentile_rank_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25687</td>\n",
       "      <td>436845786</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-06-19 02:00:36</td>\n",
       "      <td>2020-06-21 17:19:04</td>\n",
       "      <td>2020</td>\n",
       "      <td>06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.889353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25687</td>\n",
       "      <td>434921430</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-06-16 01:38:52</td>\n",
       "      <td>2020-06-27 00:51:54</td>\n",
       "      <td>2020</td>\n",
       "      <td>06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.979123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25687</td>\n",
       "      <td>434756797</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-06-15 20:09:34</td>\n",
       "      <td>2020-06-21 17:09:07</td>\n",
       "      <td>2020</td>\n",
       "      <td>06</td>\n",
       "      <td>...</td>\n",
       "      <td>193.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.945720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25687</td>\n",
       "      <td>432087613</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-06-09 21:58:42</td>\n",
       "      <td>2020-06-11 10:24:34</td>\n",
       "      <td>2020</td>\n",
       "      <td>06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.851775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25687</td>\n",
       "      <td>428731439</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-06-05 20:22:33</td>\n",
       "      <td>2020-06-06 19:56:46</td>\n",
       "      <td>2020</td>\n",
       "      <td>06</td>\n",
       "      <td>...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.803758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>25687</td>\n",
       "      <td>381049396</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-02-27 20:10:35</td>\n",
       "      <td>2020-02-28 20:50:04</td>\n",
       "      <td>2020</td>\n",
       "      <td>02</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.818018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>25687</td>\n",
       "      <td>408265106</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-04-24 00:23:44</td>\n",
       "      <td>2020-05-07 08:09:42</td>\n",
       "      <td>2020</td>\n",
       "      <td>04</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.983784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>25687</td>\n",
       "      <td>408261765</td>\n",
       "      <td>interactive</td>\n",
       "      <td>MEMBER</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-04-24 00:10:57</td>\n",
       "      <td>2020-05-02 00:55:10</td>\n",
       "      <td>2020</td>\n",
       "      <td>04</td>\n",
       "      <td>...</td>\n",
       "      <td>18504.0</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.963964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>25687</td>\n",
       "      <td>369810971</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-01-31 23:21:54</td>\n",
       "      <td>2020-02-11 14:49:53</td>\n",
       "      <td>2020</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.974775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>25687</td>\n",
       "      <td>451540760</td>\n",
       "      <td>interactive</td>\n",
       "      <td>CONTRIBUTOR</td>\n",
       "      <td>dotnet</td>\n",
       "      <td>closed</td>\n",
       "      <td>2020-07-17 23:18:38</td>\n",
       "      <td>2020-07-22 15:12:24</td>\n",
       "      <td>2020</td>\n",
       "      <td>07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>Merged / Accepted</td>\n",
       "      <td>0.927928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      repo_id  pr_src_id    repo_name pr_src_author_association repo_group  \\\n",
       "22      25687  436845786  interactive               CONTRIBUTOR     dotnet   \n",
       "27      25687  434921430  interactive                    MEMBER     dotnet   \n",
       "30      25687  434756797  interactive               CONTRIBUTOR     dotnet   \n",
       "39      25687  432087613  interactive                    MEMBER     dotnet   \n",
       "42      25687  428731439  interactive               CONTRIBUTOR     dotnet   \n",
       "...       ...        ...          ...                       ...        ...   \n",
       "1027    25687  381049396  interactive                    MEMBER     dotnet   \n",
       "1028    25687  408265106  interactive                    MEMBER     dotnet   \n",
       "1029    25687  408261765  interactive                    MEMBER     dotnet   \n",
       "1030    25687  369810971  interactive               CONTRIBUTOR     dotnet   \n",
       "1031    25687  451540760  interactive               CONTRIBUTOR     dotnet   \n",
       "\n",
       "     pr_src_state       pr_created_at        pr_closed_at  created_year  \\\n",
       "22         closed 2020-06-19 02:00:36 2020-06-21 17:19:04          2020   \n",
       "27         closed 2020-06-16 01:38:52 2020-06-27 00:51:54          2020   \n",
       "30         closed 2020-06-15 20:09:34 2020-06-21 17:09:07          2020   \n",
       "39         closed 2020-06-09 21:58:42 2020-06-11 10:24:34          2020   \n",
       "42         closed 2020-06-05 20:22:33 2020-06-06 19:56:46          2020   \n",
       "...           ...                 ...                 ...           ...   \n",
       "1027       closed 2020-02-27 20:10:35 2020-02-28 20:50:04          2020   \n",
       "1028       closed 2020-04-24 00:23:44 2020-05-07 08:09:42          2020   \n",
       "1029       closed 2020-04-24 00:10:57 2020-05-02 00:55:10          2020   \n",
       "1030       closed 2020-01-31 23:21:54 2020-02-11 14:49:53          2020   \n",
       "1031       closed 2020-07-17 23:18:38 2020-07-22 15:12:24          2020   \n",
       "\n",
       "     created_month  ... lines_added  lines_removed commit_count file_count  \\\n",
       "22              06  ...         NaN            NaN          7.0        NaN   \n",
       "27              06  ...         NaN            NaN          1.0        NaN   \n",
       "30              06  ...       193.0          330.0          4.0        9.0   \n",
       "39              06  ...         NaN            NaN          1.0        NaN   \n",
       "42              06  ...        81.0           83.0          5.0        9.0   \n",
       "...            ...  ...         ...            ...          ...        ...   \n",
       "1027            02  ...         NaN            NaN         15.0        NaN   \n",
       "1028            04  ...         NaN            NaN         11.0        NaN   \n",
       "1029            04  ...     18504.0         3613.0         50.0       64.0   \n",
       "1030            01  ...         NaN            NaN         18.0        NaN   \n",
       "1031            07  ...         NaN            NaN          3.0        NaN   \n",
       "\n",
       "      average_days_between_responses  average_hours_between_responses  \\\n",
       "22                               NaN                              NaN   \n",
       "27                               NaN                              NaN   \n",
       "30                               NaN                              NaN   \n",
       "39                               NaN                              NaN   \n",
       "42                               NaN                              NaN   \n",
       "...                              ...                              ...   \n",
       "1027                             0.0                              0.0   \n",
       "1028                             0.0                              0.0   \n",
       "1029                             0.0                              0.0   \n",
       "1030                             0.0                              0.0   \n",
       "1031                             0.0                              0.0   \n",
       "\n",
       "      created_yearmonth  closed_yearmonth        merged_flag  \\\n",
       "22           2020-06-01        2020-06-01  Merged / Accepted   \n",
       "27           2020-06-01        2020-06-01  Merged / Accepted   \n",
       "30           2020-06-01        2020-06-01  Merged / Accepted   \n",
       "39           2020-06-01        2020-06-01  Merged / Accepted   \n",
       "42           2020-06-01        2020-06-01  Merged / Accepted   \n",
       "...                 ...               ...                ...   \n",
       "1027         2020-02-01        2020-02-01  Merged / Accepted   \n",
       "1028         2020-04-01        2020-05-01  Merged / Accepted   \n",
       "1029         2020-04-01        2020-05-01  Merged / Accepted   \n",
       "1030         2020-01-01        2020-02-01  Merged / Accepted   \n",
       "1031         2020-07-01        2020-07-01  Merged / Accepted   \n",
       "\n",
       "      percentile_rank_local  \n",
       "22                 0.889353  \n",
       "27                 0.979123  \n",
       "30                 0.945720  \n",
       "39                 0.851775  \n",
       "42                 0.803758  \n",
       "...                     ...  \n",
       "1027               0.818018  \n",
       "1028               0.983784  \n",
       "1029               0.963964  \n",
       "1030               0.974775  \n",
       "1031               0.927928  \n",
       "\n",
       "[208 rows x 46 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the 80th percentile slowest PRs\n",
    "\n",
    "def filter_20_per_slowest(input_df):\n",
    "    pr_slow20_filtered = pd.DataFrame()\n",
    "    pr_slow20_x = pd.DataFrame()\n",
    "    for value in repo_set: \n",
    "        if not pr_slow20_filtered.empty: \n",
    "            pr_slow20x = input_df.query('repo_id==@value')\n",
    "            pr_slow20x['percentile_rank_local'] = pr_slow20x.days_to_close.rank(pct=True)\n",
    "            pr_slow20x = pr_slow20x.query('percentile_rank_local >= .8', )\n",
    "            pr_slow20_filtered = pd.concat([pr_slow20x, pr_slow20_filtered]) \n",
    "            reponame = str(value)\n",
    "            filename = ''.join(['output/pr_slowest20pct', reponame, '.csv'])\n",
    "            pr_slow20x.to_csv(filename)\n",
    "        else: \n",
    "            # first time\n",
    "            pr_slow20_filtered = input_df.copy()\n",
    "            pr_slow20_filtered['percentile_rank_local'] = pr_slow20_filtered.days_to_close.rank(pct=True)\n",
    "            pr_slow20_filtered = pr_slow20_filtered.query('percentile_rank_local >= .8', )\n",
    "#     print(pr_slow20_filtered.describe())\n",
    "    return pr_slow20_filtered\n",
    "\n",
    "pr_slow20_open = filter_20_per_slowest(pr_open)\n",
    "pr_slow20_closed = filter_20_per_slowest(pr_closed)\n",
    "pr_slow20_merged = filter_20_per_slowest(pr_merged)\n",
    "pr_slow20_not_merged = filter_20_per_slowest(pr_not_merged)\n",
    "pr_slow20_all = filter_20_per_slowest(pr_all)\n",
    "pr_slow20_merged#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionairy with number as the key and a letter as the value\n",
    "#this is used to alias repos when using 'compeitor' display grouping\n",
    "letters = []\n",
    "nums = []\n",
    "alpha = 'a'\n",
    "for i in range(0, 26): \n",
    "    letters.append(alpha) \n",
    "    alpha = chr(ord(alpha) + 1)\n",
    "    nums.append(i)\n",
    "letters = [x.upper() for x in letters]\n",
    "\n",
    "#create dict out of list of numbers and letters\n",
    "repo_alias_dict = {nums[i]: letters[i] for i in range(len(nums))}\n",
    "\n",
    "# create dict in the form {repo_id : repo_name}\n",
    "aliased_repos = []\n",
    "repo_dict = {}\n",
    "count = 0\n",
    "for repo_id in repo_set:\n",
    "    \n",
    "    #find corresponding repo name from each repo_id \n",
    "    repo_name = pr_all.loc[pr_all['repo_id'] == repo_id].iloc[0]['repo_name']\n",
    "    \n",
    "    #if competitor grouping is enabled turn all repo names, other than the ones in the 'not_aliased_repos' into an alias\n",
    "    if display_grouping == 'competitors' and not repo_id in not_aliased_repos:\n",
    "        repo_name =  'Repo ' + repo_alias_dict[count]\n",
    "        \n",
    "        #add repo_id to list of aliased repos, this is used for ordering\n",
    "        aliased_repos.append(repo_id)\n",
    "        count += 1\n",
    "        \n",
    "    #add repo_id and repo names as key value pairs into a dict, this is used to label the title of the visualizations\n",
    "    repo_dict.update({repo_id : repo_name})\n",
    "\n",
    "#gurantees that the non_aliased repos come first when display grouping is set as 'competitors'\n",
    "repo_list = not_aliased_repos + aliased_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Colorblind, mpl, Category20\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models.annotations import Title\n",
    "from bokeh.io import export_png\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, Legend, LabelSet, Range1d, LinearAxis, Label\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.glyphs import Rect\n",
    "from bokeh.transform import dodge\n",
    "\n",
    "try:\n",
    "    colors = Colorblind[len(repo_set)]\n",
    "except:\n",
    "    colors = Colorblind[3]\n",
    "#mpl['Plasma'][len(repo_set)]\n",
    "#['A6CEE3','B2DF8A','33A02C','FB9A99']\n",
    "\n",
    "def remove_outliers(input_df, field, num_outliers_repo_map):\n",
    "    df_no_outliers = input_df.copy()\n",
    "    for repo_name, num_outliers in num_outliers_repo_map.items():\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == repo_name].nlargest(num_outliers, field).index\n",
    "        df_no_outliers = df_no_outliers.drop(index=indices_to_drop)\n",
    "    return df_no_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime as dt\n",
    "\n",
    "def visualize_mean_days_to_close(input_df, x_axis='closed_yearmonth', description='Closed', save_file=False, num_remove_outliers=0, drop_outliers_repo=None):\n",
    "\n",
    "    # Set the df you want to build the viz's for\n",
    "    driver_df = input_df.copy()\n",
    "    \n",
    "    driver_df = driver_df[['repo_id', 'repo_name', 'pr_src_id', 'created_yearmonth', 'closed_yearmonth', 'days_to_close']]\n",
    "\n",
    "    if save_file:\n",
    "        driver_df.to_csv('output/c.westw20small {}.csv'.format(description))\n",
    "    \n",
    "    driver_df_mean = driver_df.groupby(['repo_id', x_axis, 'repo_name'],as_index=False).mean()\n",
    "        \n",
    "    # Total PRS Closed\n",
    "    fig, ax = plt.subplots()\n",
    "    # the size of A4 paper\n",
    "    fig.set_size_inches(16, 8)\n",
    "    plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean, sort=True, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close of {} Pull Requests, July 2017-January 2020\".format(description))  \n",
    "    if save_file:\n",
    "        fig.savefig('images/slow_20_mean {}.png'.format(description))\n",
    "    \n",
    "    # Copying array and deleting the outlier in the copy to re-visualize\n",
    "    def drop_n_largest(input_df, n, repo_name):\n",
    "        input_df_copy = input_df.copy()\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == 'amazon-freertos'].nlargest(n,'days_to_close').index\n",
    "        print(\"Indices to drop: {}\".format(indices_to_drop))\n",
    "        input_df_copy = input_df_copy.drop(index=indices_to_drop)\n",
    "        input_df_copy.loc[input_df['repo_name'] == repo_name]\n",
    "        return input_df_copy\n",
    "\n",
    "    if num_remove_outliers > 0 and drop_outliers_repo:\n",
    "        driver_df_mean_no_outliers = drop_n_largest(driver_df_mean, num_remove_outliers, drop_outliers_repo)\n",
    "    \n",
    "        # Total PRS Closed without outlier\n",
    "        fig, ax = plt.subplots()\n",
    "        # the size of A4 paper\n",
    "        fig.set_size_inches(16, 8)\n",
    "        plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean_no_outliers, sort=False, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close among {} Pull Requests Without Outlier, July 2017-January 2020\".format(description))\n",
    "        plotterlabels = ax.set_xticklabels(driver_df_mean_no_outliers[x_axis], rotation=90, fontsize=8)\n",
    "        if save_file:\n",
    "            fig.savefig('images/slow_20_mean_no_outlier {}.png'.format(description))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_mean_days_to_close(pr_closed, description='All Closed', save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "def vertical_grouped_bar(input_df, repo_id, group_by = 'merged_flag', x_axis='closed_year', y_axis='num_commits', description='All', title=\"{}Average Commit Counts Per Year for {} Pull Requests\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook() # let bokeh display plot in jupyter cell output\n",
    "\n",
    "        driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "\n",
    "        # Filter df by passed *repo_id* param\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "        driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "\n",
    "        # contains the closed years\n",
    "        x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "\n",
    "        # inner groups on x_axis they are merged and not_merged\n",
    "        groups = list(driver_df[group_by].unique())\n",
    "\n",
    "        # setup color pallete\n",
    "        try:\n",
    "            colors = mpl['Plasma'][len(groups)]\n",
    "        except:\n",
    "            colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "\n",
    "        merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Merged / Accepted'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "        not_merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Not Merged / Rejected'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "\n",
    "\n",
    "        # Setup data in format for grouped bar chart\n",
    "        data = {\n",
    "                'years'                   : x_groups,\n",
    "                'Merged / Accepted'       : merged_avg_values,\n",
    "                'Not Merged / Rejected'   : not_merged_avg_values,\n",
    "            }\n",
    "\n",
    "        x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "        counts = sum(zip(data['Merged / Accepted'], data['Not Merged / Rejected']), ())\n",
    "\n",
    "        source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id])\n",
    "        title=title.format(title_beginning, description)\n",
    "        \n",
    "        plot_width = len(x_groups) * 300\n",
    "        title_text_font_size = 16 \n",
    "        \n",
    "        if (len(title) * title_text_font_size / 2) > plot_width:\n",
    "            plot_width = int(len(title) * title_text_font_size / 2) + 40\n",
    "        \n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=plot_width, title=title, y_range=(0, max(merged_avg_values + not_merged_avg_values)*1.15), toolbar_location=None)\n",
    "\n",
    "        # Vertical bar glyph\n",
    "        p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "               fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "\n",
    "        # Data label \n",
    "        labels = LabelSet(x='x', y='counts', text='counts',# y_offset=-8, x_offset=34,\n",
    "                  text_font_size=\"12pt\", text_color=\"black\",\n",
    "                  source=source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "\n",
    "        p.y_range.start = 0\n",
    "        p.x_range.range_padding = 0.1\n",
    "        p.xaxis.major_label_orientation = 1\n",
    "        p.xgrid.grid_line_color = None\n",
    "\n",
    "        p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "        p.xaxis.axis_label = 'Year Closed'\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"{}px\".format(title_text_font_size)\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"15px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"15px\"\n",
    "        p.yaxis.major_label_text_font_size = \"15px\"\n",
    "        \n",
    "        plot = p\n",
    "\n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average commits per pull requests over an entire year, for merged and not merged pull requests.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "        \n",
    "        show(grid)\n",
    "\n",
    "\n",
    "        #show(p)\n",
    "\n",
    "        if save_files:\n",
    "            export_png(grid, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertical_grouped_bar(pr_all, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_grouped_bar_line_counts(input_df, repo_id, x_axis='closed_year', y_max1=600000, y_max2=1000, description=\"\", title =\"\", save_file=False):\n",
    "    output_notebook() # let bokeh display plot in jupyter cell output\n",
    "    \n",
    "    driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "    \n",
    "    # Filter df by passed *repo_id* param\n",
    "    driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "        \n",
    "    # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "    driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "    \n",
    "    # contains the closed years\n",
    "    x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "    \n",
    "    groups = ['Lines Added', 'Lines Removed', 'Files Changed']\n",
    "    \n",
    "    # setup color pallete\n",
    "    colors = mpl['Plasma'][3]\n",
    "    \n",
    "    display(pr_all[pr_all['lines_added'].notna()])#.groupby([x_axis],as_index=False).mean())\n",
    "        \n",
    "    files_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['file_count'])\n",
    "    added_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_added'])\n",
    "    removed_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_removed'])\n",
    "    display(driver_df.groupby([x_axis],as_index=False).mean())\n",
    "    print(files_avg_values)\n",
    "    print(added_avg_values)\n",
    "    print(removed_avg_values)\n",
    "    \n",
    "        \n",
    "    # Setup data in format for grouped bar chart\n",
    "    data = {\n",
    "            'years' : x_groups,\n",
    "            'Lines Added'   : added_avg_values,\n",
    "            'Lines Removed' : removed_avg_values,\n",
    "            'Files Changed' : files_avg_values\n",
    "        }\n",
    "\n",
    "    x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "    line_counts = sum(zip(data['Lines Added'], data['Lines Removed'], [0]*len(x_groups)), ())\n",
    "    file_counts = sum(zip([0]*len(x_groups),[0]*len(x_groups),data['Files Changed']), ())\n",
    "    print(line_counts)\n",
    "    print(file_counts)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(x=x, line_counts=line_counts, file_counts=file_counts))\n",
    "\n",
    "    if y_max1:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description), y_range=(0,y_max1))\n",
    "    else:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description))\n",
    "                \n",
    "    # Setting the second y axis range name and range\n",
    "    p.extra_y_ranges = {\"file_counts\": Range1d(start=0, end=y_max2)}\n",
    "    \n",
    "    # Adding the second axis to the plot.  \n",
    "    p.add_layout(LinearAxis(y_range_name=\"file_counts\"), 'right')\n",
    "    \n",
    "    # Data label for line counts\n",
    "    labels = LabelSet(x='x', y='line_counts', text='line_counts',y_offset=8,# x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # Vertical bar glyph for line counts\n",
    "    p.vbar(x='x', top='line_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "    \n",
    "    # Data label for file counts\n",
    "    labels = LabelSet(x='x', y='file_counts', text='file_counts', y_offset=0, #x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center', y_range_name=\"file_counts\")\n",
    "    p.add_layout(labels)\n",
    "    \n",
    "    # Vertical bar glyph for file counts\n",
    "    p.vbar(x='x', top='file_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2), y_range_name=\"file_counts\")\n",
    "\n",
    "    p.left[0].formatter.use_scientific = False\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    \n",
    "    p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "    p.xaxis.axis_label = 'Year Closed'\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "    \n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"16px\"\n",
    "    \n",
    "    p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.yaxis.major_label_text_font_size = \"16px\"\n",
    "    \n",
    "    show(p)\n",
    "    \n",
    "    if save_files:\n",
    "        export_png(p, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THIS VIZ IS NOT READY YET , BUT UNCOMMENT LINE BELOW IF YOU WANT TO SEE\"\"\"\n",
    "# vertical_grouped_bar_line_counts(pr_all, description='All', title=\"Average Size Metrics Per Year for {} Merged Pull Requests in Master\", save_file=False, y_max1=580000, y_max2=1100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_stacked_bar(input_df, repo_id, group_by='merged_flag', x_axis='comment_count', description=\"All Closed\", y_axis='closed_year', title=\"Mean Comments for {} Pull Requests\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        output_notebook()\n",
    "\n",
    "        try:\n",
    "            y_groups = sorted(list(driver_df[y_axis].unique()))\n",
    "        except:\n",
    "            y_groups = [repo_id]\n",
    "\n",
    "        groups = driver_df[group_by].unique()\n",
    "        try:\n",
    "            colors = mpl['Plasma'][len(groups)]\n",
    "        except:\n",
    "            colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "\n",
    "        len_not_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Not Merged / Rejected'])\n",
    "        len_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Merged / Accepted'])\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id]) \n",
    "        plot_width = 650\n",
    "        p = figure(y_range=y_groups, plot_height=450, plot_width=plot_width, # y_range=y_groups,#(pr_all[y_axis].min(),pr_all[y_axis].max()) #y_axis_type=\"datetime\",\n",
    "                   title='{} {}'.format(title_beginning, title.format(description)), toolbar_location=None)\n",
    "\n",
    "        possible_maximums= []\n",
    "        for y_value in y_groups:\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            if len(y_merged_data) > 0:\n",
    "                y_merged_data[x_axis + '_mean'] = round(y_merged_data[x_axis].mean()[-1],1)\n",
    "            else:\n",
    "                y_merged_data[x_axis + '_mean'] = 0.00\n",
    "\n",
    "            if len(y_not_merged_data) > 0:\n",
    "                y_not_merged_data[x_axis + '_mean'] = round(y_not_merged_data[x_axis].mean()[-1],1)\n",
    "            else:\n",
    "                y_not_merged_data[x_axis + '_mean'] = 0\n",
    "\n",
    "            not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "            merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "            possible_maximums.append(max(y_not_merged_data[x_axis + '_mean']))\n",
    "            possible_maximums.append(max(y_merged_data[x_axis + '_mean']))\n",
    "\n",
    "            # mean comment count for merged\n",
    "            merged_comment_count_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=x_axis + '_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                         source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                      text_font_size=\"12pt\", text_color=\"black\",\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "            # mean comment count For nonmerged\n",
    "            not_merged_comment_count_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=x_axis + '_mean', \n",
    "                                         height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                      text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "    #         p.y_range.range_padding = 0.1\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.legend.location = \"bottom_right\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        p.xaxis.axis_label = 'Average Comments / Pull Request'\n",
    "        p.yaxis.axis_label = 'Repository' if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "\n",
    "        legend = Legend(\n",
    "                items=[\n",
    "                    (\"Merged Pull Request Mean Comment Count\", [merged_comment_count_glyph]),\n",
    "                    (\"Rejected Pull Request Mean Comment Count\", [not_merged_comment_count_glyph])\n",
    "                ],\n",
    "\n",
    "                location='center', \n",
    "                orientation='vertical',\n",
    "                border_line_color=\"black\"\n",
    "            )\n",
    "        p.add_layout(legend, \"below\")\n",
    "\n",
    "        p.title.text_font_size = \"16px\"\n",
    "        p.title.align = \"center\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.x_range = Range1d(0, max(possible_maximums)*1.15)\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average number of comments per merged or not merged pull request.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        #show(p, plot_width=1200, plot_height=300*len(y_groups) + 300)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_name else repo_name\n",
    "            export_png(grid, filename=\"./images/h_stacked_bar_mean_comments_merged_status/mean_comments_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal_stacked_bar(pr_closed, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_ratio_vertical_grouped_bar(data_dict, repo_id, x_axis='closed_year', description=\"All Closed\", title=\"Count of {} Pull Requests by Merged Status\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "        \n",
    "        output_notebook()\n",
    "\n",
    "        colors = mpl['Plasma'][6]\n",
    "\n",
    "        #if repo_name == 'mbed-os':\n",
    "            #colors = colors[::-1]\n",
    "\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "            x_groups = sorted(list(input_df[x_axis].astype(str).unique()))\n",
    "            break\n",
    "\n",
    "        plot_width = 315 * len(x_groups)\n",
    "        title_beginning = repo_dict[repo_id] \n",
    "        p = figure(x_range=x_groups, plot_height=350, plot_width=plot_width,  \n",
    "                   title='{}: {}'.format(title_beginning, title.format(description)), toolbar_location=None)\n",
    "\n",
    "        dodge_amount = 0.12\n",
    "        color_index = 0\n",
    "        x_offset = 50\n",
    "\n",
    "        all_totals = []\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "            driver_df = input_df.copy()\n",
    "\n",
    "            driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "\n",
    "            groups = sorted(list(driver_df['merged_flag'].unique()))\n",
    "\n",
    "            driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "            len_merged = []\n",
    "            zeros = []\n",
    "            len_not_merged = []\n",
    "            totals = []\n",
    "\n",
    "            for x_group in x_groups:\n",
    "\n",
    "                len_merged_entry = len(driver_df.loc[(driver_df['merged_flag'] == 'Merged / Accepted') & (driver_df[x_axis] == x_group)])\n",
    "                totals += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)]) + len_merged_entry]\n",
    "                len_not_merged += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)])]\n",
    "                len_merged += [len_merged_entry]\n",
    "                zeros.append(0)\n",
    "\n",
    "            data = {'X': x_groups}\n",
    "            for group in groups:\n",
    "                data[group] = []\n",
    "                for x_group in x_groups:\n",
    "                    data[group] += [len(driver_df.loc[(driver_df['merged_flag'] == group) & (driver_df[x_axis] == x_group)])]\n",
    "\n",
    "            data['len_merged'] = len_merged\n",
    "            data['len_not_merged'] = len_not_merged\n",
    "            data['totals'] = totals\n",
    "            data['zeros'] = zeros\n",
    "\n",
    "            if data_desc == \"All\":\n",
    "                all_totals = totals\n",
    "\n",
    "            source = ColumnDataSource(data)\n",
    "\n",
    "            stacked_bar = p.vbar_stack(groups, x=dodge('X', dodge_amount, range=p.x_range), width=0.2, source=source, color=colors[1:3], legend_label=[f\"{data_desc} \" + \"%s\" % x for x in groups])\n",
    "            # Data label for merged\n",
    "\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='zeros', text='len_merged', y_offset=2, x_offset=x_offset,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1:3][0],\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            if min(data['totals']) < 400:\n",
    "                y_offset = 15\n",
    "            else:\n",
    "                y_offset = 0\n",
    "            # Data label for not merged\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='len_not_merged', y_offset=y_offset, x_offset=x_offset,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1:3][1],\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            # Data label for total\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='totals', y_offset=0, x_offset=0,\n",
    "                      text_font_size=\"12pt\", text_color='black',\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            dodge_amount *= -1\n",
    "            colors = colors[::-1]\n",
    "            x_offset *= -1\n",
    "\n",
    "        p.y_range = Range1d(0,  max(all_totals)*1.4)\n",
    "\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.legend.location = \"top_center\"\n",
    "        p.legend.orientation=\"horizontal\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        p.yaxis.axis_label = 'Count of Pull Requests'\n",
    "        p.xaxis.axis_label = 'Repository' if x_axis == 'repo_name' else 'Year Closed' if x_axis == 'closed_year' else ''\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.outline_line_color = None\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the number of closed pull requests per year in four different categories. These four categories are All Merged, All Not Merged, Slowest 20% Merged, and Slowest 20% Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "        \n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_id else repo_id\n",
    "            export_png(grid, filename=\"./images/v_stacked_bar_merged_status_count/stacked_bar_merged_status_count__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#erged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_response_times(input_df, repo_id, time_unit='days', x_max=95,  y_axis='closed_year', description=\"All Closed\", legend_position=(410, 10)):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "\n",
    "        output_notebook() # let bokeh show plot in jupyter cell output\n",
    "\n",
    "        driver_df = input_df.copy()[['repo_name', 'repo_id', 'merged_flag', y_axis, time_unit + '_to_first_response', time_unit + '_to_last_response', \n",
    "                                     time_unit + '_to_close']] # deep copy input data so we do not alter the external dataframe\n",
    "\n",
    "        # filter by repo_id param\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id])\n",
    "        plot_width = 950\n",
    "        p = figure(toolbar_location=None, y_range=sorted(driver_df[y_axis].unique()), plot_width=plot_width, \n",
    "                   plot_height=450,#75*len(driver_df[y_axis].unique()),\n",
    "                   title=\"{}Mean Response Times for Pull Requests {}\".format(title_beginning, description))\n",
    "\n",
    "        first_response_glyphs = []\n",
    "        last_response_glyphs = []\n",
    "        merged_days_to_close_glyphs = []\n",
    "        not_merged_days_to_close_glyphs = []\n",
    "\n",
    "        possible_maximums = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "\n",
    "            y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "            possible_maximums.append(max(y_merged_data[time_unit + '_to_close_mean']))\n",
    "            possible_maximums.append(max(y_not_merged_data[time_unit + '_to_close_mean']))\n",
    "            \n",
    "            maximum = max(possible_maximums)*1.15\n",
    "            ideal_difference = maximum*0.064\n",
    "            \n",
    "        for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "\n",
    "            y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "            not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "            merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "            # mean PR length for merged\n",
    "            merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                         source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "            merged_days_to_close_glyphs.append(merged_days_to_close_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=34, #34\n",
    "                      text_font_size=\"12pt\", text_color=\"black\",\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "\n",
    "            # mean PR length For nonmerged\n",
    "            not_merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', \n",
    "                                         height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "            not_merged_days_to_close_glyphs.append(not_merged_days_to_close_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=44,\n",
    "                      text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            \n",
    "            #if the difference between two values is less than 6.4 percent move the second one to the right 30 pixels\n",
    "            if (max(y_merged_data[time_unit + '_to_last_response_mean']) - max(y_merged_data[time_unit + '_to_first_response_mean'])) < ideal_difference:\n",
    "                merged_x_offset = 30\n",
    "            else:\n",
    "                merged_x_offset = 0\n",
    "                \n",
    "            #if the difference between two values is less than 6.4 percent move the second one to the right 30 pixels\n",
    "            if (max(y_not_merged_data[time_unit + '_to_last_response_mean']) - max(y_not_merged_data[time_unit + '_to_first_response_mean'])) < ideal_difference:\n",
    "                not_merged_x_offset = 30\n",
    "            else:\n",
    "                not_merged_x_offset = 0\n",
    "                \n",
    "            #if there is only one bar set the y_offsets so the labels will not overlap the bars\n",
    "            if len(driver_df[y_axis].unique()) == 1:\n",
    "                merged_y_offset = -65\n",
    "                not_merged_y_offset = 45\n",
    "            else:\n",
    "                merged_y_offset = -45\n",
    "                not_merged_y_offset = 25\n",
    "            \n",
    "            \n",
    "            # mean time to first response\n",
    "            glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "            first_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "            first_response_glyphs.append(first_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean',x_offset = 0, y_offset=merged_y_offset,#-60,\n",
    "                      text_font_size=\"12pt\", text_color=colors[0],\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            #for nonmerged\n",
    "            glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "            first_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "            first_response_glyphs.append(first_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean',x_offset = 0, y_offset=not_merged_y_offset,#40,\n",
    "                              text_font_size=\"12pt\", text_color=colors[0],\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "\n",
    "            # mean time to last response\n",
    "            glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "            last_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "            last_response_glyphs.append(last_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', x_offset=merged_x_offset, y_offset=merged_y_offset,#-60,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1],\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "            \n",
    "\n",
    "            #for nonmerged\n",
    "            glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "            last_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "            last_response_glyphs.append(last_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', x_offset = not_merged_x_offset, y_offset=not_merged_y_offset,#40,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1],\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label = \"Days to Close\"\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "        \n",
    "        #adjust the starting point and ending point based on the maximum of maximum of the graph\n",
    "        p.x_range = Range1d(maximum/30 * -1, maximum*1.15)\n",
    "\n",
    "        p.yaxis.axis_label = \"Repository\" if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.y_range.range_padding = 0.15\n",
    "\n",
    "        p.outline_line_color = None\n",
    "        p.toolbar.logo = None\n",
    "        p.toolbar_location = None\n",
    "\n",
    "        def add_legend(location, orientation, side):\n",
    "            legend = Legend(\n",
    "                items=[\n",
    "                    (\"Mean Days to First Response\", first_response_glyphs),\n",
    "                    (\"Mean Days to Last Response\", last_response_glyphs),\n",
    "                    (\"Merged Mean Days to Close\", merged_days_to_close_glyphs),\n",
    "                    (\"Not Merged Mean Days to Close\", not_merged_days_to_close_glyphs)\n",
    "                ],\n",
    "\n",
    "                location=location, \n",
    "                orientation=orientation,\n",
    "                border_line_color=\"black\"\n",
    "        #         title='Example Title'\n",
    "            )\n",
    "            p.add_layout(legend, side)\n",
    "\n",
    "    #     add_legend((150, 50), \"horizontal\", \"center\")\n",
    "        add_legend(legend_position, \"vertical\", \"right\")\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height = 200, margin = (0, 0, 0, 0))\n",
    "        caption = \"Caption Here\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            export_png(grid, filename=\"./images/hbar_response_times/mean_response_times__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_closed['repo_name'].unique():\n",
    "#visualize_mean_response_times(pr_closed, repo_id=repo_list, legend_position='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_time_between_responses(data_dict, repo_id, time_unit='Days', x_axis='closed_yearmonth', description=\"All Closed\", line_group='merged_flag', y_axis='average_days_between_responses', num_outliers_repo_map={}):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook()\n",
    "        plot_width = 950\n",
    "        p1 = figure(x_axis_type=\"datetime\", title=\"{}: Mean {} Between Comments by Month Closed for {} Pull Requests\".format(repo_dict[repo_id], time_unit, description), plot_width=plot_width, x_range=(pr_all[x_axis].min(),pr_all[x_axis].max()), plot_height=500, toolbar_location=None)\n",
    "        colors = Category20[10][6:]\n",
    "        color_index = 0\n",
    "\n",
    "        glyphs = []\n",
    "\n",
    "        possible_maximums = []\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "\n",
    "            driver_df = input_df.copy()\n",
    "\n",
    "            driver_df = remove_outliers(driver_df, y_axis, num_outliers_repo_map)\n",
    "\n",
    "            driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "            index = 0\n",
    "\n",
    "            driver_df_mean = driver_df.groupby(['repo_id', line_group, x_axis],as_index=False).mean()\n",
    "\n",
    "            title_ending = ''\n",
    "            if repo_id:\n",
    "                title_ending += ' for Repo: {}'.format(repo_id)\n",
    "\n",
    "            for group_num, line_group_value in enumerate(driver_df[line_group].unique(), color_index):\n",
    "                glyphs.append(p1.line(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][x_axis], driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis], color=colors[group_num], line_width = 3))\n",
    "                color_index += 1\n",
    "                possible_maximums.append(max(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis].dropna()))\n",
    "        for repo, num_outliers in num_outliers_repo_map.items():\n",
    "            if repo_name == repo:\n",
    "                p1.add_layout(Title(text=\"** {} outliers for {} were removed\".format(num_outliers, repo), align=\"center\"), \"below\")\n",
    "\n",
    "        p1.grid.grid_line_alpha = 0.3\n",
    "        p1.xaxis.axis_label = 'Month Closed'\n",
    "        p1.xaxis.ticker.desired_num_ticks = 15\n",
    "        p1.yaxis.axis_label = 'Mean {} Between Responses'.format(time_unit)\n",
    "        p1.legend.location = \"top_left\"\n",
    "\n",
    "        legend = Legend(\n",
    "            items=[\n",
    "                (\"All Not Merged / Rejected\", [glyphs[0]]),\n",
    "                (\"All Merged / Accepted\", [glyphs[1]]),\n",
    "                (\"Slowest 20% Not Merged / Rejected\", [glyphs[2]]),\n",
    "                (\"Slowest 20% Merged / Accepted\", [glyphs[3]])\n",
    "            ],\n",
    "\n",
    "            location='center_right', \n",
    "            orientation='vertical',\n",
    "            border_line_color=\"black\"\n",
    "        )\n",
    "\n",
    "        p1.add_layout(legend, 'right')\n",
    "\n",
    "        p1.title.text_font_size = \"16px\"\n",
    "\n",
    "        p1.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p1.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p1.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p1.yaxis.major_label_text_font_size = \"16px\"\n",
    "        p1.xaxis.major_label_orientation = 45.0\n",
    "        \n",
    "        p1.y_range = Range1d(0,  max(possible_maximums)*1.15)\n",
    "        \n",
    "        plot = p1\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average number of days between comments for all closed pull requests per month in four categories. These four categories are All Merged, All Not Merged, Slowest 20% Merged, and Slowest 20% Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_name else repo_name\n",
    "            export_png(grid, filename=\"./images/line_mean_time_between_comments/line_mean_time_between_comments__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_to_first_comment(input_df, repo_id, x_axis='pr_closed_at', y_axis='days_to_first_response', description='All', num_outliers_repo_map={}, group_by='merged_flag', same_scales=True, columns=2, legend_position='top_right', remove_outliers = 0):\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook()\n",
    "\n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        group_by_groups = sorted(driver_df[group_by].unique())\n",
    "\n",
    "        seconds = ((driver_df[x_axis].max() + datetime.timedelta(days=25))- (driver_df[x_axis].min() - datetime.timedelta(days=30))).total_seconds()\n",
    "        quarter_years = seconds / 10506240\n",
    "        quarter_years = round(quarter_years)\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id]) \n",
    "        plot_width = 180 * 5\n",
    "        p = figure(x_range=(driver_df[x_axis].min() - datetime.timedelta(days=30), driver_df[x_axis].max() + datetime.timedelta(days=25)), \n",
    "                  #(driver_df[y_axis].min(), driver_df[y_axis].max()), \n",
    "                   toolbar_location=None,\n",
    "                   title='{}Days to First Response for {} Closed Pull Requests'.format(title_beginning, description), plot_width=plot_width, \n",
    "                   plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "        for index, group_by_group in enumerate(group_by_groups):\n",
    "            p.scatter(x_axis, y_axis, color=colors[index], marker=\"square\", source=driver_df.loc[driver_df[group_by] == group_by_group], legend_label=group_by_group)\n",
    "\n",
    "            if group_by_group == \"Merged / Accepted\":\n",
    "                merged_values = driver_df.loc[driver_df[group_by] == group_by_group][y_axis].dropna().values.tolist()\n",
    "            else:\n",
    "                not_merged_values = driver_df.loc[driver_df[group_by] == group_by_group][y_axis].dropna().values.tolist()\n",
    "\n",
    "        values = not_merged_values + merged_values\n",
    "        #values.fillna(0)\n",
    "\n",
    "        for value in range(0, remove_outliers):\n",
    "            values.remove(max(values))\n",
    " \n",
    "        #determine y_max by finding the max of the values and scaling it up a small amoutn\n",
    "        y_max = max(values)*1.0111\n",
    "        outliers = driver_df.loc[driver_df[y_axis] > y_max]\n",
    "        if len(outliers) > 0:\n",
    "            if repo_id:\n",
    "                p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) for {} were removed **\".format(y_max, len(outliers), repo_name), align=\"center\"), \"below\")\n",
    "            else:\n",
    "                p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) were removed **\".format(y_max, len(outliers)), align=\"center\"), \"below\")\n",
    "\n",
    "        p.xaxis.axis_label = 'Date Closed' if x_axis == 'pr_closed_at' else 'Date Created' if x_axis == 'pr_created_at' else 'Date'\n",
    "        p.yaxis.axis_label = 'Days to First Response'\n",
    "        p.legend.location = legend_position\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.y_range = Range1d(0, y_max)\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the days to first reponse for individual pull requests, either Merged or Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = repo_ids\n",
    "            export_png(grid, filename=\"./images/first_comment_times/scatter_first_comment_times__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#visualize_time_to_first_comment(pr_closed, repo_id= repo_list, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_RGB(hex):\n",
    "    ''' \"#FFFFFF\" -> [255,255,255] '''\n",
    "    # Pass 16 to the integer function for change of base\n",
    "    return [int(hex[i:i+2], 16) for i in range(1,6,2)]\n",
    "\n",
    "def color_dict(gradient):\n",
    "    ''' Takes in a list of RGB sub-lists and returns dictionary of\n",
    "    colors in RGB and hex form for use in a graphing function\n",
    "    defined later on '''\n",
    "    return {\"hex\":[RGB_to_hex(RGB) for RGB in gradient],\n",
    "      \"r\":[RGB[0] for RGB in gradient],\n",
    "      \"g\":[RGB[1] for RGB in gradient],\n",
    "      \"b\":[RGB[2] for RGB in gradient]}\n",
    "\n",
    "def RGB_to_hex(RGB):\n",
    "    ''' [255,255,255] -> \"#FFFFFF\" '''\n",
    "    # Components need to be integers for hex to make sense\n",
    "    RGB = [int(x) for x in RGB]\n",
    "    return \"#\"+\"\".join([\"0{0:x}\".format(v) if v < 16 else\n",
    "            \"{0:x}\".format(v) for v in RGB])\n",
    "\n",
    "def linear_gradient(start_hex, finish_hex=\"#FFFFFF\", n=10):\n",
    "    ''' returns a gradient list of (n) colors between\n",
    "    two hex colors. start_hex and finish_hex\n",
    "    should be the full six-digit color string,\n",
    "    inlcuding the number sign (\"#FFFFFF\") '''\n",
    "    # Starting and ending colors in RGB form\n",
    "    s = hex_to_RGB(start_hex)\n",
    "    f = hex_to_RGB(finish_hex)\n",
    "    # Initilize a list of the output colors with the starting color\n",
    "    RGB_list = [s]\n",
    "    # Calcuate a color at each evenly spaced value of t from 1 to n\n",
    "    for t in range(1, n):\n",
    "        # Interpolate RGB vector for color at the current value of t\n",
    "        curr_vector = [\n",
    "          int(s[j] + (float(t)/(n-1))*(f[j]-s[j]))\n",
    "          for j in range(3)\n",
    "        ]\n",
    "        # Add it to our list of output colors\n",
    "        RGB_list.append(curr_vector)\n",
    "\n",
    "    return color_dict(RGB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, LogTicker, Label\n",
    "from bokeh.transform import transform\n",
    "        \n",
    "def events_types_heat_map(input_df, repo_id, include_comments=True, x_axis='closed_year', facet=\"merged_flag\",columns=2, x_max=1100, same_scales=True, y_axis='repo_name', description=\"All Closed\", title=\"Average Pull Request Event Types for {} Pull Requests\"):\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        colors = linear_gradient('#f5f5dc', '#fff44f', 150)['hex']\n",
    "\n",
    "        driver_df = input_df.copy()\n",
    "        driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        if facet == 'closed_year' or y_axis == 'closed_year':\n",
    "            driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "\n",
    "        optional_comments = ['comment_count'] if include_comments else []\n",
    "        driver_df = driver_df[['repo_id', 'repo_name',x_axis, 'assigned_count',\n",
    "              'review_requested_count',\n",
    "              'labeled_count',\n",
    "              'subscribed_count',\n",
    "              'mentioned_count',\n",
    "              'referenced_count',\n",
    "              'closed_count',\n",
    "              'head_ref_force_pushed_count',\n",
    "              'merged_count',\n",
    "              'milestoned_count',\n",
    "              'unlabeled_count',\n",
    "              'head_ref_deleted_count', facet ] + optional_comments]\n",
    "        y_groups = [\n",
    "              'review_requested_count',\n",
    "              'labeled_count',\n",
    "              'subscribed_count',\n",
    "              'referenced_count',\n",
    "              'closed_count',\n",
    "    #           'milestoned_count',\n",
    "              ] + optional_comments\n",
    "        output_notebook()\n",
    "        optional_group_comments = ['comment'] if include_comments else []\n",
    "    #     y_groups = ['subscribed', 'mentioned', 'labeled', 'review_requested', 'head_ref_force_pushed', 'referenced', 'closed', 'merged', 'unlabeled', 'head_ref_deleted', 'milestoned', 'assigned'] + optional_group_comments\n",
    "\n",
    "        x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "\n",
    "        grid_array = []\n",
    "        grid_row = []  \n",
    "\n",
    "        for index, facet_group in enumerate(sorted(driver_df[facet].unique())):\n",
    "\n",
    "            facet_data = driver_df.loc[driver_df[facet] == facet_group]\n",
    "    #         display(facet_data.sort_values('merged_count', ascending=False).head(50))\n",
    "            driver_df_mean = facet_data.groupby(['repo_id', 'repo_name', x_axis], as_index=False).mean().round(1)\n",
    "    #         data = {'Y' : y_groups}\n",
    "    #         for group in y_groups:\n",
    "    #             data[group] = driver_df_mean[group].tolist()\n",
    "            plot_width = 700\n",
    "            p = figure(y_range=y_groups, plot_height=500, plot_width=plot_width, x_range=x_groups, \n",
    "                       title='{}'.format(format(facet_group)))\n",
    "\n",
    "            for y_group in y_groups:\n",
    "                driver_df_mean['field'] = y_group\n",
    "                source = ColumnDataSource(driver_df_mean)\n",
    "                mapper = LinearColorMapper(palette=colors, low=driver_df_mean[y_group].min(), high=driver_df_mean[y_group].max())\n",
    "\n",
    "                p.rect(y='field', x=x_axis, width=1, height=1, source=source,\n",
    "                       line_color=None, fill_color=transform(y_group, mapper))\n",
    "                # Data label \n",
    "                labels = LabelSet(x=x_axis, y='field', text=y_group, y_offset=-8,\n",
    "                          text_font_size=\"12pt\", text_color='black',\n",
    "                          source=source, text_align='center')\n",
    "                p.add_layout(labels)\n",
    "\n",
    "                color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                                     ticker=BasicTicker(desired_num_ticks=9),\n",
    "                                     formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "    #         p.add_layout(color_bar, 'right')\n",
    "\n",
    "\n",
    "            p.y_range.range_padding = 0.1\n",
    "            p.ygrid.grid_line_color = None\n",
    "\n",
    "            p.legend.location = \"bottom_right\"\n",
    "            p.axis.minor_tick_line_color = None\n",
    "            p.outline_line_color = None\n",
    "\n",
    "            p.xaxis.axis_label = 'Year Closed'\n",
    "            p.yaxis.axis_label = 'Event Type'\n",
    "\n",
    "            p.title.align = \"center\"\n",
    "            p.title.text_font_size = \"15px\"\n",
    "\n",
    "            p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "            p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "            p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "            p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "            grid_row.append(p)\n",
    "            if index % columns == columns - 1:\n",
    "                grid_array.append(grid_row)\n",
    "                grid_row = []\n",
    "        grid = gridplot(grid_array)\n",
    "        \n",
    "        #add title, the title changes its x value based on the number of x_groups so that it stays centered\n",
    "        label=Label(x=-len(x_groups), y=6.9, text='{}: Average Pull Request Event Types for {} Closed Pull Requests'.format(repo_dict[repo_id], description), render_mode='css', text_font_size = '17px', text_font_style= 'bold')\n",
    "        p.add_layout(label)\n",
    "\n",
    "        show(grid, plot_width=1200, plot_height=1200)\n",
    "        if save_files:\n",
    "            comments_included = 'comments_included' if include_comments else 'comments_not_included'\n",
    "            repo_extension = 'All' if not repo_id else repo_id\n",
    "            export_png(grid, filename=\"./images/h_stacked_bar_mean_event_types/mean_event_types__facet_{}__{}_PRs__yaxis_{}__{}__repo_{}.png\".format(facet, description, y_axis, comments_included, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#events_types_heat_map(pr_closed, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_green_gradient = linear_gradient('#0080FF', '#DC143C', 150)['hex']\n",
    "    #32CD32\n",
    "def heat_map(input_df, repo_id, x_axis='repo_name', group_by='merged_flag', y_axis='closed_yearmonth', same_scales=True, description=\"All Closed\", heat_field='pr_duration_days', columns=2, remove_outliers = 0):\n",
    "\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    driver_df = input_df.copy()[['repo_id', y_axis, group_by, x_axis, heat_field]]\n",
    "    \n",
    "    print(driver_df)\n",
    "\n",
    "    if display_grouping == 'repo':\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "    driver_df[y_axis] = driver_df[y_axis].astype(str)\n",
    "\n",
    "    # add new group by + xaxis column \n",
    "    driver_df['grouped_x'] = driver_df[x_axis] + ' - ' + driver_df[group_by]\n",
    "\n",
    "    driver_df_mean = driver_df.groupby(['grouped_x', y_axis], as_index=False).mean()\n",
    "\n",
    "    colors = red_green_gradient\n",
    "    y_groups = driver_df_mean[y_axis].unique()\n",
    "    x_groups = sorted(driver_df[x_axis].unique())\n",
    "    grouped_x_groups = sorted(driver_df_mean['grouped_x'].unique())\n",
    "\n",
    "    values = driver_df_mean['pr_duration_days'].values.tolist()\n",
    "    for i in range(0, remove_outliers):\n",
    "        values.remove(max(values))\n",
    "\n",
    "    heat_max = max(values)* 1.02\n",
    "\n",
    "    mapper = LinearColorMapper(palette=colors, low=driver_df_mean[heat_field].min(), high=heat_max)#driver_df_mean[heat_field].max())\n",
    "\n",
    "    source = ColumnDataSource(driver_df_mean)\n",
    "    title_beginning = repo_dict[repo_id] + ':' if not type(repo_id) == type(repo_list) else ''\n",
    "    plot_width = 1100\n",
    "    p = figure(plot_width=plot_width, plot_height=300, title=\"{} Mean Duration (Days) {} Pull Requests\".format(title_beginning,description),\n",
    "               y_range=grouped_x_groups[::-1], x_range=y_groups,\n",
    "               toolbar_location=None, tools=\"\")#, x_axis_location=\"above\")\n",
    "\n",
    "    for x_group in x_groups:\n",
    "        outliers = driver_df_mean.loc[(driver_df_mean[heat_field] > heat_max) & (driver_df_mean['grouped_x'].str.contains(x_group))]\n",
    "\n",
    "        if len(outliers) > 0:\n",
    "            p.add_layout(Title(text=\"** Outliers capped at {} days: {} outlier(s) for {} were capped at {} **\".format(heat_max, len(outliers), x_group, heat_max), align=\"center\"), \"below\")\n",
    "\n",
    "    p.rect(x=y_axis, y='grouped_x', width=1, height=1, source=source,\n",
    "           line_color=None, fill_color=transform(heat_field, mapper))\n",
    "\n",
    "    color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=BasicTicker(desired_num_ticks=9),\n",
    "                         formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "\n",
    "    p.add_layout(color_bar, 'right')\n",
    "\n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"11pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.xaxis.axis_label = 'Month Closed' if y_axis[0:6] == 'closed' else 'Date Created' if y_axis[0:7] == 'created' else 'Repository' if y_axis == 'repo_name' else ''\n",
    "#     p.yaxis.axis_label = 'Merged Status'\n",
    "\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"14px\"\n",
    "\n",
    "    p.yaxis.major_label_text_font_size = \"15px\"\n",
    "\n",
    "    plot = p\n",
    "\n",
    "    p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "    caption = \"Caption Here\"\n",
    "    p.add_layout(Label(\n",
    "    x = 0, # Change to shift caption left or right\n",
    "    y = 160, \n",
    "    x_units = 'screen',\n",
    "    y_units = 'screen',\n",
    "    text='{}'.format(caption),\n",
    "    text_font = 'times', # Use same font as paper\n",
    "    text_font_size = '15pt',\n",
    "    render_mode='css'\n",
    "    ))\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    caption_plot = p\n",
    "\n",
    "    grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "    show(grid)\n",
    "\n",
    "    if save_files:\n",
    "        repo_extension = 'All' if not repo_id else repo_id\n",
    "        export_png(grid, filename=\"./images/heat_map_pr_duration_merged_status/heat_map_duration_by_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat_map(pr_closed, repo_id=25502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"5e5c33a5-437f-49b8-b4fe-37134d93ecee\" data-root-id=\"1095\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"d99bacf2-088c-4b6c-b0ed-5998036afdec\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1094\"},{\"id\":\"1092\"}]},\"id\":\"1095\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"\"},\"id\":\"1084\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis\":{\"id\":\"1014\"},\"grid_line_color\":null,\"ticker\":null},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"overlay\":{\"id\":\"1066\"}},\"id\":\"1062\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1027\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_color\":{\"field\":\"x\",\"transform\":{\"id\":\"1035\"}},\"line_color\":{\"value\":\"white\"},\"top\":{\"field\":\"counts\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"x\"}},\"id\":\"1037\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1060\"},{\"id\":\"1061\"},{\"id\":\"1062\"},{\"id\":\"1063\"},{\"id\":\"1064\"},{\"id\":\"1065\"}]},\"id\":\"1067\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"LinearScale\"},{\"attributes\":{\"data\":{\"counts\":[1.9,2.0],\"x\":[[\"2020\",\"Merged / Accepted\"],[\"2020\",\"Not Merged / Rejected\"]]},\"selected\":{\"id\":\"1081\"},\"selection_policy\":{\"id\":\"1080\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1037\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1038\"},\"selection_glyph\":null,\"view\":{\"id\":\"1040\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"toolbar\":{\"id\":\"1093\"},\"toolbar_location\":\"above\"},\"id\":\"1094\",\"type\":\"ToolbarBox\"},{\"attributes\":{\"below\":[{\"id\":\"1014\"}],\"center\":[{\"id\":\"1016\"},{\"id\":\"1020\"},{\"id\":\"1041\"}],\"left\":[{\"id\":\"1017\"}],\"plot_height\":450,\"plot_width\":568,\"renderers\":[{\"id\":\"1039\"}],\"title\":{\"id\":\"1005\"},\"toolbar\":{\"id\":\"1028\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"1003\"},\"x_scale\":{\"id\":\"1010\"},\"y_range\":{\"id\":\"1008\"},\"y_scale\":{\"id\":\"1012\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis\":{\"id\":\"1056\"},\"dimension\":1,\"ticker\":null},\"id\":\"1059\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1015\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"1061\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1040\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"DataRange1d\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"LinearScale\"},{\"attributes\":{\"formatter\":{\"id\":\"1086\"},\"ticker\":{\"id\":\"1057\"}},\"id\":\"1056\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1052\"},\"ticker\":null},\"id\":\"1055\",\"type\":\"Grid\"},{\"attributes\":{\"factors\":[[\"2020\",\"Merged / Accepted\"],[\"2020\",\"Not Merged / Rejected\"]],\"range_padding\":0.1},\"id\":\"1003\",\"type\":\"FactorRange\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"x\",\"transform\":{\"id\":\"1035\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"white\"},\"top\":{\"field\":\"counts\"},\"width\":{\"value\":0.9},\"x\":{\"field\":\"x\"}},\"id\":\"1038\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1057\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1063\",\"type\":\"SaveTool\"},{\"attributes\":{\"render_mode\":\"css\",\"text\":\"This graph shows the average commits per pull requests over an entire year, for merged and not merged pull requests.\",\"text_font\":\"times\",\"text_font_size\":\"15pt\",\"x\":0,\"x_units\":\"screen\",\"y\":160,\"y_units\":\"screen\"},\"id\":\"1074\",\"type\":\"Label\"},{\"attributes\":{\"axis_label\":\"Year Closed\",\"axis_label_text_font_size\":\"16px\",\"formatter\":{\"id\":\"1078\"},\"major_label_orientation\":1,\"major_label_text_font_size\":\"15px\",\"ticker\":{\"id\":\"1015\"}},\"id\":\"1014\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"end\":2,\"factors\":[\"Merged / Accepted\",\"Not Merged / Rejected\"],\"palette\":[\"#0C0786\",\"#CA4678\"],\"start\":1},\"id\":\"1035\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{\"axis_label\":\"Average Commits / Pull Request\",\"axis_label_text_font_size\":\"15px\",\"formatter\":{\"id\":\"1076\"},\"major_label_text_font_size\":\"15px\",\"ticker\":{\"id\":\"1018\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1053\",\"type\":\"BasicTicker\"},{\"attributes\":{\"overlay\":{\"id\":\"1027\"}},\"id\":\"1023\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"children\":[[{\"id\":\"1004\"},0,0],[{\"id\":\"1043\"},1,0]]},\"id\":\"1092\",\"type\":\"GridBox\"},{\"attributes\":{},\"id\":\"1044\",\"type\":\"DataRange1d\"},{\"attributes\":{\"below\":[{\"id\":\"1052\"}],\"center\":[{\"id\":\"1055\"},{\"id\":\"1059\"},{\"id\":\"1074\"}],\"left\":[{\"id\":\"1056\"}],\"margin\":[0,0,0,0],\"outline_line_color\":null,\"plot_height\":200,\"plot_width\":568,\"title\":{\"id\":\"1084\"},\"toolbar\":{\"id\":\"1067\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"1044\"},\"x_scale\":{\"id\":\"1048\"},\"y_range\":{\"id\":\"1046\"},\"y_scale\":{\"id\":\"1050\"}},\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1066\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1065\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"axis\":{\"id\":\"1017\"},\"dimension\":1,\"ticker\":null},\"id\":\"1020\",\"type\":\"Grid\"},{\"attributes\":{\"end\":2.3},\"id\":\"1008\",\"type\":\"Range1d\"},{\"attributes\":{\"toolbars\":[{\"id\":\"1028\"},{\"id\":\"1067\"}],\"tools\":[{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1060\"},{\"id\":\"1061\"},{\"id\":\"1062\"},{\"id\":\"1063\"},{\"id\":\"1064\"},{\"id\":\"1065\"}]},\"id\":\"1093\",\"type\":\"ProxyToolbar\"},{\"attributes\":{},\"id\":\"1064\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{\"align\":\"center\",\"text\":\"llvm-project: Average Commit Counts Per Year for All Pull Requests\",\"text_font_size\":{\"value\":\"16px\"}},\"id\":\"1005\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1060\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1086\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1002\"},\"text\":{\"field\":\"counts\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"counts\"}},\"id\":\"1041\",\"type\":\"LabelSet\"},{\"attributes\":{\"formatter\":{\"id\":\"1088\"},\"ticker\":{\"id\":\"1053\"}},\"id\":\"1052\",\"type\":\"LinearAxis\"}],\"root_ids\":[\"1095\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"d99bacf2-088c-4b6c-b0ed-5998036afdec\",\"root_ids\":[\"1095\"],\"roots\":{\"1095\":\"5e5c33a5-437f-49b8-b4fe-37134d93ecee\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1095"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1201\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1201\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1201\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1201\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1201\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c62ee1bc159c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepo_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mvertical_grouped_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mhorizontal_stacked_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_closed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mmerged_ratio_vertical_grouped_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'All'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpr_closed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Slowest 20%'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpr_slow20_not_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_slow20_merged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvisualize_mean_response_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_closed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-3cc09ca22133>\u001b[0m in \u001b[0;36mhorizontal_stacked_bar\u001b[0;34m(input_df, repo_id, group_by, x_axis, description, y_axis, title)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_merged_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0my_merged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_merged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0my_merged_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "if display_grouping == 'repo':\n",
    "    for repo_id in repo_set:\n",
    "        vertical_grouped_bar(pr_all, repo_id=repo_id)\n",
    "        horizontal_stacked_bar(pr_closed, repo_id=repo_id)\n",
    "        merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_id)\n",
    "        visualize_mean_response_times(pr_closed, repo_id=repo_id, legend_position='center')\n",
    "        visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_id)\n",
    "        visualize_time_to_first_comment(pr_closed, repo_id= repo_id, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)\n",
    "        events_types_heat_map(pr_closed, repo_id=repo_id)\n",
    "        #print(pr_closed)\n",
    "        pr_duration_frame = pr_closed.assign(pr_duration=(pr_closed['pr_closed_at'] - pr_closed['pr_created_at']))\n",
    "        pr_duration_frame = pr_duration_frame.assign(pr_duration_days = (pr_duration_frame['pr_duration'] / datetime.timedelta(minutes=1))/60/24)\n",
    "        heat_map(pr_duration_frame, repo_id=repo_id)\n",
    "\n",
    "elif display_grouping == 'competitors':\n",
    "    vertical_grouped_bar(pr_all, repo_id=repo_list)\n",
    "    horizontal_stacked_bar(pr_closed, repo_id=repo_list)\n",
    "    merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)\n",
    "    visualize_mean_response_times(pr_closed, repo_id=repo_list, legend_position='center')\n",
    "    visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)\n",
    "    visualize_time_to_first_comment(pr_closed, repo_id= repo_list, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)\n",
    "    events_types_heat_map(pr_closed, repo_id=repo_list)\n",
    "    pr_duration_frame = pr_closed.assign(pr_duration=(pr_closed['pr_closed_at'] - pr_closed['pr_created_at']))\n",
    "    pr_duration_frame = pr_duration_frame.assign(pr_duration_days = (pr_duration_frame['pr_duration'] / datetime.timedelta(minutes=1))/60/24)\n",
    "    heat_map(pr_duration_frame, repo_id=repo_list)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
