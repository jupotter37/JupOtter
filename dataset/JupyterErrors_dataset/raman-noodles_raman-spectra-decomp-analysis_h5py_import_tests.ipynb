{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is early experimenting with importing real world data, playing around with our existing code, and determining how/if h5py (HDF5) file format will be appropriate for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ramanspectradecomp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1b6a3387db4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mramanspectradecomp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectrafit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ramanspectradecomp'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ramanspectradecomp import spectrafit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_data = pd.read_excel('../../Baseline Subtracted Data/CarbonMonoxide_Baseline_Calibration.xlsx',\n",
    "                        header=None, names=('x', 'y'))\n",
    "Me_data = pd.read_excel('../../Baseline Subtracted Data/Methane_Baseline_Calibration.xlsx',\n",
    "                        header=None, names=('x', 'y'))\n",
    "H_data = pd.read_excel('../../Baseline Subtracted Data/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                        header=None, names=('x', 'y'))\n",
    "FA_data = pd.read_excel('../../FormicAcid_3percentconc_400C_5s_00000.xlsx',\n",
    "                        header=None, names=('x', 'y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(CO_data['x'], CO_data['y'], label='Carbon Monoxide')\n",
    "plt.plot(Me_data['x'], Me_data['y'], label='Methane')\n",
    "plt.plot(H_data['x'], H_data['y'], label='Hydrogen')\n",
    "plt.plot(FA_data['x'], FA_data['y'], label='Formic Acid', alpha = 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = H_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, peak_list1 = spectrafit.peak_detect(data['x'].values, data['y'].values, height=10, prominence=20)\n",
    "peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(data['x'], data['y'], label='Experimental Data')\n",
    "for i in peak_list1[0]:\n",
    "    plt.axvline(x=data['x'][i], color='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, pars = spectrafit.set_params(peaks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = spectrafit.model_fit(data['x'].values, data['y'].values, mod, pars)\n",
    "print(out.fit_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrafit.plot_fit(data['x'].values, data['y'].values, out, plot_components=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our existing code with the baselined single compound data the curve fitting appears to work well. Expecially when a Voigt or Pseudo-Voigt model is used opposed to the Lorentzian. Next we begin to experiment with h5py (HDF5 file format) for storing the data and the result of the curve fitting for future comparison with multi-compound spectra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. In order to impliment h5py and the HDF5 file system (which I believe we should) we need to make a new .py module to replace shoyu.py. In this markdown cell I will attempt to do exactly that. First I will outline the functions that will be needed:\n",
    "\n",
    "## function brainstorm\n",
    "\n",
    "#### def new_calibration(new_filename)  \n",
    "    initialize an empty .hdp5 file with the input name\n",
    "    \n",
    "#### def add_compound(data_file, custom_label=None)\n",
    "    # file must contain only two columns, the first containing wavenumber data and the second count data.\n",
    "    # no headings\n",
    "    check if the file is csv or xlsx and import data using the appropriate pandas function\n",
    "    confirm that data is listed from smallest wavenumber to highest, correct if not\n",
    "    fit the data using the pseudo-voigt model\n",
    "    save the raw data and the fit results to the calibration .hdp5 using filename or custom_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramannoodles import dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the functions written we can now create a new calibration file (cal_file)\n",
    "# this file can only be made once and compounds can be added to it later\n",
    "cal_file = calibration.new_cal('cal_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we add methane to the cal_file\n",
    "cal_file = calibration.add_compound('cal_example.hdf5',\n",
    "                                    'cal_example compounds/Methane_Baseline_Calibration.xlsx',\n",
    "                                    label='Methane')                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we add hydrogen to the cal_file\n",
    "cal_file = calibration.add_compound('cal_example.hdf5',\n",
    "                                    'cal_example compounds/Hydrogen_Baseline_Calibration.xlsx',\n",
    "                                    label='Hydrogen') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last we add carbon monoxide to the cal_file\n",
    "cal_file = calibration.add_compound('cal_example.hdf5',\n",
    "                                    'cal_example compounds/CarbonMonoxide_Baseline_Calibration.xlsx',\n",
    "                                    label='Carbon Monoxide') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can list all the keys in cal_example.hdf5\n",
    "list(cal_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each key contains the x and y data as well as all the fit parameters for each detected peak\n",
    "print(list(cal_file['Hydrogen'].keys()))\n",
    "# each peak has parameters fraction, sigma, center, amplitude, fwhm, and height, in that order\n",
    "print(list(cal_file['Hydrogen/Peak_1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we update the peakidentify.py functions, we can read the cal_file in read only mode to ensure that we do not accidentally overwrite any of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one additional function for uploading data for a single compound at a variety of temperatures and residence times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5 = h5py.File('cal_example.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the original development for a function that will display the contents of a h5py.File\n",
    "# down at least three levels. Datasets are labeled \n",
    "\n",
    "def view_hdf5(filename):\n",
    "    \"\"\"docstring\"\"\"\n",
    "    print('**** {} ****'.format(filename))\n",
    "    hdf5 = h5py.File(filename, 'r')\n",
    "    for _,layer_1 in enumerate(list(hdf5.keys())):\n",
    "        if isinstance(hdf5[layer_1], h5py.Group):\n",
    "            print('\\033[1m{}\\033[0m'.format(layer_1))\n",
    "            for _,layer_2 in enumerate(list(hdf5[layer_1].keys())):\n",
    "                if isinstance(hdf5['{}/{}'.format(layer_1, layer_2)], h5py.Group):\n",
    "                    print('|    \\033[1m{}\\033[0m'.format(layer_2))\n",
    "                    for _,layer_3 in enumerate(list(hdf5['{}/{}'.format(layer_1, layer_2)])):\n",
    "                        if isinstance(hdf5['{}/{}/{}'.format(layer_1, layer_2, layer_3)], h5py.Group):\n",
    "                            print('|    |    \\033[1m{}\\033[0m/...'.format(layer_3))\n",
    "                        else:\n",
    "                            print('|    |    {}'.format(layer_3))\n",
    "                else:\n",
    "                    print('|    {}'.format(layer_2))\n",
    "        else:\n",
    "            print('{}'.format(layer_1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formic acid experimental data h5py test\n",
    "FA = dataprep.new_hdf5('FA_exp_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA_300 = pd.read_csv('../../Baseline Subtracted Data/Formic Acid Data/300 C/FA_3.6wt__300C_25s.csv',\n",
    "                        header=None, names=('x', 'y'))\n",
    "FA_300.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FA['300C/25s/x'] = FA_300['x']\n",
    "FA['300C/25s/y'] = FA_300['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data_filename = '../../Baseline Subtracted Data/Formic Acid Data/300 C/FA_3.6wt__300C_35s.csv'\n",
    "hdf5_filename = 'FA_exp_300.hdf5'\n",
    "FA_300_35 = pd.read_csv(data_filename, header=None, names=('x', 'y'))\n",
    "\n",
    "# extract experimental parameters from filename\n",
    "specs = data_filename.split('/')[-1].split('.')[:-1]\n",
    "if len(specs) > 1:\n",
    "    spec = ''\n",
    "    for _,element in enumerate(specs):\n",
    "        spec = str(spec+element)\n",
    "    specs = spec\n",
    "specs = specs.split('_')\n",
    "specs\n",
    "time = specs[-1]\n",
    "temp = specs[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_filename = 'FA_exp_300.hdf5'\n",
    "data_filename = '../../Baseline Subtracted Data/Formic Acid Data/300 C/FA_3.6wt__300C_55s.csv'\n",
    "\n",
    "\n",
    "# function to add a dataset to experiment hdf5\n",
    "def add_experiment(cal_filename, data_filename):\n",
    "    \"\"\"docstring\"\"\"\n",
    "    # handling input errors\n",
    "    if not isinstance(cal_filename, str):\n",
    "        raise TypeError('Passed value of `cal_filename` is not a string! Instead, it is: '\n",
    "                        + str(type(cal_filename)))\n",
    "    if not isinstance(data_filename, str):\n",
    "        raise TypeError('Passed value of `data_filename` is not a string! Instead, it is: '\n",
    "                        + str(type(data_filename)))\n",
    "    # r+ is read/write mode and will fail if the file does not exist\n",
    "    exp_file = h5py.File(exp_filename, 'r+')\n",
    "    if data_filename.split('.')[-1] == 'xlsx':\n",
    "        data = pd.read_excel(data_filename, header=None, names=('x', 'y'))\n",
    "    elif data_filename.split('.')[-1] == 'csv':\n",
    "        data = pd.read_csv(data_filename, header=None, names=('x', 'y'))\n",
    "    else:\n",
    "        print('data file type not recognized')\n",
    "    # ensure that the data is listed from smallest wavenumber first\n",
    "    if data['x'][:1].values > data['x'][-1:].values:\n",
    "        data = data.iloc[::-1]\n",
    "        data.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        pass\n",
    "    # peak detection and data fitting\n",
    "    fit_result = spectrafit.fit_data(data['x'].values, data['y'].values)\n",
    "    # extract experimental parameters from filename\n",
    "    specs = data_filename.split('/')[-1].split('.')[:-1]\n",
    "    if len(specs) > 1:\n",
    "        spec = ''\n",
    "        for _,element in enumerate(specs):\n",
    "            spec = str(spec+element)\n",
    "        specs = spec\n",
    "    specs = specs.split('_')\n",
    "    specs\n",
    "    time = specs[-1]\n",
    "    temp = specs[-2]\n",
    "    # write data to .hdf5\n",
    "    exp_file['{}/{}/wavenumber'.format(temp, time)] = data['x']\n",
    "    exp_file['{}/{}/counts'.format(temp, time)] = data['y']\n",
    "    for i, _ in enumerate(fit_result):\n",
    "        if i < 9:\n",
    "            exp_file['{}/{}/Peak_0{}'.format(temp, time, i+1)] = fit_result[i]\n",
    "        else:\n",
    "            exp_file['{}/{}/Peak_{}'.format(temp, time, i+1)] = fit_result[i]\n",
    "    exp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_experiment('FA_exp_300.hdf5', '../../Baseline Subtracted Data/Formic Acid Data/300 C/FA_3.6wt__300C_65s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_hdf5('FA_exp_300.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../../Baseline Subtracted Data/Formic Acid Data/300 C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramannoodles import dataprep\n",
    "dataprep.new_hdf5('exp_test')\n",
    "dataprep.add_experiment('exp_test.hdf5', '../../Baseline Subtracted Data/Formic Acid Data/300 C/FA_3.6wt__300C_25s.csv')\n",
    "exp_file = h5py.File('exp_test.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_hdf5('exp_test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exp_file['300C/25s']) == 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(exp_file) == 1, 'incorrect number of 1st order groups'\n",
    "assert list(exp_file.keys())[0] == '300C', '1st order group name incorrect'\n",
    "assert len(exp_file['300C']) ==1, 'incorrect number of 2nd order groups'\n",
    "assert list(exp_file['300C'].keys())[0] == '25s', '2nd order group name incorrect'\n",
    "assert '300C/25s/wavenumber' in exp_file, 'x data (wavenumber) not stored correctly'\n",
    "assert '300C/25s/counts' in exp_file, 'y data (counts) not stored correctly'\n",
    "assert len(exp_file['300C/25s']) == 18, 'incorrect number of peaks + raw_data stored'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataprep.add_experiment(4.2, 'ramannoodles/tests/test_files/CarbonMonoxide_Baseline_Calibration.xlsx')\n",
    "except TypeError:\n",
    "    print('A float was passed to the function, and it was handled well with a TypeError.')\n",
    "try:\n",
    "    dataprep.add_experiment('test.hdp5', 4.2)\n",
    "except TypeError:\n",
    "    print('A float was passed to the function, and it was handled well with a TypeError.')\n",
    "try:\n",
    "    dataprep.add_experiment('test.txt', 'ramannoodles/tests/test_files/CarbonMonoxide_Baseline_Calibration')\n",
    "except TypeError:\n",
    "    print('A .txt file was passed to the function, and it was handled will with a TypeError.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
