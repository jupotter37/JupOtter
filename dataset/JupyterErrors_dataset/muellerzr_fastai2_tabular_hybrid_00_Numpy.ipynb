{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00_Numpy\n",
    "\n",
    "> Building an example `Dataset` and `DataLoader` with `NumPy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.tabular.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data we'll first utilize `TabularPandas` for pre-processing. One potential is to use `TabularPandas` for pre-processing only, or to integrate `NumPy` directly into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "y_names = 'salary'\n",
    "splits = RandomSplitter()(range_of(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll still build our regular `TabularPandas`, as we haven't done any `NumPy` modifications yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                   y_names=y_names, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumpyDataset():\n",
    "    \"A `Numpy` dataset object from `TabularPandas`\"\n",
    "    def __init__(self, to:TabularPandas):\n",
    "        self.cats = to.cats.to_numpy().astype(np.long)\n",
    "        self.conts = to.conts.to_numpy().astype(np.float32)\n",
    "        self.ys = to.ys.to_numpy()\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        idx = idx[0]\n",
    "        return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs]\n",
    "    \n",
    "    def __len__(self): return len(self.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NumpyDataset(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.bs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = ds[[0]]\n",
    "test_eq(len(a), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumpyDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, bs=1, **kwargs):\n",
    "        \"A `DataLoader` for a `NumpyDataset`\"\n",
    "        super().__init__(dataset, bs=bs, **kwargs)\n",
    "        self.dataset.bs = bs\n",
    "        \n",
    "    def create_item(self, s): return s\n",
    "    \n",
    "    def create_batch(self, b):\n",
    "        cat, cont, y = self.dataset[b]\n",
    "        return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = NumpyDataLoader(ds, bs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(dl), len(ds)//3+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def shuffle_fn(x:NumpyDataLoader):\n",
    "    \"Shuffle the interior dataset\"\n",
    "    rng = np.random.permutation(len(x.dataset))\n",
    "    x.dataset.cats = x.dataset.cats[rng]\n",
    "    x.dataset.conts = x.dataset.conts[rng]\n",
    "    x.dataset.ys = x.dataset.ys[rng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def get_idxs(x:NumpyDataLoader):\n",
    "    \"Get index's to select\"\n",
    "    idxs = Inf.count if x.indexed else Inf.nones\n",
    "    if x.n is not None: idxs = list(range(len(x.dataset)))\n",
    "    if x.shuffle: x.shuffle_fn()\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we still see an improvement, we'll compare timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NumpyDataset(to.train)\n",
    "valid_ds = NumpyDataset(to.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = NumpyDataLoader(train_ds, bs=64, shuffle=True, drop_last=True)\n",
    "valid_dl = NumpyDataLoader(valid_ds, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = to.dataloaders(bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.2 ms ± 35.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Numpy\n",
    "for _ in train_dl: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 s ± 784 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# fastai\n",
    "for _ in dls[0]: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.35 ms ± 12.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Numpy\n",
    "for _ in valid_dl: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 ms ± 1.24 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# fastai\n",
    "for _ in dls[1]: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class NumpyDataLoaders(DataLoaders):\n",
    "    def __init__(self, to, bs=64, val_bs=None, shuffle_train=True, device='cpu', **kwargs):\n",
    "        train_ds = NumpyDataset(to.train)\n",
    "        valid_ds = NumpyDataset(to.valid)\n",
    "        val_bs = bs if val_bs is None else val_bs\n",
    "        train = NumpyDataLoader(train_ds, bs=bs, shuffle=shuffle_train, device=device, drop_last=True, **kwargs)\n",
    "        valid = NumpyDataLoader(valid_ds, bs=val_bs, shuffle=False, device=device, **kwargs)\n",
    "        super().__init__(train, valid, device=device, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_2_col = dict(zip(range(len(col_names)), col_names)); idx_2_col\n",
    "col_2_idx = {v: k for k, v in idx_2_col.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idxs = [col_2_idx[name] for name in cat_names]\n",
    "cont_idxs = [col_2_idx[name] for name in cont_names]\n",
    "y_idxs = [col_2_idx[name] for name in [y_names]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'fnlwgt', 'education-num']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NumpyFillMissing(TabularProc):\n",
    "    def __init__(self, fill_strategy=FillStrategy.median, add_col=True, fill_vals=None):\n",
    "        if fill_vals is None: fill_vals = defaultdict(int)\n",
    "        store_attr(self, 'fill_strategy,add_col,fill_vals')\n",
    "    def setups(self, dsets):\n",
    "        missing = [np.isnan(np.sum(df_np[:,idx])) for idx in cont_idxs]\n",
    "        self.na_dict = {n:self.fill_strategy(dsets[n])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, True]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = FillStrategy.median\n",
    "missing = [np.isnan(np.sum(df_np[:,idx])) for idx in cont_idxs]; missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = {}\n",
    "for idx in cont_idxs:\n",
    "    missing[idx] = np.isnan(np.sum(df_np[:,idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: False, 2: False, 4: True}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: False, 2: False, 4: True}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fastai2.tabular.core.FillStrategy.median(c, fill)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 2, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'median'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-0ed837a34b05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m {n:fs(df_np[:,n], defaultdict(int))\n\u001b[0;32m----> 2\u001b[0;31m for n in missing.keys()}\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-0ed837a34b05>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m {n:fs(df_np[:,n], defaultdict(int))\n\u001b[0;32m----> 2\u001b[0;31m for n in missing.keys()}\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai2/lib/python3.7/site-packages/fastai2/tabular/core.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(c, fill)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFillStrategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;34m\"Namespace containing the various filling strategies.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mmedian\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmode\u001b[0m    \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'median'"
     ]
    }
   ],
   "source": [
    "{n:fs(df_np[:,n], defaultdict(int))\n",
    "for n in missing.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'isnan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-defe3d07c2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcont_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'isnan'"
     ]
    }
   ],
   "source": [
    "np.all(df_np[:,cont_idxs], axis=1).isnan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49, 101320, 12.0],\n",
       "       [44, 236746, 14.0],\n",
       "       [38, 96185, nan],\n",
       "       ...,\n",
       "       [53, 157069, 12.0],\n",
       "       [32, 217296, 9.0],\n",
       "       [26, 182308, 10.0]], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np[:,cont_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNumpy(CollBase, GetAttr, FilteredBase):\n",
    "    _default, with_cont='procs',True\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_np = TabularNumpy(df_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNumpy(Tabular):\n",
    "    _default, with_cont='procs',True\n",
    "    def __init__(self, df, procs=None, cat_names=None, cont_names=None, y_names=None, y_block=None, splits=None,\n",
    "                 do_setup=True, device=None, inplace=False, reduce_memory=True):\n",
    "        self.df = df[cat_names+cont_names+y_names].to_numpy()\n",
    "        self.idx_2_col = dict(zip(range(len(df.columns)), df.columns))\n",
    "        self.col_2_idx = {v: k for k, v in self.idx_2_col.items()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tabular(CollBase, GetAttr, FilteredBase):\n",
    "    \"A `DataFrame` wrapper that knows which cols are cont/cat/y, and returns rows in `__getitem__`\"\n",
    "    _default,with_cont='procs',True\n",
    "    def __init__(self, df, procs=None, cat_names=None, cont_names=None, y_names=None, y_block=None, splits=None,\n",
    "                 do_setup=True, device=None, inplace=False, reduce_memory=True):\n",
    "        if inplace and splits is not None and pd.options.mode.chained_assignment is not None:\n",
    "            warn(\"Using inplace with splits will trigger a pandas error. Set `pd.options.mode.chained_assignment=None` to avoid it.\")\n",
    "        if not inplace: df = df.copy()\n",
    "        if splits is not None: df = df.iloc[sum(splits, [])]\n",
    "        self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders)\n",
    "        super().__init__(df)\n",
    "\n",
    "        self.y_names,self.device = L(y_names),device\n",
    "        if y_block is None and self.y_names:\n",
    "            # Make ys categorical if they're not numeric\n",
    "            ys = df[self.y_names]\n",
    "            if len(ys.select_dtypes(include='number').columns)!=len(ys.columns): y_block = CategoryBlock()\n",
    "            else: y_block = RegressionBlock()\n",
    "        if y_block is not None and do_setup:\n",
    "            if callable(y_block): y_block = y_block()\n",
    "            procs = L(procs) + y_block.type_tfms\n",
    "        self.cat_names,self.cont_names,self.procs = L(cat_names),L(cont_names),Pipeline(procs)\n",
    "        self.split = len(df) if splits is None else len(splits[0])\n",
    "        if reduce_memory:\n",
    "            if len(self.cat_names) > 0: self.reduce_cats()\n",
    "            if len(self.cont_names) > 0: self.reduce_conts()\n",
    "        if do_setup: self.setup()\n",
    "\n",
    "    def new(self, df):\n",
    "        return type(self)(df, do_setup=False, reduce_memory=False, y_block=TransformBlock(),\n",
    "                          **attrdict(self, 'procs','cat_names','cont_names','y_names', 'device'))\n",
    "\n",
    "    def subset(self, i): return self.new(self.items[slice(0,self.split) if i==0 else slice(self.split,len(self))])\n",
    "    def copy(self): self.items = self.items.copy(); return self\n",
    "    def decode(self): return self.procs.decode(self)\n",
    "    def decode_row(self, row): return self.new(pd.DataFrame(row).T).decode().items.iloc[0]\n",
    "    def reduce_cats(self): self.train[self.cat_names] = self.train[self.cat_names].astype('category')\n",
    "    def reduce_conts(self): self[self.cont_names] = self[self.cont_names].astype(np.float32)\n",
    "    def show(self, max_n=10, **kwargs): display_df(self.new(self.all_cols[:max_n]).decode().items)\n",
    "    def setup(self): self.procs.setup(self)\n",
    "    def process(self): self.procs(self)\n",
    "    def loc(self): return self.items.loc\n",
    "    def iloc(self): return _TabIloc(self)\n",
    "    def targ(self): return self.items[self.y_names]\n",
    "    def x_names (self): return self.cat_names + self.cont_names\n",
    "    def n_subsets(self): return 2\n",
    "    def y(self): return self[self.y_names[0]]\n",
    "    def new_empty(self): return self.new(pd.DataFrame({}, columns=self.items.columns))\n",
    "    def to_device(self, d=None):\n",
    "        self.device = d\n",
    "        return self\n",
    "\n",
    "    def all_col_names (self):\n",
    "        ys = [n for n in self.y_names if n in self.items.columns]\n",
    "        return self.x_names + self.y_names if len(ys) == len(self.y_names) else self.x_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
