{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/LogoWekeo_Copernicus_RGB_0.png' alt='' align='centre' width='30%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COPERNICUS MARINE BIO BLACK-SEA TRAINING\n",
    "\n",
    "<div style=\"text-align: right\"><i> 07-03-BIO </i></div>\n",
    "\n",
    "\n",
    "    License: This code is offered as open source and free-to-use in the public domain, \n",
    "             with no warranty, under the MIT license associated with this code repository.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<center><h1> How to visualize maps, sections of nutrients, chlorophyll, oxygen and CO2 in the Black-Sea </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**General Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**General Note 2**: If, for any reason, the kernel is not working anymore, in the top MENU, click on the <button class=\"btn btn-default btn-xs\"><i class=\"fa fa-repeat icon-repeat\"></i></button> button. Then, in the top MENU, click on \"Cell\" and select \"Run All Above Selected Cell\".<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Table of contents\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. About the data](#2.-About-the-data)\n",
    "- [3. Required Python modules](#3.-Required-Python-modules)\n",
    "- [4. Download data with HDA](#4.-Download-data-with-HDA)\n",
    "- [5. Exercise n.1: Vertical profiles](#5.-Exercise-n.1:-Vertical-profiles)\n",
    "- [6. Exercise n.2: Plot of transects](#6.-Exercise-n.2:-Plot-of-transects)\n",
    "- [7. Conclusion](#7.-Conclusion)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this exercise is to use the Copernicus Marine (CMEMS) BIOgeochemical products to visualize some typical coastal biogeochemical features in the Black Sea.\n",
    "\n",
    "In particular, we will display:\n",
    " \n",
    "- exercice 1: typical vertical profiles of oxygen and of pH\n",
    "- exercice 2: plot of transects\n",
    "\n",
    "We will use the near-real time (NRT) products as they already use the latest CMEMS name conventions. After July 2020, the multi-year (MY) product will also use the same conventions.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description\n",
    "\n",
    "### This example is based on the product: [EO:MO:DAT:BLKSEA_ANALYSIS_FORECAST_BIO_007_010](https://moi.wekeo.eu/data?view=dataset&dataset=EO%3AMO%3ADAT%3ABLKSEA_ANALYSIS_FORECAST_BIO_007_010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLKSEA_ANALYSIS_FORECAST_BIO_007_010** is the nominal product of the Black Sea Biogeochemistry NRT system and is generated by the NEMO-BAMHBI modelling system. Biogeochemical Model for Hypoxic and Benthic Influenced areas (BAMHBI) is an innovative biogeochemical model with a 28-variable pelagic component (including the carbonate system) and a 6-variable benthic component ; it explicitely represents processes in the anoxic layer.The product provides analysis and forecast for 3D concentration of chlorophyll, nutrients (nitrate and phosphate), dissolved oxygen, phytoplankton carbon biomass, net primary production, pH, dissolved inorganic carbon, total alkalinity, and for 2D fields of bottom oxygen concentration (for the North-Western shelf), surface partial pressure of CO2 and surface flux of CO2. These variables are computed on the same grid as the PHY product, at ~3km x 31-levels resolution, and are provided as daily and monthly means.\n",
    "\n",
    "<img src=\"https://wekeo-broker.apps.mercator.dpi.wekeo.eu/previews/EO_MO_DAT_BLKSEA_ANALYSIS_FORECAST_BIO_007_010_bs-ulg-pft-an-fc-m.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get more info on the product\n",
    "You can find more info on this product and access to the download services in the [products viewer on Wekeo](https://moi.wekeo.eu/data?view=dataset&dataset=EO%3AMO%3ADAT%3ABLKSEA_ANALYSIS_FORECAST_BIO_007_010).\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters used for downloading the data\n",
    "| Parameter | Value |\n",
    "| :---: | :---|\n",
    "| **Product** | BLKSEA_ANALYSIS_FORECAST_BIO_007_010 |\n",
    "| **Datasets** | <ul><li>bs-ulg-bio-an-fc-m</li><li>bs-ulg-ptf-an-fc-m</li><li>bs-ulg-nut-an-fc-m</li>\n",
    "| **Frequency** | monthly |\n",
    "| **Lat min** | 27.37 |\n",
    "| **Lat max** | 41.96 |\n",
    "| **Lon min** | 40.86 |\n",
    "| **Lon max** | 46.80 |\n",
    "| **Timesteps** | 2019-01-01, 2019-09-01 |\n",
    "| **Service for downloading** | HDA (FTP) |\n",
    "| **Files total dimension** | ~40 MB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Get the WEkEO User credentials</b>\n",
    "<hr>\n",
    "If you want to download the data to use this notebook, you will need WEkEO User credentials. If you do not have these, you can register <a href=\"https://www.wekeo.eu/web/guest/user-registration\" target=\"_blank\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Required Python modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find the Python modules imported for running the notebook's code. They are quite common modules adopted for handling the scientific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Module name | Description |\n",
    "| :---: | :---|\n",
    "| **os** | [ Miscellaneous operating system interfaces](https://docs.python.org/3.7/library/os.html) for managing paths, creating directories,... |\n",
    "| **sys** | [sys](https://docs.python.org/3/library/sys.html) for accessing variables used by the interpreter |\n",
    "| **json** | [json](https://docs.python.org/3/library/json.html) is for manipulating JSON files |\n",
    "| **requests** | [requests](https://requests.readthedocs.io/en/master/) is for sending HTTP requests |\n",
    "| **numpy** | [NumPy](https://numpy.org/) is the fundamental package for scientific computing with Python and for managing ND-arrays |\n",
    "| **xarray** | [Xarray](http://xarray.pydata.org/en/stable/) introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience. |\n",
    "| **matplotlib** |[Matplotlib](https://matplotlib.org/) is a Python 2D plotting library which produces publication quality figures |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code cells allow you to enter and run Python code \n",
    "Run a code cell using <code>Shift-Enter</code> or pressing the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button in the toolbar above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For avoiding the warning messages during the execution and installation process, at first remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have the right module, please install it with the command:\n",
    "```\n",
    "conda install module_name\n",
    "```\n",
    "and then re-try to execute the cell for importing it. **Please install the modules one by one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Download data with HDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),'wekeo-hda'))\n",
    "import hda_api_functions as hapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API key is:  VVNFUk5BTUU6UEFTU1dPUkQ=\n"
     ]
    }
   ],
   "source": [
    "# your WEkEO API username and password (needs to be in '  ')\n",
    "user_name = 'USERNAME'\n",
    "password = 'PASSWORD'\n",
    "\n",
    "# Generate an API key\n",
    "api_key = hapi.generate_api_key(user_name, password)\n",
    "print('Your API key is: ', api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir_path = os.path.join(os.getcwd(),'products', \"07-03\")\n",
    "\n",
    "# make the output directory if required\n",
    "if not os.path.exists(download_dir_path):\n",
    "    os.makedirs(download_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers three different data sets. If you want to look at all of them, you need to uncomment (remove the # and replace on the others) the line corresponding to the dataset you want to download and run the next 8 cells in this section, for each data set ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"EO:MO:DAT:BLKSEA_ANALYSIS_FORECAST_BIO_007_010\"\n",
    "\n",
    "## Uncooment the product with the variable (together)\n",
    "\n",
    "product = \"bs-ulg-pft-an-fc-m\";variable=\"chl\"\n",
    "#product = \"bs-ulg-bio-an-fc-m\";variable=\"o2\"\n",
    "#product = \"bs-ulg-nut-an-fc-m\";variable=\"no3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide here the parameters of the requests as described in previous section. You can prepare this request thanks to the data access feature of WEkEO data viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "  \"datasetId\": dataset_id+\":\"+product,\n",
    "  \"boundingBoxValues\": [\n",
    "    {\n",
    "      \"name\": \"bbox\",\n",
    "      \"bbox\": [\n",
    "        36.29404162525542,\n",
    "        43.07629007394045,\n",
    "        38.025762720953,\n",
    "        44.03246025765181\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"dateRangeSelectValues\": [\n",
    "    {\n",
    "      \"name\": \"position\",\n",
    "      \"start\": \"2020-09-02T00:00:00.000Z\",\n",
    "      \"end\": \"2020-11-02T00:00:00.000Z\"\n",
    "    }\n",
    "  ],\n",
    "  \"multiStringSelectValues\": [\n",
    "    {\n",
    "      \"name\": \"variable\",\n",
    "      \"value\": [\n",
    "        variable\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"stringChoiceValues\": [\n",
    "    {\n",
    "      \"name\": \"service\",\n",
    "      \"value\": \"BLKSEA_ANALYSIS_FORECAST_BIO_007_010-TDS\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"product\",\n",
    "      \"value\": product\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"startDepth\",\n",
    "      \"value\": \"22.66373634338379\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"endDepth\",\n",
    "      \"value\": \"78.25402069091797\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting an access token. This token is valid for one hour only.\n",
      "Success: Access token is c214744c-1346-38e7-be78-359431d37414\n",
      "Copernicus_General_License Terms and Conditions already accepted\n"
     ]
    }
   ],
   "source": [
    "HAPI_dict = hapi.init(dataset_id, api_key, download_dir_path)\n",
    "HAPI_dict = hapi.get_access_token(HAPI_dict)\n",
    "HAPI_dict = hapi.acceptTandC(HAPI_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job...\n",
      "Query successfully submitted. Job ID is c348w3FCqKJR0kRQIok6euDDkK0\n",
      "Next check in 5 seconds\n",
      "Query successfully submitted. Status is running\n",
      "Next check in 10 seconds\n",
      "Query successfully submitted. Status is completed\n"
     ]
    }
   ],
   "source": [
    "# launch job\n",
    "print('Launching job...')\n",
    "HAPI_dict = hapi.get_job_id(HAPI_dict, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results...\n",
      "************** Results *******************************n\n",
      "{\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"downloadUri\": null,\n",
      "            \"filename\": \"&service=BLKSEA_ANALYSIS_FORECAST_BIO_007_010-TDS&product=bs-ulg-pft-an-fc-m&z_lo=22.66373634338379&z_hi=78.25402069091797&t_lo=2020-09-02T00:00:00.000Z&t_hi=2020-11-02T00:00:00.000Z&x_lo=36.29404162525542&y_hi=43.07629007394045&x_hi=38.025762720953&y_lo=44.03246025765181&variable=chl\",\n",
      "            \"order\": null,\n",
      "            \"productInfo\": {\n",
      "                \"datasetId\": \"EO:MO:DAT:BLKSEA_ANALYSIS_FORECAST_BIO_007_010:bs-ulg-pft-an-fc-m\",\n",
      "                \"product\": \"bs-ulg-pft-an-fc-m:170b8c300f68d28ea3e6593da4fb4f02\",\n",
      "                \"productEndDate\": \"2020-11-02T00:00:00Z\",\n",
      "                \"productStartDate\": \"2020-09-02T00:00:00Z\"\n",
      "            },\n",
      "            \"size\": 134788,\n",
      "            \"url\": \"&service=BLKSEA_ANALYSIS_FORECAST_BIO_007_010-TDS&product=bs-ulg-pft-an-fc-m&z_lo=22.66373634338379&z_hi=78.25402069091797&t_lo=2020-09-02T00:00:00.000Z&t_hi=2020-11-02T00:00:00.000Z&x_lo=36.29404162525542&y_hi=43.07629007394045&x_hi=38.025762720953&y_lo=44.03246025765181&variable=chl\"\n",
      "        }\n",
      "    ],\n",
      "    \"itemsInPage\": 1,\n",
      "    \"nextPage\": null,\n",
      "    \"page\": 0,\n",
      "    \"pages\": 1,\n",
      "    \"previousPage\": null,\n",
      "    \"totItems\": 1\n",
      "}\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "print('Getting results...')\n",
    "params = {'page':'0', 'size':'12'}\n",
    "response = requests.get(HAPI_dict['broker_endpoint'] + '/datarequest/jobs/' + HAPI_dict['job_id'] + '/result', headers=HAPI_dict['headers'], params = params)\n",
    "results = json.loads(response.text)\n",
    "\n",
    "print(\"************** Results *******************************n\")\n",
    "print(json.dumps(results, indent=4, sort_keys=True))\n",
    "print(\"*******************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"content\": [],\n",
      "    \"itemsInPage\": 1,\n",
      "    \"nextPage\": null,\n",
      "    \"page\": 0,\n",
      "    \"pages\": 1,\n",
      "    \"previousPage\": null,\n",
      "    \"totItems\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Remove all unnecessary files to download the only ones we need\n",
    "results['content'] = [x for x in results['content'] if \"20190115\" in x['filename'] or \"20190915\" in x['filename']]\n",
    "print(json.dumps(results, indent=4, sort_keys=True))\n",
    "\n",
    "HAPI_dict['results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'response' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ccfde4bc96e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHAPI_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_order_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHAPI_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cfortunylombra/wekeo-jupyter-lab/wekeo-hda/hda_api_functions.py\u001b[0m in \u001b[0;36mget_order_ids\u001b[0;34m(hda_dict)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mhda_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mhda_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_sizes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mhda_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_status_response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhda_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'response' referenced before assignment"
     ]
    }
   ],
   "source": [
    "HAPI_dict = hapi.get_order_ids(HAPI_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n"
     ]
    }
   ],
   "source": [
    "print('Downloading data...')\n",
    "HAPI_dict = hapi.download_data(HAPI_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exercise n.1: Vertical profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Scientific question ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does typical vertical profiles of oxygen and pH look like in the Black Sea ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Don't change the following constants, which define the training and the notebook codes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_CODE = \"07\"\n",
    "NB_CODE = \"03\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**checkDir**: function for creating a path, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkDir(outPath):\n",
    "    if not os.path.exists(outPath):\n",
    "        os.makedirs(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getRangeIndexes**: function for getting the indexes of the array *arr* between the *var_min* and *var_max* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRangeIndexes(arr, var_min, var_max):\n",
    "    return np.where((arr >= var_min) & (arr <= var_max))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_code = REGION_CODE+'-'+NB_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for netcdf files\n",
    "data_path = './products/'+ex_code\n",
    "# Path for the output files (images, etc)\n",
    "out_path = './out/'+ex_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "checkDir(data_path)\n",
    "checkDir(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the new directories have been created... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "06-08-BIO.ipynb\n",
      "07-03-BIO.ipynb\n",
      "Cyanobacteria_Sentinel3_OLCI_RGB_plotter.ipynb\n",
      "JSON_templates\n",
      "Marine_heatwaves_S3_CMEMS.ipynb\n",
      "img\n",
      "out\n",
      "products\n",
      "tools\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir('.'):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and if the data files are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(data_path):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input netcdf file (please note - if you haven't downloaded all three different data sets, you'll need to comment the relevant lines below \n",
    "#  or go back to the section above to download the data.)\n",
    "\n",
    "# Monthly means of January and September 2019\n",
    "nut_f = [\"20190115_m-ULg--NUTR-nemo_bamhbi-BS-b20200603_an-sv09.00.nc\",\n",
    "         \"20190915_m-ULg--NUTR-nemo_bamhbi-BS-b20200603_an-sv09.00.nc\"]\n",
    "pft_f = [\"20190115_m-ULg--PFTC-nemo_bamhbi-BS-b20200603_an-sv09.00.nc\",\n",
    "         \"20190915_m-ULg--PFTC-nemo_bamhbi-BS-b20200603_an-sv09.00.nc\"]\n",
    "bio_f = [\"20190115_m-ULg--BIOL-nemo_bamhbi-BS-b20200603_an-sv09.00.nc\",\n",
    "         \"20190915_m-ULg--BIOL-nemo_bamhbi-BS-b20200603_an-sv09.00.nc\"]\n",
    "\n",
    "nut_nc = [os.path.join(data_path, f) for f in nut_f]\n",
    "pft_nc = [os.path.join(data_path, f) for f in pft_f]\n",
    "bio_nc = [os.path.join(data_path, f) for f in bio_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/home/cfortunylombra/wekeo-jupyter-lab/ocean/OceanCaseStudies/products/07-03/20190115_m-ULg--NUTR-nemo_bamhbi-BS-b20200603_an-sv09.00.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/cfortunylombra/wekeo-jupyter-lab/ocean/OceanCaseStudies/products/07-03/20190115_m-ULg--NUTR-nemo_bamhbi-BS-b20200603_an-sv09.00.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e1f505db03d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Open the nc datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnut_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnut_nc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpft_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpft_nc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbio_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbio_nc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e1f505db03d2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Open the nc datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnut_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnut_nc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpft_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpft_nc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbio_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbio_nc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0moverwrite_encoded_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite_encoded_chunks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m     backend_ds = backend.open_dataset(\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         store = NetCDF4DataStore.open(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mnetCDF4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         )\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nc4_require_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;34m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_with_cache_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;31m# ensure file doesn't get overriden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/home/cfortunylombra/wekeo-jupyter-lab/ocean/OceanCaseStudies/products/07-03/20190115_m-ULg--NUTR-nemo_bamhbi-BS-b20200603_an-sv09.00.nc'"
     ]
    }
   ],
   "source": [
    "# Open the nc datasets\n",
    "nut_ds = [ xr.open_dataset(nc) for nc in nut_nc]\n",
    "pft_ds = [ xr.open_dataset(nc) for nc in pft_nc]\n",
    "bio_ds = [ xr.open_dataset(nc) for nc in bio_nc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the array index for accessing the desired dataset in the datasets arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index for the ds arrays\n",
    "# 0 = winter, 1 = summer\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Get info about the dataset of **\"Nitrate and Phosphate 3D\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_ds[j].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Products used in this training sessions are NetCDF files (.nc files). A NetCDF file is a common way of storing scientific data. It contains:\n",
    "- The **dimensions** of the data (here depth, latitutde, longitude and time)\n",
    "- Several **variables** depending on one or more of these dimensions.\n",
    "    - Each variable comes with its own attributes such as units, long_name, FillValue...\n",
    "- General information about the product (**Attributes**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And about the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_ds[j].data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nut_ds[j].coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "**The dataset is 3D! You have depth levels!**\n",
    "\n",
    "The xarray dataset ***nut_ds*** is extracted from a **3D dataset**        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to check the depth levels. Type ```nut_ds[j].depth``` in the cell below and execute it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Get info about the dataset for **\"Phytoplankton Carbon Biomass and Chlorophyll\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please note - if you haven't downloaded all three data sets, you may not be able to run this section. \n",
    "# If you get errors saying data isn't found, go back to section 4 to download the relevant data.\n",
    "\n",
    "pft_ds[j].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And about the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_ds[j].data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_ds[j].coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. Get info about the dataset for **\"Primary Production and Oxygen\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_ds[j].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And about the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_ds[j].data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_ds[j].coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press the ***TAB*** key (on your keyboard) for obtaining the other methods and properties of the dataset: \n",
    "\n",
    "Select the cell below, press enter and then type:\n",
    "```\n",
    "bio_ds[j]. (and press TAB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Set the configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the coordinates names in the cells above (check the ***ds.coords*** outputs) and set the correct variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the coordinate names (used later for accessing the data)\n",
    "lon_name = \"longitude\"\n",
    "lat_name = \"latitude\"\n",
    "time_name = \"time\"\n",
    "depth_name = \"depth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the variables names (check the ***ds.data_vars*** outputs): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable names\n",
    "\n",
    "# mass_concentration_of_chlorophyll_a_in_sea_water (CHL)\n",
    "chl_name = \"chl\"\n",
    "\n",
    "# mole_concentration_of_dissolved_molecular_oxygen_in_sea_water (O2)\n",
    "oxy_name = \"o2\"\n",
    "\n",
    "# mole_concentration_of_nitrate_in_sea_water (NO3)\n",
    "no3_name = \"no3\"\n",
    "\n",
    "# mole_concentration_of_phosphate_in_sea_water (PO4)\n",
    "po4_name = \"po4\"\n",
    "\n",
    "# net_primary_production_of_biomass_expressed_as_carbon_per_unit_volume_in_sea_water (PP)\n",
    "npp_name = \"nppv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Plot the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Configure the variables for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected latitude and longitude point \n",
    "lat_sel, lon_sel = 43, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the season: 0 = winter, 1 = summer\n",
    "season_index = 0\n",
    "\n",
    "# extract the columns for phyc and o2\n",
    "phyc_column = np.squeeze(pft_ds[season_index].phyc.sel(latitude=lat_sel, longitude=lon_sel, method=\"nearest\"))\n",
    "o2_column = np.squeeze(bio_ds[season_index].o2.sel(latitude=lat_sel, longitude=lon_sel, method=\"nearest\"))\n",
    "\n",
    "# Plot configuration\n",
    "width_inch = 16\n",
    "height_inch = 8\n",
    "\n",
    "# Axes labels\n",
    "fontsize = 14\n",
    "xlabel = 'longitude [deg]'\n",
    "ylabel = 'latitude [deg]'\n",
    "xlabelpad = 30\n",
    "ylabelpad = 60\n",
    "\n",
    "# Colorbar configuration\n",
    "cmap = \"jet\"\n",
    "cbar_position = \"right\"\n",
    "\n",
    "title_fontstyle = {\n",
    "    \"fontsize\": \"14\",\n",
    "    \"fontstyle\": \"italic\",\n",
    "    \"fontweight\": \"bold\",\n",
    "    \"pad\": 30\n",
    "}\n",
    "\n",
    "label_fontstyle = {\n",
    "    \"fontsize\": \"12\",\n",
    "    \"labelpad\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Plot the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(width_inch, height_inch))\n",
    "\n",
    "# set depths as negative\n",
    "depths_column = -o2_column.depth\n",
    "\n",
    "# define the signals\n",
    "sig1 = o2_column\n",
    "sig2 = phyc_column\n",
    "\n",
    "# set the colors\n",
    "color1 = 'r'\n",
    "color2 = 'b'\n",
    "\n",
    "# plot sig1\n",
    "ax1.plot(sig1,depths_column,color1)\n",
    "ax1.set_xlabel('o2', fontsize=14, color=color1)\n",
    "ax1.set_ylabel(\"Depth [m]\", fontsize=14)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.set_ylim([-300, 0])\n",
    "ax1.grid()\n",
    "\n",
    "# plot sig2\n",
    "ax2 = ax1.twiny()\n",
    "ax2.plot(sig2, depths_column, color2)\n",
    "ax2.set_xlabel(\"phyc\", fontsize=14, color=color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# set title\n",
    "title = \"phyc and o2 vertical profiles\"\n",
    "plt.title(title, **title_fontstyle)\n",
    "\n",
    "# output file\n",
    "output_file = os.path.join(out_path,title.replace(' ','_')) + \".png\"\n",
    "\n",
    "# save the output file\n",
    "plt.savefig(output_file)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Exercise n.2: Plot of transects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Define the area of interest\n",
    "\n",
    "You should select a fixed latitude and a range of longitude. You can choose also the range of depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_min = 0\n",
    "depth_max = 200\n",
    "\n",
    "lat_point = 43\n",
    "\n",
    "lon_min = 27.5\n",
    "lon_max = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Access the data and set the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index for the ds arrays\n",
    "j = 0\n",
    "\n",
    "# --- Choose the variable to plot (but comment the others with the symbol #): ---\n",
    "var_sel = nut_ds[j][no3_name]\n",
    "#var_sel = pft_ds[j][chl_name]\n",
    "#var_sel = bio_ds[j][oxy_name]\n",
    "\n",
    "# Set the variable min and max values for the plot and the colorbar (otherwise assign None):\n",
    "min_value, max_value = 0, 30\n",
    "#min_value, max_value = None, None\n",
    "#min_value, max_value = 260, 330\n",
    "\n",
    "\n",
    "dataset_3D = False\n",
    "if depth_name in var_sel.coords:\n",
    "    dataset_3D = True\n",
    "\n",
    "# --- Set up the arrays of coordinates for the selected dataset ---\n",
    "# \n",
    "lats = var_sel[lat_name]\n",
    "lons = var_sel[lon_name]\n",
    "times = var_sel[time_name]\n",
    "depths = var_sel[depth_name]\n",
    "\n",
    "# Extract the coordinates subsets\n",
    "ds = pft_ds[j] # the choosen dataset is not important now: we are extrating the coordinates\n",
    "lats_ds, lons_ds, depths_ds = ds[lat_name], ds[lon_name], ds[depth_name]\n",
    "\n",
    "# Set the indexes of coordinates\n",
    "depth_indexes = getRangeIndexes(depths_ds, depth_min, depth_max) if dataset_3D else [0]\n",
    "lat_indexes = np.abs(lats_ds-lat_point).argmin()\n",
    "lon_indexes = getRangeIndexes(lons_ds, lon_min, lon_max)\n",
    "time_indexes = 0\n",
    "\n",
    "lons_sel = ds[lon_name][lon_indexes]\n",
    "lats_sel = ds[lat_name][lat_indexes]\n",
    "depths_sel = -ds[depth_name][depth_indexes] # note the minus '-ds'. Why?\n",
    "\n",
    "# Set the variables to plot\n",
    "CHL = [ds[chl_name][time_indexes, depth_indexes, lat_indexes, lon_indexes] for ds in pft_ds]\n",
    "OXY = [ds[oxy_name][time_indexes, depth_indexes, lat_indexes, lon_indexes] for ds in bio_ds]\n",
    "NPP = [ds[npp_name][time_indexes, depth_indexes, lat_indexes, lon_indexes] for ds in bio_ds]\n",
    "PHO = [ds[po4_name][time_indexes, depth_indexes, lat_indexes, lon_indexes] for ds in nut_ds]\n",
    "NO3 = [ds[no3_name][time_indexes, depth_indexes, lat_indexes, lon_indexes] for ds in nut_ds]\n",
    "\n",
    "# Set the variable min and max values for the plot and the colorbar (otherwise assign None):\n",
    "min_value, max_value = None, None\n",
    "# min_value, max_value = 0, 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check a variable content, write its name in the cell below and press RUN (or shift-enter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the meshgrid for the plot \n",
    "X, Y = np.meshgrid(lons_sel, depths_sel)\n",
    "\n",
    "# Plot configuration\n",
    "width_inch = 14\n",
    "height_inch = 8\n",
    "\n",
    "# Axes labels\n",
    "fontsize = 14\n",
    "xlabel = \"longitude [degE]\"\n",
    "ylabel = \"depth [m]\"\n",
    "\n",
    "# Colorbar configuration\n",
    "cmap = \"jet\"\n",
    "cbar_position = \"right\"\n",
    "\n",
    "contour_levels = 100\n",
    "\n",
    "title_fontstyle = {\n",
    "    \"fontsize\": \"14\",\n",
    "    \"fontstyle\": \"italic\",\n",
    "    \"fontweight\": \"bold\",\n",
    "    \"pad\": 30\n",
    "}\n",
    "label_fontstyle = {\n",
    "    \"fontsize\": \"12\",\n",
    "#     \"labelpad\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Define the plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the transect (it uses many \"hidden\" variables...)\n",
    "def plot_transect(data, min_value, max_value,step_value):\n",
    "    fig = plt.figure(figsize=(width_inch, height_inch))\n",
    "\n",
    "    # Get the timestep\n",
    "    timestep = np.datetime_as_string(data.time,'h')\n",
    "\n",
    "    # Create the meshgrid for the plot \n",
    "    xx, yy = np.meshgrid(lons_sel, depths_sel)\n",
    "\n",
    "    # set variable limits\n",
    "    min_value = data.min() if min_value is None else min_value\n",
    "    max_value = data.max() if max_value is None else max_value\n",
    "    contour_levels = np.arange(min_value, max_value, step_value)\n",
    "    \n",
    "    ## contour fill\n",
    "    plt.contourf(xx, yy, data, contour_levels, cmap=cmap, vmin=min_value, vmax=max_value)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.colorbar(extend='both')\n",
    "\n",
    "    title_sel = data.long_name\n",
    "    var_str = \"{} [{}]\".format(title_sel, data.units)\n",
    "    title = ' - '.join((var_str, timestep))\n",
    "\n",
    "    plt.title(title, **title_fontstyle)\n",
    "    plt.xlabel(xlabel, **label_fontstyle)\n",
    "    plt.ylabel(ylabel, **label_fontstyle)\n",
    "\n",
    "    # output file\n",
    "    output_file = os.path.join(out_path,title.replace(' ','_')) + \".png\"\n",
    "\n",
    "    # save the output file\n",
    "    plt.savefig(output_file)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Plot the variables along a transect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're now ready to plot the section along the chosen transect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we plot a section of phytoplankton \n",
    "\n",
    "Phytoplankton, the autotrophic component of the marine ecosystem, needs light and nutrients to growth.\n",
    "\n",
    "These drivers greatly vary throughout the year, so that the distribution of the phytoplankton in the water column has a seasonal cycle.\n",
    "\n",
    "### a winter section of chlorophyll\n",
    "\n",
    "The chlorophyll is often used to describe phytoplankton biomass, however it is worth to remind that the CMEMS model provides also the carbon biomass which is more indicated to investigate carbon cycle, carbon production and trophic transfer efficiency through the food web.\n",
    "\n",
    "The winter section of chlorophyll shows \n",
    " - the effect of the Danube input on the western-most part of the transect.\n",
    " - no chlorophyll below ~100 m depth\n",
    " - the surface layers are well mixed (vertically), and we observe a Deep Chlorophyll Maximum around 40-50m depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = CHL[0]\n",
    "\n",
    "plot_transect(data, 0, 3, 0.01)\n",
    "#plot_transect(data, None, None, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and in summer ?\n",
    "We select the second time frame of the chlorophyll dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CHL[1]\n",
    "\n",
    "plot_transect(data, 0, 3, 0.01)\n",
    "#plot_transect(data, None, None, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At surface in summer, chlorophyll content is much lower than in winter because the strong stratification prevents vertical movements of nutrients. The Deep Chlorophyll Maximum is still present, and occurs where there's still sufficient light, and also nutrients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to the \"Table of contents\"](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>CONGRATULATIONS</b><br>\n",
    "  \n",
    "--- \n",
    "\n",
    "#### And thank you for your attention! :) We hope you enjoyed this training on the Mediterranean Biogeochemical model data provided through WEkEO by Copernicus Marine Service, for free, thanks to the European Commission.\n",
    "\n",
    "#### Now let's try to download new data and variables and to access and visualize them... you can try to make new maps and plots... and don't forget to try to the others\n",
    "\n",
    "We'd love to hear from you about how we could improve it (topics, tools, storytelling, format, speed etc). \n",
    "\n",
    "We do thank you in advance for your kind collaboration :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/all_partners_wekeo.png' alt='' align='center' width='75%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left;\">This project is licensed under the <a href=\"./LICENSE\">MIT License</a> <span style=\"float:right;\"><a href=\"https://github.com/wekeo/wekeo-jupyter-lab\">View on GitHub</a> | <a href=\"https://www.wekeo.eu/\">WEkEO Website</a> | <a href=mailto:support@wekeo.eu>Contact</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1575910822768,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
