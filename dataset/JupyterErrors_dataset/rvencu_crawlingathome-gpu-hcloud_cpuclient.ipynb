{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone \"https://github.com/TheoCoombes/crawlingathome\" crawlingathome_client\n",
    "! pip3 install -r crawlingathome_client/requirements.txt --no-cache-dir\n",
    "! rm requirements.txt\n",
    "! sudo apt-get install -y --no-install-recommends g++ protobuf-compiler libprotobuf-dev\n",
    "! git clone \"https://github.com/rvencu/crawlingathome-gpu-hcloud\"\n",
    "! wget https://raw.githubusercontent.com/rvencu/crawlingathome-gpu-hcloud/main/worker-requirements.txt\n",
    "! pip3 install -r ./worker-requirements.txt --no-cache-dir\n",
    "! pip3 install random_user_agent\n",
    "! yes | pip3 uninstall pillow\n",
    "! CC=\"cc -mavx2\" pip3 install -U --force-reinstall pillow-simd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Crawling at Home - CPU Worker\n",
    "YOUR_NICKNAME_FOR_THE_LEADERBOARD = \"Colab-CPU-worker\" #@param {type:\"string\"}\n",
    "CRAWLINGATHOME_SERVER_URL = \"http://cah.io.community/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gcld3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2207926/1277132912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgcld3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muuid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muuid1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gcld3'"
     ]
    }
   ],
   "source": [
    "import gc \n",
    "import os\n",
    "import ssl\n",
    "import sys\n",
    "import time\n",
    "import trio\n",
    "import uuid\n",
    "import ftfy\n",
    "import math\n",
    "import ujson\n",
    "import shutil\n",
    "import random\n",
    "import hashlib\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gcld3\n",
    "from glob import glob\n",
    "from uuid import uuid1\n",
    "from io import BytesIO\n",
    "from requests import get\n",
    "from threading import Thread\n",
    "import crawlingathome_client as cah\n",
    "from bloom_filter2 import BloomFilter\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from PIL import Image, ImageFile, UnidentifiedImageError \n",
    "\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from crawlingathome_client.temp import TempCPUWorker\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "\n",
    "sys.path.append('./crawlingathome-worker/')\n",
    "\n",
    "import asks\n",
    "asks.init(\"trio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracer(trio.abc.Instrument):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.exceptions = 0\n",
    "        self.requests = 0\n",
    "        self.downloads = 0\n",
    "        self.imgproc_duration = 0\n",
    "        self.download_duration = 0\n",
    "        self.error_duration = 0\n",
    "        self.bloom = 0\n",
    "\n",
    "    def task_exited(self, task):\n",
    "        if task.custom_sleep_data is not None:\n",
    "            if task.custom_sleep_data[0] in [1, 3]: # this is exception\n",
    "                self.exceptions += 1\n",
    "                self.error_duration += task.custom_sleep_data[2]\n",
    "            if task.custom_sleep_data[0] == 0: # this is image downloaded\n",
    "                self.download_duration += task.custom_sleep_data[1]\n",
    "                self.imgproc_duration += task.custom_sleep_data[2]\n",
    "                self.downloads += 1\n",
    "            if task.custom_sleep_data[0] == 3:\n",
    "                self.bloom += 1\n",
    "\n",
    "    def after_run(self):\n",
    "        rate = round(self.exceptions / (self.exceptions + self.downloads + sys.float_info.epsilon), 2)\n",
    "        avg_download = round(self.download_duration / (self.downloads + sys.float_info.epsilon), 2)\n",
    "        avg_process = round(self.imgproc_duration / (self.downloads + sys.float_info.epsilon), 2)\n",
    "        avg_error = round(self.error_duration / (self.exceptions + sys.float_info.epsilon), 2)\n",
    "        print(f\"[instrumentation] While scraping there were {self.exceptions} errors within {self.downloads + self.exceptions} candidates (error rate = {round(rate * 100,2)} %). {self.downloads} images were downloaded.\")\n",
    "        print(f\"[instrumentation] Cumulative image processing duration {round(self.imgproc_duration, 2)} s.\")\n",
    "        print(f\"[instrumentation] Average downloading time {avg_download} s/img, image processing time {avg_process} s/img, exceptions processing time {avg_error} s/link\")\n",
    "        print(f\"[instrumentation] Localbloom catched {self.bloom} urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # https://stackoverflow.com/a/47958486\n",
    "ssl_ctx = ssl.create_default_context()\n",
    "ssl_ctx.check_hostname = False\n",
    "ssl_ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def log(e):\n",
    "    with open(\"errors.txt\",\"a\") as f:\n",
    "        f.write(str(e.__class__.__name__) + \" \" + str(e) + \"\\n\")\n",
    "\n",
    "def remove_bad_chars(text):\n",
    "    # cleanup text so language can be detected\n",
    "    return \"\".join(c for c in text if c.isprintable())\n",
    "\n",
    "\n",
    "def parse_wat(content, start, line_count):\n",
    "    \"\"\"\n",
    "    This function checks the wat file content and attempts to extract valid candidates of image urls and alt texts\n",
    "\n",
    "    input: content = wat file content; start = start line number; line_count = how many lines to parse\n",
    "            usually a wat file is split in 2 halfs or 2 shards. shard 0 starts at the first line and line_count is about 1/2 of wat file lines\n",
    "            shard 1 starts at the middle of wat file and ends with the last line of wat\n",
    "    \n",
    "    output: a list of tuples (url, text, license)\n",
    "    \"\"\"\n",
    "\n",
    "    bloomip = \"116.202.162.146\"\n",
    "    bloom2ip = \"94.130.167.172\"\n",
    "\n",
    "    # clipped*.bin filters domains based on previous results of CLIP filtering.\n",
    "    # the domains are not likely to pass CLIP for either bad captions or the content is almost always NSFW\n",
    "\n",
    "    # failed-domains.bin contains failed domains, i.e. domains with image links and suitable alt texts that actually\n",
    "    # do not produce any image. domains that mayb dissapeared, or are good at blocking scrapers. List is also learned from\n",
    "    # past crawling effort\n",
    "\n",
    "    #clipped = [BloomFilter(max_elements=200000000, error_rate=0.05, filename=(x,-1)) for x in glob(\"crawlingathome-gpu-hcloud/blocklists/clipped*\")]\n",
    "    # blocked = BloomFilter(max_elements=10000000, error_rate=0.01, filename=(\"crawlingathome-gpu-hcloud/blocklists/failed-domains.bin\",-1))    \n",
    "\n",
    "    clpd = 0\n",
    "    valid_data = []\n",
    "    check_flag = set() # track urls and make them unique\n",
    "    content.seek(start)\n",
    "    for _ in range(line_count):\n",
    "        line = content.readline()\n",
    "        if \"IMG@\" not in line:\n",
    "            continue\n",
    "        line_str = line.strip()\n",
    "        data = ujson.loads(line_str)\n",
    "        # find all links inside the line\n",
    "        linklist = data[\"Envelope\"][\"Payload-Metadata\"][\"HTTP-Response-Metadata\"][\n",
    "            \"HTML-Metadata\"\n",
    "        ][\"Links\"]\n",
    "        # get base url\n",
    "        base_url = os.path.dirname(\n",
    "            data[\"Envelope\"][\"WARC-Header-Metadata\"][\"WARC-Target-URI\"]\n",
    "        )\n",
    "        license = \"?\"\n",
    "        for e in linklist:\n",
    "            if \"url\" in e and \"creativecommons.org/licenses/\" in e[\"url\"]:\n",
    "                license = e[\"url\"]\n",
    "            # reject links if ALT tag is not present\n",
    "            if \"alt\" not in e:\n",
    "                continue\n",
    "            url = e[\"url\"]\n",
    "            # reject links of svg, gif or scripted images content\n",
    "            if any( x in url for x in [\".svg\", \".gif\", \"data:image\", \"javascript:\"] ):\n",
    "                continue\n",
    "            try:\n",
    "                domain = urlparse(url).netloc\n",
    "            except:\n",
    "                continue\n",
    "            # detect ALT text language, we want to retain only English captions\n",
    "            alt_text = ftfy.fix_text(e[\"alt\"].replace(\"\\n\", \" \")).strip()\n",
    "            detector = gcld3.NNetLanguageIdentifier(min_num_bytes=6, max_num_bytes=1000)\n",
    "            detlang = \"\"\n",
    "            try:\n",
    "                res = detector.FindLanguage(alt_text)\n",
    "                detlang = res.language\n",
    "            except Exception as e:\n",
    "                alt_text = remove_bad_chars(alt_text)\n",
    "                res = detector.FindLanguage(alt_text)\n",
    "                detlang = res.language            # keep pair if we made it so far\n",
    "            if detlang == \"en\":\n",
    "                if not url.startswith(\"http\"):\n",
    "                    url = urljoin(base_url, url)\n",
    "                hash = hashlib.md5((url + alt_text).encode(\"utf-8\")).hexdigest()\n",
    "                if url not in check_flag:\n",
    "                    valid_data.append((url, alt_text, license, domain, hash))\n",
    "                    check_flag.add(url)\n",
    "            \n",
    "    print(f\"[debug] lenght of pairs to filter {len(valid_data)}\")\n",
    "    s = time.time()\n",
    "\n",
    "    # remove from valid_data elements rejected by clipped bloom server\n",
    "    with open('hash.txt', 'w') as f:\n",
    "        for item in valid_data:\n",
    "            f.write(item[-1].strip()+\"\\n\")\n",
    "    post = {\n",
    "        'file': ('hash.txt', open('hash.txt', 'rb')),\n",
    "        'key': (None, 'clipped'),\n",
    "    }\n",
    "    \n",
    "    failure = True\n",
    "    for _ in range(10):\n",
    "        response = requests.post(f'http://{bloomip}:8000/deduplicate/', files=post)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"bloom server error, retrying...\")\n",
    "            time.sleep(15)            \n",
    "        else:\n",
    "            failure = False\n",
    "            break\n",
    "    if failure:\n",
    "        print(f\"crash, cannot contact the clipped bloom server, please fix\")\n",
    "        return\n",
    "\n",
    "    valid_hashes = response.content.decode(\"utf-8\").split(\"\\n\")\n",
    "\n",
    "    print(f\"[debug] clipped bloom server returned {len(valid_hashes)} in {round(time.time()-s,3)} sec\")\n",
    "\n",
    "    valid_data = [t for t in {tuple(i) for i in valid_data}]\n",
    "    kept_data = []\n",
    "    clpd = len(valid_data)\n",
    "\n",
    "    for item in valid_data:\n",
    "        if item[-1].strip() in valid_hashes:\n",
    "            kept_data.append(item)\n",
    "            clpd -= 1\n",
    "    \n",
    "    s = time.time()\n",
    "    # remove from valid_data elements rejected by parsed bloom server\n",
    "    with open('hash.txt', 'w') as f:\n",
    "        for item in kept_data:\n",
    "            f.write(item[0].strip()+\"\\n\")\n",
    "    post = {\n",
    "        'file': ('hash.txt', open('hash.txt', 'rb')),\n",
    "        'key': (None, 'parsed'),\n",
    "    }\n",
    "    \n",
    "    failure = True\n",
    "    for _ in range(10):\n",
    "        response = requests.post(f'http://{bloom2ip}:8000/deduplicate/', files=post)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"bloom server error, retrying...\")\n",
    "            time.sleep(15)            \n",
    "        else:\n",
    "            failure = False\n",
    "            break\n",
    "    if failure:\n",
    "        print(f\"crash, cannot contact the parsed bloom server, please fix\")\n",
    "        sys.exit() # maybe fallback to file based filters? too depressing...\n",
    "\n",
    "    valid_urls = response.content.decode(\"utf-8\").split(\"\\n\")\n",
    "\n",
    "    print(f\"[debug] parsed bloom server returned {len(valid_urls)} in {round(time.time()-s,3)} sec\")\n",
    "\n",
    "    valid_data = [t for t in {tuple(i) for i in kept_data}]\n",
    "    final_kept_data = []\n",
    "    prsd = len(kept_data)\n",
    "\n",
    "    for item in kept_data:\n",
    "        if item[0].strip() in valid_urls:\n",
    "            final_kept_data.append(item)\n",
    "            prsd -= 1\n",
    "\n",
    "    print(f\"[debug] lenght of deduplicated pairs to return {len(final_kept_data)}\")\n",
    "\n",
    "    return (final_kept_data, clpd, prsd)  # use a dict in order to remove duplicate tuples from list\n",
    "\n",
    "\n",
    "def process_img_content(response, alt_text, license, sample_id):\n",
    "    \"\"\"\n",
    "    Function to process downloaded image. Use use PIL from pillow-simd \n",
    "        (faster than open cv that in return is faster than original pillow)\n",
    "    \n",
    "    input: web request response, ALT text, license and sample id\n",
    "\n",
    "    output: list of image parameters or None if image is rejected\n",
    "    \"\"\"\n",
    "    img_output_folder = \"save/images/\"\n",
    "\n",
    "    def _resize(im: Image):\n",
    "        width, height = im.size\n",
    "        ratio = min(width, height) / 224\n",
    "        new_width = int(round(width/ratio,0))\n",
    "        new_height = int(round(height/ratio,0))\n",
    "        im = im.resize((new_width, new_height), resample=Image.BICUBIC)\n",
    "        if new_width > 224 or new_height > 224:\n",
    "            left = (new_width - 224)/2\n",
    "            top = (new_height - 224)/2\n",
    "            right = (new_width + 224)/2\n",
    "            bottom = (new_height + 224)/2\n",
    "            # Crop the center of the image\n",
    "            im = im.crop((left, top, right, bottom))\n",
    "        return im\n",
    "    try:\n",
    "        # reject too small images\n",
    "        if len(response.content) < 5000:\n",
    "            return\n",
    "        img_data = BytesIO(response.content)\n",
    "        with Image.open(img_data) as im:\n",
    "            width, height = im.size\n",
    "            # reject if too large (might be a DOS decompression bomb)\n",
    "            if width * height > 89478484:\n",
    "                return\n",
    "            im_format = im.format\n",
    "            out_fname = f\"{img_output_folder}{str(sample_id)}.{im_format.lower()}\"\n",
    "            # reject if format is not in this list\n",
    "            if im_format not in [\"JPEG\", \"JPG\", \"PNG\", \"WEBP\"]:\n",
    "                return\n",
    "            if min(width, height) > 224:\n",
    "                im = _resize(im)\n",
    "            \n",
    "            # convert all images to RGB (necessary for CLIP, also CLIP is doing it again so do we need it here?)\n",
    "            if im.mode != \"RGB\":\n",
    "                im = im.convert(\"RGB\")\n",
    "            im.save(out_fname)\n",
    "    except (KeyError, UnidentifiedImageError):\n",
    "        return\n",
    "\n",
    "    return [str(sample_id), out_fname, response.url, alt_text, width, height, license]\n",
    "\n",
    "\n",
    "async def request_image(datas, start_sampleid):\n",
    "    \"\"\"\n",
    "    This function initiates many parallel async connections to try download the images from provided links\n",
    "    \n",
    "    input: dataset of validated links, the sample id to start with\n",
    "\n",
    "    output: list of lists with succesfully downloaded images and their parameters. this list is dumped on disk as json file\n",
    "    \"\"\"\n",
    "    tmp_data = []\n",
    "    limit = trio.CapacityLimiter(1000)\n",
    "\n",
    "    # change the number of parallel connections based on CPU speed, network capabilities, etc.\n",
    "    # the number of 192 is optimized for 1 vCPU droplet at Hetzner Cloud (code CX11)\n",
    "    session = asks.Session(connections=164, ssl_context=ssl_ctx)\n",
    "\n",
    "    software_names = [SoftwareName.CHROME.value]\n",
    "    operating_systems = [OperatingSystem.LINUX.value]   \n",
    "\n",
    "    user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=2000)\n",
    "    user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    # try to make the bot website friendly\n",
    "    session.headers = {\n",
    "        \"User-Agent\": user_agent,\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Referer\": \"https://google.com\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    async def _request(data, sample_id):\n",
    "        while True:\n",
    "            start=time.time()\n",
    "\n",
    "            url, alt_text, license, domain, hash = data\n",
    "            # the following 2 lines are related to Trio Instrument to capture events from multiple threads\n",
    "            task = trio.lowlevel.current_task()\n",
    "            try:\n",
    "\n",
    "                response = await session.get(url, timeout=10, connection_timeout=20)\n",
    "                dltime = round(time.time()-start, 2)\n",
    "                start=time.time()\n",
    "                proces = process_img_content(\n",
    "                    # tune timeout and connection_timeout to grab more or less files. shorter timeouts will exclude bad performing websites\n",
    "                    response, alt_text, license, sample_id\n",
    "                )\n",
    "                proctime = round(time.time()-start, 2)\n",
    "                task.custom_sleep_data = (0, dltime, proctime) # for success do not count errors\n",
    "                if proces is not None:\n",
    "                    tmp_data.append(proces)\n",
    "\n",
    "            except Exception as e:\n",
    "                log(e)\n",
    "                task.custom_sleep_data = (1, 0, round(time.time()-start,2)) # when exception is hit, count it\n",
    "            return\n",
    "\n",
    "    async with trio.open_nursery() as n:\n",
    "        for data in datas:\n",
    "            async with limit:\n",
    "                n.start_soon(_request, data, start_sampleid)\n",
    "            start_sampleid += 1\n",
    "            \n",
    "    # trio makes sure at this point all async tasks were executed\n",
    "    with open(f\".tmp/{uuid1()}.json\", \"w\") as f:\n",
    "        ujson.dump(tmp_data, f)\n",
    "    gc.collect()\n",
    "    \n",
    "    # add downloaded urls to parsed bloom server\n",
    "    bloom2ip = \"94.130.167.172\"\n",
    "    with open('hash.txt', 'w') as f:\n",
    "        for item in datas:\n",
    "            f.write(item[0].strip()+\"\\n\")\n",
    "    post = {\n",
    "        'file': ('hash.txt', open('hash.txt', 'rb')),\n",
    "        'key': (None, 'parsed'),\n",
    "    }\n",
    "    \n",
    "    failure = True\n",
    "    for _ in range(10):\n",
    "        response = requests.post(f'http://{bloom2ip}:8000/add/', files=post)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"bloom server error, retrying...\")\n",
    "            time.sleep(15)            \n",
    "        else:\n",
    "            failure = False\n",
    "            break\n",
    "    if failure:\n",
    "        print(f\"crash, cannot contact the parsed bloom server, please fix\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def dl_wat(valid_data, first_sample_id):\n",
    "    \"\"\"\n",
    "    This function initiates download attempt of validated parsed links\n",
    "    It launches multithreaded tasks by using trio module\n",
    "    \n",
    "    input: dataset of validated links, the sample id to start with\n",
    "\n",
    "    output: dataframe of downloaded images and their parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download every image available\n",
    "    processed_samples = []\n",
    "    #trio.run(request_image, valid_data, first_sample_id, instruments=[TrioProgress(len(valid_data), False)] )\n",
    "    trio.run( request_image, valid_data, first_sample_id, instruments=[Tracer()] )\n",
    "\n",
    "    for tmpf in glob(\".tmp/*.json\"):\n",
    "        processed_samples.extend(ujson.load(open(tmpf)))\n",
    "    return pd.DataFrame(\n",
    "        processed_samples,\n",
    "        columns=[\"SAMPLE_ID\", \"PATH\", \"URL\", \"TEXT\", \"HEIGHT\", \"WIDTH\", \"LICENSE\"],\n",
    "    )\n",
    "\n",
    "def upload(source: str, clientType: str, target: str):\n",
    "    with tarfile.open(f\"{source}.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(source, arcname=os.path.basename(source))\n",
    "    print(f\"client type is {clientType}\")\n",
    "    result = os.system(f\"rsync -av {source}.tar.gz {target}\")\n",
    "    if os.path.exists(f\"/home/crawl/{source}.tar.gz\"):\n",
    "        os.remove(f\"/home/crawl/{source}.tar.gz\")\n",
    "    if os.path.exists(f\"/home/crawl/{source}\"):\n",
    "        shutil.rmtree(f\"/home/crawl/{source}\", ignore_errors=True)\n",
    "    return result\n",
    "\n",
    "class FileData:\n",
    "    \"\"\"\n",
    "    Helper class to easily find wat file size, mid position, etc\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._line_to_position = [0]\n",
    "        self._length = 0\n",
    "\n",
    "        with open(self._filename, 'r') as f:\n",
    "            while f.readline():\n",
    "                self._line_to_position.append(f.tell())\n",
    "                self._length += 1\n",
    "    \n",
    "    def __getitem__(self, line):\n",
    "        return self._line_to_position[line]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize working folders\n",
    "output_folder = \"./save/\"\n",
    "img_output_folder = output_folder + \"images/\"\n",
    "\n",
    "# initialize client variables\n",
    "if YOUR_NICKNAME_FOR_THE_LEADERBOARD is None:\n",
    "    YOUR_NICKNAME_FOR_THE_LEADERBOARD = \"Anonymous\"\n",
    "\n",
    "print (f\"starting session under `{YOUR_NICKNAME_FOR_THE_LEADERBOARD}` nickname\")\n",
    "\n",
    "# connect to C@H server and initialize client\n",
    "client = TempCPUWorker(url=CRAWLINGATHOME_SERVER_URL, nickname=YOUR_NICKNAME_FOR_THE_LEADERBOARD)\n",
    "\n",
    "# initialize stats variables for previous job\n",
    "last = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        lastext = f\". Last job duration: {last}\"\n",
    "\n",
    "        start = time.time()\n",
    "        start0 = start\n",
    "\n",
    "        client.newJob()\n",
    "        client.downloadWat()\n",
    "        \n",
    "        result = 0\n",
    "        prefixes = {}\n",
    "\n",
    "        fd = FileData('shard.wat')\n",
    "        lines = int(len(fd)*0.5)\n",
    "\n",
    "        for shard_of_chunk in range(2):\n",
    "            # clear working folders for a new job\n",
    "            if os.path.exists(output_folder):\n",
    "                shutil.rmtree(output_folder, ignore_errors=True)\n",
    "            if os.path.exists(\".tmp\"):\n",
    "                shutil.rmtree(\".tmp\")\n",
    "\n",
    "            os.mkdir(output_folder)\n",
    "            os.mkdir(img_output_folder)\n",
    "            os.mkdir(\".tmp\")\n",
    "\n",
    "            # retrieve job details and determine what part of the wat file to parse\n",
    "            first_sample_id = np.int64(client.shards[shard_of_chunk][1][\"start_id\"])\n",
    "            last_sample_id = np.int64(client.shards[shard_of_chunk][1][\"end_id\"])\n",
    "            shard = client.shards[shard_of_chunk][1][\"shard\"]\n",
    "\n",
    "            if shard == 0:\n",
    "                start_index = fd[0]\n",
    "            if shard == 1:\n",
    "                start_index = fd[ int(len(fd)*0.5) ]\n",
    "\n",
    "            # compute output file names base\n",
    "            out_fname = f\"FIRST_SAMPLE_ID_IN_SHARD_{str(first_sample_id)}_LAST_SAMPLE_ID_IN_SHARD_{str(last_sample_id)}_{shard}_rvencu_full_wat\"\n",
    "            print(f\"[stats {shard_of_chunk}] Shard acquired in {round(time.time()-start,2)} sec\")\n",
    "            start = time.time()\n",
    "\n",
    "            # parse valid links from wat file\n",
    "            with open(\"shard.wat\", \"r\") as infile:\n",
    "                parsed_data, clpd, prsd = parse_wat(infile, start_index, lines)\n",
    "            print (f\"[stats {shard_of_chunk}] Parsed wat in {round(time.time()-start,2)} sec\")\n",
    "            start = time.time()\n",
    "\n",
    "            # convert to dataframe and save to disk (for statistics and generating blocking lists)\n",
    "            parsed_df = pd.DataFrame(parsed_data, columns=[\"URL\",\"TEXT\",\"LICENSE\",\"DOMAIN\",\"HASH\"])\n",
    "            parsed_df = parsed_df.drop_duplicates(subset=[\"URL\"])\n",
    "            parsed_df.to_csv(output_folder + out_fname + \"_parsed.csv\", index=False, sep=\"|\")\n",
    "\n",
    "            # attempt to spread out clusters of links pointing to the same domain name, improves crawling\n",
    "            random.shuffle(parsed_data)\n",
    "            \n",
    "            lastlinks = len(parsed_data)\n",
    "            print (f\"[stats {shard_of_chunk}] This job has {lastlinks} candidates after removing {clpd} and {prsd} via bloom filters\")\n",
    "        \n",
    "            # attempt to download validated links and save to disk for stats and blocking lists\n",
    "            dlparse_df = dl_wat( parsed_data, first_sample_id)\n",
    "            dlparse_df.to_csv(output_folder + out_fname + \".csv\", index=False, sep=\"|\")\n",
    "            dlparse_df.to_csv(output_folder + out_fname + \"_unfiltered.csv\", index=False, sep=\"|\")\n",
    "            print (f\"[stats {shard_of_chunk}] pairs retained {len(dlparse_df)} in {round(time.time() - start, 2)}\")\n",
    "            print (f\"[stats {shard_of_chunk}] scraping efficiency {len(dlparse_df)/(time.time() - start)} img/sec\")\n",
    "            print (f\"[stats {shard_of_chunk}] crawling efficiency {lastlinks/(time.time() - start)} links/sec\")\n",
    "\n",
    "            # at this point we finishes the CPU node job, need to make the data available for GPU worker\n",
    "            prefix = uuid.uuid4().hex\n",
    "            prefixes[str(client.shards[shard_of_chunk][0])] = f\"rsync {prefix}\"\n",
    "            os.mkdir(prefix)\n",
    "            os.system(f\"mv save/* {prefix}/\")\n",
    "            result += upload(prefix, \"CPU\", client.upload_address)\n",
    "        if result == 0:\n",
    "            client.completeJob(prefixes)\n",
    "\n",
    "        last = round(time.time() - start0)\n",
    "\n",
    "        print(f\"[stats] WAT job completed in {last} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print (\"Worker crashed\")\n",
    "        time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ba613d68b3b18985e02519c8e1689c7243aaf59a45b2d39d56345edb6b92440"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
