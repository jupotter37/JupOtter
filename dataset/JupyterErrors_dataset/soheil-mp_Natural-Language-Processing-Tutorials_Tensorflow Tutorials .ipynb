{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorials \n",
    "\n",
    "In this jupyter notebook we will go therough 14 different topics:\n",
    "1. **Installing Tensorflow on your machine**\n",
    "2. **Hello world example**\n",
    "3. **Tensors**\n",
    "4. **Session**\n",
    "5. **Linear functions**\n",
    "6. **Softmax function**\n",
    "7. **Cross Entropy**\n",
    "8. **Mini-batching**\n",
    "9. **Epochs**\n",
    "10. **Recurrent Linear Units (ReLUs)**\n",
    "11. **Deep Neural Network**\n",
    "12. **Save and Restore TensorFlow Models**\n",
    "13. **Finetune**\n",
    "14. **Dropout regularization**\n",
    "\n",
    "Before learning about each of them let's import all of the libraries that we are going to use here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Install Tensorflow\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "### 1.1. Windows\n",
    "\n",
    "Run the following commands to setup your environment:\n",
    "\n",
    "1. <code>conda create -n tensorflow python=3.5</code>\n",
    "2. <code>activate tensorflow</code>\n",
    "3. <code>conda install pandas matplotlib jupyter notebook scipy scikit-learn</code>\n",
    "4. <code>pip install tensorflow</code>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.2. OS X or Linux\n",
    "\n",
    "As usual, we'll be using Conda to install TensorFlow. You might already have a TensorFlow environment, but check to make sure you have all the necessary packages.\n",
    "\n",
    "Run the following commands to setup your environment:\n",
    "\n",
    "1. <code>conda create -n tensorflow python=3.5</code>\n",
    "2. <code>source activate tensorflow</code>\n",
    "3. <code>conda install pandas matplotlib jupyter notebook scipy scikit-learn</code>\n",
    "4. <code>pip install tensorflow</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Hello World\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now let's write a tensorflow code for printing \"hello world\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a constant tensor \n",
    "hello_constant = tf.constant('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "# Create a Session and run the tensor inside of it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Tensor\n",
    "\n",
    "<hr>\n",
    "\n",
    "In TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. In the case of <code>hello_constant = tf.constant('Hello World!')</code>, hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234\n"
     ]
    }
   ],
   "source": [
    "# Create a 0-dimensional constant tensor \n",
    "A = tf.constant(1234) \n",
    "\n",
    "# Create a Session and run the tensor inside of it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(A)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 456 789]\n"
     ]
    }
   ],
   "source": [
    "# Create a 1-dimensional constant tensor \n",
    "B = tf.constant([123,456,789]) \n",
    "\n",
    "# Create a Session and run the tensor inside of it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(B)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123 456 789]\n",
      " [222 333 444]]\n"
     ]
    }
   ],
   "source": [
    "# Create a 2-dimensional constant tensor \n",
    "C = tf.constant([[123,456,789], [222,333,444]])\n",
    "\n",
    "# Create a Session and run the tensor inside of it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(C)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>tf.constant()</code> is one of many TensorFlow operations you will use in this lesson. The tensor returned by <code>tf.constant()</code> is called a constant tensor, because the value of the tensor never changes.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 4. Session\n",
    "\n",
    "<hr>\n",
    "\n",
    "TensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:\n",
    "\n",
    "<img width=\"500px\" src=\"assets/session.png\">\n",
    "\n",
    "A \"TensorFlow Session\", as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "# Create a constant tensor \n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "# Create a Session and run the tensor inside of it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he code has already created the tensor, <code>hello_constant</code>, from the previous lines. The next step is to evaluate the tensor in a session.\n",
    "\n",
    "The code creates a session instance, <code>sess</code>, using <code>tf.Session</code>. The <code>sess.run()</code> function then evaluates the tensor and returns the results.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5. Linear functions\n",
    "\n",
    "<hr>\n",
    "\n",
    "The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as \n",
    "\n",
    "<code style=\"background:white; font-size:24px; display:flex; margin-left:450px; margin-right:45%;\">y=Wx+b</code>\n",
    "\n",
    "Here, W is a matrix of the weights connecting two layers. The output y, the input x, and the biases b are all vectors.\n",
    "\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. This is where tf.Variable class comes in.\n",
    "\n",
    "**tf.Variable()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create a variable that holds number 5\n",
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Open a session and run the initialization in it\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the tf.truncated_normal() function to generate random numbers from a normal distribution.\n",
    "\n",
    "**tf.truncated_normal()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable which is wraped around a truncated normal\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.\n",
    "\n",
    "**tf.zeros()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable which is wraped around zeros\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6. TensorFlow Softmax\n",
    "\n",
    "<hr>\n",
    "\n",
    "The softmax function squashes it's inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.\n",
    "\n",
    "<img width=\"400px\" src=\"assets/softmax-input-output.png\">\n",
    "\n",
    "We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45659032 0.3382504  0.20515925]\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax function to the following array\n",
    "values = [1.2, 0.9, 0.4]\n",
    "x = tf.nn.softmax(values)\n",
    "\n",
    "# Open a session and run the softmax in it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy as that! tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45659032 0.3382504  0.20515925]\n"
     ]
    }
   ],
   "source": [
    "# Again use the softmax function for the following array\n",
    "# However this use placeholder\n",
    "logit_data = [1.2, 0.9, 0.4]\n",
    "logits = tf.placeholder(tf.float32)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "# Open a session and run the softmax in it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.zeros() function returns a tensor with all zeros.\n",
    "\n",
    "You'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you'll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function for returning weights\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "# Write a function for returning biases\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "# Write a function for returning linear function\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "\n",
    "# Get the number of labels\n",
    "n_labels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for features and labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights and biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the linear function (xW + b)\n",
    "logits = linear(features, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "### Make the features and labels ready for training\n",
    "\n",
    "# Initialize an empty array for features and labels\n",
    "mnist_features = []\n",
    "mnist_labels = []\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# Iterating through features and labels\n",
    "for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "    # If it's for the first <n>th labels then add features and labels\n",
    "    if mnist_label[:n_labels].any():\n",
    "        mnist_features.append(mnist_feature)\n",
    "        mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "# Assign the features and labels for training\n",
    "train_features = mnist_features\n",
    "train_labels = mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.5296101570129395\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 7. Cross Entropy\n",
    "\n",
    "<hr>\n",
    "\n",
    "As with the softmax function, TensorFlow has a function to do the cross entropy calculations for us.\n",
    "\n",
    "\n",
    "<img width=\"500px\" src=\"assets/cross-entropy-diagram.png\">\n",
    "\n",
    "Let's take what you learned from the video and create a cross entropy function in TensorFlow. To create a cross entropy function in TensorFlow, you'll need to use two new functions:\n",
    "\n",
    "- tf.reduce_sum()\n",
    "- tf.log()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Apply reduce_sum to the following array\n",
    "values = [1, 2, 3, 4, 5]\n",
    "x = tf.reduce_sum(values)  \n",
    "\n",
    "# Open a session and run the tensor in it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.reduce_sum() function takes an array of numbers and sums them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6051702\n"
     ]
    }
   ],
   "source": [
    "# Apply log to the following value\n",
    "value = 100.0\n",
    "x = tf.log(value) \n",
    "\n",
    "# Open a session and run the tensor in it\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does exactly what you would expect it to do. tf.log() takes the natural log of a number. Now print the cross entropy using softmax_data and one_hot_encod_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667497\n"
     ]
    }
   ],
   "source": [
    "# Softmax and one hot encoded data\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "# Create placeholder for softmax and one hot\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# Get the cross-entropy\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Printing the cross entropy of given arrays\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 8. Mini-batching\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this section, you'll go over what mini-batching is and how to apply it in TensorFlow.\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features \n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "# Get the labels \n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder for features and labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does None do here?\n",
    "\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the batches function to batch features and labels. The function should return each batch with a maximum size of batch_size. To help you with the quiz, look at the following example output of a working batches function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample example for features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "\n",
    "# Sample example for labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating batches\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    # Make sure the length of features and labels are the same\n",
    "    assert len(features) == len(labels)\n",
    "    \n",
    "    # Initialize an empty array for batches\n",
    "    output_batches = []\n",
    "    \n",
    "    # Get the lenght our dataset\n",
    "    sample_size = len(features)\n",
    "    \n",
    "    # Loop batch times\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        \n",
    "        # Get the next index\n",
    "        end_i = start_i + batch_size\n",
    "        \n",
    "        # Create the batch\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        \n",
    "        # Append the batch to output_batches array\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Craete Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the variable initialization\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. You'll go over this subject in the next section where we talk about \"epochs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 9. Epochs\n",
    "\n",
    "<hr>\n",
    "\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder for features and labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a placeholder for learning rate\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# Define the loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 128\n",
    "epochs = 80\n",
    "learn_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batches for the training\n",
    "train_batches = batches(batch_size, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    - Cost: 1.4      Valid Accuracy: 0.718\n",
      "Epoch: 1    - Cost: 0.97     Valid Accuracy: 0.794\n",
      "Epoch: 2    - Cost: 0.813    Valid Accuracy: 0.825\n",
      "Epoch: 3    - Cost: 0.728    Valid Accuracy: 0.845\n",
      "Epoch: 4    - Cost: 0.675    Valid Accuracy: 0.856\n",
      "Epoch: 5    - Cost: 0.639    Valid Accuracy: 0.863\n",
      "Epoch: 6    - Cost: 0.613    Valid Accuracy: 0.868\n",
      "Epoch: 7    - Cost: 0.591    Valid Accuracy: 0.871\n",
      "Epoch: 8    - Cost: 0.572    Valid Accuracy: 0.875\n",
      "Epoch: 9    - Cost: 0.554    Valid Accuracy: 0.877\n",
      "Epoch: 10   - Cost: 0.539    Valid Accuracy: 0.879\n",
      "Epoch: 11   - Cost: 0.525    Valid Accuracy: 0.881\n",
      "Epoch: 12   - Cost: 0.511    Valid Accuracy: 0.882\n",
      "Epoch: 13   - Cost: 0.499    Valid Accuracy: 0.886\n",
      "Epoch: 14   - Cost: 0.487    Valid Accuracy: 0.887\n",
      "Epoch: 15   - Cost: 0.476    Valid Accuracy: 0.889\n",
      "Epoch: 16   - Cost: 0.466    Valid Accuracy: 0.891\n",
      "Epoch: 17   - Cost: 0.456    Valid Accuracy: 0.893\n",
      "Epoch: 18   - Cost: 0.446    Valid Accuracy: 0.894\n",
      "Epoch: 19   - Cost: 0.437    Valid Accuracy: 0.895\n",
      "Epoch: 20   - Cost: 0.429    Valid Accuracy: 0.895\n",
      "Epoch: 21   - Cost: 0.421    Valid Accuracy: 0.895\n",
      "Epoch: 22   - Cost: 0.413    Valid Accuracy: 0.895\n",
      "Epoch: 23   - Cost: 0.405    Valid Accuracy: 0.898\n",
      "Epoch: 24   - Cost: 0.398    Valid Accuracy: 0.898\n",
      "Epoch: 25   - Cost: 0.391    Valid Accuracy: 0.901\n",
      "Epoch: 26   - Cost: 0.385    Valid Accuracy: 0.901\n",
      "Epoch: 27   - Cost: 0.378    Valid Accuracy: 0.901\n",
      "Epoch: 28   - Cost: 0.372    Valid Accuracy: 0.901\n",
      "Epoch: 29   - Cost: 0.366    Valid Accuracy: 0.901\n",
      "Epoch: 30   - Cost: 0.361    Valid Accuracy: 0.901\n",
      "Epoch: 31   - Cost: 0.355    Valid Accuracy: 0.901\n",
      "Epoch: 32   - Cost: 0.35     Valid Accuracy: 0.902\n",
      "Epoch: 33   - Cost: 0.345    Valid Accuracy: 0.902\n",
      "Epoch: 34   - Cost: 0.34     Valid Accuracy: 0.904\n",
      "Epoch: 35   - Cost: 0.335    Valid Accuracy: 0.904\n",
      "Epoch: 36   - Cost: 0.331    Valid Accuracy: 0.904\n",
      "Epoch: 37   - Cost: 0.326    Valid Accuracy: 0.905\n",
      "Epoch: 38   - Cost: 0.322    Valid Accuracy: 0.905\n",
      "Epoch: 39   - Cost: 0.318    Valid Accuracy: 0.906\n",
      "Epoch: 40   - Cost: 0.314    Valid Accuracy: 0.906\n",
      "Epoch: 41   - Cost: 0.311    Valid Accuracy: 0.906\n",
      "Epoch: 42   - Cost: 0.307    Valid Accuracy: 0.906\n",
      "Epoch: 43   - Cost: 0.303    Valid Accuracy: 0.906\n",
      "Epoch: 44   - Cost: 0.3      Valid Accuracy: 0.906\n",
      "Epoch: 45   - Cost: 0.297    Valid Accuracy: 0.906\n",
      "Epoch: 46   - Cost: 0.294    Valid Accuracy: 0.906\n",
      "Epoch: 47   - Cost: 0.291    Valid Accuracy: 0.907\n",
      "Epoch: 48   - Cost: 0.288    Valid Accuracy: 0.907\n",
      "Epoch: 49   - Cost: 0.285    Valid Accuracy: 0.907\n",
      "Epoch: 50   - Cost: 0.282    Valid Accuracy: 0.908\n",
      "Epoch: 51   - Cost: 0.28     Valid Accuracy: 0.908\n",
      "Epoch: 52   - Cost: 0.277    Valid Accuracy: 0.908\n",
      "Epoch: 53   - Cost: 0.275    Valid Accuracy: 0.908\n",
      "Epoch: 54   - Cost: 0.272    Valid Accuracy: 0.908\n",
      "Epoch: 55   - Cost: 0.27     Valid Accuracy: 0.908\n",
      "Epoch: 56   - Cost: 0.268    Valid Accuracy: 0.908\n",
      "Epoch: 57   - Cost: 0.266    Valid Accuracy: 0.908\n",
      "Epoch: 58   - Cost: 0.264    Valid Accuracy: 0.908\n",
      "Epoch: 59   - Cost: 0.262    Valid Accuracy: 0.908\n",
      "Epoch: 60   - Cost: 0.26     Valid Accuracy: 0.909\n",
      "Epoch: 61   - Cost: 0.258    Valid Accuracy: 0.909\n",
      "Epoch: 62   - Cost: 0.256    Valid Accuracy: 0.909\n",
      "Epoch: 63   - Cost: 0.255    Valid Accuracy: 0.91 \n",
      "Epoch: 64   - Cost: 0.253    Valid Accuracy: 0.91 \n",
      "Epoch: 65   - Cost: 0.251    Valid Accuracy: 0.91 \n",
      "Epoch: 66   - Cost: 0.25     Valid Accuracy: 0.911\n",
      "Epoch: 67   - Cost: 0.248    Valid Accuracy: 0.911\n",
      "Epoch: 68   - Cost: 0.247    Valid Accuracy: 0.911\n",
      "Epoch: 69   - Cost: 0.245    Valid Accuracy: 0.911\n",
      "Epoch: 70   - Cost: 0.244    Valid Accuracy: 0.911\n",
      "Epoch: 71   - Cost: 0.243    Valid Accuracy: 0.911\n",
      "Epoch: 72   - Cost: 0.241    Valid Accuracy: 0.911\n",
      "Epoch: 73   - Cost: 0.24     Valid Accuracy: 0.911\n",
      "Epoch: 74   - Cost: 0.239    Valid Accuracy: 0.91 \n",
      "Epoch: 75   - Cost: 0.238    Valid Accuracy: 0.911\n",
      "Epoch: 76   - Cost: 0.237    Valid Accuracy: 0.911\n",
      "Epoch: 77   - Cost: 0.236    Valid Accuracy: 0.911\n",
      "Epoch: 78   - Cost: 0.234    Valid Accuracy: 0.911\n",
      "Epoch: 79   - Cost: 0.233    Valid Accuracy: 0.912\n",
      "Test Accuracy: 0.9090999960899353\n"
     ]
    }
   ],
   "source": [
    "### Training\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the variable initialization\n",
    "    sess.run(init)\n",
    "\n",
    "    # Loop epoch times\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches and get the feature and label\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            \n",
    "            # Create a feed_dict for training optimizer\n",
    "            train_feed_dict = {features: batch_features,\n",
    "                               labels: batch_labels,\n",
    "                               learning_rate: learn_rate}\n",
    "            \n",
    "            # Train the optimizer\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Get the cost\n",
    "        current_cost = sess.run(cost, feed_dict={features: batch_features, labels: batch_labels})\n",
    "        \n",
    "        # Get the accuracy for validation set\n",
    "        valid_accuracy = sess.run(accuracy, feed_dict={features: valid_features, labels: valid_labels})\n",
    "        \n",
    "        # Print the cost and accuracy\n",
    "        print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(epoch_i,\n",
    "                                                                        current_cost,\n",
    "                                                                        valid_accuracy))\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 10. ReLUs\n",
    "\n",
    "<hr>\n",
    "\n",
    "TensorFlow provides the ReLU function as tf.nn.relu(), as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code># Hidden Layer with ReLU activation function</code>\n",
    "\n",
    "<code>hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)</code>\n",
    "\n",
    "<code>hidden_layer = tf.nn.relu(hidden_layer)</code>\n",
    "\n",
    "<code>output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code applies the tf.nn.relu() function to the hidden_layer, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n",
    "Below you'll use the ReLU function to turn a linear single layer network into a non-linear multilayer network.\n",
    "\n",
    "<img width=\"500px\" src=\"assets/relu-network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for hidden layer & output\n",
    "hidden_layer_weights = [[0.1, 0.2, 0.4],\n",
    "                        [0.4, 0.6, 0.6],\n",
    "                        [0.5, 0.9, 0.1],\n",
    "                        [0.8, 0.2, 0.8]]\n",
    "\n",
    "out_weights = [[0.1, 0.6],\n",
    "               [0.2, 0.1],\n",
    "               [0.7, 0.9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "weights = [tf.Variable(hidden_layer_weights),\n",
    "           tf.Variable(out_weights)]\n",
    "\n",
    "biases = [tf.Variable(tf.zeros(3)),\n",
    "          tf.Variable(tf.zeros(2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], \n",
    "                        [-1.0, -2.0, -3.0, -4.0], \n",
    "                        [11.0, 12.0, 13.0, 14.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st hidden layer\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "\n",
    "# Apply RELU\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# Get the output\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11      8.440001]\n",
      " [ 0.        0.      ]\n",
      " [24.010002 38.239998]]\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Print the logits\n",
    "    print(sess.run(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 11. Deep Neural Network\n",
    "\n",
    "<hr>\n",
    "\n",
    "You've seen how to build a logistic classifier using TensorFlow. Now you're going to see how to use the logistic classifier to build a deep neural network. In the following walkthrough, we'll step through TensorFlow code written to classify the letters in the MNIST database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "n_hidden_layer = 256 # layer number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "           'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))}\n",
    "biases = {'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "          'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder for input and output\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Hidden layer\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "\n",
    "# Apply RELU\n",
    "layer_1 = tf.nn.relu(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Session (launching the graph)\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the variable initialization\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Loop epoch times\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Get the total number of batches\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            # Get the input and output\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Going from one layer to two is easy. Adding more layers to the network allows you to solve more complicated problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 12. Save and Restore TensorFlow Models\n",
    "\n",
    "<hr>\n",
    "\n",
    "Training a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again!\n",
    "\n",
    "Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system.\n",
    "\n",
    "Let's start with a simple example of saving weights and bias Tensors. For the first example you'll just save two variables. Later examples will save all the weights in a practical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: \n",
      " [[ 1.0848122   0.48442674 -0.21687426]\n",
      " [-0.89066917  0.35860372 -0.05830318]] \n",
      "\n",
      "Bias: \n",
      " [ 0.5093955   0.02881323 -0.9548363 ]\n"
     ]
    }
   ],
   "source": [
    "# Path of the saved model\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Create two tensor variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights\n",
    "    print(\"Weights: \\n\", sess.run(weights), \"\\n\")\n",
    "    \n",
    "    # Show the values of bias\n",
    "    print(\"Bias: \\n\", sess.run(bias))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensors weights and bias are set to random values using the tf.truncated_normal() function. The values are then saved to the save_file location, \"model.ckpt\", using the tf.train.Saver.save() function. (The \".ckpt\" extension stands for \"checkpoint\".)\n",
    "\n",
    "If you're using TensorFlow 0.11.0RC1 or newer, a file called \"model.ckpt.meta\" will also be created. This file contains the TensorFlow graph.\n",
    "\n",
    "Now that the Tensor Variables are saved, let's load them back into a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Weights: \n",
      " [[ 1.1212208  -0.5115597  -0.2281799 ]\n",
      " [ 0.9504565   0.49948028 -0.7823087 ]] \n",
      "\n",
      "Bias: \n",
      " [-0.0563532  -1.6072345   0.83857113]\n"
     ]
    }
   ],
   "source": [
    "# Path of the saved model\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create two tensor variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights\n",
    "    print(\"Weights: \\n\", sess.run(weights), \"\\n\")\n",
    "    \n",
    "    # Show the values of bias\n",
    "    print(\"Bias: \\n\", sess.run(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice you still need to create the weights and bias Tensors in Python. The tf.train.Saver.restore() function loads the saved data into weights and bias.\n",
    "\n",
    "Since tf.train.Saver.restore() sets all the TensorFlow Variables, you don't need to call tf.global_variables_initializer().\n",
    "\n",
    "Let's see how to train a model and save its weights.\n",
    "\n",
    "First start with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "# Create placeholder fo features and labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define the loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate the accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train that model, then save the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Validation Accuracy: 0.18\n",
      "Epoch 10  - Validation Accuracy: 0.32\n",
      "Epoch 20  - Validation Accuracy: 0.46\n",
      "Epoch 30  - Validation Accuracy: 0.54\n",
      "Epoch 40  - Validation Accuracy: 0.59\n",
      "Epoch 50  - Validation Accuracy: 0.63\n",
      "Epoch 60  - Validation Accuracy: 0.66\n",
      "Epoch 70  - Validation Accuracy: 0.69\n",
      "Epoch 80  - Validation Accuracy: 0.70\n",
      "Epoch 90  - Validation Accuracy: 0.72\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "# Path for the saved model\n",
    "save_file = './train_model.ckpt'\n",
    "\n",
    "# Hyper parameters\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Loop epoch times\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Get the total number of batches\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            # Get the feature and label\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Train the optimizer\n",
    "            sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print validation accuracy every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(accuracy, feed_dict={features: mnist.validation.images,\n",
    "                                                           labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {:.2f}'.format(epoch, valid_accuracy))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the weights and bias from memory, then check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./train_model.ckpt\n",
      "Test Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Get the accuracy for the test set\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You now know how to save and load a trained model in TensorFlow. Let's look at loading weights and biases into modified models in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 13. Finetune\n",
    "\n",
    "<hr>\n",
    "\n",
    "Sometimes you might want to adjust, or \"finetune\" a model that you have already trained and saved.\n",
    "\n",
    "However, loading saved Variables directly into a modified model can generate errors. Let's go over how to avoid these problems.\n",
    "\n",
    "TensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name <Type>, and then give the name <Type>_<number> for the subsequent nodes. Let's see how this can affect loading a model with a different order of weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: Variable:0\n",
      "Save Bias: Variable_1:0\n",
      "Load Weights: Variable_1:0\n",
      "Load Bias: Variable:0\n",
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-14-703b4ac2493d>:34) ]]\n\nCaused by op 'save/Assign', defined at:\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-703b4ac2493d>\", line 34, in <module>\n    saver = tf.train.Saver()\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-14-703b4ac2493d>:34) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[{{node save/Assign}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1276\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-14-703b4ac2493d>:34) ]]\n\nCaused by op 'save/Assign', defined at:\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-703b4ac2493d>\", line 34, in <module>\n    saver = tf.train.Saver()\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-14-703b4ac2493d>:34) ]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-703b4ac2493d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Load the weights and bias - ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1312\u001b[0;31m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-14-703b4ac2493d>:34) ]]\n\nCaused by op 'save/Assign', defined at:\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/anaconda3/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-703b4ac2493d>\", line 34, in <module>\n    saver = tf.train.Saver()\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 64, in assign\n    use_locking=use_locking, name=name)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[node save/Assign (defined at <ipython-input-14-703b4ac2493d>:34) ]]\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Path to the saved model\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Create two tensor variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load the weights and bias - ERROR\n",
    "    saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above returns out an error. You'll notice that the name properties for weights and bias are different than when you saved the model. This is why the code produces the \"Assign requires shapes of both tensors to match\" error. The code saver.restore(sess, save_file) is trying to load weight data into bias and bias data into weights.\n",
    "\n",
    "Instead of letting TensorFlow set the name property, let's set it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: weights_0:0\n",
      "Save Bias: bias_0:0\n",
      "Load Weights: weights_0:0\n",
      "Load Bias: bias_0:0\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "Loaded Weights and Bias successfully.\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Path to the saved model\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Create two tensor variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create two tensor variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! The Tensor names match and the data loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 14. Dropout\n",
    "\n",
    "<hr>\n",
    "\n",
    "Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units' incoming and outgoing connections. Figure 1 illustrates how dropout works.\n",
    "\n",
    "<img width=\"600px\" src=\"assets/dropout-node.jpeg\">\n",
    "\n",
    "TensorFlow provides the tf.nn.dropout() function, which you can use to implement dropout.\n",
    "\n",
    "Let's look at an example of how to use tf.nn.dropout().\n",
    "\n",
    "<code>keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])</code>\n",
    "\n",
    "The code above illustrates how to apply dropout to a neural network.\n",
    "\n",
    "The tf.nn.dropout() function takes in two parameters:\n",
    "\n",
    "hidden_layer: the tensor to which you would like to apply dropout\n",
    "keep_prob: the probability of keeping (i.e. not dropping) any given unit\n",
    "keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.\n",
    "\n",
    "During training, a good starting value for keep_prob is 0.5.\n",
    "\n",
    "During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model.\n",
    "\n",
    "Now let's start with the code from the ReLU Quiz and applying a dropout layer. Build a model with a ReLU layer and dropout layer using the keep_prob placeholder to pass in a probability of 0.5. Print the logits from the model.\n",
    "\n",
    "Note: Output will be different every time the code is run. This is caused by dropout randomizing the units it drops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for hidden layer and output layer\n",
    "hidden_layer_weights = [[0.1, 0.2, 0.4],\n",
    "                        [0.4, 0.6, 0.6],\n",
    "                        [0.5, 0.9, 0.1],\n",
    "                        [0.8, 0.2, 0.8]]\n",
    "\n",
    "out_weights = [[0.1, 0.6],\n",
    "               [0.2, 0.1],\n",
    "               [0.7, 0.9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "weights = [tf.Variable(hidden_layer_weights),\n",
    "           tf.Variable(out_weights)]\n",
    "\n",
    "biases = [tf.Variable(tf.zeros(3)),\n",
    "          tf.Variable(tf.zeros(2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], \n",
    "                        [0.1, 0.2, 0.3, 0.4], \n",
    "                        [11.0, 12.0, 13.0, 14.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder for keep_prob\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st hidden layer\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "\n",
    "# Apply RELU activation function\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "# Apply dropout\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "# Get the logits\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.1         6.6000004 ]\n",
      " [ 0.30800003  0.7700001 ]\n",
      " [48.020004   76.479996  ]]\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Print the logits\n",
    "    print(sess.run(logits, feed_dict={keep_prob: 0.5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
