{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 2.2.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Visible devices cannot be modified after being initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-198946d35cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mlogical_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_logical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Physical GPUs,\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogical_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Logical GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/imgentf2/lib/python3.6/site-packages/tensorflow/python/framework/config.py\u001b[0m in \u001b[0;36mset_visible_devices\u001b[0;34m(devices, device_type)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRuntime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malready\u001b[0m \u001b[0minitialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \"\"\"\n\u001b[0;32m--> 443\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/imgentf2/lib/python3.6/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mset_visible_devices\u001b[0;34m(self, devices, device_type)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m       raise RuntimeError(\n\u001b[0;32m-> 1294\u001b[0;31m           \"Visible devices cannot be modified after being initialized\")\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_visible_device_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisible_device_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Visible devices cannot be modified after being initialized"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Dense, Input, LeakyReLU, Reshape, Conv2D, Dense, Embedding\n",
    "from tensorflow.keras.layers import UpSampling2D, AveragePooling2D, GlobalAveragePooling2D, Flatten\n",
    "from tensorflow.keras.activations import relu, tanh\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train,  ds_info = tfds.load('cifar10', split='train', \n",
    "                                         as_supervised=True, shuffle_files=True, with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = IMAGE_WIDTH = 32\n",
    "IMAGE_SHAPE = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 200\n",
    "\n",
    "def preprocess(image, label):\n",
    "\n",
    "    # normalize\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image-127.5)/127.5\n",
    "    \n",
    "    label = tf.cast(label, tf.int32)\n",
    "    return image, label\n",
    "\n",
    "train_dataset = ds_train.map(preprocess, \n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).repeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNorm(tf.keras.constraints.Constraint):\n",
    "    def __init__(self, n_iter=5):\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def call(self, input_weights):\n",
    "        w = tf.reshape(input_weights, (-1, input_weights.shape[-1]))\n",
    "        u = tf.random.normal((w.shape[0], 1))\n",
    "        for _ in range(self.n_iter):\n",
    "            v = tf.matmul(w, u, transpose_a=True)\n",
    "            v /= tf.norm(v)\n",
    "            \n",
    "            u = tf.matmul(w, v)\n",
    "            u /= tf.norm(u)\n",
    "            \n",
    "        spec_norm = tf.matmul(u, tf.matmul(w, v), transpose_a=True)\n",
    "        return input_weights/spec_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spectral normalization, orthogonal initialization and orthogonal reguralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0028087753>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OrthogonalReguralizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, beta=1e-4):\n",
    "        self.beta = beta\n",
    "        \n",
    "    def __call__(self, input_tensor):\n",
    "        c = input_tensor.shape[-1]\n",
    "        x = tf.reshape(input_tensor, (-1, c))\n",
    "        \n",
    "        ortho_loss = tf.matmul(x, x, transpose_a=True) * (1 -tf.eye(c))\n",
    "        return self.beta * tf.norm(ortho_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'beta': self.beta}\n",
    "\n",
    "x = tf.random.normal((4, 3, 3, 5))\n",
    "OrthogonalReguralizer()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_kernel_cfg={\n",
    "    'kernel_initializer' : tf.keras.initializers.Orthogonal,\n",
    "    'kernel_constraint' : SpectralNorm(),\n",
    "    'kernel_regularizer' : OrthogonalReguralizer()\n",
    "}\n",
    "\n",
    "d_kernel_cfg={\n",
    "    'kernel_initializer' : tf.keras.initializers.Orthogonal,\n",
    "    'kernel_constraint' : SpectralNorm(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Layer):\n",
    "    def __init__(self):\n",
    "        super(SelfAttention, self).__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        n, h, w, c = input_shape\n",
    "        self.n_feats = h * w\n",
    "        self.conv_theta = Conv2D(c//8, 1, padding='same', **g_kernel_cfg,  name='Conv_Theta')\n",
    "        self.conv_phi = Conv2D(c//8, 1, padding='same', **g_kernel_cfg, name='Conv_Phi')\n",
    "        self.conv_g = Conv2D(c//2, 1, padding='same', **g_kernel_cfg, name='Conv_G')\n",
    "        self.conv_attn_g = Conv2D(c, 1, padding='same', **g_kernel_cfg, name='Conv_AttnG')\n",
    "        self.sigma = self.add_weight(shape=[1],\n",
    "                                initializer='zeros',\n",
    "                                trainable=True, name='sigma')\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        n, h, w, c = x.shape\n",
    "        theta = self.conv_theta(x)\n",
    "        theta = tf.reshape(theta, (-1, self.n_feats, theta.shape[-1]))\n",
    "        \n",
    "        phi = self.conv_phi(x)\n",
    "        phi = tf.nn.max_pool2d(phi, ksize=2, strides=2, padding='VALID')\n",
    "        phi = tf.reshape(phi, (-1, self.n_feats//4, phi.shape[-1]))\n",
    "        \n",
    "        attn = tf.matmul(theta, phi, transpose_b=True)\n",
    "        attn = tf.nn.softmax(attn)\n",
    "\n",
    "        g = self.conv_g(x)\n",
    "        g = tf.nn.max_pool2d(g, ksize=2, strides=2, padding='VALID')\n",
    "        g = tf.reshape(g, (-1, self.n_feats//4, g.shape[-1]))\n",
    "\n",
    "        attn_g = tf.matmul(attn, g)\n",
    "        attn_g = tf.reshape(attn_g, (-1, h, w, attn_g.shape[-1]))\n",
    "        attn_g = self.conv_attn_g(attn_g)\n",
    "        \n",
    "        output = x + self.sigma * attn_g\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionBatchNorm(Layer):\n",
    "    def __init__(self, decay_rate=0.999, eps=1e-4):\n",
    "        super(ConditionBatchNorm, self).__init__()\n",
    "        self.decay = decay_rate\n",
    "        self.eps = 1e-5\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        c = input_shape[-1]\n",
    "        self.dense_beta = Dense(c, **g_kernel_cfg,)\n",
    "        self.dense_gamma = Dense(c, **g_kernel_cfg,)\n",
    "        self.moving_mean = self.add_weight(shape=[1, 1, 1, c], initializer='zeros',\n",
    "                                          trainable=False, name='moving_mean')\n",
    "    \n",
    "        self.moving_var = self.add_weight(shape=[1, 1, 1, c], initializer='ones',\n",
    "                                          trainable=False, name='moving_var')\n",
    "\n",
    "    def call(self, x, z_y, training=False):\n",
    "        beta = self.dense_beta(z_y)\n",
    "        gamma = self.dense_gamma(z_y)\n",
    "        for _ in range(2):\n",
    "            beta = tf.expand_dims(beta, 1)\n",
    "            gamma = tf.expand_dims(gamma, 1)\n",
    "                \n",
    "        if training:\n",
    "            mean, var = tf.nn.moments(x, axes=(0,1,2), keepdims=True)\n",
    "            self.moving_mean.assign(self.decay * self.moving_mean + (1-self.decay)*mean)\n",
    "            self.moving_var.assign(self.decay * self.moving_var + (1-self.decay)*var)\n",
    "            output = tf.nn.batch_normalization(x, mean, var, beta, gamma, self.eps)\n",
    " \n",
    "        else:\n",
    "            output = tf.nn.batch_normalization(x, \n",
    "                                               self.moving_mean, self.moving_var,\n",
    "                                               beta, gamma, self.eps)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build SAGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resblock(Layer):\n",
    "    def __init__(self, filters, n_class):\n",
    "        super(Resblock, self).__init__(name=f'g_resblock_{filters}x{filters}')\n",
    "        self.filters = filters\n",
    "        self.n_class = n_class\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_filter = input_shape[-1]\n",
    "        self.conv_1 = Conv2D(self.filters, 3, padding='same', **g_kernel_cfg, name='conv2d_1')\n",
    "        self.conv_2 = Conv2D(self.filters, 3, padding='same', **g_kernel_cfg, name='conv2d_2')\n",
    "        self.cbn_1 = ConditionBatchNorm(self.n_class)\n",
    "        self.cbn_2 = ConditionBatchNorm(self.n_class)\n",
    "        self.learned_skip = False\n",
    "        \n",
    "        if self.filters != input_filter:\n",
    "            self.learned_skip = True\n",
    "            self.conv_3 = Conv2D(self.filters, 1, padding='same', **g_kernel_cfg, name='conv2d_3')\n",
    "            self.cbn_3 = ConditionBatchNorm(self.n_class)\n",
    "        \n",
    "    def call(self, input_tensor, labels):\n",
    "        x = self.conv_1(input_tensor)\n",
    "        x = self.cbn_1(x, labels)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.cbn_2(x, labels)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "                \n",
    "        if self.learned_skip:\n",
    "            skip = self.conv_3(input_tensor)\n",
    "            skip = self.cbn_3(skip, labels)\n",
    "            skip = tf.nn.leaky_relu(skip, 0.2)            \n",
    "        else:\n",
    "            skip = input_tensor\n",
    "            \n",
    "        output = skip + x\n",
    "        return output\n",
    "\n",
    "def build_generator(z_dim, n_class, y_dim):\n",
    "\n",
    "    DIM = 64\n",
    "    \n",
    "    z_input = layers.Input(shape=(z_dim))\n",
    "    labels = layers.Input(shape=(1), dtype='int32')\n",
    "\n",
    "    z = tf.split(z_input, 4, axis=1)\n",
    "\n",
    "    y = Embedding(n_class, y_dim)(tf.squeeze(labels, [1]))\n",
    "\n",
    "    x = Dense(4*4*4*DIM, **g_kernel_cfg)(z[0])\n",
    "    x = layers.Reshape((4, 4, 4*DIM))(x)\n",
    "    \n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    y_z = tf.concat((y, z[1]), axis=-1)\n",
    "    x = Resblock(4*DIM, n_class)(x, y_z)\n",
    "    \n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    y_z = tf.concat((y, z[2]), axis=-1)\n",
    "    x = Resblock(2*DIM, n_class)(x, y_z)\n",
    "    \n",
    "    x = SelfAttention()(x)\n",
    "\n",
    "    x = layers.UpSampling2D((2,2))(x)\n",
    "    y_z = tf.concat((y, z[3]), axis=-1)\n",
    "    x = Resblock(DIM, n_class)(x, y_z)\n",
    "    \n",
    "    output_image = tanh(Conv2D(3, 3, padding='same')(x))\n",
    "\n",
    "    return Model([z_input, labels], \n",
    "                 output_image, \n",
    "                 name='generator')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResblockDown(Layer):\n",
    "    def __init__(self, filters, downsample=True):\n",
    "        super(ResblockDown, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_filter = input_shape[-1]\n",
    "        self.conv_1 = Conv2D(self.filters, 3, padding='same', **d_kernel_cfg)\n",
    "        self.conv_2 = Conv2D(self.filters, 3, padding='same', **d_kernel_cfg)\n",
    "        self.learned_skip = False\n",
    "        \n",
    "        if self.filters != input_filter:\n",
    "            self.learned_skip = True\n",
    "            self.conv_3 = Conv2D(self.filters, 1, padding='same', **d_kernel_cfg)        \n",
    "\n",
    "    def down(self, x):\n",
    "        return tf.nn.avg_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], 'VALID')\n",
    "            \n",
    "    def call(self, input_tensor):\n",
    "        x = self.conv_1(input_tensor)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = tf.nn.leaky_relu(x, 0.2)\n",
    "\n",
    "        if self.downsample:\n",
    "            x = self.down(x)\n",
    "            \n",
    "        if self.learned_skip:\n",
    "            skip = self.conv_3(input_tensor)\n",
    "            skip = tf.nn.leaky_relu(skip, 0.2)            \n",
    "            if self.downsample:\n",
    "                skip = self.down(skip)\n",
    "        else:\n",
    "            skip = input_tensor\n",
    "        output = skip + x\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(n_class):\n",
    "    DIM = 64\n",
    "    input_image = Input(shape=IMAGE_SHAPE)\n",
    "    input_labels = Input(shape=(1))\n",
    "\n",
    "    embedding = Embedding(n_class, 4*DIM)(input_labels)\n",
    "\n",
    "    embedding = Flatten()(embedding)\n",
    "\n",
    "    x = ResblockDown(DIM)(input_image) # 64\n",
    "    \n",
    "    x = ResblockDown(2*DIM)(x) # 32\n",
    "    \n",
    "    x = SelfAttention()(x)\n",
    "    \n",
    "    x = ResblockDown(4*DIM)(x) # 16\n",
    "    \n",
    "    x = ResblockDown(4*DIM, False)(x) # 4\n",
    "    \n",
    "    x = tf.reduce_sum(x, (1, 2))\n",
    "\n",
    "    embedded_x  = tf.reduce_sum(x * embedding, axis=1,  keepdims=True)\n",
    "\n",
    "    output = Dense(1)(x)\n",
    "    \n",
    "    output += embedded_x\n",
    "    \n",
    "    return Model([input_image, input_labels], output, name='discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_d(y_true, y_pred):\n",
    "    return tf.keras.losses.Hinge()(y_pred, y_true)\n",
    "    \n",
    "def hinge_loss_g(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_pred)\n",
    "\n",
    "\n",
    "class BigGAN():\n",
    "    def __init__(self, image_shape, n_class, z_dim=128, y_dim=32):\n",
    "        self.z_dim = z_dim\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        # Build models\n",
    "        self.optimizer_d = Adam(4e-4, 0.0, 0.9)\n",
    "        self.optimizer_g = Adam(1e-4, 0.0, 0.9)\n",
    "        self.discriminator = build_discriminator(n_class)\n",
    "        \n",
    "        self.generator = build_generator(z_dim, n_class, y_dim)\n",
    "        \n",
    "        pred = self.discriminator([self.generator.output, self.generator.input[1]])\n",
    "        self.model = Model(self.generator.input, pred, name='model')\n",
    "        self.model.compile(optimizer=self.optimizer_g, loss=hinge_loss_g)\n",
    "        \n",
    "        \n",
    "        self.hinge_loss = tf.keras.losses.Hinge()\n",
    "        \n",
    "    def hinge_loss_d(self, y, is_real):\n",
    "        label = 1. if is_real else -1.\n",
    "        loss = self.hinge_loss(y, label)\n",
    "        return loss\n",
    "    \n",
    "    def hinge_loss_g(self, y):\n",
    "        return -tf.reduce_mean(y)\n",
    "\n",
    "    def train_step(self, train_gen):\n",
    "        real_images, real_class_labels = next(train_gen)\n",
    "        batch_size = real_class_labels.shape[0]\n",
    "        real_labels = 1\n",
    "        fake_labels = 0\n",
    "        \n",
    "        z = tf.random.normal((batch_size, self.z_dim))\n",
    "\n",
    "        fake_class_labels = real_class_labels\n",
    "        \n",
    "        with tf.GradientTape() as d_tape, \\\n",
    "             tf.GradientTape() as g_tape:\n",
    "            \n",
    "            # forward pass\n",
    "            fake_images = self.generator([z, fake_class_labels])            \n",
    "            pred_real = self.discriminator([real_images, real_class_labels])\n",
    "            pred_fake = self.discriminator([fake_images, fake_class_labels])\n",
    "            \n",
    "            # discriminator losses      \n",
    "            loss_fake = self.hinge_loss_d(pred_fake, False)\n",
    "            loss_real = self.hinge_loss_d(pred_real, True)\n",
    "            \n",
    "            # total loss\n",
    "            d_loss = 0.5*(loss_fake + loss_real)\n",
    "            d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)            \n",
    "            self.optimizer_d.apply_gradients(zip(d_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "            # Generator Loss\n",
    "            g_loss = self.hinge_loss_g(pred_fake)\n",
    "            g_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)            \n",
    "            self.optimizer_g.apply_gradients(zip(g_gradients, self.generator.trainable_variables))\n",
    "\n",
    "            \n",
    "        return g_loss, d_loss\n",
    "                           \n",
    "    def show_val(self):\n",
    "        images_per_class = 10\n",
    "        z = tf.random.normal((images_per_class*self.n_class, self.z_dim))\n",
    "        labels = []\n",
    "        for i in range(self.n_class):\n",
    "            labels += [i]*images_per_class\n",
    "        labels = np.array(labels, dtype=np.int32)\n",
    "        images = self.generator.predict([z, labels])\n",
    "        images = images * 0.5 + 0.5\n",
    "        grid_row = self.n_class\n",
    "        grid_col = images_per_class\n",
    "        \n",
    "        scale = 2\n",
    "        f, axarr = plt.subplots(grid_row, grid_col, \n",
    "                                figsize=(grid_col*scale, grid_row*scale))\n",
    "\n",
    "        for row in range(grid_row):\n",
    "            ax = axarr if grid_row==1 else axarr[row]\n",
    "            for col in range(grid_col):\n",
    "                ax[col].imshow(images[row*grid_col + col])\n",
    "                ax[col].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def train(self, train_gen, steps, interval=100):\n",
    "        for i in range(steps):\n",
    "            g_loss, d_loss = self.train_step(train_gen)\n",
    "            if i% interval == 0:\n",
    "                msg = f'Step {i} g_loss {g_loss:.4f} d_loss {d_loss:.4f}'\n",
    "                print(msg)\n",
    "                self.show_val()\n",
    "                \n",
    "\n",
    "tf.keras.backend.clear_session()                                         \n",
    "gan = BigGAN(IMAGE_SHAPE, 10)\n",
    "gan.train(iter(train_dataset), 50000, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.train(iter(train_dataset), 50000, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(gan.generator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(gan.discriminator, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgentf2",
   "language": "python",
   "name": "imgentf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
