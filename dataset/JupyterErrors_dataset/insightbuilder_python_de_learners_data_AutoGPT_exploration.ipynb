{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tiktoken faiss-cpu > /dev/null"
      ],
      "metadata": {
        "id": "QPK--l4KPdmK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xA8QBnA1JcOc"
      },
      "outputs": [],
      "source": [
        "# General \n",
        "import os\n",
        "import pandas as pd\n",
        "from langchain.experimental.autonomous_agents.autogpt.agent import AutoGPT\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\n",
        "from langchain.docstore.document import Document\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "\n",
        "# Needed synce jupyter runs an async eventloop\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "qLZYo2EvPuJQ",
        "outputId": "0d6fd0c5-0e99-4b7c-ad2c-31cbe508d774"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-01b78135b6c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pydantic/main.cpython-39-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools\n",
        "import os\n",
        "from contextlib import contextmanager\n",
        "from typing import Optional\n",
        "from langchain.agents import tool\n",
        "from langchain.tools.file_management.read import ReadFileTool\n",
        "from langchain.tools.file_management.write import WriteFileTool"
      ],
      "metadata": {
        "id": "A-3rXLZSPuAu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = \"./data/\"\n",
        "\n",
        "@contextmanager\n",
        "def pushd(new_dir):\n",
        "    \"\"\"Context manager for changing the current working directory.\"\"\"\n",
        "    prev_dir = os.getcwd()\n",
        "    os.chdir(new_dir)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        os.chdir(prev_dir)\n",
        "\n",
        "@tool\n",
        "def process_csv(\n",
        "    csv_file_path: str, instructions: str, output_path: Optional[str] = None\n",
        ") -> str:\n",
        "    \"\"\"Process a CSV by with pandas in a limited REPL.\\\n",
        " Only use this after writing data to disk as a csv file.\\\n",
        " Any figures must be saved to disk to be viewed by the human.\\\n",
        " Instructions should be written in natural language, not code. Assume the dataframe is already loaded.\"\"\"\n",
        "    with pushd(ROOT_DIR):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file_path)\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "        agent = create_pandas_dataframe_agent(llm, df, max_iterations=30, verbose=True)\n",
        "        if output_path is not None:\n",
        "            instructions += f\" Save output to disk at {output_path}\"\n",
        "        try:\n",
        "            result = agent.run(instructions)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\""
      ],
      "metadata": {
        "id": "t1gFDyxgPt2L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playwright bs4 duckduckgo_search > /dev/null"
      ],
      "metadata": {
        "id": "LzTM0yiISkFf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def async_load_playwright(url: str) -> str:\n",
        "    \"\"\"Load the specified URLs using Playwright and parse using BeautifulSoup.\"\"\"\n",
        "    from bs4 import BeautifulSoup\n",
        "    from playwright.async_api import async_playwright\n",
        "\n",
        "    results = \"\"\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        try:\n",
        "            page = await browser.new_page()\n",
        "            await page.goto(url)\n",
        "\n",
        "            page_source = await page.content()\n",
        "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.extract()\n",
        "\n",
        "            text = soup.get_text()\n",
        "            lines = (line.strip() for line in text.splitlines())\n",
        "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "            results = \"\\n\".join(chunk for chunk in chunks if chunk)\n",
        "        except Exception as e:\n",
        "            results = f\"Error: {e}\"\n",
        "        await browser.close()\n",
        "    return results\n",
        "\n",
        "def run_async(coro):\n",
        "    event_loop = asyncio.get_event_loop()\n",
        "    return event_loop.run_until_complete(coro)\n",
        "\n",
        "@tool\n",
        "def browse_web_page(url: str) -> str:\n",
        "    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n",
        "    return run_async(async_load_playwright(url))"
      ],
      "metadata": {
        "id": "bxk3NtzCSi7d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool #, DuckDuckGoSearchRun\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from pydantic import Field\n",
        "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n",
        "\n",
        "def _get_text_splitter():\n",
        "    return RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = 500,\n",
        "        chunk_overlap  = 20,\n",
        "        length_function = len,\n",
        "    )\n",
        "\n",
        "\n",
        "class WebpageQATool(BaseTool):\n",
        "    name = \"query_webpage\"\n",
        "    description = \"Browse a webpage and retrieve the information relevant to the question.\"\n",
        "    text_splitter: RecursiveCharacterTextSplitter = Field(default_factory=_get_text_splitter)\n",
        "    qa_chain: BaseCombineDocumentsChain\n",
        "    \n",
        "    def _run(self, url: str, question: str) -> str:\n",
        "        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n",
        "        result = browse_web_page.run(url)\n",
        "        docs = [Document(page_content=result, metadata={\"source\": url})]\n",
        "        web_docs = self.text_splitter.split_documents(docs)\n",
        "        results = []\n",
        "        # TODO: Handle this with a MapReduceChain\n",
        "        for i in range(0, len(web_docs), 4):\n",
        "            input_docs = web_docs[i:i+4]\n",
        "            window_result = self.qa_chain({\"input_documents\": input_docs, \"question\": question}, return_only_outputs=True)\n",
        "            results.append(f\"Response from window {i} - {window_result}\")\n",
        "        results_docs = [Document(page_content=\"\\n\".join(results), metadata={\"source\": url})]\n",
        "        return self.qa_chain({\"input_documents\": results_docs, \"question\": question}, return_only_outputs=True)\n",
        "    \n",
        "    async def _arun(self, url: str, question: str) -> str:\n",
        "        raise NotImplementedError\n",
        "      "
      ],
      "metadata": {
        "id": "_WGSiqfqSi2g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_website_tool = WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))"
      ],
      "metadata": {
        "id": "4hGcmyvySivt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.tools.human.tool import HumanInputRun\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, \n",
        "                    index, \n",
        "                    InMemoryDocstore({}), \n",
        "                    {})"
      ],
      "metadata": {
        "id": "LCwbvEFDTSbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "web_search = DuckDuckGoSearchRun()"
      ],
      "metadata": {
        "id": "ZlvS2S7mTSVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    web_search,\n",
        "    WriteFileTool(root_dir=\"./data\"),\n",
        "    ReadFileTool(root_dir=\"./data\"),\n",
        "    process_csv,\n",
        "    query_website_tool,\n",
        "    # HumanInputRun(), # Activate if you want the permit asking for help from the human\n",
        "]"
      ],
      "metadata": {
        "id": "KavCwZIfTSOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = AutoGPT.from_llm_and_tools(\n",
        "    ai_name=\"Tom\",\n",
        "    ai_role=\"Assistant\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    memory=vectorstore.as_retriever(search_kwargs={\"k\": 8}),\n",
        "    # human_in_the_loop=True, # Set to True if you want to add feedback at each step.\n",
        ")\n",
        "# agent.chain.verbose = True"
      ],
      "metadata": {
        "id": "zg9aAIu5T4Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run([\"\"\"What were the winning boston marathon times for\n",
        " the past 5 years (ending in 2022)? Generate a table of\n",
        "  the year, name, country of origin, and times.\"\"\"])"
      ],
      "metadata": {
        "id": "IkPRnKn-T6l6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}