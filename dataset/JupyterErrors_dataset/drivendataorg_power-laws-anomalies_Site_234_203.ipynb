{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrivenData - Power Laws: Detecting Anomalies in Usage\n",
    "## Levi Viana\n",
    "\n",
    "### Introduction :\n",
    "\n",
    "This notebook takes as input the raw data from the DrivenData Power Laws Challenge sponsored by Schneider Eletric and produces anomaly scores as well as an algorithm to help detecting energy overconsumption.\n",
    "\n",
    "### Folders :\n",
    "\n",
    "This notebook relies on the following organization of project folders:\n",
    "\n",
    "Project -\n",
    "    - Models (where PyTorch will save the models)\n",
    "    - Data (where the raw data files should be in)\n",
    "    - Notebooks (where this notebook should be located)\n",
    "\n",
    "### Dependencies :\n",
    "\n",
    "<ol><b>Anaconda3</b>: \n",
    "    <li>Download: https://www.anaconda.com/download/#linux</li>\n",
    "    <li>This code runs in the python 3.5 environment: https://docs.anaconda.com/anaconda/faq#how-do-i-get-the-latest-anaconda-with-python-3-5</li>\n",
    "</ol>\n",
    "<ol><b>PyTorch</b>: \n",
    "    <li>Download: http://pytorch.org/</li>\n",
    "    <li>This notebook uses the version 0.3.0.post4 of Pytorch, if is shows errors for newer releases, you should be able to install previous versions of pytorch following these steps http://pytorch.org/previous-versions/</li>\n",
    "    <li>Be sure that you install the Python 3.5 environment version</li>\n",
    "</ol>\n",
    "\n",
    "### Hardware requirement :\n",
    "\n",
    "<li>CPU RAM: This Notebooks needs about 5 Gb of CPU RAM to run, it can be optimized to run with less memory, but it wasn't the priority of the competition</li>\n",
    "<li>GPU RAM: It needs about 3,4 Gb of GPU memory to run. The code however should work even if you don't have any GPU installed</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from random import sample, seed, shuffle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "#Checking the PyTorch version installed\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From raw data to a tidy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../Data/weather.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3eb08b167994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/weather.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mholidays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/holidays.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data/metadata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomaly-2nd/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomaly-2nd/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomaly-2nd/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomaly-2nd/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anomaly-2nd/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../Data/weather.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Loading the raw data\n",
    "weather = pd.read_csv(\"../Data/weather.csv\")\n",
    "holidays = pd.read_csv(\"../Data/holidays.csv\")\n",
    "train = pd.read_csv(\"../Data/train.csv\")\n",
    "metadata = pd.read_csv(\"../Data/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train[train['meter_id'] == '234_203']\n",
    "train['Timestamp'] = pd.to_datetime(train['Timestamp'])\n",
    "train['Weekday'] = train['Timestamp'].dt.weekday\n",
    "train['Hour'] = train['Timestamp'].dt.hour\n",
    "train['Month'] = train['Timestamp'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some datasets have duplicates, this cell it take care of it\n",
    "train = train.groupby(['Timestamp']).first()\n",
    "train.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train['Values'].iloc[:1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Adding the Holidays column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No holidays reported for this site\n",
    "holidays['site_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Adding the Temperature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['Timestamp'] = pd.to_datetime(weather['Timestamp'])\n",
    "weather['Date'] = weather['Timestamp'].apply(lambda dt: datetime.datetime(dt.year, \n",
    "                                                                      dt.month, \n",
    "                                                                      dt.day, \n",
    "                                                                      dt.hour))\n",
    "weather = pd.DataFrame(\n",
    "    weather[weather['site_id'] == \"234_203\"].groupby(['Date'])['Temperature'].mean())\n",
    "weather.reset_index(level=0, inplace=True)\n",
    "train['Date'] = train['Timestamp'].apply(lambda dt: datetime.datetime(dt.year, \n",
    "                                                                      dt.month, \n",
    "                                                                      dt.day, \n",
    "                                                                      dt.hour))\n",
    "train = train.merge(weather, how='left', on='Date')\n",
    "del train['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Determining buckets of energy consumption (for the predictive model below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumption_label(x):\n",
    "    if x >= np.percentile(train['Values'], 99):\n",
    "        return 'VeryHigh'\n",
    "    elif x >= np.percentile(train['Values'], 95):\n",
    "        return 'High'\n",
    "    elif x >= np.percentile(train['Values'], 80):\n",
    "        return 'MediumHigh'\n",
    "    elif x >= np.percentile(train['Values'], 60):\n",
    "        return 'Medium'\n",
    "    elif x >= np.percentile(train['Values'], 40):\n",
    "        return 'MediumLow'\n",
    "    elif x >= np.percentile(train['Values'], 20):\n",
    "        return 'Low'\n",
    "    elif x >= np.percentile(train['Values'], 5):\n",
    "        return 'VeryLow'\n",
    "    elif x >= np.percentile(train['Values'], 1):\n",
    "        return 'StandBy'\n",
    "    else:\n",
    "        return 'Off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Values'] = train['Values'].fillna(-1)\n",
    "train['target'] = train['Values'].apply(lambda row: consumption_label(row), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Handling temperature missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def impute_values(indexes, x):\n",
    "    \"\"\"\n",
    "    This function will be used to fill na's in the temperature column. It will firstly find add \"blocks\" \n",
    "    of na's, i.e, sequence of consecutives na's. Then, it interpolates the missing values by using the \n",
    "    temperature values before and after the beggining of the na block.\n",
    "    \n",
    "    Arguments:\n",
    "        indexes: Pandas DataFrame of na's indexes\n",
    "        x: the Pandas DataFrame column with temperatures\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp = []\n",
    "    \n",
    "    blocks = []\n",
    "    \n",
    "    start_block = indexes[0]\n",
    "    \n",
    "    for idx in indexes:\n",
    "        \n",
    "        if start_block < 0:\n",
    "            start_block = idx\n",
    "        \n",
    "        if (idx+1) in indexes:\n",
    "            continue\n",
    "        else:\n",
    "            blocks.append([start_block, idx])\n",
    "            start_block = -1\n",
    "    \n",
    "    tmp = np.array([])\n",
    "    for begin, end in blocks:\n",
    "        \n",
    "        if begin == 0:\n",
    "            last_val = x[end+1]\n",
    "            next_val = x[end+1]\n",
    "        elif end == len(x)-1:\n",
    "            last_val = x[begin-1]\n",
    "            next_val = x[begin-1]\n",
    "        else:\n",
    "            last_val = x[begin-1]\n",
    "            next_val = x[end+1]\n",
    "\n",
    "        imputation = np.repeat((last_val+next_val)/2, end-begin +1)\n",
    "        \n",
    "        tmp = np.concatenate((tmp, imputation))\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_temperatures = impute_values(train[train['Temperature'].isnull()].index, \n",
    "                                     train['Temperature'])\n",
    "train.loc[train['Temperature'].isnull(), 'Temperature'] = imputed_temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Pickling the dataset to gain some time for the next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train, open(\"../Data/train_prediction_234_203.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"../Data/train_prediction_234_203.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Preliminar steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Values', 'Temperature', 'Weekday', 'Hour', 'Month']\n",
    "feature_scaler = StandardScaler()\n",
    "feature_scaler.fit(train[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(df, sequence_length):\n",
    "    \"\"\"\n",
    "    This function builds a tidy dataset to be used as input to a LSTM. \n",
    "    It returns a tensor of the shape [nb_records, 3*sequence_length+2, feature_size] and a vector\n",
    "    of shape [nb_records] having the target values to predict.\n",
    "    \n",
    "    The main idea is to give the model an intraday pattern twice (two previous weeks) and an\n",
    "    same but incomplete pattern for the current week so that the model has to predict only the\n",
    "    last value of the pattern.\n",
    "    \n",
    "    Arguments:\n",
    "        df: Pandas DataFrame containing the energy consumption data\n",
    "        sequence_length: length of the pattern to recognize\n",
    "    \"\"\"\n",
    "    \n",
    "    target_labels = ['Off', 'StandBy', 'VeryLow', 'Low', 'MediumLow', 'Medium', 'MediumHigh', 'High','VeryHigh']\n",
    "    target_labels = np.array(target_labels)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(48*7*2, len(df) - sequence_length - 1):\n",
    "        for j in range(3):\n",
    "            if j == 0:\n",
    "                tmp = feature_scaler.transform(df[features].iloc[i:(i+sequence_length)])\n",
    "            else:\n",
    "                ts = feature_scaler.transform(df[features].iloc[(i-j*48*7):(i+sequence_length-j*48*7+1)])\n",
    "#                 print(ts.shape)\n",
    "                tmp = np.concatenate((tmp, ts), axis=0)\n",
    "            \n",
    "        X.append(tmp)\n",
    "        y.append(np.where(target_labels == df['target'].iloc[i+sequence_length + 1])[0][0])\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, train_labels = generate_df(train, 8)\n",
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Splitting train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, _, _ = train_test_split(list(range(train_sequences.shape[0])), \n",
    "                                         np.zeros(train_sequences.shape[0]), \n",
    "                                         test_size=0.5, \n",
    "                                         random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_sequences = train_sequences[idx_train,:,:]\n",
    "rnn_labels = train_labels[idx_train]\n",
    "\n",
    "val_sequences = train_sequences[idx_val,:,:] \n",
    "val_labels = train_labels[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Defining the predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runValidation(model_dict, cm_print = False):\n",
    "    model = WinticsRNN(hidden_size=hidden_size, \n",
    "                       features_size=len(features))\n",
    "    \n",
    "    model.load_state_dict(model_dict)    \n",
    "    model.eval()\n",
    "    \n",
    "    sequences = torch.FloatTensor(val_sequences)\n",
    "    sequences = autograd.Variable(sequences)\n",
    "    \n",
    "    labels_true = torch.LongTensor(val_labels)\n",
    "    labels_true = autograd.Variable(labels_true)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        sequences = sequences.cuda()\n",
    "        labels_true = labels_true.cuda()\n",
    "    \n",
    "    labels_predicted = model(sequences)\n",
    "    loss = loss_function(labels_predicted, labels_true)\n",
    "    \n",
    "    _, labels_predicted = labels_predicted.max(1)\n",
    "    labels_predicted = labels_predicted.data.cpu().numpy()\n",
    "    \n",
    "    if cm_print:\n",
    "        print(\"Current Validation loss: %.3f\" % loss.data.cpu().numpy()[0])\n",
    "        print(confusion_matrix(labels_predicted, val_labels))\n",
    "    \n",
    "    return accuracy_score(labels_predicted, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WinticsRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, features_size):\n",
    "        super(WinticsRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.features_size = features_size\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.features_size, \n",
    "                          self.hidden_size, \n",
    "                          1, # This argument stands for the nb of layers \n",
    "                          dropout=0, \n",
    "                          batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h0 = autograd.Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "        c0 = autograd.Variable(torch.zeros(1, x.size(0), self.hidden_size))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h0, c0 = h0.cuda(), c0.cuda()\n",
    "\n",
    "        _, (x, _) = self.rnn(x, (h0, c0))\n",
    "        \n",
    "        x = torch.squeeze(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbEpochs = 282\n",
    "hidden_size = 80\n",
    "batch_size = 256\n",
    "\n",
    "net = WinticsRNN(hidden_size=hidden_size, \n",
    "                 features_size=len(features))\n",
    "\n",
    "#### Uncomment the following line if you want to retrain a previously saved model\n",
    "# net.load_state_dict(torch.load(\"../Models/LSTM_80_234_203\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "epoch_losses = []\n",
    "\n",
    "best_model = net.state_dict()\n",
    "epoch_acc_list = [] \n",
    "\n",
    "for ep in range(nbEpochs):\n",
    "\n",
    "    avg_loss = []        \n",
    "            \n",
    "    net.train()\n",
    "\n",
    "    for i in range(rnn_sequences.shape[0] // batch_size):\n",
    "        \n",
    "        X_batch = rnn_sequences[i*batch_size:(i+1)*batch_size, :, :]\n",
    "        y_batch = rnn_labels[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        # [batch_size, sequence_length, features_size]\n",
    "        sequences = torch.FloatTensor(X_batch)\n",
    "        sequences = autograd.Variable(sequences)\n",
    "\n",
    "        labels_true = torch.LongTensor(y_batch)\n",
    "        labels_true = autograd.Variable(labels_true)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            sequences = sequences.cuda()\n",
    "            labels_true = labels_true.cuda()\n",
    "        \n",
    "        labels_predicted = net(sequences)        \n",
    "        \n",
    "        net.zero_grad()\n",
    "        loss = loss_function(labels_predicted, labels_true)\n",
    "        loss.backward()    \n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss.append(loss.data.cpu().numpy()[0]) \n",
    "    \n",
    "    #<Early stopping>\n",
    "    epoch_acc = runValidation(net.state_dict())\n",
    "    epoch_acc_list.append(epoch_acc)\n",
    "\n",
    "    if max(epoch_acc_list) <= epoch_acc:\n",
    "        best_model = net.state_dict()\n",
    "\n",
    "    if max(epoch_acc_list)/1.2 > epoch_acc:\n",
    "        print(\"Training stopped by early stopping !\")\n",
    "        print(\"Validation MSE: %.2f\" % (max(epoch_acc_list)))\n",
    "        plt.plot(epoch_acc_list)\n",
    "        plt.show()\n",
    "        break\n",
    "    #</Early stopping>\n",
    "    \n",
    "    if ep % 10 == 1:     \n",
    "        print(\"Epoch losses: %.2f\" % (np.mean(epoch_losses)))\n",
    "        if ep % 40 == 1:\n",
    "            print(\"Epoch nb: {}\".format(ep))\n",
    "            \n",
    "            print(\"Validation Accuracy: %.2f\" % runValidation(net.state_dict(), True))\n",
    "            \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['lr'] * 0.8\n",
    "  \n",
    "        epoch_losses = []\n",
    "            \n",
    "    epoch_losses.append(np.mean(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, \"../Models/LSTM_80_234_203\")\n",
    "best_model = torch.load(\"../Models/LSTM_80_234_203\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Evaluating the entropy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(model_dict, sequences, labels):\n",
    "    \"\"\"\n",
    "    This function returns (i) the labels predicted by the model as well as (ii) the entropy \n",
    "    of predictions, which will serve as an indicator of the prediction's confidence.\n",
    "    Small entropies mean that the model is sure of what it is predicting, regardless\n",
    "    the target value.\n",
    "    \n",
    "    Arguments:\n",
    "        model_dict: PyTorch model dictionary, i.e., the output of net.state_dict()\n",
    "        sequences: RNN inputs, i.e., tensors of the form [nb_records, 3*sequence_length+2, feature_size]\n",
    "        labels: Target values (only used for some metric evaluation)\n",
    "    \"\"\"\n",
    "    model = WinticsRNN(hidden_size=hidden_size, \n",
    "                       features_size=len(features))\n",
    "    \n",
    "    model.load_state_dict(model_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    sequences = torch.FloatTensor(sequences)\n",
    "    sequences = autograd.Variable(sequences)\n",
    "    \n",
    "    labels_true = torch.LongTensor(labels)\n",
    "    labels_true = autograd.Variable(labels_true)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        sequences = sequences.cuda()\n",
    "        labels_true = labels_true.cuda()\n",
    "\n",
    "    labels_predicted = model(sequences)\n",
    "    \n",
    "    entropy = [exp_logits/torch.sum(exp_logits) for exp_logits in torch.exp(labels_predicted)]\n",
    "    entropy = [-torch.log(probs).dot(probs) for probs in entropy]\n",
    "    entropy = np.array([e.data.cpu().numpy() for e in entropy])\n",
    "    \n",
    "    _, labels_predicted = labels_predicted.max(1)\n",
    "    labels_predicted = labels_predicted.data.cpu().numpy()\n",
    "    \n",
    "    print(\"Dataset accuracy: %.3f\" % accuracy_score(labels_predicted, labels))\n",
    "    cm = confusion_matrix(labels_predicted, labels)\n",
    "    extended_diagonal = cm.trace()+cm[1:, :-1].trace()+cm[:-1, 1:].trace()\n",
    "    extended_diagonal_2 = extended_diagonal+cm[2:, :-2].trace()+cm[:-2, 2:].trace()\n",
    "    \n",
    "    print(\"Dataset extended accuracy (max gap 1): %.3f\" % (extended_diagonal/np.sum(cm)))\n",
    "    print(\"Dataset extended accuracy (max gap 2): %.3f\" % (extended_diagonal_2/np.sum(cm)))\n",
    "    \n",
    "    return labels_predicted, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the entropies for the training dataset\n",
    "overconsumption_train, entropy_train = entropy(best_model, rnn_sequences, rnn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the entropies for the test dataset\n",
    "overconsumption_val, entropy_val = entropy(best_model, val_sequences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column 'entropy' in the original dataset \n",
    "train['entropy'] = 3.0\n",
    "train.loc[48*7*2+8+1+np.array(idx_val), 'entropy'] = entropy_val[:, 0]\n",
    "train.loc[48*7*2+8+1+np.array(idx_train), 'entropy'] = entropy_train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column 'predicted' in the original dataset\n",
    "target_labels = ['Off', 'StandBy', 'VeryLow', 'Low', 'MediumLow', 'Medium', 'MediumHigh', 'High','VeryHigh']\n",
    "train['predicted'] = \"None\"\n",
    "train.loc[48*7*2+8+1+np.array(idx_val), 'predicted'] = [target_labels[v] for v in overconsumption_val]\n",
    "train.loc[48*7*2+8+1+np.array(idx_train), 'predicted'] = [target_labels[v] for v in overconsumption_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Determining the anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gap(row):\n",
    "    \"\"\"\n",
    "    This function returns the number of buckets lying between the prediction and the target value.\n",
    "    It will be applied to all the main dataset's lines.\n",
    "    \n",
    "    Arguments:\n",
    "        row: row of the dataset.\n",
    "    \"\"\"\n",
    "    if row['predicted'] in target_labels:\n",
    "        idx_pred = np.where(np.array(target_labels) == row['predicted'])[0][0]\n",
    "        idx_true = np.where(np.array(target_labels) == row['target'])[0][0]\n",
    "\n",
    "        return idx_true - idx_pred\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['gap'] = train.apply(lambda row: get_gap(row), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CRITERIA : the anomaly score\n",
    "\n",
    "The main motivation for that anomaly score is to catch two behaviours :\n",
    "\n",
    "    - Low entropy and low prediction gap:\n",
    "        The model is sure of what it is predicting, and still, there is a gap. This effect\n",
    "        increases the risk that the current measurement is actually an anomaly.\n",
    "    - Moderate enropy and high prediction gap:\n",
    "        If the gap is somehow higher than what the entropy measurement is expecting,\n",
    "        then the this energy measurement is a potential anomaly.\n",
    "\n",
    "Since there are 9 buckets, the maximum entropy we can possibly get is ln(9), which is about 2.2.\n",
    "\n",
    "Let's consider an arbitrary prediction. Suppose that its entropy is equal to 0.69, then, \n",
    "the mass of probabilities is very likely distributed to only 2 buckets (because ln(2) is about 0.69). \n",
    "Suppose that the gap between the prediction and the target value is 3. We have then a suspicious\n",
    "energy measurement: the probabilities are very high for only two buckets (probably one next \n",
    "to the other), and still the target value is 3 buckets away from the prediction.\n",
    "\n",
    "The same logic can be drawn for all gaps. I came up with the following formula:\n",
    "$$gap*exp(ln(9)-entropy)$$\n",
    "\n",
    "It will have high absolute values whenever something unexpected happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['criteria'] = train['gap']*np.exp(np.log(9) - train['entropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Rule-based approach (Percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['temp_bucket'] = train['Temperature'].apply(lambda row: np.round(row/5)*5)\n",
    "train['working_day'] = train['Weekday'] < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile(row):\n",
    "    \"\"\"\n",
    "    This function returns the percentile of the energy consumption measurement with respect\n",
    "    to (i) the Temperature, (ii) the type of day and (iii) the hour.\n",
    "    \n",
    "    Arguments:\n",
    "        row: row of the dataset.\n",
    "    \"\"\"\n",
    "    bucket = row['temp_bucket']\n",
    "    weekday = row['working_day']\n",
    "    hour = row['Hour']\n",
    "    \n",
    "    cond = (train['temp_bucket'] == bucket)&(train['working_day'] == weekday)&(train['Hour'] == hour)    \n",
    "    range_of_values = train.loc[cond, 'Values']\n",
    "    \n",
    "    return stats.percentileofscore(range_of_values, row['Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['percentile'] = train.apply(lambda row: get_percentile(row), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Rule-based approach (Checking the context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def check_context(idx):\n",
    "    \"\"\"\n",
    "    The main idea of that function is to detect and eliminate anomalies that come from abnormal\n",
    "    contexts. For instance, if there is a conference takes place in the building for an entire\n",
    "    week, the energy consumption levels will be higher than the 'normal' days but it won't\n",
    "    necessarily be classified as overconsumption. So the idea is to detect these kind of weeks \n",
    "    and ignore any alarm that the predictive model might throw in that period of time.\n",
    "    \n",
    "    This function checks whether the pattern of a certain meter neighbourhood value is high \n",
    "    across the entire week. It compares the pattern percentiles for the precedent week against\n",
    "    the two former weeks. It returns the number of days it went higher.\n",
    "    \n",
    "    Arguments:\n",
    "        idx: index of the meter measurement, i.e., dataset row.\n",
    "    \"\"\"\n",
    "    ans = 0\n",
    "    \n",
    "    for j in range(7):\n",
    "        week_n   = train.loc[idx-8-j*96       :idx+8-j*96, 'percentile'].mean()\n",
    "        week_n_1 = train.loc[idx-8-j*96-96*7*1:idx+8-j*96-96*7*1, 'percentile'].mean()\n",
    "        week_n_2 = train.loc[idx-8-j*96-96*7*2:idx+8-j*96-96*7*2, 'percentile'].mean()\n",
    "        \n",
    "        ans += (week_n > week_n_1 + 10)*1 + (week_n > week_n_2 + 10)*1\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an illustration of what can be detected by this model\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "begin = 28896-96\n",
    "end = 28896+1+96\n",
    "consumption = train.loc[begin:end, 'Values'].values - train.loc[begin:end, 'Values'].min()\n",
    "consumption = 100*consumption/(train.loc[begin:end, 'Values'].max()-train.loc[begin:end, 'Values'].min())\n",
    "slide = 28896\n",
    "ax1.plot(train.loc[begin:end, 'percentile'].values)\n",
    "ax1.plot(consumption, color = \"green\")\n",
    "ax1.set_ylim([0, 100])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(train.loc[begin:end, 'criteria'].values, color=\"red\", linestyle=\":\", linewidth=3)\n",
    "ax2.set_ylim([-16, 16])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9 The algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "train['is_abnormal'] = False\n",
    "\n",
    "# Detecting the beggining of anomalies (Prediction-Based approach)\n",
    "train.loc[train['criteria']>11, 'is_abnormal'] = True\n",
    "\n",
    "# Filtering process (Rule-based approach)\n",
    "for idx in train[train['is_abnormal']].index:\n",
    "    \n",
    "    # Eliminating abnormal energy consumption contexts\n",
    "    if check_context(idx) > 8:\n",
    "        train.loc[idx, 'is_abnormal'] = False\n",
    "        continue\n",
    "    \n",
    "    # Eliminating low percentile anomalies\n",
    "    if train.loc[idx, 'percentile'] < 90:\n",
    "        train.loc[idx, 'is_abnormal'] = False\n",
    "        continue\n",
    "    \n",
    "    # Eliminating anomalies that come just after na values\n",
    "    if train.loc[idx-5:idx-1, 'Values'].min() < 0:\n",
    "        train.loc[idx, 'is_abnormal'] = False\n",
    "        continue\n",
    "    \n",
    "    # Anomalies should begin just after a ascending percentile pattern\n",
    "    if train.loc[idx-5:idx-1, 'percentile'].mean() >= 80:\n",
    "        train.loc[idx, 'is_abnormal'] = False\n",
    "\n",
    "# Extending the duration of the anomalies\n",
    "for idx in train[train['is_abnormal']].index:\n",
    "    k=0\n",
    "    moving_average = train.loc[idx, 'percentile']\n",
    "    while (moving_average >= 80):\n",
    "        train.loc[idx+k, 'is_abnormal'] = True\n",
    "        k+=1\n",
    "        moving_average = 0.2*moving_average + 0.8*train.loc[idx+k, 'percentile']\n",
    "\n",
    "    # Eliminating anomalies that last less than an hour\n",
    "    if k < 2:\n",
    "        train.loc[idx:idx+k, 'is_abnormal'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train['is_abnormal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10 Illustration of the anomalies found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "begin = 56325-96\n",
    "end = 56325+1+96\n",
    "ax1.plot(train.loc[begin:end, 'percentile'])\n",
    "ax1.plot(train[train['is_abnormal']].loc[begin:end, 'percentile'], color = \"green\", linewidth=10, linestyle=\":\")\n",
    "ax1.set_ylim([0, 100])\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(train.loc[begin:end, 'criteria'], color=\"red\")\n",
    "ax2.set_ylim([-16, 16])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
