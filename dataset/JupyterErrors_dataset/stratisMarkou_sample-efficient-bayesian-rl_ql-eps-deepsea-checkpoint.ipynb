{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (Agents.py, line 89)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1e3f44a9c45e>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from Agents import QLearningAgent, BayesianQAgent, PSRLAgent, MomentMatchingAgent, UbeNoUnrollAgent\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/stratis/Documents/research/nips-2019-rl-workshop/code/Agents.py\"\u001b[0;36m, line \u001b[0;32m89\u001b[0m\n\u001b[0;31m    self.Qlog = []\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/'.join(os.getcwd().split('/')[:-1]))\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "from Agents import QLearningAgent, BayesianQAgent, PSRLAgent, MomentMatchingAgent, UbeNoUnrollAgent\n",
    "from Environments import DeepSea, WideNarrow, PriorMDP\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/big/download_figures/QL-epsilon-greedy’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 6.0)\n",
    "\n",
    "plt.rc('xtick', labelsize=16)\n",
    "plt.rc('ytick', labelsize=16)\n",
    "plt.rc('legend', fontsize=16)\n",
    "plt.rc('figure', titlesize=50)\n",
    "\n",
    "figsave_loc = '/big/download_figures/QL-epsilon-greedy/'\n",
    "method = 'Q-Learning'\n",
    "!mkdir /big/download_figures/QL-epsilon-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_offline_experiment(environment, pi, num_offline_frames, seed, num_oracle_iter):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Save current environment state\n",
    "    s0, t0 = environment.s, environment.t\n",
    "    \n",
    "    # Initial state\n",
    "    environment.reset()\n",
    "    s, t = 0, 0\n",
    "    \n",
    "    states, actions, rewards, states_ = [], [], [], []\n",
    "    \n",
    "    for n in range(num_offline_frames):\n",
    "        \n",
    "        a = pi[s]\n",
    "        \n",
    "        # Step environment\n",
    "        s_, r, t, done = environment.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        states_.append(s_)\n",
    "        \n",
    "        s = s_\n",
    "        \n",
    "    # Initial state\n",
    "    environment.reset()\n",
    "    s, t = 0, 0\n",
    "    \n",
    "    agent_sars_ = np.array(states), np.array(actions), np.array(rewards), np.array(states_)\n",
    "    \n",
    "    oracle_sars_ = run_oracle_experiment(environment,\n",
    "                                         seed,\n",
    "                                         gamma=agent.gamma, \n",
    "                                         num_iter=num_oracle_iter,\n",
    "                                         num_episodes=1,\n",
    "                                         num_frames_per_episode=num_offline_frames-1)\n",
    "    \n",
    "    offline_regret = np.sum((oracle_sars_[2] - agent_sars_[2]))\n",
    "    \n",
    "    # Restore environment state\n",
    "    environment.s, environment.t = s0, t0\n",
    "    \n",
    "    return [offline_regret, oracle_sars_, agent_sars_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment,\n",
    "                   agent,\n",
    "                   seed,\n",
    "                   num_episodes,\n",
    "                   num_frames_per_episode,\n",
    "                   save_every,\n",
    "                   num_offline_frames,\n",
    "                   num_oracle_iter,\n",
    "                   max_buffer_length=0):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Initial state\n",
    "    environment.reset()\n",
    "    s, t = 0, 0\n",
    "\n",
    "    # Save location for agent\n",
    "    save_loc = '/big/tabular_results/{}/{}/seed-{}/'.format(environment.get_name(), agent.get_name(), seed)\n",
    "    pathlib.Path(save_loc).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for n in range(num_episodes):\n",
    "        for i in range(num_frames_per_episode + 1):\n",
    "            \n",
    "            # Take action\n",
    "            a = agent.take_action(s, t)\n",
    "\n",
    "            # Step environment\n",
    "            s_, r, t, done = environment.step(a)\n",
    "\n",
    "            # Update agent\n",
    "            agent.observe([t, s, a, r, s_])\n",
    "            agent.update_after_step(max_buffer_length)\n",
    "\n",
    "            # Update current state (for agent)\n",
    "            s = s_\n",
    "            \n",
    "            if i % save_every == 0:\n",
    "                result = run_offline_experiment(environment,\n",
    "                                                agent.get_greedy_policy(),\n",
    "                                                num_offline_frames,\n",
    "                                                np.random.randint(low=0, high=int(1e6)),\n",
    "                                                num_oracle_iter)\n",
    "                \n",
    "                agent.offline_regret = result[0]\n",
    "                agent.agent_sars_ = result[1]\n",
    "                agent.oracle_sars_ = result[2]\n",
    "                \n",
    "                agent.save_copy(save_loc, '{}_{}'.format(n, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_oracle_experiment(environment, seed, gamma, num_iter, num_episodes, num_frames_per_episode):\n",
    "    \n",
    "    np.random.seed(seed * 10)\n",
    "    \n",
    "    # Initial state\n",
    "    environment.reset()\n",
    "    s, t = 0, 0\n",
    "    \n",
    "    pi, Q = environment.get_optimal_policy(gamma=gamma, num_iter=num_iter)\n",
    "    \n",
    "    states, actions, rewards, states_ = [], [], [], []\n",
    "    \n",
    "    for n in range(num_episodes):\n",
    "        for i in range(num_frames_per_episode + 1):\n",
    "            \n",
    "            # Take action\n",
    "            a = pi[s]\n",
    "\n",
    "            # Step environment\n",
    "            s_, r, t, done = environment.step(a)\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            states_.append(s_)\n",
    "\n",
    "            # Update current state (for agent)\n",
    "            s = s_\n",
    "        \n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(states_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(environment, agent, seed, episode, frame):\n",
    "\n",
    "    # Load location\n",
    "    load_name = '/big/tabular_results/{}/{}/seed-{}/chkpt_{}_{}'.format(environment.get_name(),\n",
    "                                                                        agent.get_name(),\n",
    "                                                                        seed,\n",
    "                                                                        episode,\n",
    "                                                                        frame)\n",
    "    \n",
    "    # Load the agent\n",
    "    fhandle = open(load_name, 'rb')\n",
    "    agent = pickle.load(fhandle)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab255ab6a2a43a09381b5aa015c9024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8f1fc454e3489192bfb330e3b23b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fbbcb0ec944c1e8e6912e98fdc3062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede7caf850934cde8537cb819b1f232d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d913a211e6430894f9d9b8d4eb895c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb53b8934f8b4bd7930024fe7e8433c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45b9c1ace3849e6b75a9f9c2d0d30e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904d440cb6cb4f408b43572bb84038f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4934d0a97144b79a92df09a5132e9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a874d9210fdb4a38af93d6469ba2ccc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b6f4eb1a3d4c14a8fffb54b7fb0c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c41db330e44e82a43f5cd017a727eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f2e8aeeca14480bb534405d0a1bd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4a588aca4440d2aff4fe0a3b81fa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed4547707b84d7dbd8a6626065ec9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for anneal_timescale in tqdm([10000, float('inf')]):\n",
    "    for dither_param in tqdm([1.0]):\n",
    "        for L in tqdm([4, 8, 12, 16, 20]):\n",
    "            for seed in tqdm(range(10)):\n",
    "\n",
    "                # Environment constants\n",
    "                rew_params = ((0., 0.), (-1e-1 * np.exp(- L / 4), 0.), (1., 0.))\n",
    "                env_params = {'L'          :  L,\n",
    "                              'episodic'   :  False,\n",
    "                              'rew_params' :  rew_params}\n",
    "\n",
    "                # Define environment\n",
    "                environment = DeepSea(env_params)\n",
    "                environment.reset()\n",
    "\n",
    "                # Agent constants\n",
    "                agent_params = {'gamma'            : 0.9,\n",
    "                                'dither_mode'      : 'boltzmann',\n",
    "                                'dither_param'     : dither_param,\n",
    "                                'lr'               : 0.1,\n",
    "                                'Q0'               : 0.0,\n",
    "                                'T'                : float('inf'),\n",
    "                                'anneal_timescale' : anneal_timescale,\n",
    "                                'sa_list'          : environment.sa_list()}\n",
    "\n",
    "                # Define agent\n",
    "                agent = QLearningAgent(agent_params)\n",
    "\n",
    "                # Run experiment\n",
    "                run_experiment(environment,\n",
    "                               agent,\n",
    "                               seed=seed,\n",
    "                               num_episodes=1,\n",
    "                               num_frames_per_episode=30000,\n",
    "                               num_offline_frames=1000,\n",
    "                               num_oracle_iter=3*L,\n",
    "                               save_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1d9707f9a744ec8b5bf19c948b6c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2a1488b45e47929dcce47610fde180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/big/tabular_results/DeepSea-E-L_4-mul_0.0-mur_-0.036787944117144235-mut_1.0-sigl_0.0-sigr_0.0-sigt_0.0/QLearningAgent_dither-epsilon-greedy_ditherparam-0.2_gamma-0.9_lr-0.1_Q0-0.0-tscale-10000/seed-1/chkpt_0_10000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-790ffc497805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                              num_frames_per_episode=num_frames)[2]\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0magent_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0monline_regrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle_r\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0magent_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-957fd5a4b92d>\u001b[0m in \u001b[0;36mload_agent\u001b[0;34m(environment, agent, seed, episode, frame)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Load the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/big/tabular_results/DeepSea-E-L_4-mul_0.0-mur_-0.036787944117144235-mut_1.0-sigl_0.0-sigr_0.0-sigt_0.0/QLearningAgent_dither-epsilon-greedy_ditherparam-0.2_gamma-0.9_lr-0.1_Q0-0.0-tscale-10000/seed-1/chkpt_0_10000'"
     ]
    }
   ],
   "source": [
    "env_name = 'DeepSea-{}'\n",
    "\n",
    "num_frames = 30000\n",
    "save_every = 500\n",
    "color = 'orange'\n",
    "\n",
    "for epsilon in [0.2]:\n",
    "    for L in tqdm([4, 8, 12, 16, 20]):\n",
    "\n",
    "        online_regrets = []\n",
    "        offline_regrets = []\n",
    "\n",
    "        for seed in tqdm(range(10)):\n",
    "\n",
    "            offline_regrets.append([])\n",
    "\n",
    "            # Environment constants\n",
    "            rew_params = ((0., 0.), (-1e-1 * np.exp(- L / 4), 0.), (1., 0.))\n",
    "            env_params = {'L'          :  L,\n",
    "                          'episodic'   :  False,\n",
    "                          'rew_params' :  rew_params}\n",
    "\n",
    "            # Define environment\n",
    "            environment = DeepSea(env_params)\n",
    "            environment.reset()\n",
    "\n",
    "            # Agent constants\n",
    "            agent_params = {'gamma'            : 0.9,\n",
    "                            'kappa'            : 1.0,\n",
    "                            'mu0'              : 0.0,\n",
    "                            'lamda'            : 1.0,\n",
    "                            'alpha'            : 2.0,\n",
    "                            'beta'             : 2.0,\n",
    "                            'num_pi_iter'      : 40,\n",
    "                            'T'                : float('inf'),\n",
    "                            'num_dyn_samples'  : 100,\n",
    "                            'sa_list'          : environment.sa_list()}\n",
    "\n",
    "            # Define agent\n",
    "            agent = PSRLAgent(agent_params)\n",
    "\n",
    "            oracle_r = run_oracle_experiment(environment,\n",
    "                                             seed=seed,\n",
    "                                             gamma=0.9,\n",
    "                                             num_iter=2*L,\n",
    "                                             num_episodes=1,\n",
    "                                             num_frames_per_episode=num_frames)[2]\n",
    "\n",
    "            agent_ = load_agent(environment, agent, seed=seed, episode=0, frame=num_frames)\n",
    "\n",
    "            online_regrets.append(np.cumsum(oracle_r - agent_.train_r))\n",
    "\n",
    "            for frame in np.arange(0, num_frames+1, save_every):\n",
    "                agent_ = load_agent(environment, agent, seed=seed, episode=0, frame=frame)\n",
    "                offline_regrets[-1].append(agent_.offline_regret / 1000)\n",
    "\n",
    "        online_regrets = np.array(online_regrets)\n",
    "        online_mu = online_regrets.mean(axis=0)\n",
    "        online_std = online_regrets.var(axis=0)**0.5\n",
    "\n",
    "        offline_regrets = np.array(offline_regrets)\n",
    "        offline_mu = offline_regrets.mean(axis=0)\n",
    "        offline_std = offline_regrets.var(axis=0)**0.5\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(online_mu, color=color, label='Mean')\n",
    "        plt.fill_between(np.arange(len(online_mu)),\n",
    "                         online_mu + online_std,\n",
    "                         online_mu - online_std,\n",
    "                         color=color,\n",
    "                         alpha=0.2,\n",
    "                         label='$\\pm$ St. dev.')\n",
    "\n",
    "        title = 'QL ($\\epsilon = {}$) on DeepSea ($N$ = {})\\nOnline oracle regret (cumulative)'.format(epsilon, L)\n",
    "        plt.title(title.format(L), fontsize=22)\n",
    "        plt.xlabel('# of timesteps', fontsize=20)\n",
    "        plt.ylabel('Oracle regret (cumulative)', fontsize=20)\n",
    "        plt.locator_params(axis='y', nbins=5)\n",
    "        plt.locator_params(axis='x', nbins=5)\n",
    "        plt.legend()\n",
    "        plt.gca().set_xlim(left=0, right=num_frames)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figsave_loc + 'online-epsilon-{}-'.format(str(epsilon).replace('.', '_')) \\\n",
    "                     + env_name.format(L) + '.pdf')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(offline_mu)) * save_every, offline_mu, color=color, label='Mean')\n",
    "        plt.fill_between(np.arange(len(offline_mu)) * save_every,\n",
    "                         offline_mu + offline_std,\n",
    "                         offline_mu - offline_std,\n",
    "                         color=color,\n",
    "                         alpha=0.2,\n",
    "                         label='$\\pm$ St. dev.')\n",
    "\n",
    "        title = 'PSRL on DeepSea ($N$ = {})\\nOffline oracle regret'.format(epsilon, L)\n",
    "        plt.title(title.format(L), fontsize=22)\n",
    "        plt.xlabel('# of timesteps', fontsize=20)\n",
    "        plt.ylabel('Oracle regret (per step)', fontsize=20)\n",
    "        plt.locator_params(axis='y', nbins=5)\n",
    "        plt.locator_params(axis='x', nbins=5)\n",
    "        plt.gca().set_xlim(left=0, right=num_frames)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figsave_loc + 'offline-epsilon-{}-'.format(str(epsilon).replace('.', '_')) \\\n",
    "                     + env_name.format(L) + '.pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
