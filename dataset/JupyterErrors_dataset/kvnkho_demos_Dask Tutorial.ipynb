{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c02e53",
   "metadata": {},
   "source": [
    "# Introduction to Dask (and Distributed Computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b5fcbb",
   "metadata": {},
   "source": [
    "## Limitations of Pandas\n",
    "\n",
    "This is taken from the [Fugue docs](https://fugue-tutorials.readthedocs.io/tutorials/beginner/introduction.html)\n",
    "\n",
    "pandas is great for small datasets, but unfortunately does not scale well large datasets. The primary reason is that pandas is single core, and does not take advantage of all available compute resources. A lot of operations also generate [intermediate copies of data](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets), utilizing more memory than necessary. To effectively handle data with pandas, users preferably need to have [5x to 10x times as much RAM](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) as the size of the dataset.\n",
    "\n",
    "Spark and Dask allow us to split compute jobs across multiple machines. They also can handle datasets that donâ€™t fit into memory by spilling data over to disk in some cases. But ultimately, moving to Spark or Dask still requires significant code changes to port existing pandas code. Added to changing code, there is also a lot of knowledge required to use these frameworks effectively.\n",
    "\n",
    "Pandas enforces vertical scaling by default. When the data exceeds the capcity of the current machine, you need to increase the underlying machine.\n",
    "\n",
    "<img src=\"https://www.cloudzero.com/hubfs/blog/horizontal-vs-vertical-scaling.webp\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a91a8",
   "metadata": {},
   "source": [
    "## Memory Bound versus Compute Bound Problems\n",
    "\n",
    "<img src=\"https://ml.dask.org/_images/dimensions_of_scale.svg\" width=600/>\n",
    "\n",
    "The [dask-ml](https://ml.dask.org/) has a good introduction to the cases that need distributed compute. They either tend to be compute-bound problems or memory-bound problems. For memory-bound problems, the data literally does not fit on a single machine so we need to divide across multiple machinses and keep track where each partition of data lives.\n",
    "\n",
    "On the other hand, compute-bound problems are about tasks that take long to execute to we can speed up execution by running them in parallel or distributedly over a cluster. The point is to utilize untapped resoucres that are not consumed when executing sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85483e6b",
   "metadata": {},
   "source": [
    "## Landscape of Distributed Computing Frameworks\n",
    "\n",
    "### [Dask](https://dask.org/)\n",
    "\n",
    "![img](data:image/png;base64,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)\n",
    "\n",
    "### [Spark](https://spark.apache.org/docs/latest/api/python/)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1280px-Apache_Spark_logo.svg.png\" width=300/>\n",
    "\n",
    "### [Ray](https://www.ray.io/)\n",
    "\n",
    "<img src=\"https://softwareengineeringdaily.com/wp-content/uploads/2020/02/ray-logo.png\" width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d5457",
   "metadata": {},
   "source": [
    "## Parallel Computing versus Distributed Computing\n",
    "\n",
    "[This O'Reilly page](https://learning.oreilly.com/library/view/distributed-computing-in/9781787126992/7478b64c-8de4-4db3-b473-66e1d1fcba77.xhtml) describes the different of parallel and distributed computing. Parallel computing normally is confided to one machine where the processes can share the same memory, while distributed computing happens over multiple machines. In the distributed scenario, a task and the dependencies are sent to other machines through a network connection.\n",
    "\n",
    "![img](https://camo.githubusercontent.com/d35e159e77d69d60395afb6023374f5f4decd6a6f7a5aa08d9d17415d75e63db/68747470733a2f2f7777772e6f7265696c6c792e636f6d2f6c6962726172792f766965772f64697374726962757465642d636f6d707574696e672d696e2f393738313738373132363939322f6173736574732f65313135333733392d663535312d346631662d613434612d3865666663643139333039392e706e67)\n",
    "\n",
    "With that, we can introduce Dask, a distributed computing framework that allows us to scale workflows. Something convenient about Dask is that the local mode is similar to a multiprocessing pool while the distributed mode executes code across a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b59f0",
   "metadata": {},
   "source": [
    "## Introduction to Dask\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/3c64951581aa07190e26664835ef4c5b1e0d108e6f3b1d945a374719e2fd68e9/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3330363338302f3132393033313337352d38333534376561322d623366642d343632332d616439612d6535376464633233613965362e6a7067\" width=500/>\n",
    "\n",
    "We can see example Dask DataFrame code below, notice the wildcard shows we can load multiple files in parallel. The API was also designed to mirror the Pandas API.\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('2014-*.csv')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "Under the hood, Dask DataFrame then takes care of submitting this for execution to the Dask cluster.\n",
    "\n",
    "In the diagram below, note how:\n",
    "- package versions and serialization\n",
    "- reading in files can be optimized\n",
    "- data actually lives on a physical machine\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/3b83d2f3637a19be38f50b7776035cd1b24d730f821f3d3fcdccfc5fae3c5310/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3330363338302f3132393033313431362d66653131376236322d383366362d343763652d393232372d6261386135306462336266382e6a7067\" width=600/>\n",
    "\n",
    "The Client can give on your local computer, and can submit tasks to the Dask cluster. The Schedeuler is the entrypoint that receives this task and decides which worker to send it to. When using the Dask DataFrame API, Dask handles the lower level managing of sending partitions to workers or rearranging the data across the cluster. This is called a shuffle in distributed compute terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef31851",
   "metadata": {},
   "source": [
    "# Map_Partition example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a8ba17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = pd.DataFrame({\"x_1\": [1, 1, 2, 2], \"x_2\":[1, 2, 2, 3]})\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "reg = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5b7a8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_1  x_2  predicted\n",
       "0    3    3       12.0\n",
       "1    4    3       13.0\n",
       "2    6    6       21.0\n",
       "3    6    6       21.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(df: pd.DataFrame, model: LinearRegression) -> pd.DataFrame:\n",
    "    return df.assign(predicted=model.predict(df))\n",
    "\n",
    "input_df = pd.DataFrame({\"x_1\": [3, 4, 6, 6], \"x_2\":[3, 3, 6, 6]})\n",
    "\n",
    "predict(input_df, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86271eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_1  x_2  predicted\n",
       "0    3    3       12.0\n",
       "1    4    3       13.0\n",
       "2    6    6       21.0\n",
       "3    6    6       21.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dask_df = dd.from_pandas(input_df, npartitions=2)\n",
    "ddf = dask_df.map_partitions(predict, reg)\n",
    "ddf.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90346d",
   "metadata": {},
   "source": [
    "## Basic Syntax, Spark versus Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "626e136c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Carrot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   value\n",
       "0   0   Apple\n",
       "1   1  Banana\n",
       "2   2  Carrot"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "df = pd.DataFrame({\"id\":[0,1,2], \"value\": ([\"A\", \"B\", \"C\"])})\n",
    "map_dict = {\"A\": \"Apple\", \"B\": \"Banana\", \"C\": \"Carrot\"}\n",
    "\n",
    "def map_letter_to_food(df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:\n",
    "    df[\"value\"] = df[\"value\"].map(mapping)\n",
    "    return df\n",
    "\n",
    "map_letter_to_food(df, map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814808e",
   "metadata": {},
   "source": [
    "**Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef58a748",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exactly one of npartitions and chunksize must be specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m])})\n\u001b[0;32m----> 2\u001b[0m ddf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/fugue-tutorials/lib/python3.8/site-packages/dask/dataframe/io/io.py:216\u001b[0m, in \u001b[0;36mfrom_pandas\u001b[0;34m(data, npartitions, chunksize, sort, name)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a pandas DataFrame or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (npartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m (chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one of npartitions and chunksize must be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m nrows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exactly one of npartitions and chunksize must be specified."
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"id\":[0,1,2], \"value\": ([\"A\", \"B\", \"C\"])})\n",
    "ddf = dd.from_pandas(df, npartitions=2)\n",
    "ddf[\"value\"] = ddf[\"value\"].map(map_dict)\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b4011",
   "metadata": {},
   "source": [
    "**Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42974b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id|value|  food|\n",
      "+---+-----+------+\n",
      "|  0|    A| Apple|\n",
      "|  1|    B|Banana|\n",
      "|  2|    C|Carrot|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "mapping = create_map([lit(x) for x in chain(*map_dict.items())])\n",
    "sdf = spark.createDataFrame(df)\n",
    "sdf = sdf.withColumn(\"food\", mapping[sdf['value']])\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf28f0d",
   "metadata": {},
   "source": [
    "## Dask collections\n",
    "\n",
    "But while we have discussed this in the context of DataFrames, the DataFrame concept is just one of the available collections. In fact, it is a collection more associated with memory-bound problems while workflow orchestration usage of Dask tends to be compute-based problems.\n",
    "\n",
    "The diagram below shows the other available collections.\n",
    "\n",
    "![img](https://camo.githubusercontent.com/62741530885939ba21ad5a3f72bf130fbf880423fa331136004e95f78d73a948/68747470733a2f2f757365722d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3330363338302f3132393033313338382d36666364306364312d393634332d346633642d623462312d6336653334336262636630382e6a7067)\n",
    "\n",
    "The Dask Bag is like a distributed dictionary or JSON. The Dask Array builds on top of xarray. But the collection that is most relevant to workflow orchestration is Futures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6055f",
   "metadata": {},
   "source": [
    "## Dask Futures\n",
    "\n",
    "Dask Delayed and Dask Futures allow for the submission of arbitrary code to the Dask scheduler. The Dask scheduler then directs the execution to a worker. The difference is that Dask Delayed is evaluated lazily, allowing the computation graph to compile before execution. Knowing the execution graph ahead of time allows Dask to optimize it by analyzing the data dependencies and ensuring that workers have all the dependencies they need to execute tasks. Dask Futures, on the other hand are executed immediately.\n",
    "\n",
    "Below is an example of using the Future interface, which is what Prefect does to submit tasks to a Dask cluster. Here, we perform the execution on a local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dc0f23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: pending, key: inc-88110521fcd41acdd80fda11113e42d2>\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes=False)\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "a = client.submit(inc, )\n",
    "print(a)\n",
    "print(a.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52a8fb",
   "metadata": {},
   "source": [
    "## Cloudpickle Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4dbcc",
   "metadata": {},
   "source": [
    "## Prefect Using Dask\n",
    "\n",
    "Take the following DAG structure. We have two upstream tasks that then fork out into three independent tasks.\n",
    "\n",
    "![img](https://camo.githubusercontent.com/e93383a93eedd9c05d9875ecb39f350acb8b3c796a6ddb9b89e22bedd6b1ccaf/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313430302f312a5934573670486b505162637a6f6d6e685237505a64512e6a706567)\n",
    "\n",
    "If each of the three tasks takes 10 minutes to run, then running them sequentially will take 30 minutes. But if we have enough compute resources, we can just run them in parallel instead and reduce the execution time by 2/3rds. When tasks are independent, we can run them concurrently.\n",
    "\n",
    "Note that the independent tasks do not have to be homogenous tasks. They can be entirely different functions as long as they are independent.\n",
    "\n",
    "This section will cover how to execute tasks parallelly or distributedly.\n",
    "\n",
    "Before talking about how to parallelize, we just need to clarify two terms that will be used a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fafe60",
   "metadata": {},
   "source": [
    "## When to use Dask versus Prefect\n",
    "\n",
    "People already familiar with Dask wonder what Prefect adds because Dask already seemingly has a lot of the features Prefect brings. For example, Dask already has\n",
    "\n",
    "* Retries of code\n",
    "* Scheduling of tasks\n",
    "* A Directed Acyclic Graph (DAG)\n",
    "* Handling of dependencies\n",
    "\n",
    "So when do we use Dask versus Prefect? Or how are they used together?\n",
    "\n",
    "**The Prefect DAG**\n",
    "\n",
    "The Prefect DAG is more robust in handling state. For example, it is superior in the following cases:\n",
    "\n",
    "* restarting a failed Flow from the checkpoint\n",
    "* caching across multiple flow runs\n",
    "* linking results to Flow runs (and parameters) through the GraphQL API\n",
    "* observability into why a Flow failed (additional logging)\n",
    "\n",
    "So what is the relationship of the Dask DAG and the Prefect DAG? Prefect is a more macro-level workflow orchestrator while Dask focuses on compute (micro-level). Prefect cares more across Flow runs while you would need to add extra code for Dask to Handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow() as flow:\n",
    "    a = task_a.map()\n",
    "    b = task_b.map(a)\n",
    "    task_c.map(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6257da4",
   "metadata": {},
   "source": [
    "## Mapping in Prefect 1.0 (Depth First Execution)\n",
    "\n",
    "There is actually an edge case when the Dask DAG is used. This is when we have multiple stages of mapping. For example, check the two-stage mapping operation below. By submitting multiple stages to Dask together, we can execute them in a depth-first fashion. This eliminates the need to collect intermediate results before running the next task.\n",
    "\n",
    "![img](https://camo.githubusercontent.com/ea770293514c19e912414779412502c99178aa24bbb2f585c9585ca082c24b02/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f3630322f312a4b4f692d6b5361516444496454534768614f47656e412e706e67)\n",
    "\n",
    "When these are submitted together, the Dask DAG is used (and sometimes overlaps with the Prefect DAG). For example, if a Dask worker dies, the different stages can be executed again (sometimes duplicated). This is because Dask revives the worker and from it's perspective, does not realize that earlier tasks have been completed and the checkpoint can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce70ea0",
   "metadata": {},
   "source": [
    "## Failure mode in distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f9c99",
   "metadata": {},
   "source": [
    "## Dask versus other distributed computing frameworks\n",
    "\n",
    "When Prefect started out, it specifically was exploring performing Airflow-like workflow orchestration semantics on top of the Dask milli-second latency scheduler. This is because the Airflow scheduler took 10 seconds to submit a task. But why Dask specifically compared to Spark?\n",
    "\n",
    "Spark and Dask are the most widely adopted distributed compute interfaces for Python. The difference though is that Spark has leaned more heavily towards the DataFrame abstraction and has optimized for that. It's very easy in Dask to submit an abitrary function with\n",
    "\n",
    "client.submit(fn)\n",
    "whereas in Spark it's not even straightforward how to perform this operation. In [this](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) blog by Databricks, they recommend using the DataFrame API.\n",
    "\n",
    "Ray, though has been gaining steam as a distributed computing framework so we added a task runner for it in Orion. The docs can be [found here](https://orion-docs.prefect.io/tutorials/dask-ray-task-runners/). The benefits of Ray over Dask were discussed [here](https://github.com/PrefectHQ/prefect/issues/3963). Namely (mentioned by Github user Hoeze):\n",
    "\n",
    "* Ray would make it easy to define resources that a certain task needs (e.g. GPU, memory):\n",
    "* Also, it's shared-memory object store is just superior to Dask's deserialization for large objects when working with Dask tasks (e.g. check [dask-on-ray](https://github.com/ray-project/ray/issues/13620) Zero-copy deserialization of dask.delayed() objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14196c41",
   "metadata": {},
   "source": [
    "## CSV versus Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c1d5ed",
   "metadata": {},
   "source": [
    "## Setting-up a Dask cluster\n",
    "One reason why Dask is widely adopted is because of the ease of spinning up your own ephemeral cluster for the duration of an application (or flow run). This documentation page contains the various ways that a Dask cluster can be deployed. The choice of cluster largely depeneds on your infrastructure. The most commons ones are:\n",
    "\n",
    "* Dask on Kubernetes\n",
    "* Dask on AWS/ECS or Fargate\n",
    "* Managed services like [Coiled](https://coiled.io/) or [Saturn Cloud](https://saturncloud.io/)\n",
    "\n",
    "The important thing to note in distributed systems is that the package versions on the workers need to be the same as the scheduler and client. Otherwise, it's very easy to run into inconsistent execution, or programs will raise an error. Most Dask cluster initialization method will take in a Docker base image that can be used to spin-up the workers. This guarantees execution.\n",
    "\n",
    "For example, this is what spinning up a Dask cluster looks like with a KubeCluster\n",
    "\n",
    "```python\n",
    "from dask_kubernetes import KubeCluster, make_pod_spec\n",
    "\n",
    "pod_spec = make_pod_spec(image='prefecthq/prefect:latest')\n",
    "cluster = KubeCluster(pod_spec)\n",
    "cluster.scale(10)\n",
    "```\n",
    "This is why it's important to know how to build your own image to include the dependencies\n",
    "\n",
    "After this, you can connect the Client with:\n",
    "\n",
    "```python\n",
    "client = Client(cluster)\n",
    "```\n",
    "but Prefect does it for you under the hood in order to submit your tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91862c",
   "metadata": {},
   "source": [
    "## Using DaskExecutor in Prefect 1\n",
    "\n",
    "```python\n",
    "executor = DaskExecutor(\n",
    "    cluster_class=\"dask_cloudprovider.FargateCluster\",\n",
    "    cluster_kwargs={\n",
    "        \"image\": \"prefecthq/prefect:latest\",\n",
    "        \"n_workers\": 5,\n",
    "        ...\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431264e",
   "metadata": {},
   "source": [
    "## Using the Dask TaskRunner for Orion\n",
    "\n",
    "Utilizing the DaskTaskRunner in Prefect Orion is as simple as adding it to the @flow decorator. If not specified, the default Task Runner used is the ConcurrentTaskRunner, which actually runs tasks asynchronously already. It should be comparable to Dask on local, and now Dask can scale beyond one machine.\n",
    "\n",
    "The ConcurrentTaskRunner was introduced to remove the Dask dependency for local parallelism.\n",
    "\n",
    "```python\n",
    "from prefect import flow, task\n",
    "from prefect.task_runners import DaskTaskRunner\n",
    "\n",
    "@task\n",
    "def say_hello(name):\n",
    "    print(f\"hello {name}\")\n",
    "\n",
    "@task\n",
    "def say_goodbye(name):\n",
    "    print(f\"goodbye {name}\")\n",
    "\n",
    "@flow(task_runner=DaskTaskRunner())\n",
    "def greetings(names):\n",
    "    for name in names:\n",
    "        say_hello(name)\n",
    "        say_goodbye(name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    greetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e77698",
   "metadata": {},
   "source": [
    "We can then pass a callable or address to the DaskTaskRunner to create an ephemeral cluster or connec to an already existing cluster.\n",
    "\n",
    "```python\n",
    "DaskTaskRunner(\n",
    "    cluster_class=\"dask_cloudprovider.FargateCluster\",\n",
    "    cluster_kwargs={\n",
    "         \"image\": \"prefecthq/prefect:latest\",\n",
    "         \"n_workers\": 5,\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "Note we can also do the same for Ray. In the next section, we'll see how to create our own image that we can use to guarantee consistency across Flow Runs and when creating a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532ec58",
   "metadata": {},
   "source": [
    "## [Reference on Partitions](https://blog.scottlogic.com/2018/03/22/apache-spark-performance.html) by Scott Logic\n",
    "\n",
    "This reference has a lot of good images and explanations\n",
    "\n",
    "### Ideal Partitioning Strategy\n",
    "![Partitioning](https://blog.scottlogic.com/mdebeneducci/assets/Ideal-Partitioning.png)\n",
    "### Skewed Partitions\n",
    "![Skewed Partitions](https://blog.scottlogic.com/mdebeneducci/assets/Skewed-Partitions.png)\n",
    "### Inefficient Scheduling\n",
    "![Inefficient Scheduling](https://blog.scottlogic.com/mdebeneducci/assets/Inefficient-Scheduling.png)\n",
    "### Data Shuffling\n",
    "![Shuffle](https://blog.scottlogic.com/mdebeneducci/assets/Shuffle-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914fcc3",
   "metadata": {},
   "source": [
    "## Pitfalls of Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6146e4",
   "metadata": {},
   "source": [
    "### Inefficient Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1324c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4\n",
       "5  5\n",
       "6  6\n",
       "7  7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from time import sleep\n",
    "\n",
    "def delay(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    sleep(df.shape[0]*3)\n",
    "    return df.assign(b=df.shape[0])\n",
    "\n",
    "pdf = pd.DataFrame([[0],[1],[2],[3],[4],[5],[6],[7]], columns=[\"a\"])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e48483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 592 ms, sys: 71.8 ms, total: 664 ms\n",
      "Wall time: 6.05 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  0  2\n",
       "1  1  2\n",
       "2  2  2\n",
       "3  3  2\n",
       "4  4  2\n",
       "5  5  2\n",
       "6  6  2\n",
       "7  7  2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.from_pandas(pdf, npartitions=4)\n",
    "ddf.map_partitions(delay, meta={\"a\":\"int32\",\"b\":\"int32\"}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3ba0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 514 ms, sys: 55.5 ms, total: 569 ms\n",
      "Wall time: 6.04 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  0  1\n",
       "1  1  1\n",
       "2  2  2\n",
       "3  3  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pdf = pd.DataFrame([[0],[1],[2],[3]], columns=[\"a\"])\n",
    "ddf = dd.from_pandas(pdf, npartitions=4)\n",
    "ddf.map_partitions(delay, meta={\"a\":\"int32\",\"b\":\"int32\"}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45f0ba",
   "metadata": {},
   "source": [
    "## Lineage and Persisting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254bd06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0aa233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a         b\n",
      "0  0  0.651542\n",
      "1  1  0.810021\n",
      "2  2  0.076631\n",
      "3  3  0.138025\n",
      "4  4  0.202097\n",
      "5  5  0.956513\n",
      "6  6  0.738100\n",
      "7  7  0.793247\n",
      "   a         b\n",
      "0  0  0.651542\n",
      "1  1  0.810021\n",
      "2  2  0.076631\n",
      "CPU times: user 1.32 s, sys: 183 ms, total: 1.5 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def gen_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sleep(df.shape[0]*3)\n",
    "    return df.assign(b=np.random.random((df.shape[0], 1)))\n",
    "\n",
    "pdf = pd.DataFrame([[0],[1],[2],[3],[4],[5],[6],[7]], columns=[\"a\"])\n",
    "result = gen_data(pdf)\n",
    "print(result)\n",
    "print(result.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa07cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a         b\n",
      "0  0  0.608975\n",
      "1  1  0.082308\n",
      "2  2  0.577186\n",
      "3  3  0.724430\n",
      "4  4  0.822322\n",
      "5  5  0.159231\n",
      "6  6  0.980479\n",
      "7  7  0.696390\n",
      "   a         b\n",
      "0  0  0.486628\n",
      "1  1  0.161387\n",
      "CPU times: user 1.05 s, sys: 120 ms, total: 1.17 s\n",
      "Wall time: 12.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/fugue-tutorials/lib/python3.8/site-packages/dask/dataframe/core.py:7352: UserWarning: Insufficient elements for `head`. 3 elements requested, only 2 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.from_pandas(pdf, npartitions=4)\n",
    "result = ddf.map_partitions(gen_data, meta={\"a\": \"int32\", \"b\":\"i8\"})\n",
    "print(result.compute())\n",
    "print(result.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b5221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a         b\n",
      "0  0  0.924826\n",
      "1  1  0.434186\n",
      "2  2  0.533162\n",
      "3  3  0.919182\n",
      "4  4  0.520236\n",
      "5  5  0.027585\n",
      "6  6  0.524088\n",
      "7  7  0.169923\n",
      "   a         b\n",
      "0  0  0.924826\n",
      "1  1  0.434186\n",
      "CPU times: user 637 ms, sys: 64.3 ms, total: 701 ms\n",
      "Wall time: 6.07 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/fugue-tutorials/lib/python3.8/site-packages/dask/dataframe/core.py:7352: UserWarning: Insufficient elements for `head`. 3 elements requested, only 2 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.from_pandas(pdf, npartitions=4)\n",
    "result = ddf.map_partitions(gen_data, meta={\"a\": \"int32\", \"b\":\"i8\"}).persist()\n",
    "print(result.compute())\n",
    "print(result.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d1a20",
   "metadata": {},
   "source": [
    "## Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd648054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4\n",
       "5  5\n",
       "6  6\n",
       "7  7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_col(df):\n",
    "    print(df[\"a\"].iloc[0])\n",
    "    if df[\"a\"].iloc[0] == 7:\n",
    "        return df.assign(b=None)\n",
    "    else:\n",
    "        return df.assign(b=1)\n",
    "    \n",
    "pdf = pd.DataFrame([[x] for x in range(8)], columns=[\"a\"])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db93cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a     b\n",
       "0  0     1\n",
       "1  1     1\n",
       "2  2     1\n",
       "3  3     1\n",
       "4  4     1\n",
       "5  5     1\n",
       "6  6     1\n",
       "7  7  None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.groupby(\"a\").apply(add_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faea2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a     int64\n",
       "b    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.groupby(\"a\").apply(add_col).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee641d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/f4r6ylss0k7dwh8c_0dmzd_40000gn/T/ipykernel_5975/2870419602.py:2: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  ddf.groupby(\"a\").apply(add_col).dtypes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a    int64\n",
       "b    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.from_pandas(pdf, npartitions=2)\n",
    "ddf.groupby(\"a\").apply(add_col).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bef87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col_2(df):\n",
    "    if df[\"a\"].iloc[0] == 1:\n",
    "        sleep(5)\n",
    "    return df.assign(b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796d7458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 324 ms, sys: 44.2 ms, total: 369 ms\n",
      "Wall time: 5.03 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed eval>:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a    int64\n",
       "b    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf.groupby(\"a\").apply(add_col_2).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ac9138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed eval>:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 759 ms, sys: 91.9 ms, total: 851 ms\n",
      "Wall time: 10.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "1  1  1\n",
       "0  0  1\n",
       "2  2  1\n",
       "3  3  1\n",
       "4  4  1\n",
       "5  5  1\n",
       "6  6  1\n",
       "7  7  1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf.groupby(\"a\").apply(add_col_2).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
