{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from typing import *\n",
    "\n",
    "from typet5.utils import proj_root, get_data_dir\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = get_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets:  tk_dataset-all_labels-drop_comments\n"
     ]
    }
   ],
   "source": [
    "# experiment configurations\n",
    "\n",
    "import torch\n",
    "\n",
    "from typet5.data import (\n",
    "    TokenizedSrcSet,\n",
    "    get_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from typet5.model import CtxArgs, DecodingArgs, ModelSPOT, ModelWrapper\n",
    "from copy import copy\n",
    "from typet5.train import TrainingConfig, TypeCheckArgs\n",
    "\n",
    "config = TrainingConfig(quicktest=False, all_labels=True)\n",
    "train_R1: bool = True\n",
    "load_R0: bool = True\n",
    "load_critic: bool = False\n",
    "gpu_id = 0\n",
    "TypeCheckSettings.temp_path = f\"GPU-{gpu_id}\"\n",
    "\n",
    "project_name = \"test-SPOT\" if config.quicktest else \"SPOT\"\n",
    "train_ctx_args = config.train_ctx_args()\n",
    "tc_args = TypeCheckArgs(check_in_isolation=config.check_in_isolation)\n",
    "\n",
    "max_tokens_per_file = config.ctx_size\n",
    "dec_args = DecodingArgs(\n",
    "    sampling_max_tokens=8 * max_tokens_per_file,\n",
    "    ctx_args=config.dec_ctx_args(),\n",
    "    max_workers=20,\n",
    ")\n",
    "\n",
    "\n",
    "datasets_name = get_dataset_name(\n",
    "    drop_comments=config.drop_comments,\n",
    "    all_labels=config.all_labels,\n",
    ")\n",
    "\n",
    "r0_model_name = \"R0-model--\" + config.as_name()\n",
    "\n",
    "tk_dataset = load_tokenized_srcsets(\n",
    "    datadir,\n",
    "    datasets_name,\n",
    "    data_reduction=config.data_reduction,\n",
    "    quicktest=config.quicktest,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from typet5.train import ModelTrainingArgs, train_spot_model, TypeCheckArgs\n",
    "import wandb\n",
    "\n",
    "train_args = ModelTrainingArgs(\n",
    "    train_ctx_args,\n",
    "    dec_args,\n",
    "    train_max_tokens=max_tokens_per_file,\n",
    "    eval_max_tokens=2 * max_tokens_per_file,\n",
    "    max_epochs=2,\n",
    "    tc_args=tc_args,\n",
    ")\n",
    "\n",
    "if not load_R0:\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=r0_model_name,\n",
    "        config=config.as_dict(),\n",
    "        dir=str(datadir),\n",
    "    )\n",
    "    r0_wrapper, r0_extra = train_spot_model(\n",
    "        tk_dataset,\n",
    "        r0_model_name,\n",
    "        train_args=train_args,\n",
    "        record_batches=train_R1,\n",
    "        gpus=[gpu_id],\n",
    "        quicktest=config.quicktest,\n",
    "        use_small_model=config.use_small_model,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecodingArgs(ctx_args=CtxArgs(ctx_size=4096, left_margin=2048, right_margin=1024), sampling_max_tokens=32768, max_workers=20)\n"
     ]
    }
   ],
   "source": [
    "# load trained model\n",
    "from typet5.utils import pickle_load, pickle_dump\n",
    "\n",
    "r0_wrapper = ModelWrapper.from_pretrained(\n",
    "    datadir / f\"checkpoints/lit-saved/{r0_model_name}\"\n",
    ")\n",
    "# if train_R1:\n",
    "    # r0_extra = pickle_load(datadir / f\"checkpoints/lit-saved/{r0_model_name}/extra.pkl\")\n",
    "    # r1_tk_dataset: dict[str, TokenizedSrcSet] = r0_extra[\"R1-tk_dataset\"]\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "r0_wrapper.to(device)\n",
    "r0_wrapper.args.do_sample = False\n",
    "print(r0_wrapper.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute_preexisting_fdbks: 100%|██████████| 3/3 [00:02<00:00,  1.25it/s]\n",
      "Evaluating: 100%|██████████| 219/219 [01:03<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: nan\n",
      "acc: 0.73973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>avg_time</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type checking</td>\n",
       "      <td>219</td>\n",
       "      <td>0.564880</td>\n",
       "      <td>123.708648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>predict next type</td>\n",
       "      <td>219</td>\n",
       "      <td>0.306826</td>\n",
       "      <td>67.194790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generate new src</td>\n",
       "      <td>219</td>\n",
       "      <td>0.206394</td>\n",
       "      <td>45.200321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  count  avg_time  total_time\n",
       "1      type checking    219  0.564880  123.708648\n",
       "0  predict next type    219  0.306826   67.194790\n",
       "2   generate new src    219  0.206394   45.200321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test DAgger\n",
    "from typet5.dagger import DAggerModel\n",
    "from typet5.utils import print_limited, display, pretty_print_dict\n",
    "\n",
    "dmodel = DAggerModel(r0_wrapper)\n",
    "\n",
    "metrics = await dmodel.eval_on_data(tk_dataset[\"test\"][1:10], concurrency=8)\n",
    "pretty_print_dict(metrics)\n",
    "\n",
    "display(dmodel.t_logger.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute_preexisting_fdbks: 100%|██████████| 612/612 [00:59<00:00, 10.33it/s]\n",
      "Training:   0%|          | 252/295457 [01:15<31:46:38,  2.58it/s]/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/libcst/_nodes/whitespace.py:93: RuntimeWarning: coroutine 'throttled_async_run' was never awaited\n",
      "  if SIMPLE_WHITESPACE_RE.fullmatch(self.value) is None:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Training:   0%|          | 260/295457 [01:16<19:20:40,  4.24it/s]/home/jiayi/anaconda3/envs/py3.10/lib/python3.10/dataclasses.py:1203: RuntimeWarning: coroutine 'throttled_async_run' was never awaited\n",
      "  return tuple(f for f in fields.values() if f._field_type is _FIELD)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Training:   0%|          | 277/295457 [01:24<33:05:52,  2.48it/s]/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/libcst/metadata/position_provider.py:60: RuntimeWarning: coroutine 'throttled_async_run' was never awaited\n",
      "  self._stack.append(CodePosition(self.line, self.column))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Training:   0%|          | 279/295457 [01:24<31:53:30,  2.57it/s]/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/libcst/_nodes/base.py:301: RuntimeWarning: coroutine 'throttled_async_run' was never awaited\n",
      "  self._codegen_impl(state, **kwargs)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Training:   0%|          | 286/295457 [01:26<21:45:15,  3.77it/s]/home/jiayi/anaconda3/envs/py3.10/lib/python3.10/contextlib.py:103: RuntimeWarning: coroutine 'throttled_async_run' was never awaited\n",
      "  self.gen = func(*args, **kwds)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Training:   0%|          | 315/295457 [01:37<28:33:17,  2.87it/s]/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/libcst/metadata/position_provider.py:102: RuntimeWarning: coroutine 'throttled_async_run' was never awaited\n",
      "  start = CodePosition(self.line, self.column)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Training:   0%|          | 316/295457 [01:40<25:59:28,  3.15it/s]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/SPOT/src/spot/dagger.py:239\u001b[0m, in \u001b[0;36mtask\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks)\n\u001b[0;32m--> 239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_typechecked_src\u001b[39m(src: TokenizedSrc, assignment, check_r) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TokenizedSrc:\n\u001b[1;32m    240\u001b[0m     errors, current_code \u001b[39m=\u001b[39m check_r\n",
      "File \u001b[0;32m~/Projects/SPOT/src/spot/dagger.py:177\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(src)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m progress \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m    173\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_on_src(\n\u001b[1;32m    174\u001b[0m     src,\n\u001b[1;32m    175\u001b[0m     env,\n\u001b[1;32m    176\u001b[0m     state,\n\u001b[0;32m--> 177\u001b[0m     callback\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m _: pbar\u001b[39m.\u001b[39mupdate(),\n\u001b[1;32m    178\u001b[0m     expert_rate\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m progress,\n\u001b[1;32m    179\u001b[0m )\n\u001b[1;32m    180\u001b[0m preds \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mtype_assignment\n",
      "File \u001b[0;32m~/Projects/SPOT/src/spot/dagger.py:116\u001b[0m, in \u001b[0;36mrun_on_src\u001b[0;34m(self, src, typecheck_env, model_executor, cpu_executor, state, callback, expert_rate)\u001b[0m\n\u001b[1;32m    108\u001b[0m     check_r \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m eloop\u001b[39m.\u001b[39mrun_in_executor(\n\u001b[1;32m    109\u001b[0m         cpu_executor,\n\u001b[1;32m    110\u001b[0m         type_check_src_in_project,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m         typecheck_env\u001b[39m.\u001b[39mpre_fdbks[src\u001b[39m.\u001b[39mfile],\n\u001b[1;32m    115\u001b[0m     )\n\u001b[0;32m--> 116\u001b[0m \u001b[39mwith\u001b[39;00m t_logger\u001b[39m.\u001b[39mtimed(\u001b[39m\"\u001b[39m\u001b[39mgenerate new src\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    117\u001b[0m     new_src \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m eloop\u001b[39m.\u001b[39mrun_in_executor(\n\u001b[1;32m    118\u001b[0m         cpu_executor, get_typechecked_src, src, assignment, check_r\n\u001b[1;32m    119\u001b[0m     )\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jiayi/Projects/SPOT/scripts/train_typet5.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/train_typet5.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m dmodel \u001b[39m=\u001b[39m DAggerModel(r0_wrapper)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/train_typet5.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m dmodel\u001b[39m.\u001b[39mt_logger\u001b[39m.\u001b[39mclear()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/train_typet5.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mawait\u001b[39;00m dmodel\u001b[39m.\u001b[39mtrain_on_data(tk_dataset, DAggerArgs())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Butopia3/home/jiayi/Projects/SPOT/scripts/train_typet5.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m display(dmodel\u001b[39m.\u001b[39mt_logger\u001b[39m.\u001b[39mas_dataframe())\n",
      "File \u001b[0;32m~/Projects/SPOT/src/spot/dagger.py:197\u001b[0m, in \u001b[0;36mtrain_on_data\u001b[0;34m(self, tk_dataset, dagger_args, concurrency)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_on_data\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     dataset: TokenizedSrcSet,\n\u001b[1;32m    194\u001b[0m     concurrency: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[1;32m    195\u001b[0m ):\n\u001b[1;32m    196\u001b[0m     correct_seq \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 197\u001b[0m     loss_seq \u001b[39m=\u001b[39m []\n\u001b[1;32m    199\u001b[0m     \u001b[39mwith\u001b[39;00m dataset\u001b[39m.\u001b[39msetup_typechecking(dataset\u001b[39m.\u001b[39mall_srcs) \u001b[39mas\u001b[39;00m env, tqdm(\n\u001b[1;32m    200\u001b[0m         total\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m(\u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39mtypes) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mall_srcs), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluating\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar, ThreadPoolExecutor(\u001b[39m1\u001b[39m) \u001b[39mas\u001b[39;00m model_executor, ProcessPoolExecutor(\n\u001b[1;32m    202\u001b[0m         DefaultWorkers\n\u001b[1;32m    203\u001b[0m     ) \u001b[39mas\u001b[39;00m cpu_executor:\n\u001b[1;32m    205\u001b[0m         \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mtask\u001b[39m(src):\n",
      "File \u001b[0;32m~/Projects/SPOT/src/spot/dagger.py:242\u001b[0m, in \u001b[0;36mthrottled_async_run\u001b[0;34m(f, xs, concurrency)\u001b[0m\n\u001b[1;32m    240\u001b[0m errors, current_code \u001b[39m=\u001b[39m check_r\n\u001b[1;32m    241\u001b[0m errors \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(errors, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m errors\n\u001b[0;32m--> 242\u001b[0m new_src \u001b[39m=\u001b[39m feedbacks_to_tokenized_src(\n\u001b[1;32m    243\u001b[0m     src,\n\u001b[1;32m    244\u001b[0m     current_code,\n\u001b[1;32m    245\u001b[0m     errors,\n\u001b[1;32m    246\u001b[0m     patch_predictions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    247\u001b[0m )\n\u001b[1;32m    248\u001b[0m new_src\u001b[39m.\u001b[39mprev_types \u001b[39m=\u001b[39m assignment\n\u001b[1;32m    249\u001b[0m new_src \u001b[39m=\u001b[39m new_src\u001b[39m.\u001b[39minline_prev_predictions(as_comment\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the DAgger model\n",
    "from typet5.dagger import DAggerModel, DAggerArgs\n",
    "from typet5.utils import display, pretty_print_dict\n",
    "\n",
    "dmodel = DAggerModel(r0_wrapper)\n",
    "dmodel.t_logger.clear()\n",
    "await dmodel.train_on_data(tk_dataset, DAggerArgs())\n",
    "display(dmodel.t_logger.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dmodel.t_logger.as_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from typet5.train import evaluate_model\n",
    "from typet5.utils import PickleCache\n",
    "from typet5.visualization import display_persist, dict_widget\n",
    "\n",
    "r0_cache = PickleCache(datadir / f\"checkpoints/lit-saved/{r0_model_name}/eval_cache\")\n",
    "r0_eval = evaluate_model(\n",
    "    r0_wrapper,\n",
    "    None,\n",
    "    tk_dataset[\"test\"],\n",
    "    eval_cache=r0_cache,\n",
    "    tc_args=train_args.tc_args,\n",
    ")\n",
    "r0_accs = r0_eval[0][1].accuracies\n",
    "display_persist(dict_widget(r0_accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close wandb\n",
    "from typet5.utils import pretty_show_dict\n",
    "from typet5.visualization import string_to_html\n",
    "import wandb\n",
    "\n",
    "\n",
    "def wandb_string(s: str):\n",
    "    return wandb.Html(string_to_html(s))\n",
    "\n",
    "\n",
    "if not load_R0:\n",
    "    for i, e in enumerate(r0_eval):\n",
    "        wandb.log({f\"test/R{i}\": wandb_string(pretty_show_dict(e[1].accuracies))})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the code with inlined predictions as HTML\n",
    "\n",
    "from typet5.visualization import export_preds_on_code, display_persist, proj_root\n",
    "\n",
    "export_preds = False\n",
    "\n",
    "if export_preds:\n",
    "    pr = r0_eval[0][1]\n",
    "    sub_ids = range(0, len(pr.chunks), 10)\n",
    "    export_preds_on_code(\n",
    "        pr.chunks[sub_ids],\n",
    "        [pr.predictions[i] for i in sub_ids],\n",
    "        {},\n",
    "        export_to=proj_root() / \"R0_predictions\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the critic\n",
    "from typet5.critic import (\n",
    "    CriticModel,\n",
    "    ModelSPOT,\n",
    "    train_critic_model,\n",
    "    CriticTrainArgs,\n",
    "    get_critic_name,\n",
    ")\n",
    "from typet5.utils import pickle_load, run_long_task, PickleCache\n",
    "from typet5.train import R1_srcs_from_extra, R1_srcs_from_model\n",
    "import wandb\n",
    "\n",
    "critic_new_data = True\n",
    "critic_no_feedback = False\n",
    "critic_name = get_critic_name(critic_no_feedback, critic_new_data, config)\n",
    "\n",
    "with run_long_task(f\"Training Critic: {critic_name}\", notify=not load_critic):\n",
    "    critic_train_args = CriticTrainArgs(\n",
    "        ctx_args=train_ctx_args,\n",
    "        train_max_tokens=max_tokens_per_file,\n",
    "        eval_max_tokens=2 * max_tokens_per_file,\n",
    "        max_epochs=1,\n",
    "    )\n",
    "\n",
    "    critic_tc_args = tc_args._replace(no_feedback=critic_no_feedback)\n",
    "    critic_cache = PickleCache(\n",
    "        datadir / f\"checkpoints/lit-saved/CriticData-{critic_name}\"\n",
    "    )\n",
    "    # critic_cache.remove(\"tk_dataset\")\n",
    "    critic_tk_dataset: dict[str, TokenizedSrcSet]\n",
    "\n",
    "    if critic_new_data:\n",
    "        # use sampling to increase example diversity\n",
    "        r0_wrapper.args.do_sample = True\n",
    "        r0_wrapper.args.top_p = 0.9\n",
    "\n",
    "    critic_tk_dataset = critic_cache.cached(\n",
    "        \"tk_dataset\",\n",
    "        lambda: {\n",
    "            k: v.inline_predictions(as_comment=False)\n",
    "            for k, v in (\n",
    "                R1_srcs_from_model(\n",
    "                    r0_wrapper,\n",
    "                    tk_dataset,\n",
    "                    critic_tc_args,\n",
    "                )\n",
    "                if critic_new_data\n",
    "                else R1_srcs_from_extra(\n",
    "                    r0_wrapper,\n",
    "                    tk_dataset,\n",
    "                    extra=pickle_load(\n",
    "                        datadir / f\"checkpoints/lit-saved/{r0_model_name}/extra.pkl\"\n",
    "                    ),\n",
    "                    tc_args=critic_tc_args,\n",
    "                )\n",
    "            ).items()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if not load_critic:\n",
    "        wandb.init(\n",
    "            project=project_name,\n",
    "            name=critic_name,\n",
    "            config=config.as_dict(),\n",
    "            dir=str(datadir),\n",
    "        )\n",
    "        critic, critic_extra = train_critic_model(\n",
    "            critic_tk_dataset,\n",
    "            critic_train_args,\n",
    "            critic_name,\n",
    "            gpus=[gpu_id],\n",
    "            quicktest=config.quicktest,\n",
    "            use_early_stop=False,\n",
    "            use_small_model=config.use_small_model,\n",
    "        )\n",
    "        # critic.save_pretrained(\"CriticSaved\")\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained critic\n",
    "from typet5.utils import pickle_load, pickle_dump\n",
    "from typet5.critic import CriticModel\n",
    "\n",
    "critic = CriticModel.load(datadir / f\"checkpoints/lit-saved/{critic_name}\")\n",
    "if train_R1 and (\"r1_tk_dataset\" not in globals()):\n",
    "    r0_extra = pickle_load(datadir / f\"checkpoints/lit-saved/{r0_model_name}/extra.pkl\")\n",
    "    r1_tk_dataset: dict[str, TokenizedSrcSet] = r0_extra[\"R1-tk_dataset\"]\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "critic.to(device)\n",
    "print(\"Critic loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show critic performance\n",
    "\n",
    "from typet5.visualization import visualize_preds_on_code, pretty_print_dict\n",
    "\n",
    "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "critic.to(device)\n",
    "r1_testset = critic_tk_dataset[\"test\"]\n",
    "critic_eval = critic.eval_on_src_dataset(\n",
    "    r1_testset, train_ctx_args, dec_args.sampling_max_tokens\n",
    ")\n",
    "nicer_preds = [[f\"{x:.1%}\" for x in xs] for xs in critic_eval[1]]\n",
    "pretty_print_dict(critic_eval[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The performance achieved by always predicting true or random values\n",
    "\n",
    "from typet5.utils import not_none, pretty_print_dict\n",
    "from typet5.type_check import normalize_type\n",
    "from typet5.critic import CriticModel\n",
    "import random\n",
    "\n",
    "\n",
    "def dummy_performance(dataset: TokenizedSrcSet, pred_f):\n",
    "    targets = list[bool]()\n",
    "    for s in dataset.all_srcs:\n",
    "        for p, t in zip(not_none(s.prev_types).values(), s.types):\n",
    "            targets.append(normalize_type(t) == normalize_type(p))\n",
    "\n",
    "    preds = [pred_f() for _ in range(len(targets))]\n",
    "    return CriticModel.compute_metrics(preds, targets)\n",
    "\n",
    "\n",
    "pretty_print_dict(dummy_performance(r1_testset, lambda: True))\n",
    "pretty_print_dict(dummy_performance(r1_testset, lambda: random.choice([True, False])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.utils import DefaultTokenizer, decode_tokens, np\n",
    "\n",
    "\n",
    "def chunk_has_fdbk(tks):\n",
    "    return \"/* error:\" in decode_tokens(tks)\n",
    "\n",
    "\n",
    "test_chunks = r1_tk_dataset[\"test\"].to_chunks(DefaultTokenizer, dec_args.ctx_args)\n",
    "fraction_chunks_with_fdbk = np.mean(\n",
    "    [chunk_has_fdbk(tks) for tks in test_chunks.data[\"input_ids\"]]\n",
    ")\n",
    "print(\"Fraction of chunks with feedback:\", fraction_chunks_with_fdbk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking mypy feedbacks\n",
    "from typet5.visualization import show_feedback_stats\n",
    "\n",
    "if train_R1:\n",
    "    error_groups = show_feedback_stats(r1_tk_dataset[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize feedback samples\n",
    "\n",
    "from typet5.utils import seq_flatten, add_line_numbers\n",
    "from typet5.visualization import code_inline_type_masks, visualize_sequence, display\n",
    "\n",
    "\n",
    "if train_R1:\n",
    "    to_display = []\n",
    "    for xs in error_groups[\"return-value\"]:  # seq_flatten(error_groups.values()):\n",
    "        src = xs[1]\n",
    "        code = code_inline_type_masks(src.origin_code, src.types)\n",
    "        to_display.append(\n",
    "            f\"feedback: {xs[0]}\\n\" + \"=========code=========\\n\" + add_line_numbers(code)\n",
    "        )\n",
    "    if len(to_display) > 0:\n",
    "        display(visualize_sequence(to_display))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1 training\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from typet5.data import TokenizedSrcSet, get_dataset_name\n",
    "from typet5.model import CtxArgs, DecodingArgs, ModelSPOT, ModelWrapper\n",
    "\n",
    "load_R1 = False\n",
    "r1_model_name = \"R1-model--\" + config.as_name()\n",
    "\n",
    "if not load_R1:\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=r1_model_name,\n",
    "        config=config.as_dict(),\n",
    "        dir=str(datadir),\n",
    "    )\n",
    "\n",
    "    r1_train_args = copy(train_args)\n",
    "    r1_train_args.max_epochs = 1\n",
    "\n",
    "    r1_wrapper, r1_extra = train_spot_model(\n",
    "        r1_tk_dataset,\n",
    "        r1_model_name,\n",
    "        train_args=r1_train_args,\n",
    "        gpus=[gpu_id],\n",
    "        record_batches=False,\n",
    "        quicktest=config.quicktest,\n",
    "        use_early_stop=False,\n",
    "        use_small_model=config.use_small_model,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model and evaluate\n",
    "from typet5.train import evaluate_model\n",
    "from typet5.visualization import visualize_dicts\n",
    "\n",
    "r1_wrapper = ModelWrapper.from_pretrained(\n",
    "    datadir / f\"checkpoints/lit-saved/{r1_model_name}\"\n",
    ")\n",
    "r1_wrapper.to(device)\n",
    "\n",
    "r1_cache = PickleCache(datadir / f\"checkpoints/lit-saved/{r1_model_name}/eval_cache\")\n",
    "r1_cache.clear()\n",
    "r1_eval = evaluate_model(\n",
    "    r0_wrapper,\n",
    "    r1_wrapper,\n",
    "    tk_dataset[\"test\"],\n",
    "    tc_args=tc_args,\n",
    "    eval_cache=r1_cache,\n",
    ")\n",
    "visualize_dicts([x[1].accuracies for x in r1_eval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.visualization import export_preds_on_code, display_persist, proj_root\n",
    "\n",
    "eval_to_viz = r1_eval[1][1]\n",
    "sub_ids = range(0, len(eval_to_viz.chunks), 10)\n",
    "export_preds_on_code(\n",
    "    eval_to_viz.chunks[sub_ids],\n",
    "    [eval_to_viz.predictions[i] for i in sub_ids],\n",
    "    {},\n",
    "    export_to=proj_root() / \"caches/R1_predictions\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.visualization import visualize_conf_matrix\n",
    "\n",
    "visualize_conf_matrix({n: x[1] for n, x in zip([\"R0\", \"R1\"], r1_eval)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.utils import pretty_show_dict\n",
    "\n",
    "if not load_R1:\n",
    "    for i, e in enumerate(r1_eval):\n",
    "        wandb.log({f\"test/R{i}\": wandb_string(pretty_show_dict(e[1].accuracies))})\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from typet5.visualization import visualize_preds_on_code\n",
    "\n",
    "round = 1\n",
    "pred_dataset = r1_eval[round][1].chunks\n",
    "visualize_preds_on_code(pred_dataset, r1_eval[round][1].predictions, dict())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
