{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Activation Map Examples for Image Classification\n",
    "\n",
    "Much of the code in this blog was adapted from [Jacob Gil's Github repository for class activation mapping](https://github.com/jacobgil/keras-cam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist # load in our MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help with typing, we'll define some common shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "ImageShape = Tuple[int, int]\n",
    "GrayScaleImageShape = Tuple[int, int, int] # a grayscale image should have a H x W x 1 dimensionality\n",
    "Dataset = Tuple[np.ndarray, np.ndarray]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data and Train/Test Splits\n",
    "We first split the MNIST dataset into train and test splits, and print out their shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (60000, 28, 28)\n",
      "The shape of y_train is (60000,)\n",
      "The shape of X_test is (10000, 28, 28)\n",
      "The shape of y_test is (10000,) - some example targets: [7 2 1 0 4]\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "#download mnist data and split into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape} - some example targets: {y_test[:5]}\")\n",
    "mnist_image_shape: ImageShape = X_train.shape[1:]\n",
    "print(mnist_image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encode Categorical Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding y_train (60000,) -> (60000, 10)\n",
      "One-hot encoding y_test (10000,) -> (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "OneHotEncodedTarget = np.ndarray\n",
    "Categories = int\n",
    "encoded_y_train: OneHotEncodedTarget = to_categorical(y_train)\n",
    "encoded_y_test: OneHotEncodedTarget = to_categorical(y_test)\n",
    "print(f\"One-hot encoding y_train {y_train.shape} -> {encoded_y_train.shape}\")\n",
    "print(f\"One-hot encoding y_test {y_test.shape} -> {encoded_y_test.shape}\")\n",
    "\n",
    "CLASSES: Categories = encoded_y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to reshape the tensors and expand their dimensionality in order for Keras to accept their tensor shape as input. Essentially, this involves adding an extra dimension to the current tensor shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (60000, 28, 28) to (60000, 28, 28, 1)\n",
      "Expanding shape from (10000, 28, 28) to (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def expand_tensor_shape(X_train: np.ndarray)-> np.ndarray:\n",
    "    new_shape: Tuple = X_train.shape + (1,)\n",
    "    new_tensor = X_train.reshape(new_shape)\n",
    "    print(f\"Expanding shape from {X_train.shape} to {new_tensor.shape}\")\n",
    "    return new_tensor\n",
    "\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Vanilla CNN Implementation (With No Attention / Class-Activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla architecture we will build as a baseline \"Hello World\" contains two 2D Convolutional layers, followed by a `Flatten` aggregation layer:\n",
    "<img src=\"public/vanilla_architecture.png\" width=\"500\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 02:53:26.200469 140047326390016 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0903 02:53:26.217101 140047326390016 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0903 02:53:26.219845 140047326390016 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0903 02:53:26.265810 140047326390016 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0903 02:53:26.289904 140047326390016 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_1/Softmax:0\", shape=(?, 10), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                184330    \n",
      "=================================================================\n",
      "Total params: 203,434\n",
      "Trainable params: 203,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# define model architecture and hyperparameters\n",
    "NUM_FILTERS_L1 = 64\n",
    "NUM_FILTERS_L2 = 32\n",
    "KERNEL_SIZE = 3\n",
    "\n",
    "# the images are 28 x 28 (pixel size) x 1 (grayscale - if RGB, then 3)\n",
    "input_dims: GrayScaleImageShape = (28,28,1)\n",
    "\n",
    "def build_vanilla_cnn(filters_layer1:int, filters_layer2:int, kernel_size:int, input_dims: GrayScaleImageShape)-> Model:\n",
    "    inputs: Tensor = Input(shape=input_dims)\n",
    "    x: Tensor = Conv2D(filters=filters_layer1, kernel_size=kernel_size, activation='relu')(inputs)\n",
    "    x: Tensor = Conv2D(filters=filters_layer2, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x: Tensor = Flatten()(x)\n",
    "    predictions = Dense(CLASSES, activation=\"softmax\")(x)\n",
    "    print(predictions)\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model: Model = build_vanilla_cnn(NUM_FILTERS_L1, NUM_FILTERS_L2, KERNEL_SIZE, input_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Run the Model\n",
    "Keras will return a history object from the `model.fit()` call. This object contains training loss, validation loss, and the performance metrics you've selected to evaluate (in this case accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History\n",
    "\n",
    "RUN_VANILLA = False # set this to True to actually run the model\n",
    "\n",
    "if RUN_VANILLA:\n",
    "    history: History = model.fit(X_train_expanded, encoded_y_train, \n",
    "                                 validation_data=(X_test_expanded, encoded_y_test), epochs=2, batch_size=2058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Class Activation Map for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D, Layer, Lambda\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "import cv2\n",
    "\n",
    "def global_average_pooling(x: Layer):\n",
    "    return K.mean(x, axis = (2,3))\n",
    "\n",
    "def global_average_pooling_shape(input_shape):\n",
    "    # return the dimensions corresponding with batch size and number of filters\n",
    "    return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def build_global_average_pooling_layer(function, output_shape):\n",
    "    return Lambda(pooling_function, output_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture for Basic Class Activation Mapped CNN\n",
    "<img src=\"public/cam_architecture.png\" width=\"600\" height=\"600\">\n",
    "\n",
    "The original `28x28` image has a depth of 1 (grayscale) and is mapped to 32 filter feature maps of size `24x24`. These are then aggregated in the global average pooling layer to 1 scalar value per feature map (32 feature maps in total $\\rightarrow$ 32 scalar averages. These are then run through a dense softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c5eff524b67f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_class_activation_map_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     '''\n\u001b[1;32m      3\u001b[0m     \u001b[0mI\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mve\u001b[0m \u001b[0mcommented\u001b[0m \u001b[0mout\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCNN\u001b[0m \u001b[0marchitecture\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHowever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeel\u001b[0m \u001b[0mfree\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0mthem\u001b[0m \u001b[0mback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0madd\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mcomplexity\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoing\u001b[0m \u001b[0mso\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mdramatically\u001b[0m \u001b[0mincrease\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "def build_class_activation_map_cnn()-> Model:\n",
    "    '''\n",
    "    I've commented out different internal layers of the CNN architecture. However, feel free to comment them back in to\n",
    "    add more complexity to the model. Note that doing so may dramatically increase training time.\n",
    "    '''\n",
    "\n",
    "    inputs: Tensor = Input(shape=(28,28,1))\n",
    "    x: Tensor = Conv2D(filters=32, \n",
    "                       kernel_size=5, \n",
    "                       activation='relu',\n",
    "                       name='final_convolution_layer')(inputs)\n",
    "    # x: Tensor = MaxPool2D()(x)\n",
    "    # x: Tensor = Conv2D(filters=64, kernel_size=5, activation='relu')(x)\n",
    "    x: Tensor = Lambda(lambda x: K.mean(x, axis=(1,2)), output_shape=global_average_pooling_shape)(x)\n",
    "    # x: Tensor = Dense(128, activation=\"relu\")(x)\n",
    "    predictions: Tensor = Dense(10, activation=\"softmax\")(x)\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def run_cnn_cam(model, save_filepath):\n",
    "    history: History = model.fit(X_train_expanded, encoded_y_train, \n",
    "                                 validation_data=(X_test_expanded, encoded_y_test), epochs=100, batch_size=5126)\n",
    "\n",
    "    model.save(save_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "final_convolution_layer (Con (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 1,162\n",
      "Trainable params: 1,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f5ed9a4e978>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a dry run of the model construction to see its architecture and parameter distributions\n",
    "build_class_activation_map_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we purposely gave the final `Conv2D` layer an explicit name: `final_convolution_layer` - we do this so that after our model has finished training, we can explicitly call `model.get_layer(final_convolution_layer)` to grab that layer's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models/basic_cam.h5\n"
     ]
    }
   ],
   "source": [
    "RUN_CAM = False\n",
    "MODEL_FOLDER = \"saved_models\"\n",
    "save_filepath: str = \"basic_cam.h5\"\n",
    "    \n",
    "import os\n",
    "saved_model_relative_path: str = os.path.join(MODEL_FOLDER, save_filepath)\n",
    "print(saved_model_relative_path)\n",
    "if RUN_CAM:\n",
    "    cam_model = build_class_activation_map_cnn()\n",
    "    run_cnn_cam(cam_model, saved_model_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Persisted Model and Visualize Class Activation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 02:56:53.363850 139653920888576 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0903 02:56:53.384502 139653920888576 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0903 02:56:53.421128 139653920888576 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0903 02:56:53.421756 139653920888576 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0903 02:56:53.422426 139653920888576 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0903 02:56:54.329708 139653920888576 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0903 02:56:54.433799 139653920888576 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "warnings.filterwarnings('ignore')\n",
    "model = load_model(saved_model_relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Grab Outputs of a Particular Keras Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two main inputs:\n",
    "\n",
    "1. The `class_weights`, which should be of dimension size `number of nodes in final feedforward layer x number of classes`. This is easy to get because we can simply call `model.layers[-1]` to retrieve the last layer in the model architecture, and then call `get_weights` to fetch the Numpy array of weights. Note that `get_weights` actually returns a list of two weight matrices - one for the incoming weights (which is what we care about) and the other the outbound weights (which aren't as useful for us in this case).\n",
    "\n",
    "\n",
    "2. The **output of the final convolutional layer**. Since our model could be of any arbitrary depth, we need to get this particular layer by its name, which - if you remember - we conveniently labelled `final_convolutional_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer(model, layer_name: str):\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name]\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Class Activation Maps for One Sample Image\n",
    "We'll randomly pick the 6th image of the dataset and visualize it. It happens to be a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f01fc040e10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_image = X_train[5]\n",
    "first_image = first_image.reshape(28,28,1)\n",
    "img = np.array(first_image).reshape(1, 28, 28, 1)\n",
    "img.shape\n",
    "plt.imshow(img.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final convolution layer has outputs of (24, 24, 32)\n",
      "Final dense layer class weights have output of (32, 10)\n"
     ]
    }
   ],
   "source": [
    "class_weights = model.layers[-1].get_weights()[0]\n",
    "\n",
    "conv_layer_name = \"conv2d_1\" # I accidentally saved the model with this as the conv2d layer name - oops!\n",
    "final_conv_layer = get_output_layer(model, \"conv2d_1\")\n",
    "\n",
    "# define a Keras function to accept as input the model image input and return the final dense layer weights and\n",
    "# convolution layer weights\n",
    "get_output = K.function([model.layers[0].input], [final_conv_layer.output, model.layers[-1].output])\n",
    "\n",
    "[conv_outputs, predictions] = get_output([img])\n",
    "conv_outputs = conv_outputs[0,:,:,:]\n",
    "print(f\"Final convolution layer has outputs of {conv_outputs.shape}\")\n",
    "print(f\"Final dense layer class weights have output of {class_weights.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function to Generate Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cam(conv_outputs: np.ndarray, class_weights: np.ndarray, original_shape: Tuple[int, int], target_class: int):\n",
    "    \n",
    "    '''\n",
    "    Our class activation map should be the same shape as the first two dimensions of our Conv2D output (24 x 24).\n",
    "    We finally also need to interpolate our CAM back to the original image (which we do with cv2.resize)\n",
    "    '''\n",
    "    \n",
    "    cam = np.zeros(dtype=np.float32, shape = conv_outputs.shape[0:2])\n",
    "\n",
    "    # we select ONLY the weight vectors from our class weight matrix (of size 32 x 10) that apply to the target class\n",
    "    # that we care about (in this case, 2)\n",
    "    for i, w in enumerate(class_weights[:, target_class]):\n",
    "        cam += w * conv_outputs[:,:,i]\n",
    "    cam /= np.max(cam)\n",
    "    return cv2.resize(cam, original_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Generate a Heatmap from Class Activation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(cam: np.ndarray)-> np.ndarray:\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "    heatmap[np.where(cam < 0.1)] = 0\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Heatmaps for True Class and a Few False Classes to See Where the CNN Model Is \"Focusing\" On\n",
    "\n",
    "We'll visualize the heatmaps for\n",
    "\n",
    "* The true class 2 - this is what the model is \"looking for\" when it tries to determine if the image fits a handwritten 2\n",
    "* A handwritten 4\n",
    "* A handwritten 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=2)\n",
    "false_cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=4)\n",
    "false2_cam = make_cam(conv_outputs, class_weights, original_shape=(28,28), target_class=5)\n",
    "\n",
    "heatmap = make_heatmap(cam)\n",
    "false_heatmap = make_heatmap(false_cam)\n",
    "false2_heatmap = make_heatmap(false2_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'True Image')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_img = heatmap*0.5 + img\n",
    "final_img = new_img.reshape((28,28,3))\n",
    "\n",
    "imgs = [heatmap, img.reshape(28,28)]\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 15))\n",
    "axes[0].imshow(heatmap)\n",
    "axes[0].set_title(\"Activation Map for 2\")\n",
    "axes[1].imshow(false_heatmap)\n",
    "axes[1].set_title(\"Activation Map for 4\")\n",
    "axes[2].imshow(false2_heatmap)\n",
    "axes[2].set_title(\"Activation Map for 5\")\n",
    "axes[3].imshow(img.reshape((28,28)))\n",
    "axes[3].set_title(\"True Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate Results on FEI Faces Dataset\n",
    "\n",
    "### Import AWS Boto3 API Library\n",
    "\n",
    "We first need to instantiate a `boto3` object to retrieve the face images from the S3 bucket. Within the S3 bucket, several JSON mappings files exists that provides the mapping between an image filename and its actual class. We will use `classification/easy_gender.json` in our example.\n",
    "\n",
    "The `easy` portion of this task stems from the limited number of images (barely more than 100) and the fact that all the faces are centered and aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1-01.jpg', 'male'), ('1-02.jpg', 'male'), ('1-03.jpg', 'male')]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "S3_BUCKET_NAME = \"fei-faces-sao-paulo\"\n",
    "mapping = 'classification/gender.json'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "faces_bucket = s3.Bucket(S3_BUCKET_NAME) # instantiate the bucket object\n",
    "\n",
    "obj = s3.Object(S3_BUCKET_NAME, mapping) # fetch the mapping dictionary\n",
    "\n",
    "json_string: str = obj.get()['Body'].read().decode('utf-8')\n",
    "mappings_dict: Dict[str, str] = json.loads(json_string) # this mappings_dict contains filename -> gender class mapping\n",
    "print(list(mappings_dict.items())[:3]) # print the first three entries of the mappings dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the Images Locally (If Exists) or Download from S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is C:\\Users\\ychen\\Desktop\\Github Repositories\\attention-facial-recognition\n",
      "Downloading 1-11.jpg, saving as C:\\Users\\ychen\\Desktop\\Github Repositories\\attention-facial-recognition\\faces\\1-11.jpg\n",
      "Error downloading 1-11.jpg\n",
      "Downloading 1-12.jpg, saving as C:\\Users\\ychen\\Desktop\\Github Repositories\\attention-facial-recognition\\faces\\1-12.jpg\n",
      "Error downloading 1-12.jpg\n",
      "Downloading 1-13.jpg, saving as C:\\Users\\ychen\\Desktop\\Github Repositories\\attention-facial-recognition\\faces\\1-13.jpg\n",
      "Error downloading 1-13.jpg\n",
      "Downloading 10-02.jpg, saving as C:\\Users\\ychen\\Desktop\\Github Repositories\\attention-facial-recognition\\faces\\10-02.jpg\n",
      "Error downloading 10-02.jpg\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input\n",
    "cwd = os.getcwd()\n",
    "import warnings\n",
    "print(f\"Current working directory is {cwd}\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import logging\n",
    "from keras.preprocessing.image import load_img\n",
    "s3 = boto3.client('s3')\n",
    "warnings.filterwarnings('ignore')\n",
    "IMAGE_LIMIT = 3000\n",
    "LOCAL_IMAGES_FOLDER = \"faces\"\n",
    "\n",
    "target: List[np.ndarray] = [] # this list will contain our actual tensors (as N-dimensional numpy arrays)\n",
    "images: List[str] = [] # this list will contain our classes (male or female)\n",
    "    \n",
    "for filename, gender in mappings_dict.items():\n",
    "    \n",
    "    if \"-14\" in filename or \"-10\" in filename:\n",
    "        continue\n",
    "    \n",
    "    local_filename: str = os.path.join(cwd, LOCAL_IMAGES_FOLDER, filename)    \n",
    "    try:\n",
    "        if not os.path.isfile(local_filename): # if file does not exist locally\n",
    "            print(f\"Downloading {filename}, saving as {local_filename}\")\n",
    "            s3.download_file(S3_BUCKET_NAME, filename, local_filename)\n",
    "        else:\n",
    "            logging.debug(f\"Found a local copy of {local_filename}\")\n",
    "\n",
    "        # use the Keras image API to load in an image\n",
    "        img = load_img(local_filename)\n",
    "        img = img.convert('L') # convert to gray scale\n",
    "        # report details about the image\n",
    "        images.append(np.array(img))\n",
    "        target.append(gender)\n",
    "        if len(images) == IMAGE_LIMIT:\n",
    "            print(\"Breaking after reaching image limit.\")\n",
    "            break\n",
    "    except Exception:\n",
    "        print(f\"Error downloading {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encode Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Images into Training / Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding target vector (2396,) -> (2396, 2)\n",
      "There are 2 classes to predict.\n"
     ]
    }
   ],
   "source": [
    "binary_target = np.array(list(map(lambda gender: 0 if gender == 'male' else 1, target)))\n",
    "encoded_target: OneHotEncodedTarget = to_categorical(binary_target)\n",
    "    \n",
    "print(f\"One-hot encoding target vector {binary_target.shape} -> {encoded_target.shape}\")\n",
    "NUM_CLASSSES: Categories = encoded_target.shape[1]\n",
    "print(f\"There are {NUM_CLASSSES} classes to predict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5008347245409015"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(binary_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.linspace(0,len(binary_target) - 1,len(binary_target))\n",
    "validation_indices = np.random.choice(indices, size=int(len(binary_target) * 0.15), replace=False).astype(int)\n",
    "training_indices = set(indices).difference(set(validation_indices))\n",
    "training_indices = np.array(list(training_indices)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "combined = list(zip(images, binary_target))\n",
    "random.shuffle(combined)\n",
    "\n",
    "images[:], binary_target[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding shape from (2037, 480, 640) to (2037, 480, 640, 1)\n",
      "Expanding shape from (359, 480, 640) to (359, 480, 640, 1)\n",
      "Expanding shape from (2396, 480, 640) to (2396, 480, 640, 1)\n"
     ]
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "X_train = images[training_indices]\n",
    "y_train = binary_target[training_indices]\n",
    "X_test = images[validation_indices]\n",
    "y_test = binary_target[validation_indices]\n",
    "X_train_expanded: np.ndarray = expand_tensor_shape(X_train)\n",
    "X_test_expanded: np.ndarray = expand_tensor_shape(X_test)\n",
    "images_expanded = expand_tensor_shape(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (2037, 480, 640)\n",
      "The shape of y_train is (2037,)\n",
      "The shape of X_test is (359, 480, 640)\n",
      "The shape of y_test is (359,) - some example targets:\n",
      " [0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# print(f\"The shape of X_train_expanded is {X_train_expanded.shape}\")\n",
    "# print(f\"The shape of X_test_expanded is {X_test_expanded.shape}\")\n",
    "print(f\"The shape of X_train is {X_train.shape}\")\n",
    "print(f\"The shape of y_train is {y_train.shape}\")\n",
    "print(f\"The shape of X_test is {X_test.shape}\")\n",
    "print(f\"The shape of y_test is {y_test.shape} - some example targets:\\n {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 01:52:39.919575 16388 deprecation_wrapper.py:119] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3012: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0814 01:52:39.931543 16388 deprecation.py:506] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1062: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0814 01:52:39.954481 16388 deprecation_wrapper.py:119] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:675: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0814 01:52:39.971436 16388 deprecation_wrapper.py:119] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2612: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0814 01:52:39.974438 16388 deprecation.py:323] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 480, 640, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 449, 609, 64)      65600     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 152, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 109, 149, 32)      32800     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1728)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                55328     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 153,761.0\n",
      "Trainable params: 153,761.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_dims = (480, 640, 1)\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, MaxPooling2D, Layer, Lambda\n",
    "\n",
    "def build_vanilla_cnn(filters_layer1:int, filters_layer2:int, kernel_size:int, input_dims: GrayScaleImageShape)-> Model:\n",
    "    inputs: Tensor = Input(shape=input_dims)\n",
    "    x: Tensor = Conv2D(filters=filters_layer1, kernel_size=32, activation='relu')(inputs)\n",
    "    x: Tensor = MaxPooling2D(pool_size=(4,4))(x)\n",
    "    x: Tensor = Conv2D(filters=filters_layer2, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x: Tensor = MaxPooling2D(pool_size=(16,16))(x)\n",
    "    x: Tensor = Flatten()(x)\n",
    "    x: Tensor = Dense(32, activation=\"relu\")(x)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "    print(predictions)\n",
    "\n",
    "    #compile model using accuracy to measure model performance\n",
    "    model: Model = Model(inputs=inputs, outputs=predictions)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model: Model = build_vanilla_cnn(64, 32, 4, input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2396, 480, 640, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 01:52:47.531331 16388 deprecation_wrapper.py:119] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:766: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0814 01:52:47.539278 16388 deprecation.py:506] From c:\\users\\ychen\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:519: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import History\n",
    "\n",
    "RUN_VANILLA = True # set this to True to actually run the model\n",
    "if RUN_VANILLA:\n",
    "    history: History = model.fit(X_train_expanded, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
