{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/extract-knowledge-from-text-end-to-end-information-extraction-pipeline-with-spacy-and-neo4j-502b2b1e0754\n",
        "\n",
        "https://github.com/tomasonjo/blogs/blob/master/ie_pipeline/SpaCy_informationextraction.ipynb"
      ],
      "metadata": {
        "id": "oCt14ZG0Nrya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crosslingual-coreference > /dev/null"
      ],
      "metadata": {
        "id": "X1MocPtMOXyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers > /dev/null"
      ],
      "metadata": {
        "id": "XTOfMjBMdhGu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy > /dev/null"
      ],
      "metadata": {
        "id": "VvN9_PxBenku"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers > /dev/null"
      ],
      "metadata": {
        "id": "FGhMxKPb2JDw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "transformers.__version__"
      ],
      "metadata": {
        "id": "WLgPcCG22fjF",
        "outputId": "0bbfb85c-f2a3-4ef5-ad13-e9a32317ba14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.20.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers wikipedia neo4j > /dev/null\n",
        "!pip install --upgrade google-cloud-storage > /dev/null\n",
        "!python -m spacy download en_core_web_sm > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXC51UIgNqXd",
        "outputId": "60f5964c-2c3d-4b4d-9577-dddadb361d94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.2.0 Requires-Python ==3.6; 0.2.6 Requires-Python >=3.7.1,<3.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch<1.11.0,>=1.10.0 (from crosslingual-coreference) (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch<1.11.0,>=1.10.0\u001b[0m\u001b[31m\n",
            "\u001b[0m2023-06-21 04:30:57.616001: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-21 04:30:58.765261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f9NHrriEOxS",
        "outputId": "3a847186-2909-4b3b-f5d0-064b5a1733ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import crosslingual_coreference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
        "import requests\n",
        "import re\n",
        "import hashlib\n",
        "from spacy import Language\n",
        "from typing import List\n",
        "\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "2JUaR_ATQDAB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_wiki_api(item):\n",
        "  try:\n",
        "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
        "    data = requests.get(url).json()\n",
        "    # Return the first id (Could upgrade this in the future)\n",
        "    return data['search'][0]['id']\n",
        "  except:\n",
        "    return 'id-less'\n"
      ],
      "metadata": {
        "id": "aEbL_MDlQC9r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_triplets(text):\n",
        "    \"\"\"\n",
        "    Function to parse the generated text and extract the triplets\n",
        "    \"\"\"\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"\", \"\").replace(\"\", \"\").replace(\"\", \"\").split():\n",
        "        if token == \"\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "\n",
        "    return triplets"
      ],
      "metadata": {
        "id": "2JigtW9EQC7X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.factory(\n",
        "    \"rebel\",\n",
        "    requires=[\"doc.sents\"],\n",
        "    assigns=[\"doc._.rel\"],\n",
        "    default_config={\n",
        "        \"model_name\": \"Babelscape/rebel-large\",\n",
        "        \"device\": 0,\n",
        "    },\n",
        ")\n",
        "class RebelComponent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        nlp,\n",
        "        name,\n",
        "        model_name: str,\n",
        "        device: int,\n",
        "    ):\n",
        "        assert model_name is not None, \"\"\n",
        "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
        "        self.entity_mapping = {}\n",
        "        # Register custom extension on the Doc\n",
        "        if not Doc.has_extension(\"rel\"):\n",
        "          Doc.set_extension(\"rel\", default={})\n",
        "\n",
        "    def get_wiki_id(self, item: str):\n",
        "        mapping = self.entity_mapping.get(item)\n",
        "        if mapping:\n",
        "          return mapping\n",
        "        else:\n",
        "          res = call_wiki_api(item)\n",
        "          self.entity_mapping[item] = res\n",
        "          return res\n",
        "\n",
        "\n",
        "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
        "          output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
        "          extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
        "          extracted_triplets = extract_triplets(extracted_text[0])\n",
        "          return extracted_triplets\n",
        "\n",
        "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
        "        for triplet in triplets:\n",
        "\n",
        "            # Remove self-loops (relationships that start and end at the entity)\n",
        "            if triplet['head'] == triplet['tail']:\n",
        "                continue\n",
        "\n",
        "            # Use regex to search for entities\n",
        "            head_span = re.search(triplet[\"head\"], doc.text)\n",
        "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
        "\n",
        "            # Skip the relation if both head and tail entities are not present in the text\n",
        "            # Sometimes the Rebel model hallucinates some entities\n",
        "            if not head_span or not tail_span:\n",
        "              continue\n",
        "\n",
        "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
        "            if index not in doc._.rel:\n",
        "                # Get wiki ids and store results\n",
        "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
        "\n",
        "    def __call__(self, doc: Doc) -> Doc:\n",
        "        for sent in doc.sents:\n",
        "            sentence_triplets = self._generate_triplets(sent)\n",
        "            self.set_annotations(doc, sentence_triplets)\n",
        "        return doc"
      ],
      "metadata": {
        "id": "K74L3Sv2QC48"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = -1 # Number of the GPU, -1 if want to use CPU"
      ],
      "metadata": {
        "id": "bIOc-C_XceNo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add coreference resolution model\n",
        "coref = spacy.load('en_core_web_sm')\n",
        "\n",
        "coref.add_pipe(\"xx_coref\",\n",
        "               config={\"chunk_size\": 2500,\n",
        "                       \"chunk_overlap\": 2,\n",
        "                       \"device\": DEVICE})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAAe6vvPchVB",
        "outputId": "bc4aeaa0-3246-4403-a4c3-94a458f8ca52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<crosslingual_coreference.CrossLingualPredictorSpacy.CrossLingualPredictorSpacy at 0x7fa350d04b80>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coref(\"What is the subject of this line\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiPYuhz7k41k",
        "outputId": "ffcd8714-5b99-4942-98eb-a22a62339ee7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "What is the subject of this line"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define rel extraction model\n",
        "rel_ext = spacy.load('en_core_web_sm')\n",
        "\n",
        "rel_ext.add_pipe(\"rebel\", config={\n",
        "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
        "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXjHiJtFQC28",
        "outputId": "1dc25dc2-3a2e-4dcc-912e-b0dc8e6df916"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RebelComponent at 0x7fa34d315210>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Why no data is getting populated.\"\n",
        "\n",
        "coref_text = coref(input_text)._.resolved_text\n",
        "\n",
        "doc = rel_ext(coref_text)"
      ],
      "metadata": {
        "id": "6LQvb0PbQCyQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc._.rel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgLvm_CXftSf",
        "outputId": "c948bcfb-bdaf-49d2-cab6-0c5e07ce5caa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for value, rel_dict in doc._.rel.items():\n",
        "    print(f\"{value}: {rel_dict}\")"
      ],
      "metadata": {
        "id": "qPy7_0D6QCt_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large',\n",
        "                             tokenizer='Babelscape/rebel-large')\n",
        "text = \"Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic\"\n",
        "\n",
        "# We need to use the tokenizer manually since we need special tokens.\n",
        "extracted_text = triplet_extractor.tokenizer.batch_decode([[\"generated_token_ids\"]])\n",
        "\n",
        "print(extracted_text[0])\n"
      ],
      "metadata": {
        "id": "ZFTbaxYynH5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data = triplet_extractor(text,\n",
        "                  return_tensors=True,\n",
        "                  return_text=False)[0]"
      ],
      "metadata": {
        "id": "hiv--E89pRXw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gG1utACnH2V",
        "outputId": "60666d20-ee57-4c8e-c689-34dfedc54481"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'generated_token_ids': {'output_ids': tensor([[[    0, 50267,   221, 20339,  2615,   102,  1437, 50266,  1587,  7330,\n",
              "             1073, 13249,   493, 16517,  1437, 50265,  2034,    11,     5,  6833,\n",
              "            15752, 10014,  1437, 50266, 18978,  3497,  1437, 50265,   247,  1437,\n",
              "            50267, 19664,  1780,   219,  1437, 50266,  1587,  7330,  1073, 13249,\n",
              "              493, 16517,  1437, 50265,  2034,    11,     5,  6833, 15752, 10014,\n",
              "             1437, 50266, 18978,  3497,  1437, 50265,   247,  1437, 50267,  1587,\n",
              "             7330,  1073, 13249,   493, 16517,  1437, 50266, 18978,  3497,  1437,\n",
              "            50265,   247,  1437, 50267, 18978,  3497,  1437, 50266,  1587,  7330,\n",
              "             1073, 13249,   493, 16517,  1437, 50265,  6308,  6833, 15752, 10014,\n",
              "                2]]])}}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "triplet_extractor.tokenizer.decode(extracted_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "GUK9_3ConH0N",
        "outputId": "db1421f1-9d59-440c-9be9-a364ff887568"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-3457dfded667>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtriplet_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3302\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3304\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3305\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't convert {'generated_token_ids': {'output_ids': [[[0, 50267, 221, 20339, 2615, 102, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 2034, 11, 5, 6833, 15752, 10014, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 19664, 1780, 219, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 2034, 11, 5, 6833, 15752, 10014, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50266, 18978, 3497, 1437, 50265, 247, 1437, 50267, 18978, 3497, 1437, 50266, 1587, 7330, 1073, 13249, 493, 16517, 1437, 50265, 6308, 6833, 15752, 10014, 2]]]}} to Sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zA0DajWinHxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4D2EvDRWnHvL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}