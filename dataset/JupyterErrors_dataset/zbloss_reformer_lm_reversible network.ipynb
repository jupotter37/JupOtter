{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,0,0,0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(math.sqrt(2 / math.pi) * (\n",
    "                x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevNetBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, dropout=0.1, lol=[]):\n",
    "        super(RevNetBlock, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.dropout = dropout\n",
    "\n",
    "        layers = []\n",
    "        if lol == list():\n",
    "            layers.append(nn.LayerNorm((d_in,d_out)))\n",
    "            layers.append(nn.Linear(d_in, d_out))\n",
    "            layers.append(GeLU())\n",
    "            layers.append(nn.Linear(d_in, d_out))\n",
    "        else:\n",
    "            for layer in lol:\n",
    "                layers.append(layer)\n",
    "        \n",
    "        self.bottleneck_block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat((x, x), dim=1)\n",
    "        x1, x2 = self.split(x)\n",
    "        Fx2 = self.bottleneck_block(x2)\n",
    "        y1 = Fx2 + x1\n",
    "        return (x2, y1)\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        x2, y1 = x[0], x[1]\n",
    "        Fx2 = - self.bottleneck_block(x2)\n",
    "        x1 = Fx2 + y1\n",
    "        return (x1, x2)\n",
    "\n",
    "    @staticmethod\n",
    "    def split(x):\n",
    "        n = int(x.size()[1] / 2)\n",
    "        x1 = x[:, :n].contiguous()\n",
    "        x2 = x[:, n:].contiguous()\n",
    "        return (x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward: (tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]), tensor([[ 0.4402,  0.3203,  0.4012,  0.4259],\n",
      "        [-0.2684,  1.1148,  0.3594, -0.0601],\n",
      "        [-0.8052,  0.1181,  1.4960, -0.2988],\n",
      "        [-0.5242,  0.2818,  0.2205,  0.5753]], grad_fn=<AddBackward0>))\n",
      "\n",
      "inverse: (tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]], grad_fn=<AddBackward0>), tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "rb = RevNetBlock(d_in=4, d_out=4)\n",
    "output = rb(x)\n",
    "print(f'forward: {output}\\n')\n",
    "input_ = rb.inverse(output)\n",
    "print(f'inverse: {input_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevNet(nn.Module):\n",
    "    def __init__(self, d_in, d_out, dropout=0.0):\n",
    "        super(RevNet, self).__init__()\n",
    "        self.d_in = d_in \n",
    "        self.d_out = d_out\n",
    "        self.dropout = dropout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = d_out = 4\n",
    "\n",
    "layers = []\n",
    "layers.append(nn.LayerNorm((d_in,d_out)))\n",
    "layers.append(nn.Linear(d_in, d_out))\n",
    "layers.append(GeLU())\n",
    "layers.append(nn.Linear(d_in, d_out))\n",
    "\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1839,  0.4475, -0.4725,  0.0502],\n",
       "        [-0.4128,  0.6098, -0.6251,  0.4258],\n",
       "        [ 0.0905,  0.5267, -0.1511, -0.0575],\n",
       "        [-0.0612,  0.4543, -0.2061, -0.1074]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,0,0,0], [0,1,0,0],\n",
    "    [0,0,1,0], [0,0,0,1]]\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gelu(): argument 'input' (position 1) must be Tensor, not Linear",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-399ff22fb314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#layers.append(nn.ReLU())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGaussian\u001b[0m \u001b[0mError\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mUnits\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGELUs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \"\"\"\n\u001b[0;32m-> 1126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: gelu(): argument 'input' (position 1) must be Tensor, not Linear"
     ]
    }
   ],
   "source": [
    "layers = []\n",
    "layers.append(nn.LayerNorm((4,4)))\n",
    "layers.append(F.gelu(nn.Linear(4, 4)))\n",
    "#layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        layers.append(nn.Linear(d_in, d_out))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(4,4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
