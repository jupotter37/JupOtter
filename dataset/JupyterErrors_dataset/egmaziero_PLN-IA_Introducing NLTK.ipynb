{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "Its a platform with a lot of modules to work with natural language. It provides tokenization, stemming, tagging, parsing and semantic analysis.\n",
    "\n",
    "Run the command bellow and download the following data:\n",
    "\n",
    "**In Corpora**\n",
    "- mac_morpho\n",
    "\n",
    "- floresta\n",
    "\n",
    "- stopwords\n",
    "\n",
    "**In Models**\n",
    "- punkt\n",
    "\n",
    "- rslp\n",
    "\n",
    "The files will be downloaded to user home folder inside *nltk_data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "## sentence & word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hoje está um ótimo dia para estudar NLP com python 3.*! Explore o NLTK, que tem rotinas para tratar a linguagem natural.\"\n",
    "sentences =nltk.tokenize.sent_tokenize(text, language='portuguese')\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence, language='portuguese')\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "## keeping word stem (*tronco*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "print(stemmer.stem(\"canto\"))\n",
    "print(stemmer.stem(\"cantar\"))\n",
    "print(stemmer.stem(\"computador\"))\n",
    "\n",
    "sent = \"O canto é retângulo!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(tokens)\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "## keeping content words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"O ônibus parou no centro do campo de futebol\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "tokens_no_stopwords = []\n",
    "for token in tokens:\n",
    "    if token.lower() not in stopwords:\n",
    "        tokens_no_stopwords.append(token)\n",
    "new_sent = \" \".join(tokens_no_stopwords)\n",
    "print(new_sent)\n",
    "print(\" \".join([token for token in sent.lower().split() if token not in stopwords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging\n",
    "## morphosyntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import mac_morpho\n",
    "print(mac_morpho.words())\n",
    "print(mac_morpho.sents())\n",
    "print(mac_morpho.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training a tagger\n",
    "### unigram based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = mac_morpho.tagged_sents()\n",
    "unigram_tagger = nltk.UnigramTagger(tagged_sentences)\n",
    "print(unigram_tagger.evaluate(tagged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"O canto é retângulo!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(unigram_tagger.tag(tokens))\n",
    "sent = \"Eu canto pra você!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(unigram_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"retangulo\" in mac_morpho.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger = nltk.BigramTagger(tagged_sentences, backoff=unigram_tagger)\n",
    "print(bigram_tagger.evaluate(tagged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"O canto é retângulo!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(bigram_tagger.tag(tokens))\n",
    "sent = \"Eu canto pra você!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(bigram_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigram based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(tagged_sentences, backoff=bigram_tagger)\n",
    "print(trigram_tagger.evaluate(tagged_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, tag in mac_morpho.tagged_words():\n",
    "    if word == 'canto':\n",
    "        print(word,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"O canto é retângulo!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(trigram_tagger.tag(tokens))\n",
    "sent = \"Eu canto pra você!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(trigram_tagger.tag(tokens))\n",
    "sent = \"Eu compro pra você!\"\n",
    "tokens = nltk.word_tokenize(sent, language='portuguese')\n",
    "print(trigram_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "## Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202124\n",
      "8829\n"
     ]
    }
   ],
   "source": [
    "print(len(floresta.words()))\n",
    "print(len(floresta.sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floresta.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(STA+fcl\n",
      "  (SUBJ+np (>N+art O) (H+prop 7_e_Meio))\n",
      "  (P+v-fin é)\n",
      "  (SC+np\n",
      "    (>N+art um)\n",
      "    (H+n ex-libris)\n",
      "    (N<+pp\n",
      "      (H+prp de)\n",
      "      (P<+np (>N+art a) (H+n noite) (N<+adj algarvia))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "print(floresta.parsed_sents()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "floresta.parsed_sents()[1].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"viu\" | \"comeu\" | \"andou\"\n",
    "  NP -> Det N | Det N PP | N\n",
    "  Det -> \"o\" | \"a\" | \"um\" | \"uma\" | \"meu\" | \"minha\"\n",
    "  N -> \"homem\" | \"cachorro\" | \"gato\" | \"telescópio\" | \"parque\" | \"João\" | \"Maria\" | \"Pedro\" \n",
    "  P -> \"no\" | \"sobre\" | \"por\" | \"com\"\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (Det o) (N homem)) (VP (V viu) (NP (Det o) (N parque))))\n",
      "-------\n",
      "(S (NP (Det o) (N homem)) (VP (V viu) (NP (Det o) (N parque))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"o homem viu o parque\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "for tree in rd_parser.parse(sent):\n",
    "    print(tree)\n",
    "print(\"-------\")\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "sent = \"o homem viu\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "for tree in rd_parser.parse(sent):\n",
    "    print(tree)\n",
    "print(\"-------\")\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'foi', 'visto', 'pelo'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bfb941420f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msr_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mShiftReduceParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/NLP-IA_env/lib/python3.7/site-packages/nltk/parse/recursivedescent.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/NLP-IA_env/lib/python3.7/site-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 660\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'foi', 'visto', 'pelo'\"."
     ]
    }
   ],
   "source": [
    "sent = \"o parque foi visto pelo homem\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "for tree in rd_parser.parse(sent):\n",
    "    print(tree)\n",
    "print(\"-------\")\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
