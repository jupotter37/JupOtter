{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6287\n",
      "6287\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# เรียกใช้งานโมดูล\n",
    "file_name=\"data\"\n",
    "import codecs\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "#import deepcut\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "# thai cut\n",
    "thaicut=\"newmm\"\n",
    "from sklearn_crfsuite import scorers,metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate,train_test_split\n",
    "import sklearn_crfsuite\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "stopwords = list(thai_stopwords())\n",
    "#จัดการประโยคซ้ำ\n",
    "data_not=[]\n",
    "def Unique(p):\n",
    " text=re.sub(\"<[^>]*>\",\"\",p)\n",
    " text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    " text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    " if text not in data_not:\n",
    "  data_not.append(text)\n",
    "  return True\n",
    " else:\n",
    "  return False\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    " text=text.strip().replace(\"FACILITY\",\"LOCATION\").replace(\"[AGO]\",\"\").replace(\"[/AGO]\",\"\").replace(\"[T]\",\"\").replace(\"[/T]\",\"\")\n",
    " text=re.sub(\"<[^>]*>\",\"\",text)\n",
    " text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')# text.replace('>','>***') # ตัดการกับพวกไม่มี tag word\n",
    " text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    " text2=[]\n",
    " for i in text.split('***'):\n",
    "  if \"[\" in i:\n",
    "   text2.append(i)\n",
    "  else:\n",
    "   text2.append(\"[word]\"+i+\"[/word]\")\n",
    " text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    " return text.replace(\"[word][/word]\",\"\")\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text)\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text)\n",
    "    j=0\n",
    "    conll2002=\"\"\n",
    "    for tagopen,text,tagclose in tag:\n",
    "        word_cut=word_tokenize(text,engine=thaicut) # ใช้ตัวตัดคำ newmm\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut):\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen\n",
    "            else:\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n'\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002)\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\", corpus=\"orchid_ud\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'\n",
    "        i+=1\n",
    "    return text\n",
    "# เขียนไฟล์ข้อมูล conll2002\n",
    "def write_conll2002(file_name,data):\n",
    "    \"\"\"\n",
    "    ใช้สำหรับเขียนไฟล์\n",
    "    \"\"\"\n",
    "    with codecs.open(file_name, \"w\", \"utf-8-sig\") as temp:\n",
    "        temp.write(data)\n",
    "    return True\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "\t\"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "\twith codecs.open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "\t\tlines = f.read().splitlines()\n",
    "\treturn [a for a in lines if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def alldata(lists):\n",
    "    text=\"\"\n",
    "    for data in lists:\n",
    "        text+=text2conll2002(data)\n",
    "        text+='\\n'\n",
    "    return text\n",
    "\n",
    "def alldata_list(lists):\n",
    "    data_all=[]\n",
    "    for data in lists:\n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=True).split('\\n')\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2]))\n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "    #print(data_all)\n",
    "    return data_all\n",
    "\n",
    "def alldata_list_str(lists):\n",
    "\tstring=\"\"\n",
    "\tfor data in lists:\n",
    "\t\tstring1=\"\"\n",
    "\t\tfor j in data:\n",
    "\t\t\tstring1+=j[0]+\"\t\"+j[1]+\"\t\"+j[2]+\"\\n\"\n",
    "\t\tstring1+=\"\\n\"\n",
    "\t\tstring+=string1\n",
    "\treturn string\n",
    "\n",
    "def get_data_tag(listd):\n",
    "\tlist_all=[]\n",
    "\tc=[]\n",
    "\tfor i in listd:\n",
    "\t\tif i !='':\n",
    "\t\t\tc.append((i.split(\"\\t\")[0],i.split(\"\\t\")[1],i.split(\"\\t\")[2]))\n",
    "\t\telse:\n",
    "\t\t\tlist_all.append(c)\n",
    "\t\t\tc=[]\n",
    "\treturn list_all\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in lista:\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll\n",
    "\n",
    "data1=getall(get_data(file_name+\".txt\"))\n",
    "print(len(data1))\n",
    "'''\n",
    "import dill\n",
    "with open('datatrain.data', 'rb') as file:\n",
    " datatofile = dill.load(file)\n",
    "'''\n",
    "#del datatofile[0]\n",
    "datatofile=alldata_list(data1)\n",
    "tt=[]\n",
    "#datatofile.reverse()\n",
    "import random\n",
    "#random.shuffle(datatofile)\n",
    "print(len(datatofile))\n",
    "#training_samples = datatofile[:int(len(datatofile) * 0.8)]\n",
    "#test_samples = datatofile[int(len(datatofile) * 0.8):]\n",
    "'''training_samples = datatofile[:2822]\n",
    "test_samples = datatofile[2822:]'''\n",
    "#print(test_samples[0])\n",
    "#tag=TrainChunker(training_samples,test_samples) # Train\n",
    "\n",
    "#run(training_samples,test_samples)\n",
    "with open(file_name+\"-pos.conll\",\"w\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile):\n",
    "        for j in datatofile[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1\n",
    "\n",
    "with open(file_name+\".conll\",\"w\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile):\n",
    "        for j in datatofile[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1\n",
    "\n",
    "\n",
    "def isThai(chr):\n",
    " cVal = ord(chr)\n",
    " if(cVal >= 3584 and cVal <= 3711):\n",
    "  return True\n",
    " return False\n",
    "def isThaiWord(word):\n",
    " t=True\n",
    " for i in word:\n",
    "  l=isThai(i)\n",
    "  if l!=True and i!='.':\n",
    "   t=False\n",
    "   break\n",
    " return t\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word in stopwords\n",
    "def is_s(word):\n",
    "    if word == \" \" or word ==\"\\t\" or word==\"\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def lennum(word,num):\n",
    "    if len(word)==num:\n",
    "        return True\n",
    "    return False\n",
    "def doc2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "    # Features from current word\n",
    "    features={\n",
    "        'word.word': word,\n",
    "        'word.stopword': is_stopword(word),\n",
    "        'word.isthai':isThaiWord(word),\n",
    "        'word.isspace':word.isspace(),\n",
    "        'postag':postag,\n",
    "        'word.isdigit()': word.isdigit()\n",
    "    }\n",
    "    if word.isdigit() and len(word)==5:\n",
    "        features['word.islen5']=True\n",
    "    if i > 0:\n",
    "        prevword = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        features['word.prevword'] = prevword\n",
    "        features['word.previsspace']=prevword.isspace()\n",
    "        features['word.previsthai']=isThaiWord(prevword)\n",
    "        features['word.prevstopword']=is_stopword(prevword)\n",
    "        features['word.prepostag'] = postag1\n",
    "        features['word.prevwordisdigit'] = prevword.isdigit()\n",
    "    else:\n",
    "        features['BOS'] = True # Special \"Beginning of Sequence\" tag\n",
    "    # Features from next word\n",
    "    if i < len(doc)-1:\n",
    "        nextword = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        features['word.nextword'] = nextword\n",
    "        features['word.nextisspace']=nextword.isspace()\n",
    "        features['word.nextpostag'] = postag1\n",
    "        features['word.nextisthai']=isThaiWord(nextword)\n",
    "        features['word.nextstopword']=is_stopword(nextword)\n",
    "        features['word.nextwordisdigit'] = nextword.isdigit()\n",
    "    else:\n",
    "        features['EOS'] = True # Special \"End of Sequence\" tag\n",
    "    return features\n",
    "\n",
    "def extract_features(doc):\n",
    "    return [doc2features(doc, i) for i in range(len(doc))]\n",
    "\n",
    "def get_labels(doc):\n",
    "    return [tag for (token,postag,tag) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8779222018924607\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        B-DATE      0.905     0.864     0.884       354\n",
      "        I-DATE      0.944     0.940     0.942       714\n",
      "       B-EMAIL      1.000     1.000     1.000         4\n",
      "       I-EMAIL      1.000     1.000     1.000        28\n",
      "         B-LAW      0.897     0.591     0.712        44\n",
      "         I-LAW      0.904     0.677     0.774       167\n",
      "         B-LEN      0.950     0.704     0.809        27\n",
      "         I-LEN      0.958     0.793     0.868        58\n",
      "    B-LOCATION      0.876     0.790     0.831       901\n",
      "    I-LOCATION      0.837     0.783     0.809       879\n",
      "       B-MONEY      1.000     0.917     0.957        96\n",
      "       I-MONEY      0.995     0.887     0.938       247\n",
      "B-ORGANIZATION      0.920     0.803     0.857      1130\n",
      "I-ORGANIZATION      0.840     0.775     0.806      1275\n",
      "     B-PERCENT      1.000     0.966     0.982        29\n",
      "     I-PERCENT      1.000     0.978     0.989        46\n",
      "      B-PERSON      0.950     0.867     0.906       653\n",
      "      I-PERSON      0.920     0.920     0.920      2349\n",
      "       B-PHONE      1.000     0.619     0.765        21\n",
      "       I-PHONE      1.000     0.870     0.930        46\n",
      "        B-TIME      0.912     0.771     0.836       188\n",
      "        I-TIME      0.945     0.903     0.924       362\n",
      "         B-URL      0.955     1.000     0.977        21\n",
      "         I-URL      0.970     1.000     0.985       318\n",
      "         B-ZIP      1.000     0.833     0.909         6\n",
      "\n",
      "   avg / total      0.908     0.852     0.878      9963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_data = [extract_features(doc) for doc in datatofile]\n",
    "y_data = [get_labels(doc) for doc in datatofile]\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True,\n",
    "    model_filename=file_name+\"-pos.model0\"\n",
    ")\n",
    "crf.fit(X, y);\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "y_pred = crf.predict(X_test)\n",
    "e=metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n",
    "print(e)\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_data[0]\n",
    "del y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 2-10: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f3c48aba0fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcrf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn_crfsuite/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_dev, y_dev)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.append\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycrfsuite/_pycrfsuite.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mvector.from_py.__pyx_convert_vector_from_py_std_3a__3a_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycrfsuite/_pycrfsuite.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mstring.from_py.__pyx_convert_string_from_py_std__in_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 2-10: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "crf2 = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True,\n",
    "    model_filename=file_name+\".model\"\n",
    ")\n",
    "crf2.fit(X_data, y_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"datatrain.data\", \"wb\") as dill_file:\n",
    " dill.dump(datatofile, dill_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport dill\\nwith open(\"datatrain.data\", \"wb\") as dill_file:\\n dill.dump(datatofile, dill_file)\\nf1_scorer = make_scorer(metrics.flat_f1_score, average=\\'macro\\') \\n\\nscores = cross_validate(crf, X, y, scoring=f1_scorer, cv=5)\\n# save data\\nprint(scores)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross_validate\n",
    "\"\"\"\n",
    "import dill\n",
    "with open(\"datatrain.data\", \"wb\") as dill_file:\n",
    " dill.dump(datatofile, dill_file)\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score, average='macro') \n",
    "\n",
    "scores = cross_validate(crf, X, y, scoring=f1_scorer, cv=5)\n",
    "# save data\n",
    "print(scores)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
