{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_it(x):\n",
    "    return re.sub(\"( [,.;\\)]|[\\(] | ['] | [-] | [:])\", lambda m: m.group(1).strip(), \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trwiki-67/trwiki-67.train.txt\") as fi:\n",
    "    content = fi.read()\n",
    "    wiki_articles = [ { \"title\": ti.strip(\"==\").strip(), \"text\": tx.strip()} for ti, tx in zip(re.findall(\"== .* ==\", content), re.split(\"== .* == \\n\\n\", content)[1:]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trnews-64/trnews-64.train.raw\") as fi:\n",
    "    news_articles = re.split(\"\\n\\n\", fi.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in open(\"/scratch/users/user/nlp_datasets/mukayese_tokenization/clean/train_news_clean.jsonl\"):\n",
    "    texts.append(json.loads(i)[\"text\"])\n",
    "\n",
    "for i in open(\"/scratch/users/user/nlp_datasets/mukayese_tokenization/clean/abstracts_train_clean.jsonl\"):\n",
    "    texts.append(json.loads(i)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = \" \".join([do_it(i) for i in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [1.3551] t-1\n",
      "  Abbreviation: [0.9034] w-1\n",
      "  Abbreviation: [0.4517] h-j\n",
      "  Abbreviation: [1.2278] r,\n",
      "  Abbreviation: [0.4517] l-8\n",
      "  Abbreviation: [1.8068] a.-v\n",
      "  Collocation: [505.1390] '##number##'+'##number##'\n",
      "  Collocation: [31.7329] 'r'+'i̇'\n",
      "  Collocation: [149.2189] 'i̇'+'ö'\n",
      "  Collocation: [66.8443] 'y'+\"'ı\"\n"
     ]
    }
   ],
   "source": [
    "trainer.train(articles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = \" \".join([do_it(i) for i in news_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [0.9062] l.-t\n",
      "  Abbreviation: [1.2316] ş,\n",
      "  Abbreviation: [0.4531] v-w\n",
      "  Removed abbreviation: [-0.0006] r,\n",
      "  Removed abbreviation: [-0.0031] h-j\n",
      "  Abbreviation: [0.9062] s.-f\n",
      "  Abbreviation: [1.3593] ü-4\n",
      "  Abbreviation: [2.4632] r.,\n",
      "  Removed abbreviation: [-0.0031] l-8\n",
      "  Abbreviation: [0.9062] t.-e\n",
      "  Abbreviation: [0.9062] l.-e\n",
      "  Removed abbreviation: [-0.0000] t-1\n",
      "  Abbreviation: [2.1493] z,\n",
      "  Removed abbreviation: [0.1496] w-1\n",
      "  Abbreviation: [0.9177] e,\n",
      "  Abbreviation: [0.4531] ü-3\n",
      "  Abbreviation: [0.9062] y.-a\n",
      "  Abbreviation: [0.6098] m,\n",
      "  Abbreviation: [4.9265] ı.,\n",
      "  Abbreviation: [0.4531] ğ-9\n",
      "  Abbreviation: [0.9062] v.-p\n",
      "  Abbreviation: [0.4531] o-8\n",
      "  Collocation: [131858.7206] '##number##'+'##number##'\n",
      "  Collocation: [192.7244] 'o'+'ö.-'\n",
      "  Collocation: [237.4095] 'b'+'y.-'\n",
      "  Collocation: [14.8016] 'w'+'w'\n",
      "  Collocation: [88.7160] 'a'+'ş.-'\n",
      "  Collocation: [149.0605] 'h'+'b.-'\n",
      "  Collocation: [125.5559] 'b'+'h.-'\n",
      "  Collocation: [37.3967] 'e'+'g.-'\n",
      "  Collocation: [41.3696] 'a'+'t.-'\n",
      "  Collocation: [72.5172] 'ü'+'k.-'\n",
      "  Collocation: [89.0126] 'ç'+'k.-'\n",
      "  Collocation: [116.9794] 's'+'u.-'\n",
      "  Collocation: [88.8220] 'h'+'k.-'\n",
      "  Collocation: [138.7385] 'f'+'g.-'\n",
      "  Collocation: [20.3144] 'a'+'a.-'\n",
      "  Collocation: [97.9582] 'b'+'c.-'\n",
      "  Collocation: [70.2395] 'a'+'ü.-'\n",
      "  Collocation: [75.0716] 'b'+'s.-'\n",
      "  Collocation: [31.1509] 'a'+'ç.-'\n",
      "  Collocation: [111.6183] 'v'+'v.-'\n",
      "  Collocation: [101.4510] 'i̇'+'k.-'\n",
      "  Collocation: [25.7661] 'y'+'e.-'\n",
      "  Collocation: [52.1389] 'o'+'s.-'\n",
      "  Collocation: [170.1338] 'j'+'i.-'\n",
      "  Collocation: [19.0827] 't'+'e.-'\n",
      "  Collocation: [20.2031] 'm'+'b.-'\n",
      "  Collocation: [18.5870] 'r'+'a.-'\n",
      "  Collocation: [34.2443] 'e'+'w.-'\n",
      "  Collocation: [27.7354] 'd'+'h.-'\n",
      "  Collocation: [12.6386] 'd'+'b.-'\n",
      "  Collocation: [13.0674] 'd'+'e.-'\n",
      "  Collocation: [15.3296] 's'+'e.-'\n",
      "  Collocation: [44.8611] 'h'+'g.-'\n",
      "  Collocation: [57.8375] 'm'+'m.-'\n",
      "  Collocation: [44.1177] 'g'+'t.-'\n",
      "  Collocation: [87.7968] 'g'+'j.-'\n",
      "  Collocation: [32.4440] 'm'+'w.-'\n",
      "  Collocation: [38.9151] 'a'+'ş.-f'\n",
      "  Collocation: [9.6164] 'y'+'b.-'\n",
      "  Collocation: [26.7813] 'h'+'e.-'\n",
      "  Collocation: [10.4851] 'e'+'ç.-'\n",
      "  Collocation: [28.0522] 'ş'+'ö.-'\n",
      "  Collocation: [109.7555] 'i̇'+'o.-'\n",
      "  Collocation: [34.6101] 'o'+'v.-'\n",
      "  Collocation: [34.5912] 'a'+'ş.-t'\n",
      "  Collocation: [54.5203] 'm'+'f.-'\n",
      "  Collocation: [44.8734] 'i̇'+'e.-'\n",
      "  Collocation: [10.1058] 'm'+'a.-'\n",
      "  Collocation: [30.2673] 'a'+'ş.-g'\n",
      "  Collocation: [30.2673] 'a'+'ş.-k'\n",
      "  Collocation: [19.0193] 'b'+'ş.-'\n",
      "  Collocation: [8.1603] 'r'+'3-b'\n",
      "  Collocation: [25.9434] 'a'+'ş.-b'\n",
      "  Collocation: [12.8681] 'ş'+'g.-'\n",
      "  Collocation: [19.3993] 'h'+'ç.-'\n",
      "  Collocation: [21.6195] 'a'+'ş.-o'\n",
      "  Collocation: [21.6195] 'a'+'ş.-m'\n",
      "  Collocation: [26.2799] 'n'+'p.-'\n",
      "  Collocation: [41.7098] 'j'+'h.-'\n",
      "  Collocation: [11.6587] 'h'+'t.-'\n",
      "  Collocation: [17.2956] 'a'+'ş.-d'\n",
      "  Collocation: [12.9717] 'a'+'ş.-a'\n",
      "  Collocation: [34.5204] 'w'+'g-l'\n",
      "  Collocation: [14.3183] '##number##'+'5-i'\n",
      "  Collocation: [8.6478] 'a'+'ş.-ç'\n",
      "  Collocation: [8.6478] 'a'+'ş.-y'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(news_articles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_articles = \" \".join([do_it(i) for i in wiki_articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collocation: [132271.4600] '##number##'+'##number##'\n",
      "  Collocation: [192.8814] 'o'+'ö.-'\n",
      "  Collocation: [237.5594] 'b'+'y.-'\n",
      "  Collocation: [14.8796] 'w'+'w'\n",
      "  Collocation: [88.8432] 'a'+'ş.-'\n",
      "  Collocation: [149.1851] 'h'+'b.-'\n",
      "  Collocation: [125.6632] 'b'+'h.-'\n",
      "  Collocation: [37.4844] 'e'+'g.-'\n",
      "  Collocation: [41.4545] 'a'+'t.-'\n",
      "  Collocation: [72.6068] 'ü'+'k.-'\n",
      "  Collocation: [89.1002] 'ç'+'k.-'\n",
      "  Collocation: [117.0698] 's'+'u.-'\n",
      "  Collocation: [88.9096] 'h'+'k.-'\n",
      "  Collocation: [138.8286] 'f'+'g.-'\n",
      "  Collocation: [20.3774] 'a'+'a.-'\n",
      "  Collocation: [98.0389] 'b'+'c.-'\n",
      "  Collocation: [70.3208] 'a'+'ü.-'\n",
      "  Collocation: [75.1425] 'b'+'s.-'\n",
      "  Collocation: [31.2154] 'a'+'ç.-'\n",
      "  Collocation: [111.6906] 'v'+'v.-'\n",
      "  Collocation: [101.5184] 'i̇'+'k.-'\n",
      "  Collocation: [25.8195] 'y'+'e.-'\n",
      "  Collocation: [52.1956] 'o'+'s.-'\n",
      "  Collocation: [170.1929] 'j'+'i.-'\n",
      "  Collocation: [19.1297] 't'+'e.-'\n",
      "  Collocation: [20.2507] 'm'+'b.-'\n",
      "  Collocation: [18.6343] 'r'+'a.-'\n",
      "  Collocation: [34.2993] 'e'+'w.-'\n",
      "  Collocation: [27.7838] 'd'+'h.-'\n",
      "  Collocation: [12.6782] 'd'+'b.-'\n",
      "  Collocation: [13.1074] 'd'+'e.-'\n",
      "  Collocation: [15.3687] 's'+'e.-'\n",
      "  Collocation: [44.9089] 'h'+'g.-'\n",
      "  Collocation: [57.8869] 'm'+'m.-'\n",
      "  Collocation: [44.1616] 'g'+'t.-'\n",
      "  Collocation: [87.8423] 'g'+'j.-'\n",
      "  Collocation: [32.4829] 'm'+'w.-'\n",
      "  Collocation: [38.9561] 'a'+'ş.-f'\n",
      "  Collocation: [9.6460] 'y'+'b.-'\n",
      "  Collocation: [26.8187] 'h'+'e.-'\n",
      "  Collocation: [10.5165] 'e'+'ç.-'\n",
      "  Collocation: [28.0899] 'ş'+'ö.-'\n",
      "  Collocation: [109.7964] 'i̇'+'o.-'\n",
      "  Collocation: [34.6491] 'o'+'v.-'\n",
      "  Collocation: [34.6276] 'a'+'ş.-t'\n",
      "  Collocation: [54.5567] 'm'+'f.-'\n",
      "  Collocation: [44.9090] 'i̇'+'e.-'\n",
      "  Collocation: [10.1309] 'm'+'a.-'\n",
      "  Collocation: [30.2992] 'a'+'ş.-g'\n",
      "  Collocation: [30.2992] 'a'+'ş.-k'\n",
      "  Collocation: [19.0482] 'b'+'ş.-'\n",
      "  Collocation: [8.1818] 'r'+'3-b'\n",
      "  Collocation: [25.9707] 'a'+'ş.-b'\n",
      "  Collocation: [12.8916] 'ş'+'g.-'\n",
      "  Collocation: [19.4209] 'h'+'ç.-'\n",
      "  Collocation: [21.6423] 'a'+'ş.-o'\n",
      "  Collocation: [21.6423] 'a'+'ş.-m'\n",
      "  Collocation: [26.3026] 'n'+'p.-'\n",
      "  Collocation: [41.7324] 'j'+'h.-'\n",
      "  Collocation: [11.6753] 'h'+'t.-'\n",
      "  Collocation: [17.3138] 'a'+'ş.-d'\n",
      "  Collocation: [12.9854] 'a'+'ş.-a'\n",
      "  Collocation: [34.5340] 'w'+'g-l'\n",
      "  Collocation: [14.3274] '##number##'+'5-i'\n",
      "  Collocation: [8.6569] 'a'+'ş.-ç'\n",
      "  Collocation: [8.6569] 'a'+'ş.-y'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(wiki_articles, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60794505it [01:33, 647893.68it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "for j,line in tqdm(enumerate(open(\"tagged_tweets.xml\"))):\n",
    "    if line.startswith(\"<tweet>\"):\n",
    "        start = j\n",
    "        sentence = []\n",
    "    elif line.startswith(\"</tweet>\"):\n",
    "        tweets.append(do_it(sentence))\n",
    "    else:\n",
    "        sentence.append(line.split(\"\\t\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets = do_it([do_it(i) for i in tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [13.5072] 𝓜\n",
      "  Abbreviation: [0.5045] z.,,.,\n",
      "  Abbreviation: [0.9142] e.-3\n",
      "  Abbreviation: [0.4571] w-5\n",
      "  Removed abbreviation: [-0.0009] r.,\n",
      "  Abbreviation: [0.6186] i.,.,.,.,\n",
      "  Abbreviation: [3.3776] 𝑘\n",
      "  Abbreviation: [0.6201] m.,\n",
      "  Removed abbreviation: [-0.0003] l.-e\n",
      "  Removed abbreviation: [-0.0000] e,\n",
      "  Removed abbreviation: [-0.0001] o-8\n",
      "  Removed abbreviation: [-0.0000] z,\n",
      "  Abbreviation: [2.4851] c.,\n",
      "  Removed abbreviation: [-0.0000] m,\n",
      "  Abbreviation: [2.4851] a.,\n",
      "  Abbreviation: [2.4839] l.,\n",
      "  Abbreviation: [0.9142] m.-2\n",
      "  Abbreviation: [1.3713] r.,.,\n",
      "  Abbreviation: [0.4571] _-7\n",
      "  Removed abbreviation: [0.1522] ü-3\n",
      "  Removed abbreviation: [-0.1018] l.-t\n",
      "  Abbreviation: [0.3363] 'a-2\n",
      "  Abbreviation: [0.4571] כ-3\n",
      "  Abbreviation: [0.4571] —-k\n",
      "  Abbreviation: [1.2414] s.,\n",
      "  Abbreviation: [1.3713] i.,.,\n",
      "  Removed abbreviation: [-0.0000] ş,\n",
      "  Abbreviation: [1.8284] ü-4\n",
      "  Abbreviation: [0.5045] l,.,.,\n",
      "  Abbreviation: [17.3958] h.,\n",
      "  Abbreviation: [0.6201] ı.,\n",
      "  Abbreviation: [0.6726] l.,.,.,\n",
      "  Rare Abbrev: .,.,.,.,,.,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,.,.,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: l.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: .,,,.\n",
      "  Rare Abbrev: .,.,.,.,.\n",
      "  Rare Abbrev: .,.,,.,.,.\n",
      "  Rare Abbrev: .,.,.,,.,.,.,.,.,.,.,.,.,.\n",
      "  Rare Abbrev: i.,.,.,,.,.,.,,.,.,.,.,.\n",
      "  Collocation: [94787.7323] 't'+'c'\n",
      "  Collocation: [15032.7253] '##number##'+'##number##'\n",
      "  Collocation: [187.9531] 'o'+'ö.-'\n",
      "  Collocation: [230.8251] 'b'+'y.-'\n",
      "  Collocation: [71.2489] 'a'+'ş.-'\n",
      "  Collocation: [138.4856] 'h'+'b.-'\n",
      "  Collocation: [124.6193] 'b'+'h.-'\n",
      "  Collocation: [39.7670] 'e'+'g.-'\n",
      "  Collocation: [36.8035] 'a'+'t.-'\n",
      "  Collocation: [72.3849] 'ü'+'k.-'\n",
      "  Collocation: [85.8922] 'ç'+'k.-'\n",
      "  Collocation: [94.8543] 's'+'u.-'\n",
      "  Collocation: [73.8735] 'h'+'k.-'\n",
      "  Collocation: [134.0577] 'f'+'g.-'\n",
      "  Collocation: [16.1459] 'a'+'a.-'\n",
      "  Collocation: [95.5142] 'b'+'c.-'\n",
      "  Collocation: [60.4604] 'a'+'ü.-'\n",
      "  Collocation: [71.1225] 'b'+'s.-'\n",
      "  Collocation: [30.4445] 'a'+'ç.-'\n",
      "  Collocation: [107.0133] 'v'+'v.-'\n",
      "  Collocation: [99.2255] 'i̇'+'k.-'\n",
      "  Collocation: [22.4384] 'y'+'e.-'\n",
      "  Collocation: [47.9291] 'o'+'s.-'\n",
      "  Collocation: [136.2410] 'j'+'i.-'\n",
      "  Collocation: [17.2849] 't'+'e.-'\n",
      "  Collocation: [18.0099] 'm'+'b.-'\n",
      "  Collocation: [16.8360] 'r'+'a.-'\n",
      "  Collocation: [35.7251] 'e'+'w.-'\n",
      "  Collocation: [29.6580] 'd'+'h.-'\n",
      "  Collocation: [14.2288] 'd'+'b.-'\n",
      "  Collocation: [12.9826] 'd'+'e.-'\n",
      "  Collocation: [11.2731] 's'+'e.-'\n",
      "  Collocation: [40.8196] 'h'+'g.-'\n",
      "  Collocation: [30.1885] 'm'+'m.-'\n",
      "  Collocation: [41.5872] 'g'+'t.-'\n",
      "  Collocation: [86.8552] 'g'+'j.-'\n",
      "  Collocation: [8.1279] 'n'+'ö.-'\n",
      "  Collocation: [30.6314] 'm'+'w.-'\n",
      "  Collocation: [38.4659] 'a'+'ş.-f'\n",
      "  Collocation: [9.0010] 'y'+'b.-'\n",
      "  Collocation: [22.1225] 'h'+'e.-'\n",
      "  Collocation: [11.3354] 'e'+'ç.-'\n",
      "  Collocation: [29.1958] 'ş'+'ö.-'\n",
      "  Collocation: [105.3862] 'i̇'+'o.-'\n",
      "  Collocation: [33.4254] 'o'+'v.-'\n",
      "  Collocation: [28.1641] 'a'+'ş.-t'\n",
      "  Collocation: [52.8202] 'm'+'f.-'\n",
      "  Collocation: [45.2505] 'i̇'+'e.-'\n",
      "  Collocation: [24.1408] 'a'+'ş.-g'\n",
      "  Collocation: [29.9179] 'a'+'ş.-k'\n",
      "  Collocation: [16.4174] 'b'+'ş.-'\n",
      "  Collocation: [17.1489] 'a'+'ş.-b'\n",
      "  Collocation: [13.5813] 'ş'+'g.-'\n",
      "  Collocation: [17.5742] 'h'+'ç.-'\n",
      "  Collocation: [16.2144] 'a'+'ş.-o'\n",
      "  Collocation: [11.5384] 'a'+'ş.-m'\n",
      "  Collocation: [21.5553] 'n'+'p.-'\n",
      "  Collocation: [39.9873] 'j'+'h.-'\n",
      "  Collocation: [161.4797] '𝓚'+'𝓐'\n",
      "  Collocation: [9.7141] 'h'+'t.-'\n",
      "  Collocation: [8.2887] 'a'+'ş.-d'\n",
      "  Collocation: [83.6343] 'ת'+'ב'\n",
      "  Collocation: [27.8703] 'w'+'g-l'\n",
      "  Collocation: [25.0070] 'z'+'⛔-i'\n",
      "  Collocation: [8.6108] '##number##'+'9-g'\n",
      "  Collocation: [8.5480] 'a'+'ş.-ç'\n",
      "  Collocation: [18.3735] 'د'+'ا'\n",
      "  Collocation: [22.5937] 'ه'+'م'\n",
      "  Collocation: [19.6263] 'م'+'م'\n",
      "  Collocation: [29.9622] 'ς'+'α'\n",
      "  Collocation: [25.6592] 'د'+'ع'\n",
      "  Collocation: [11.0678] 'r'+'❞-m'\n",
      "  Collocation: [12.8967] 'ı'+'h-a-m-s-i̇'\n",
      "  Collocation: [33.1463] 'а'+'к'\n",
      "  Collocation: [21.0030] 'ن'+'و'\n",
      "  Collocation: [34.9196] 'с'+'в'\n",
      "  Collocation: [16.8337] 'ا'+'ن'\n",
      "  Collocation: [35.1704] 'ה'+'ה'\n",
      "  Collocation: [34.6525] 'у'+'о'\n"
     ]
    }
   ],
   "source": [
    "trainer.train(raw_tweets, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.finalize_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer,open(\"all_punkt.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zemberek import TurkishSentenceExtractor\n",
    "extractor = TurkishSentenceExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2561955/2561955 [02:11<00:00, 19466.17it/s]\n"
     ]
    }
   ],
   "source": [
    "punkt_tokenized = []\n",
    "for tweet in tqdm(tweets):\n",
    "    punkt_tokenized.append(tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2561955/2561955 [02:02<00:00, 20884.74it/s]\n"
     ]
    }
   ],
   "source": [
    "zemberek_tokenized = []\n",
    "for tweet in tqdm(tweets):\n",
    "    zemberek_tokenized.append(extractor.from_paragraph(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_punkt = [i for i in punkt_tokenized if len(i) > 1]\n",
    "ms_zemberek = [i for i in zemberek_tokenized if len(i) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1177457, 1274854)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms_punkt), len(ms_zemberek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisentence_tweets = random.choices(ms_zemberek, k=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlesentence_tweets = random.choices(zemberek_tokenized, k=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = multisentence_tweets[:4000] + singlesentence_tweets[:1000]\n",
    "test = multisentence_tweets[4000:] + singlesentence_tweets[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pandemi sürecinde var olan öğretmen ihtiyacı ücretli yerine 2019 KPSSde karma atama mağdurları olan öğretmenlerle giderilmeli.',\n",
       " '@user @user @user @user @user #MaliyedenÖğrt20Bin #MaliyedenÖğrt20Bin']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[4] = ['#AlaattinCakıcı yla ilgili olumsuz yazdım diye sokak arası mafyacılık oynayan oğlancıklar hemen tehditlere başladı 😂',\n",
    "            'Sizin özünüz bu işte Tehdit, Şantaj, Gasp...',\n",
    " 'ama kuru sıkısınız arkanızda biri varken sesiniz çıkar, yokken kuyruğunuz bacaklarınızın arasında gezersiniz !!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can you see the writings on the wall some.url'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"\"\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\"\", \"some.url\", \"can you see the writings on the wall https://t.co/3MxhHoTB6t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13886, 121505)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in train), sum(sum([len(j.split(\" \")) for j in i]) for i in train  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13821, 121090)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(i) for i in test), sum(sum([len(j.split(\" \")) for j in i]) for i in test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/nlp_datasets/trseg-41/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"clean\", \"train_tweets_clean.jsonl\"), \"w\") as data:\n",
    "    for t in train:\n",
    "        data.write(json.dumps(t, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"clean\", \"test_tweets_clean.jsonl\"), \"w\") as data:\n",
    "    for t in test:\n",
    "        data.write(json.dumps(t, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' asd @user'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"@\\w+\", \"@user\", \" asd @mansuryavas06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"\"\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\"\"\n",
    "for i in range(len(train)):\n",
    "    for j in range(len(train[i])):\n",
    "        train[i][j] = re.sub( \"@\\w+\", \"@user\", re.sub(f, \"some.url\", train[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9475174040325887, 2.9539594337861432)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i) for i in ms_punkt]), np.mean([len(i) for i in ms_zemberek]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.tr import Turkish\n",
    "nlp = Turkish()\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []\n",
    "for line in open(\"/scratch/users/user/nlp_datasets/mukayese_tokenization/clean/train_news_clean.jsonl\"):\n",
    "    news.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_news_clean.jsonl\")]\n",
    "test_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_news_clean.jsonl\")]\n",
    "\n",
    "train_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_abstracts_clean.jsonl\")]\n",
    "test_abstract = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_abstracts_clean.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = [{\"text\" : do_it(i), \"sentences\" : i} for i in train]\n",
    "test_ = [{\"text\" : do_it(i), \"sentences\" : i} for i in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = train_ + train_news + train_abstracts\n",
    "final_test = test_ + test_news + test_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, \"clean\", \"train_all_clean.jsonl\"), \"w\") as data:\n",
    "    for i in final_train:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"clean\", \"test_all_clean.jsonl\"), \"w\") as data:\n",
    "    for i in final_test:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.tr import Turkish\n",
    "nlp = Turkish()\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def handle_metrics(values,predictor):\n",
    "    nlp = Turkish()\n",
    "    sentences = []\n",
    "    \n",
    "    try:\n",
    "        for j,i in enumerate(values):\n",
    "            sentences.append(Doc(nlp.vocab, words=i[\"sentences\"], spaces=np.ones(len(i[\"sentences\"]), dtype=bool)))\n",
    "    except:\n",
    "        print(\"During docing : \" , j)\n",
    "    \n",
    "    tokenized = []\n",
    "    try:\n",
    "        for j,p in enumerate(values):\n",
    "            tokens = predictor(p[\"text\"])\n",
    "            tokenized.append(Doc(nlp.vocab, words=tokens, spaces=np.ones(len(tokens), dtype=bool)))\n",
    "    except:\n",
    "        print(\"In the loop: \", j)\n",
    "    examples = [Example(predicted, reference) for predicted, reference in zip(tokenized, sentences)]\n",
    "    scorer = Scorer()\n",
    "    return Scorer.score_tokenization(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\")]\n",
    "test_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_all_clean.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "trainer = PunktTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [25.4046] m.s\n",
      "  Abbreviation: [0.8246] t.a.b\n",
      "  Abbreviation: [1.6493] şek\n",
      "  Abbreviation: [11.9551] m.ö\n",
      "  Abbreviation: [0.7472] dn\n",
      "  Abbreviation: [1.0995] a.-v\n",
      "  Abbreviation: [47.1928] m\n",
      "  Abbreviation: [3.1215] iv\n",
      "  Abbreviation: [1.4944] re\n",
      "  Abbreviation: [1.4944] z.r\n",
      "  Abbreviation: [2.9888] n.a\n",
      "  Abbreviation: [1.6493] lev\n",
      "  Abbreviation: [1.4944] bk\n",
      "  Abbreviation: [0.3720] subsp\n",
      "  Abbreviation: [1.4944] th\n",
      "  Abbreviation: [0.7472] mv\n",
      "  Abbreviation: [2.9888] b.b\n",
      "  Abbreviation: [0.8246] doç\n",
      "  Abbreviation: [0.8246] xix\n",
      "  Abbreviation: [2.9888] r.e\n",
      "  Abbreviation: [4.0071] g\n",
      "  Abbreviation: [2.8858] ş\n",
      "  Abbreviation: [3.2985] //t\n",
      "  Abbreviation: [1.4944] o.o\n",
      "  Abbreviation: [11.8648] c\n",
      "  Abbreviation: [3.7360] sp\n",
      "  Abbreviation: [0.5498] abs\n",
      "  Abbreviation: [11.9764] i̇.ö\n",
      "  Abbreviation: [1.4944] xi\n",
      "  Abbreviation: [0.5498] env\n",
      "  Abbreviation: [0.5498] ioh\n",
      "  Abbreviation: [1.4944] a.b\n",
      "  Abbreviation: [1.4944] pf\n",
      "  Abbreviation: [4.8296] i̇.s\n",
      "  Abbreviation: [0.7472] tı\n",
      "  Abbreviation: [1.4944] y.y\n",
      "  Abbreviation: [0.3034] i̇.s.4\n",
      "  Abbreviation: [0.3034] a.h.i̇\n",
      "  Abbreviation: [1.4944] hz\n",
      "  Abbreviation: [0.8246] req\n",
      "  Abbreviation: [1.4944] a.y\n",
      "  Abbreviation: [2.2416] vd\n",
      "  Abbreviation: [1.4944] ır\n",
      "  Abbreviation: [2.7525] j\n",
      "  Abbreviation: [1.4944] ö.y\n",
      "  Abbreviation: [0.7472] 'z\n",
      "  Abbreviation: [2.6566] yy\n",
      "  Abbreviation: [2.2416] xx\n",
      "  Abbreviation: [0.5498] i̇.b\n",
      "  Abbreviation: [0.8246] m.a.b\n",
      "  Abbreviation: [0.7472] uz\n",
      "  Abbreviation: [1.2535] st\n",
      "  Abbreviation: [0.7472] mk\n",
      "  Abbreviation: [1.4944] g.y\n",
      "  Abbreviation: [2.9888] s.k\n",
      "  Abbreviation: [0.8246] cad\n",
      "  Abbreviation: [2.9888] t.c\n",
      "  Abbreviation: [0.7472] nr\n",
      "  Abbreviation: [0.4045] 'tur\n",
      "  Abbreviation: [0.7472] là\n",
      "  Abbreviation: [0.7472] aw\n",
      "  Abbreviation: [1.4944] m.a\n",
      "  Abbreviation: [0.7472] md\n",
      "  Abbreviation: [2.9888] a.ş\n",
      "  Abbreviation: [12.3695] res\n",
      "  Abbreviation: [0.7472] up\n",
      "  Rare Abbrev: tùrkiye.\n",
      "  Rare Abbrev: kastetmemiştim.\n",
      "  Rare Abbrev: raporunu.\n",
      "  Rare Abbrev: adamlardan.\n",
      "  Rare Abbrev: çoğalıyor.\n",
      "  Rare Abbrev: yaratıldı.\n",
      "  Rare Abbrev: unutuldu.\n",
      "  Rare Abbrev: ağliyor.\n",
      "  Rare Abbrev: çoğalır.\n",
      "  Rare Abbrev: çalışmalıyız.\n",
      "  Rare Abbrev: getirdi.\n",
      "  Rare Abbrev: oynasın.\n",
      "  Rare Abbrev: alacaksın.\n",
      "  Rare Abbrev: bitiremediniz.\n",
      "  Rare Abbrev: araştırıyormuş.\n",
      "  Rare Abbrev: alamaz.\n",
      "  Rare Abbrev: konuşuyorlar.\n",
      "  Rare Abbrev: keyifsizim.\n",
      "  Rare Abbrev: azalıyor.\n",
      "  Rare Abbrev: baksın.\n",
      "  Rare Abbrev: göremiyorum.\n",
      "  Rare Abbrev: kazanır.\n",
      "  Rare Abbrev: bilirsin.\n",
      "  Rare Abbrev: deği̇ldi̇r.\n",
      "  Rare Abbrev: lütfusun.\n",
      "  Rare Abbrev: bırakarak.\n",
      "  Rare Abbrev: öğret.\n",
      "  Rare Abbrev: üzüldüm.\n",
      "  Rare Abbrev: gözüküyor.\n",
      "  Rare Abbrev: siyasetci.\n",
      "  Rare Abbrev: bulundular.\n",
      "  Rare Abbrev: geliniyor.\n",
      "  Rare Abbrev: akpartiyi.\n",
      "  Rare Abbrev: çıkarmadı.\n",
      "  Rare Abbrev: ispatladiniz.\n",
      "  Rare Abbrev: eleştirmedim.\n",
      "  Rare Abbrev: boğulursunuz.\n",
      "  Rare Abbrev: uydurdular.\n",
      "  Rare Abbrev: yoruldum.\n",
      "  Rare Abbrev: yaparız.\n",
      "  Rare Abbrev: yetiştiriniz.\n",
      "  Rare Abbrev: karşılaşabilirsin.\n",
      "  Rare Abbrev: örn.\n",
      "  Rare Abbrev: gitmeyin.\n",
      "  Rare Abbrev: kazanacaklar.\n",
      "  Rare Abbrev: uçuyor.\n",
      "  Rare Abbrev: yüzdendir.\n",
      "  Rare Abbrev: artmıştı.\n",
      "  Rare Abbrev: bekledik.\n",
      "  Rare Abbrev: yiyor.\n",
      "  Rare Abbrev: zorlaştırmakta.\n",
      "  Rare Abbrev: veremiyoruz.\n",
      "  Rare Abbrev: bulunsun.\n",
      "  Rare Abbrev: gelmezdi.\n",
      "  Rare Abbrev: gönderilecek.\n",
      "  Rare Abbrev: xviii.\n",
      "  Rare Abbrev: boiss.\n",
      "  Rare Abbrev: bal.\n",
      "  Rare Abbrev: hub.-mor.\n",
      "  Rare Abbrev: sint.\n",
      "  Rare Abbrev: bieb.\n",
      "  Rare Abbrev: xiv.\n",
      "  Sent Starter: [1169.6559] 'bu'\n",
      "  Sent Starter: [330.2720] 'ancak'\n",
      "  Sent Starter: [54.0367] 'mansur'\n",
      "  Sent Starter: [107.3979] 'ama'\n",
      "  Sent Starter: [286.1316] 'ayrıca'\n",
      "  Sent Starter: [51.4590] 'her'\n",
      "  Sent Starter: [59.5287] 'o'\n",
      "  Sent Starter: [37.3793] 'pandemi'\n",
      "  Sent Starter: [65.5599] 'ankara'\n",
      "  Sent Starter: [103.5563] 'biz'\n",
      "  Sent Starter: [51.6848] 'türkiye'\n",
      "  Sent Starter: [69.0097] 'ben'\n",
      "  Sent Starter: [123.7092] 'bunun'\n",
      "  Sent Starter: [112.8084] 'şimdi'\n",
      "  Sent Starter: [115.5124] 'çünkü'\n",
      "  Sent Starter: [106.2883] 'buna'\n",
      "  Sent Starter: [53.4471] 'özellikle'\n",
      "  Sent Starter: [44.1705] 'allah'\n",
      "  Sent Starter: [80.6937] 'dolayısıyla'\n",
      "  Sent Starter: [91.5937] 'sonuç'\n",
      "  Sent Starter: [56.0503] 'bunlar'\n",
      "  Sent Starter: [46.5750] 'burada'\n",
      "  Sent Starter: [37.0440] 'yani'\n",
      "  Sent Starter: [33.4633] 'lütfen'\n",
      "  Sent Starter: [75.7375] 'i̇lk'\n",
      "  Sent Starter: [73.0561] 'eğer'\n",
      "  Sent Starter: [105.1134] 'böylece'\n",
      "  Sent Starter: [44.9623] 'fakat'\n",
      "  Sent Starter: [43.7066] 'bunların'\n",
      "  Sent Starter: [56.1890] 'i̇şte'\n",
      "  Sent Starter: [36.4867] 'onlar'\n",
      "  Sent Starter: [46.8557] 'umarım'\n",
      "  Sent Starter: [70.7500] 'i̇ki'\n",
      "  Sent Starter: [72.9681] 'i̇kinci'\n",
      "  Sent Starter: [99.2205] 'nitekim'\n",
      "  Sent Starter: [51.4395] 'i̇yi'\n",
      "  Sent Starter: [52.8602] 'amaç'\n",
      "  Sent Starter: [68.0923] 'vekhssere'\n",
      "  Sent Starter: [40.6762] 'örneğin'\n",
      "  Sent Starter: [42.8539] 'bununla'\n",
      "  Sent Starter: [33.8643] 'hele'\n",
      "  Sent Starter: [44.7956] 'oysa'\n",
      "  Sent Starter: [46.6767] 'bunlardan'\n",
      "  Sent Starter: [37.3185] 'rhodiapolis'\n",
      "  Sent Starter: [32.0912] 'zira'\n",
      "  Sent Starter: [30.0006] 'önümüzdeki'\n",
      "  Sent Starter: [46.2023] 'bulgular'\n",
      "  Sent Starter: [35.8168] 'üstelik'\n",
      "  Sent Starter: [60.6305] 'platon'\n",
      "  Sent Starter: [30.3646] 'binanın'\n",
      "  Sent Starter: [44.5672] 'pekgüzel'\n",
      "  Sent Starter: [30.2032] 'i̇nsanlar'\n",
      "  Sent Starter: [37.9459] 'tarımda'\n",
      "  Sent Starter: [38.5815] 'ailelerimiz'\n",
      "  Collocation: [905.6203] '##number##'+'yy.'\n",
      "  Collocation: [99.8108] '##number##'+'yüzyılın'\n",
      "  Collocation: [96.9717] '##number##'+'yüzyılda'\n",
      "  Collocation: [70.2145] '##number##'+'yüzyıla'\n",
      "  Collocation: [28.6968] 's'+'##number##'\n",
      "  Collocation: [55.6583] '##number##'+'dakikada'\n",
      "  Collocation: [41.7253] '##number##'+'yüzyıl'\n",
      "  Collocation: [46.1923] '##number##'+'yüzyıldan'\n",
      "  Collocation: [84.1135] 'a'+'ş.'\n",
      "  Collocation: [73.8136] 'i̇'+'s'\n",
      "  Collocation: [44.7702] 'i̇'+'melih'\n",
      "  Collocation: [14.2586] '##number##'+'sınıf'\n",
      "  Collocation: [22.3514] 'ö'+'##number##'\n",
      "  Collocation: [19.6226] '##number##'+'yüzyıllarda'\n",
      "  Collocation: [10.1821] '##number##'+'gününde'\n",
      "  Collocation: [54.7198] 'b'+'i̇plikçioğlu'\n",
      "  Collocation: [17.1939] '##number##'+'satırda'\n",
      "  Collocation: [45.6196] 'i'+'alâeddin'\n",
      "  Collocation: [19.3874] '##number##'+'yüzyıllar'\n",
      "  Collocation: [48.8352] 'p'+'w'\n",
      "  Collocation: [63.8468] 'w'+'ball'\n",
      "  Collocation: [15.8033] 'i'+'melih'\n",
      "  Collocation: [12.1173] '##number##'+'ayet'\n",
      "  Collocation: [16.7517] 's'+'s'\n",
      "  Collocation: [15.8984] '##number##'+'maddesine'\n",
      "  Collocation: [41.2351] 'f'+'tö'\n",
      "  Collocation: [15.8984] '##number##'+'maddesinde'\n",
      "  Collocation: [9.2821] '##number##'+'http'\n",
      "  Collocation: [15.8984] '##number##'+'maddesini'\n",
      "  Collocation: [30.7453] 'n'+'çevik'\n",
      "  Collocation: [32.7284] 't'+'kodakarensis'\n",
      "  Collocation: [34.0202] 'r'+'heberdey'\n",
      "  Collocation: [15.8984] '##number##'+'satırlar'\n",
      "  Collocation: [28.6094] 'i̇'+'ö'\n"
     ]
    }
   ],
   "source": [
    "text = do_it([i[\"text\"] for i in train_all])\n",
    "trainer.train(text, verbose=True)\n",
    "trainer.finalize_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer,open(\"mukayese_punkt_clean.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = train_ + train_news + train_abstracts\n",
    "final_test = test_ + test_news + test_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9743769743769745,\n",
       " 'token_p': 0.849907108634008,\n",
       " 'token_r': 0.8397256303738769,\n",
       " 'token_f': 0.8447856934590339}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, pickle.load(open(\"/scratch/users/user/nltk_data/tokenizers/punkt/turkish.pickle\", \"rb\")).tokenize) # clean punkt train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.973001534861485,\n",
       " 'token_p': 0.8409447275578205,\n",
       " 'token_r': 0.8290020287894889,\n",
       " 'token_f': 0.8349306738019946}"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, pickle.load(open(\"all_punkt.pkl\", \"rb\")).tokenize) # clean punkt train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9721615299591291,\n",
       " 'token_p': 0.8856099634872205,\n",
       " 'token_r': 0.8552796831224037,\n",
       " 'token_f': 0.8701806118687799}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, tokenizer.tokenize) # clean punkt train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9734536283507969,\n",
       " 'token_p': 0.8886581226888203,\n",
       " 'token_r': 0.8641871921182266,\n",
       " 'token_f': 0.8762518418620914}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, tokenizer.tokenize) # clean punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9939250259297674,\n",
       " 'token_p': 0.9534609720176731,\n",
       " 'token_r': 0.9498239436619719,\n",
       " 'token_f': 0.9516389828017052}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_news, tokenizer.tokenize) # clean punkt train news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9950161243037233,\n",
       " 'token_p': 0.9539089848308051,\n",
       " 'token_r': 0.9586631486367634,\n",
       " 'token_f': 0.9562801579178242}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_news, tokenizer.tokenize) # clean punkt test news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9971996639596751,\n",
       " 'token_p': 0.8754537838592572,\n",
       " 'token_r': 0.9198943661971831,\n",
       " 'token_f': 0.897124052081843}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_abstracts, tokenizer.tokenize) # clean punkt train abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.996057404194922,\n",
       " 'token_p': 0.9110901665095822,\n",
       " 'token_r': 0.9452411994784876,\n",
       " 'token_f': 0.9278515437529995}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_abstract, tokenizer.tokenize) # clean punkt test abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9591396129857668,\n",
       " 'token_p': 0.8707075362986864,\n",
       " 'token_r': 0.8162177732968457,\n",
       " 'token_f': 0.8425826116046538}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_, tokenizer.tokenize) # clean punkt train tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9619732785200412,\n",
       " 'token_p': 0.8661843107387662,\n",
       " 'token_r': 0.8228782287822878,\n",
       " 'token_f': 0.8439761047827539}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_, tokenizer.tokenize) # clean punkt test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer = Sentencizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2ab86f7b6f80>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = Turkish()\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bu bir cümledir.\n",
      "bu da bir cümledir\n"
     ]
    }
   ],
   "source": [
    "for i in nlp(\"bu bir cümledir. bu da bir cümledir\").sents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_wrapper(x):\n",
    "    return [i.text for i in nlp(x).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bu bir cümledir.', 'bu da bir cümledir']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_wrapper(\"bu bir cümledir. bu da bir cümledir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9478418223428603,\n",
       " 'token_p': 0.7480904446355405,\n",
       " 'token_r': 0.7143754226644768,\n",
       " 'token_f': 0.7308443082701193}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_all, dummy_wrapper) # clean spacy train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9479053022037967,\n",
       " 'token_p': 0.7649712241406128,\n",
       " 'token_r': 0.7267980295566503,\n",
       " 'token_f': 0.7453962159294718}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, dummy_wrapper) # clean spacy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9485213581599123,\n",
       " 'token_p': 0.5050595238095238,\n",
       " 'token_r': 0.4979460093896714,\n",
       " 'token_f': 0.5014775413711584}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_news, dummy_wrapper) # clean spacy train news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9633995037220844,\n",
       " 'token_p': 0.5951526032315978,\n",
       " 'token_r': 0.58311345646438,\n",
       " 'token_f': 0.5890715237672146}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_news, dummy_wrapper) # clean spacy test news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.994535519125683,\n",
       " 'token_p': 0.7758152173913043,\n",
       " 'token_r': 0.8377347417840375,\n",
       " 'token_f': 0.80558690744921}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_abstracts, dummy_wrapper) # clean spacy train abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.990764331210191,\n",
       " 'token_p': 0.8239192174187441,\n",
       " 'token_r': 0.8510430247718384,\n",
       " 'token_f': 0.8372615039281706}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_abstract, dummy_wrapper) # clean spacy test abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9333389198474882,\n",
       " 'token_p': 0.8042265692513159,\n",
       " 'token_r': 0.737217341206971,\n",
       " 'token_f': 0.7692654518128875}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(train_, dummy_wrapper) # clean spacy train tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9324866310160428,\n",
       " 'token_p': 0.7947714464621165,\n",
       " 'token_r': 0.7346791114969973,\n",
       " 'token_f': 0.7635447606872955}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_, dummy_wrapper) # clean spacy test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'caustic are the ties that bind  '"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\w\\s]', '', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(sentences):\n",
    "    new = []\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        if np.random.uniform() > .5:\n",
    "            s = re.sub(r'[^\\w\\s@]', '', s)\n",
    "        if np.random.uniform() > .50:\n",
    "            u = np.random.uniform()\n",
    "            if u > .5:\n",
    "                s = s.lower()\n",
    "            else:\n",
    "                s = s.upper()\n",
    "        new.append(s)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_abstracts_clean.jsonl\")]\n",
    "test_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_abstracts_clean.jsonl\")]\n",
    "\n",
    "train_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_news_clean.jsonl\")]\n",
    "test_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_news_clean.jsonl\")]\n",
    "\n",
    "train_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_tweets_clean.jsonl\")]\n",
    "test_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_tweets_clean.jsonl\")]\n",
    "\n",
    "\n",
    "train_all = train_abstracts + train_news + train_tweets\n",
    "test_all = test_abstracts + test_news + test_tweets\n",
    "\n",
    "\n",
    "\n",
    "train_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_abstracts_clean.jsonl\")]\n",
    "test_abstracts = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_abstracts_clean.jsonl\")]\n",
    "\n",
    "train_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_news_clean.jsonl\")]\n",
    "test_news = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_news_clean.jsonl\")]\n",
    "\n",
    "train_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_tweets_clean.jsonl\")]\n",
    "test_tweets = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_tweets_clean.jsonl\")]\n",
    "\n",
    "\n",
    "train_all = train_abstracts + train_news + train_tweets\n",
    "test_all = test_abstracts + test_news + test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweets):\n",
    "    return [re.sub(f, \"some.url\", re.sub(\"@\\w+\", \"@user\", i)) for i in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@mansuryavas06 Başkanım Rte talimatı vermiştir \" tm itirazları reddedin \" diye.',\n",
       " \"YSK akp'nin maşası durumundadır şu an, AHİM'e gider bu hırsızlık\"]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets_ = [preprocess_tweets(i) for i in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_tweets = [{\"text\" : do_it(i), \"sentences\" : i} for i in test_tweets_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Başkanım Rte talimatı vermiştir \" tm itirazları reddedin \" diye. YSK akp\\'nin maşası durumundadır şu an, AHİM\\'e gider bu hırsızlık',\n",
       " 'sentences': ['@user Başkanım Rte talimatı vermiştir \" tm itirazları reddedin \" diye.',\n",
       "  \"YSK akp'nin maşası durumundadır şu an, AHİM'e gider bu hırsızlık\"]}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Başkanım Rte talimatı vermiştir \" tm itirazları reddedin \" diye. YSK akp\\'nin maşası durumundadır şu an, AHİM\\'e gider bu hırsızlık',\n",
       " 'sentences': ['@user Başkanım Rte talimatı vermiştir \" tm itirazları reddedin \" diye.',\n",
       "  \"YSK akp'nin maşası durumundadır şu an, AHİM'e gider bu hırsızlık\"]}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets_ = []\n",
    "for i in train_tweets:\n",
    "    train_tweets_.append({\"text\" : do_it(i), \"sentences\" : i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Doğum Gününüz Kutlu Olsun Başkanım. Iyi ki Varsiniz. Allah yolunuzu açık etsin. Ankara icin yaptiklariniz inanılmaz 🙏 🏻 💐 #BirlikteBasaracagiz Saygilarimla... 💐 some.url',\n",
       " 'sentences': ['@user Doğum Gününüz Kutlu Olsun Başkanım.',\n",
       "  'Iyi ki Varsiniz.',\n",
       "  'Allah yolunuzu açık etsin.',\n",
       "  'Ankara icin yaptiklariniz inanılmaz 🙏 🏻 💐 #BirlikteBasaracagiz Saygilarimla...',\n",
       "  '💐 some.url']}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_tweets_clean_v2.jsonl\", \"w\") as data:\n",
    "     for line in train_tweets_:\n",
    "            data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_tweets_clean_v2.jsonl\", \"w\") as data:\n",
    "     for line in final_test_tweets:\n",
    "            data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_abstracts + train_news + train_tweets_\n",
    "test_all = test_abstracts + test_news + final_test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\", \"w\") as data:\n",
    "    for line in train_all:\n",
    "        data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_all_clean.jsonl\", \"w\") as data:\n",
    "    for line in test_all:\n",
    "        data.write(json.dumps(line, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [25.4046] m.s\n",
      "  Abbreviation: [0.8246] t.a.b\n",
      "  Abbreviation: [1.6493] şek\n",
      "  Abbreviation: [11.9551] m.ö\n",
      "  Abbreviation: [0.7472] dn\n",
      "  Abbreviation: [1.0995] a.-v\n",
      "  Abbreviation: [47.1928] m\n",
      "  Abbreviation: [3.1215] iv\n",
      "  Abbreviation: [1.4944] re\n",
      "  Abbreviation: [1.4944] z.r\n",
      "  Abbreviation: [2.9888] n.a\n",
      "  Abbreviation: [1.6493] lev\n",
      "  Abbreviation: [1.4944] bk\n",
      "  Abbreviation: [0.3720] subsp\n",
      "  Abbreviation: [1.4944] th\n",
      "  Abbreviation: [0.7472] mv\n",
      "  Abbreviation: [2.9888] b.b\n",
      "  Abbreviation: [0.8246] doç\n",
      "  Abbreviation: [0.8246] xix\n",
      "  Abbreviation: [2.9888] r.e\n",
      "  Abbreviation: [4.0071] g\n",
      "  Abbreviation: [2.8858] ş\n",
      "  Abbreviation: [3.2985] //t\n",
      "  Abbreviation: [1.4944] o.o\n",
      "  Abbreviation: [11.8648] c\n",
      "  Abbreviation: [3.7360] sp\n",
      "  Abbreviation: [0.5498] abs\n",
      "  Abbreviation: [11.9764] i̇.ö\n",
      "  Abbreviation: [1.4944] xi\n",
      "  Abbreviation: [0.5498] env\n",
      "  Abbreviation: [0.5498] ioh\n",
      "  Abbreviation: [1.4944] a.b\n",
      "  Abbreviation: [1.4944] pf\n",
      "  Abbreviation: [4.8296] i̇.s\n",
      "  Abbreviation: [0.7472] tı\n",
      "  Abbreviation: [1.4944] y.y\n",
      "  Abbreviation: [0.3034] i̇.s.4\n",
      "  Abbreviation: [0.3034] a.h.i̇\n",
      "  Abbreviation: [1.4944] hz\n",
      "  Abbreviation: [0.8246] req\n",
      "  Abbreviation: [1.4944] a.y\n",
      "  Abbreviation: [2.2416] vd\n",
      "  Abbreviation: [1.4944] ır\n",
      "  Abbreviation: [2.7525] j\n",
      "  Abbreviation: [1.4944] ö.y\n",
      "  Abbreviation: [0.7472] 'z\n",
      "  Abbreviation: [2.6566] yy\n",
      "  Abbreviation: [2.2416] xx\n",
      "  Abbreviation: [0.5498] i̇.b\n",
      "  Abbreviation: [0.8246] m.a.b\n",
      "  Abbreviation: [0.7472] uz\n",
      "  Abbreviation: [1.2535] st\n",
      "  Abbreviation: [0.7472] mk\n",
      "  Abbreviation: [1.4944] g.y\n",
      "  Abbreviation: [2.9888] s.k\n",
      "  Abbreviation: [0.8246] cad\n",
      "  Abbreviation: [2.9888] t.c\n",
      "  Abbreviation: [0.7472] nr\n",
      "  Abbreviation: [0.4045] 'tur\n",
      "  Abbreviation: [0.7472] là\n",
      "  Abbreviation: [0.7472] aw\n",
      "  Abbreviation: [1.4944] m.a\n",
      "  Abbreviation: [0.7472] md\n",
      "  Abbreviation: [2.9888] a.ş\n",
      "  Abbreviation: [12.3695] res\n",
      "  Abbreviation: [0.7472] up\n",
      "  Rare Abbrev: xviii.\n",
      "  Rare Abbrev: boiss.\n",
      "  Rare Abbrev: bal.\n",
      "  Rare Abbrev: hub.-mor.\n",
      "  Rare Abbrev: sint.\n",
      "  Rare Abbrev: bieb.\n",
      "  Rare Abbrev: xiv.\n",
      "  Rare Abbrev: tùrkiye.\n",
      "  Rare Abbrev: kastetmemiştim.\n",
      "  Rare Abbrev: raporunu.\n",
      "  Rare Abbrev: adamlardan.\n",
      "  Rare Abbrev: çoğalıyor.\n",
      "  Rare Abbrev: yaratıldı.\n",
      "  Rare Abbrev: unutuldu.\n",
      "  Rare Abbrev: ağliyor.\n",
      "  Rare Abbrev: çoğalır.\n",
      "  Rare Abbrev: çalışmalıyız.\n",
      "  Rare Abbrev: getirdi.\n",
      "  Rare Abbrev: oynasın.\n",
      "  Rare Abbrev: alacaksın.\n",
      "  Rare Abbrev: bitiremediniz.\n",
      "  Rare Abbrev: araştırıyormuş.\n",
      "  Rare Abbrev: alamaz.\n",
      "  Rare Abbrev: konuşuyorlar.\n",
      "  Rare Abbrev: keyifsizim.\n",
      "  Rare Abbrev: azalıyor.\n",
      "  Rare Abbrev: baksın.\n",
      "  Rare Abbrev: göremiyorum.\n",
      "  Rare Abbrev: kazanır.\n",
      "  Rare Abbrev: bilirsin.\n",
      "  Rare Abbrev: deği̇ldi̇r.\n",
      "  Rare Abbrev: lütfusun.\n",
      "  Rare Abbrev: bırakarak.\n",
      "  Rare Abbrev: öğret.\n",
      "  Rare Abbrev: üzüldüm.\n",
      "  Rare Abbrev: gözüküyor.\n",
      "  Rare Abbrev: siyasetci.\n",
      "  Rare Abbrev: bulundular.\n",
      "  Rare Abbrev: geliniyor.\n",
      "  Rare Abbrev: akpartiyi.\n",
      "  Rare Abbrev: çıkarmadı.\n",
      "  Rare Abbrev: ispatladiniz.\n",
      "  Rare Abbrev: eleştirmedim.\n",
      "  Rare Abbrev: boğulursunuz.\n",
      "  Rare Abbrev: uydurdular.\n",
      "  Rare Abbrev: yoruldum.\n",
      "  Rare Abbrev: yaparız.\n",
      "  Rare Abbrev: yetiştiriniz.\n",
      "  Rare Abbrev: karşılaşabilirsin.\n",
      "  Rare Abbrev: örn.\n",
      "  Rare Abbrev: gitmeyin.\n",
      "  Rare Abbrev: kazanacaklar.\n",
      "  Rare Abbrev: uçuyor.\n",
      "  Rare Abbrev: yüzdendir.\n",
      "  Rare Abbrev: artmıştı.\n",
      "  Rare Abbrev: bekledik.\n",
      "  Rare Abbrev: yiyor.\n",
      "  Rare Abbrev: zorlaştırmakta.\n",
      "  Rare Abbrev: veremiyoruz.\n",
      "  Rare Abbrev: bulunsun.\n",
      "  Rare Abbrev: gelmezdi.\n",
      "  Rare Abbrev: gönderilecek.\n",
      "  Sent Starter: [1169.6559] 'bu'\n",
      "  Sent Starter: [330.2720] 'ancak'\n",
      "  Sent Starter: [54.0367] 'mansur'\n",
      "  Sent Starter: [107.3979] 'ama'\n",
      "  Sent Starter: [286.1316] 'ayrıca'\n",
      "  Sent Starter: [51.4590] 'her'\n",
      "  Sent Starter: [59.5287] 'o'\n",
      "  Sent Starter: [37.3793] 'pandemi'\n",
      "  Sent Starter: [65.5599] 'ankara'\n",
      "  Sent Starter: [103.5563] 'biz'\n",
      "  Sent Starter: [51.6848] 'türkiye'\n",
      "  Sent Starter: [123.7092] 'bunun'\n",
      "  Sent Starter: [69.0097] 'ben'\n",
      "  Sent Starter: [112.8084] 'şimdi'\n",
      "  Sent Starter: [115.5124] 'çünkü'\n",
      "  Sent Starter: [106.2883] 'buna'\n",
      "  Sent Starter: [53.4471] 'özellikle'\n",
      "  Sent Starter: [80.6937] 'dolayısıyla'\n",
      "  Sent Starter: [44.1705] 'allah'\n",
      "  Sent Starter: [91.5937] 'sonuç'\n",
      "  Sent Starter: [56.0503] 'bunlar'\n",
      "  Sent Starter: [46.5750] 'burada'\n",
      "  Sent Starter: [37.0440] 'yani'\n",
      "  Sent Starter: [105.1134] 'böylece'\n",
      "  Sent Starter: [75.7375] 'i̇lk'\n",
      "  Sent Starter: [73.0561] 'eğer'\n",
      "  Sent Starter: [33.4633] 'lütfen'\n",
      "  Sent Starter: [43.7066] 'bunların'\n",
      "  Sent Starter: [44.9623] 'fakat'\n",
      "  Sent Starter: [56.1890] 'i̇şte'\n",
      "  Sent Starter: [72.9681] 'i̇kinci'\n",
      "  Sent Starter: [70.7500] 'i̇ki'\n",
      "  Sent Starter: [36.4867] 'onlar'\n",
      "  Sent Starter: [46.8557] 'umarım'\n",
      "  Sent Starter: [99.2205] 'nitekim'\n",
      "  Sent Starter: [52.8602] 'amaç'\n",
      "  Sent Starter: [68.0923] 'vekhssere'\n",
      "  Sent Starter: [51.4395] 'i̇yi'\n",
      "  Sent Starter: [40.6762] 'örneğin'\n",
      "  Sent Starter: [42.8539] 'bununla'\n",
      "  Sent Starter: [46.6767] 'bunlardan'\n",
      "  Sent Starter: [44.7956] 'oysa'\n",
      "  Sent Starter: [37.3185] 'rhodiapolis'\n",
      "  Sent Starter: [33.8643] 'hele'\n",
      "  Sent Starter: [46.2023] 'bulgular'\n",
      "  Sent Starter: [32.0912] 'zira'\n",
      "  Sent Starter: [30.0006] 'önümüzdeki'\n",
      "  Sent Starter: [35.8168] 'üstelik'\n",
      "  Sent Starter: [30.3646] 'binanın'\n",
      "  Sent Starter: [60.6305] 'platon'\n",
      "  Sent Starter: [44.5672] 'pekgüzel'\n",
      "  Sent Starter: [30.2032] 'i̇nsanlar'\n",
      "  Sent Starter: [37.9459] 'tarımda'\n",
      "  Sent Starter: [38.5815] 'ailelerimiz'\n",
      "  Collocation: [905.6203] '##number##'+'yy.'\n",
      "  Collocation: [99.8108] '##number##'+'yüzyılın'\n",
      "  Collocation: [96.9717] '##number##'+'yüzyılda'\n",
      "  Collocation: [70.2145] '##number##'+'yüzyıla'\n",
      "  Collocation: [28.6968] 's'+'##number##'\n",
      "  Collocation: [55.6583] '##number##'+'dakikada'\n",
      "  Collocation: [41.7253] '##number##'+'yüzyıl'\n",
      "  Collocation: [46.1923] '##number##'+'yüzyıldan'\n",
      "  Collocation: [73.8136] 'i̇'+'s'\n",
      "  Collocation: [84.1135] 'a'+'ş.'\n",
      "  Collocation: [22.3514] 'ö'+'##number##'\n",
      "  Collocation: [19.6226] '##number##'+'yüzyıllarda'\n",
      "  Collocation: [14.2586] '##number##'+'sınıf'\n",
      "  Collocation: [44.7702] 'i̇'+'melih'\n",
      "  Collocation: [10.1821] '##number##'+'gününde'\n",
      "  Collocation: [54.7198] 'b'+'i̇plikçioğlu'\n",
      "  Collocation: [17.1939] '##number##'+'satırda'\n",
      "  Collocation: [45.6196] 'i'+'alâeddin'\n",
      "  Collocation: [19.3874] '##number##'+'yüzyıllar'\n",
      "  Collocation: [48.8352] 'p'+'w'\n",
      "  Collocation: [63.8468] 'w'+'ball'\n",
      "  Collocation: [30.7453] 'n'+'çevik'\n",
      "  Collocation: [32.7284] 't'+'kodakarensis'\n",
      "  Collocation: [34.0202] 'r'+'heberdey'\n",
      "  Collocation: [15.8984] '##number##'+'satırlar'\n",
      "  Collocation: [28.6094] 'i̇'+'ö'\n",
      "  Collocation: [15.8984] '##number##'+'maddesine'\n",
      "  Collocation: [15.8984] '##number##'+'maddesinde'\n",
      "  Collocation: [9.2821] '##number##'+'http'\n",
      "  Collocation: [15.8984] '##number##'+'maddesini'\n",
      "  Collocation: [15.8033] 'i'+'melih'\n",
      "  Collocation: [12.1173] '##number##'+'ayet'\n",
      "  Collocation: [16.7517] 's'+'s'\n",
      "  Collocation: [41.2351] 'f'+'tö'\n"
     ]
    }
   ],
   "source": [
    "trainer = PunktTrainer()\n",
    "text = do_it([i[\"text\"] for i in train_all])\n",
    "trainer.train(text, verbose=True)\n",
    "trainer.finalize_training()\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "pickle.dump(tokenizer,open(\"mukayese_punkt_clean.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.tr import Turkish\n",
    "nlp = Turkish()\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def handle_metrics(values,predictor):\n",
    "    nlp = Turkish()\n",
    "    sentences = [Doc(nlp.vocab, words=i[\"sentences\"], spaces=np.ones(len(i[\"sentences\"]), dtype=bool)) for i in values]\n",
    "    tokenized = []\n",
    "    for p in values:\n",
    "        tokens = predictor(p[\"text\"])\n",
    "        tokenized.append(Doc(nlp.vocab, words=tokens, spaces=np.ones(len(tokens), dtype=bool)))\n",
    "        \n",
    "    examples = [Example(predicted, reference) for predicted, reference in zip(tokenized, sentences)]\n",
    "    scorer = Scorer()\n",
    "    return Scorer.score_tokenization(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt(sentences):\n",
    "    new = []\n",
    "    for i in range(len(sentences)):\n",
    "        s = sentences[i]\n",
    "        if np.random.uniform() > .5:\n",
    "            s = re.sub(r'[^\\w\\s@]', '', s)\n",
    "        if np.random.uniform() > .50:\n",
    "            u = np.random.uniform()\n",
    "            if u > .5:\n",
    "                s = s.lower()\n",
    "            else:\n",
    "                s = s.upper()\n",
    "        new.append(s)\n",
    "\n",
    "    return [i for i in new if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_abstracts = [corrupt(i[\"sentences\"]) for i in train_abstracts]\n",
    "corrupted_test_abstracts = [corrupt(i[\"sentences\"]) for i in test_abstracts]\n",
    "corrupted_train_news = [corrupt(i[\"sentences\"]) for i in train_news]\n",
    "corrupted_test_news = [corrupt(i[\"sentences\"]) for i in test_news]\n",
    "corrupted_train_tweets = [corrupt(i[\"sentences\"]) for i in train_tweets_]\n",
    "corrupted_test_tweets = [corrupt(i[\"sentences\"]) for i in final_test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_abstracts = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_train_abstracts]\n",
    "corrupted_test_abstracts = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_test_abstracts]\n",
    "corrupted_train_news = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_train_news]\n",
    "corrupted_test_news = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_test_news]\n",
    "corrupted_train_tweets = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_train_tweets]\n",
    "corrupted_test_tweets = [{\"text\" : do_it(i), \"sentences\" : i } for i in corrupted_test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'i̇nternet doğduğu günden bu yana i̇ngilizcenin hakim olduğu bir dünyadır. SON YILLARDA DIĞER DILLERINDE KATILIMIYLA SANAL DÜNYA ÇOK DILLILIKLE TANIŞMIŞTIR MAKALEDE SANAL DÜNYADA DILLERIN KULLANILMA ORANINI TESPIT EDILMEYE ÇALIŞILMIŞTIR örneğin kaç milyon kişi interneti türkçe olarak ya da fransızca olarak kullanmaktadır. günümüzde büyük bir pazar haline gelen internette tüketiciye ulaşmanın önündeki en büyük engellerden birisi de dil engelidir. İNTERNET ORTAMINDA KULLANILAN DILLERIN ORANI ETICARET VE INTERNET REKLAMCILIĞI SEKTÖRÜNDE ÇALIŞANLAR IÇIN BÜYÜK ÖNEM TAŞIMAKTADIR İnternet kullanıcılarının dil profili için Global Reach Internet World Statsın 1996 yılından beri yapmış oldukları ölçümler esas alınmıştır Global Reach ise verilerini Uluslararası Telekominikasyon birliğinin (UIT) her ülkedeki kullanıcılara göre verdiği rakamlara dayandırmaktadır. Elde edilen veriler dil atlasları dikkate alınarak rakamlara dönüştürülmektedir Bu çalışmada internet kullanıcılarının dil profili ile Arama motorlarında indekslenen 15 milyarın üzerindeki web sayfasının içerikleri ile ilgili sayısal rakamlar elde edilmeye çalışılmıştır. BUNUN IÇIN DILBILIMSEL BAZI ÖLÇÜTLERDEN YARARLANILMIŞTIR Web sitelerinin dil profili ise arama motorlarından elde edilen rakamlardan yola çıkılarak varsayımlar üretilmiştir her bir dilde kullanılan frekansı yüksek kelime, hece ve ekler tespit edilerek buna göre arama motorundan i̇ngilizce, fransızca vb. dillerdeki sayfalar görüntülenmiştir.', 'sentences': ['i̇nternet doğduğu günden bu yana i̇ngilizcenin hakim olduğu bir dünyadır.', 'SON YILLARDA DIĞER DILLERINDE KATILIMIYLA SANAL DÜNYA ÇOK DILLILIKLE TANIŞMIŞTIR', 'MAKALEDE SANAL DÜNYADA DILLERIN KULLANILMA ORANINI TESPIT EDILMEYE ÇALIŞILMIŞTIR', 'örneğin kaç milyon kişi interneti türkçe olarak ya da fransızca olarak kullanmaktadır.', 'günümüzde büyük bir pazar haline gelen internette tüketiciye ulaşmanın önündeki en büyük engellerden birisi de dil engelidir.', 'İNTERNET ORTAMINDA KULLANILAN DILLERIN ORANI ETICARET VE INTERNET REKLAMCILIĞI SEKTÖRÜNDE ÇALIŞANLAR IÇIN BÜYÜK ÖNEM TAŞIMAKTADIR', 'İnternet kullanıcılarının dil profili için Global Reach Internet World Statsın 1996 yılından beri yapmış oldukları ölçümler esas alınmıştır', 'Global Reach ise verilerini Uluslararası Telekominikasyon birliğinin (UIT) her ülkedeki kullanıcılara göre verdiği rakamlara dayandırmaktadır.', 'Elde edilen veriler dil atlasları dikkate alınarak rakamlara dönüştürülmektedir Bu çalışmada internet kullanıcılarının dil profili ile Arama motorlarında indekslenen 15 milyarın üzerindeki web sayfasının içerikleri ile ilgili sayısal rakamlar elde edilmeye çalışılmıştır.', 'BUNUN IÇIN DILBILIMSEL BAZI ÖLÇÜTLERDEN YARARLANILMIŞTIR', 'Web sitelerinin dil profili ise arama motorlarından elde edilen rakamlardan yola çıkılarak varsayımlar üretilmiştir', 'her bir dilde kullanılan frekansı yüksek kelime, hece ve ekler tespit edilerek buna göre arama motorundan i̇ngilizce, fransızca vb. dillerdeki sayfalar görüntülenmiştir.']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': 'Bu çalışmanın hedefi üç farklı gazetenin çocuk haberlerine yer veriş biçimlerini değerlendirerek olası farklılıkları belirlemek ve söz konusu farklılıkları çocuğun zarar görebilirliği bakımından incelemektir Bu amaçla Hürriyet, Cumhuriyet ve Zaman gazetelerinde 1 Temmuz 2006-31 Aralık 2006 tarihleri arasında yer alan çocuk haberleri incelenmiştir. Önceden yapmış olduğumuz pilot çalışmanın sonuçları doğrultusunda anılan üç gazete seçilmiştir. pilot çalışma aynı çocuk haberinde en büyük ifade farkının bu gazeteler arasında olduğunu ortaya koymuştur çalışmanın hedefinin çocuğun zarar görebilirliğini ortaya koymak olması bakımından aynı haberi zıt üslup ve ifade ile sunan gazeteler seçilmiştir bu gazetelerdeki çocuk haberleri içerik analizi yöntemi kullanılarak değerlendirilmiştir. Anılan süre içinde üç gazetede 1006 çocuk haberinin yayınlandığı tesbit edilmiştir En önemli bulgulara baktığımızda çocukların en çok adli haberlerde yer aldığı ve çocukların başarısına ilişkin haberlerin yok denecek kadar az olduğu görülmektedir Yasalarla düzenlenmiş olmasına rağmen bazı haberlerde çocuğun kimliği ve fotoğrafı açık, bir kısmında da kısmen açık bir biçimde belirtilmiştir. ÇOCUKLARIN ÖNEMLI BIR BÖLÜMÜ HABERLERDE SUÇ MAĞDURU KONUMUNDA YER ALMAKTADIR Çoğunluğu üçüncü sayfada yer alan haberlerde suç mağduru ve şüpheli çocukların kimliklerinin sosyal çevreleri tarafından tanınmalarına yol açabilecek şekilde verildiği görülmektedir. Çocukların suç mağduru ya da şüpheli konumunda oldukları haberlerde çocuğun kendisinin ve ailesinin kimliğinin ya da fotoğrafının gerektiği şekilde verilmediği gözlenmektedir. FOTOĞRAF KULLANIMI VE KIMLIK KONUSUNDA EN DUYARLI GAZETE CUMHURIYET GAZETESIDIR Ancak bu gazetede yer alan çocuk haberi sayısının azlığı göz ardı edilmemelidir HÜRRIYET GAZETESININ BU KONUDA ÇOK FAZLA ÖZEN GÖSTERMEDIĞI GÖZLENMEKTEDIR. Çocukların büyük bir çoğunluğu için herhangi bir tanımlama kullanılmazken, şüpheli konumunda yer alan çocukların tamamına yakını için olumsuz tanımlamaların kullanıldığı tesbit edilmiştir.', 'sentences': ['Bu çalışmanın hedefi üç farklı gazetenin çocuk haberlerine yer veriş biçimlerini değerlendirerek olası farklılıkları belirlemek ve söz konusu farklılıkları çocuğun zarar görebilirliği bakımından incelemektir', 'Bu amaçla Hürriyet, Cumhuriyet ve Zaman gazetelerinde 1 Temmuz 2006-31 Aralık 2006 tarihleri arasında yer alan çocuk haberleri incelenmiştir.', 'Önceden yapmış olduğumuz pilot çalışmanın sonuçları doğrultusunda anılan üç gazete seçilmiştir.', 'pilot çalışma aynı çocuk haberinde en büyük ifade farkının bu gazeteler arasında olduğunu ortaya koymuştur', 'çalışmanın hedefinin çocuğun zarar görebilirliğini ortaya koymak olması bakımından aynı haberi zıt üslup ve ifade ile sunan gazeteler seçilmiştir', 'bu gazetelerdeki çocuk haberleri içerik analizi yöntemi kullanılarak değerlendirilmiştir.', 'Anılan süre içinde üç gazetede 1006 çocuk haberinin yayınlandığı tesbit edilmiştir', 'En önemli bulgulara baktığımızda çocukların en çok adli haberlerde yer aldığı ve çocukların başarısına ilişkin haberlerin yok denecek kadar az olduğu görülmektedir', 'Yasalarla düzenlenmiş olmasına rağmen bazı haberlerde çocuğun kimliği ve fotoğrafı açık, bir kısmında da kısmen açık bir biçimde belirtilmiştir.', 'ÇOCUKLARIN ÖNEMLI BIR BÖLÜMÜ HABERLERDE SUÇ MAĞDURU KONUMUNDA YER ALMAKTADIR', 'Çoğunluğu üçüncü sayfada yer alan haberlerde suç mağduru ve şüpheli çocukların kimliklerinin sosyal çevreleri tarafından tanınmalarına yol açabilecek şekilde verildiği görülmektedir.', 'Çocukların suç mağduru ya da şüpheli konumunda oldukları haberlerde çocuğun kendisinin ve ailesinin kimliğinin ya da fotoğrafının gerektiği şekilde verilmediği gözlenmektedir.', 'FOTOĞRAF KULLANIMI VE KIMLIK KONUSUNDA EN DUYARLI GAZETE CUMHURIYET GAZETESIDIR', 'Ancak bu gazetede yer alan çocuk haberi sayısının azlığı göz ardı edilmemelidir', 'HÜRRIYET GAZETESININ BU KONUDA ÇOK FAZLA ÖZEN GÖSTERMEDIĞI GÖZLENMEKTEDIR.', 'Çocukların büyük bir çoğunluğu için herhangi bir tanımlama kullanılmazken, şüpheli konumunda yer alan çocukların tamamına yakını için olumsuz tanımlamaların kullanıldığı tesbit edilmiştir.']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': 'Twitter yasağı kalktı ama bu kez büyük bir iddia ortaya atıldı. BILGISAYAR UZMANLARI, TELEKOM\\'UN TWITTER\\'DE IP\\'LERI (BILGISAYARLARIN KIMLIK NUMARASI) TESBIT ETMEK IÇIN BIR YÖNTEM UYGULADIĞI ÖNE SÜRÜLÜYOR. TWITTER\\'DAKI YAVAŞLAMANIN DA BURADAN KAYNAKLANDIĞI BELIRTILIYOR. Azmedilen blog sitesinde Krototip adlı yazar durumu tüm açıklığıyla anlatıyor: \"Twitter Yasağı Kalktı (!) Az önce gördüğüm bir başlıktı \" Twitter yasağı kalktı!\" aklıma nedense TIB ve AKPli kurmayların yaptığı açıklama geldi  twitter mahkeme kararlarını uygulamazsa yasağı kaldırmayacağız PEKI NE DEĞIŞMIŞTI? TWITTER HALA MAHKEME KARARLARINI UYGULAMIYORDU HEMEN BILDIĞIM BIR KAÇ BILGISAYAR KOMUTUYLA OLAYI ARAŞTIRMAYA BAŞLADIM. İlk olarak twitter sitesine bir ping atayım dedim. SONUÇLAR BIRAZ TUHAFTI Tuhaflığın nedenini açıklayacağım. sonra google a da ping attım farkı anlayabildiniz mi? Anlayamadıysanız şöyle açıklayayım Veri alışverişindeki zaman farklılıkları. Tabi bu zaman farklılıklarına bir çok etken sebep olabilir Lakin Twitter ın kullandığı trafik ağı google a neredeyse aynı ÇÜNKÜ HIZ KAVRAMI GÜNÜMÜZ BÜYÜK INTERNET SITELERINDE ÇOK ÖNEMLI BIR KAVRAMDIR Twitter ın sürelerine bakarsanız eğer 220 ile 250 ms arasında zamanlarda veri alışverişi yaptığını görürsünüz oysa google.com 25 ile 40 saniye arasında bu işlemi yapıyor. BU SÜRELER SANIYENIN MILYONDA BIRINE DENK GELSE DE INTERNET KULLANICILARI IÇIN ÇOK ÖNEMSIZ GÖRÜNSE DE BIZLERE ÇOK BILGI VERIYOR Bununla yetinmedim tor kullanan arkadaşın bilgisayarından bir ping attım ARKADAŞIMIN BILGISAYARINDA TWITTERCOM IÇIN ÇIKAN SÜRE DEĞERLERI 45 ILE 60 SANIYE ARASINDA SÜRELERDI TABI BU AÇIKLAMA ILE YETINMEYECEĞIM BAŞKA BIR KAÇ IŞLEMDEN DAHA GEÇIRDIM TWITTER.COM U SONUÇLARI MI? hadi beraber görelim Önce bir nslookup komutu ile siteye baktım  NSLOOKUP  DOMAIN ADLARI VE IP ADRESLERI ILE ILGILI ÇEŞITLI SORGULAR YAPMAMIZA YARDIMCI OLAN BIR DOS KOMUTUDUR VE TCPIP ILE BERABER GELIR gözlerime inanamadım. Açıkçası içimden \" Vayy Köy Kurnazlarına bak Sen \" dedim. Hala verilerimiz 195.175.254.2 duvarından geçiyordu. BU IP ADRESI ÖNCEKI YAZILARDA BELIRTTIĞIM GIBI TELEKOM’UN YASAKLANAN SITELER IÇIN KULLANDIĞI BIR IP ADRESIYDI. Sonra bir de Tracert komutuna baktım (TRACERT KOMUTU ILE GÖNDERILEN ÜÇ PAKETIN HANGI AĞ GEÇITLERINDEN GEÇTIĞI TAKIP EDILIR VE BU YOLLA BIR PAKETIN UÇ NOKTAYA HANGI YOLLARI IZLEYEREK GITTIĞI GÖRÜLÜR) HADI HEP BERABER GÖRELIM ASLINDA TWITTERA GIRERKEN BILGILERIMIZ NERELERE KAYDOLUYOR. Şaşırmaya hazır mısınız vayy Gezintimiz kendi yerel hostlarımızdan başlıyor sonra Ulus Incesu Türk Telekom’a gidiyor verilerimiz. Orada çay ikram ediyorlar. sonra onların bir yedeği alınıyor Ordan hoop Frankurta biraz orda geziniyoruz SONRA TWITTERIN SUNUCULARINA YÖNLENEBILIYOR VERILERIMIZ EEE BUNUN NE ZARARI VAR DIYECEKSINIZ? Twittera yaptırıma gücü yetmeyen AKP hükümeti bu şekilde bir DNS Hijacking olayı ile IPlerimizi öğrenme fırsatını bu şekilde elde etmeye çalışıyor Kişisel güvenliğiniz her şeyden önemlidir Not Bilişim uzmanlarının da uyarılarını dikkate alarak siz siz olun VPNi kullanmaya devam edin', 'sentences': ['Twitter yasağı kalktı ama bu kez büyük bir iddia ortaya atıldı.', \"BILGISAYAR UZMANLARI, TELEKOM'UN TWITTER'DE IP'LERI (BILGISAYARLARIN KIMLIK NUMARASI) TESBIT ETMEK IÇIN BIR YÖNTEM UYGULADIĞI ÖNE SÜRÜLÜYOR.\", \"TWITTER'DAKI YAVAŞLAMANIN DA BURADAN KAYNAKLANDIĞI BELIRTILIYOR.\", 'Azmedilen blog sitesinde Krototip adlı yazar durumu tüm açıklığıyla anlatıyor: \"Twitter Yasağı Kalktı (!)', 'Az önce gördüğüm bir başlıktı \" Twitter yasağı kalktı!\"', 'aklıma nedense TIB ve AKPli kurmayların yaptığı açıklama geldi ', 'twitter mahkeme kararlarını uygulamazsa yasağı kaldırmayacağız', 'PEKI NE DEĞIŞMIŞTI?', 'TWITTER HALA MAHKEME KARARLARINI UYGULAMIYORDU', 'HEMEN BILDIĞIM BIR KAÇ BILGISAYAR KOMUTUYLA OLAYI ARAŞTIRMAYA BAŞLADIM.', 'İlk olarak twitter sitesine bir ping atayım dedim.', 'SONUÇLAR BIRAZ TUHAFTI', 'Tuhaflığın nedenini açıklayacağım.', 'sonra google a da ping attım', 'farkı anlayabildiniz mi?', 'Anlayamadıysanız şöyle açıklayayım', 'Veri alışverişindeki zaman farklılıkları.', 'Tabi bu zaman farklılıklarına bir çok etken sebep olabilir', 'Lakin Twitter ın kullandığı trafik ağı google a neredeyse aynı', 'ÇÜNKÜ HIZ KAVRAMI GÜNÜMÜZ BÜYÜK INTERNET SITELERINDE ÇOK ÖNEMLI BIR KAVRAMDIR', 'Twitter ın sürelerine bakarsanız eğer 220 ile 250 ms arasında zamanlarda veri alışverişi yaptığını görürsünüz oysa google.com 25 ile 40 saniye arasında bu işlemi yapıyor.', 'BU SÜRELER SANIYENIN MILYONDA BIRINE DENK GELSE DE INTERNET KULLANICILARI IÇIN ÇOK ÖNEMSIZ GÖRÜNSE DE BIZLERE ÇOK BILGI VERIYOR', 'Bununla yetinmedim tor kullanan arkadaşın bilgisayarından bir ping attım', 'ARKADAŞIMIN BILGISAYARINDA TWITTERCOM IÇIN ÇIKAN SÜRE DEĞERLERI 45 ILE 60 SANIYE ARASINDA SÜRELERDI', 'TABI BU AÇIKLAMA ILE YETINMEYECEĞIM BAŞKA BIR KAÇ IŞLEMDEN DAHA GEÇIRDIM TWITTER.COM U SONUÇLARI MI?', 'hadi beraber görelim Önce bir nslookup komutu ile siteye baktım', ' NSLOOKUP  DOMAIN ADLARI VE IP ADRESLERI ILE ILGILI ÇEŞITLI SORGULAR YAPMAMIZA YARDIMCI OLAN BIR DOS KOMUTUDUR VE TCPIP ILE BERABER GELIR', 'gözlerime inanamadım.', 'Açıkçası içimden \" Vayy Köy Kurnazlarına bak Sen \" dedim.', 'Hala verilerimiz 195.175.254.2 duvarından geçiyordu.', 'BU IP ADRESI ÖNCEKI YAZILARDA BELIRTTIĞIM GIBI TELEKOM’UN YASAKLANAN SITELER IÇIN KULLANDIĞI BIR IP ADRESIYDI.', 'Sonra bir de Tracert komutuna baktım', '( TRACERT KOMUTU ILE GÖNDERILEN ÜÇ PAKETIN HANGI AĞ GEÇITLERINDEN GEÇTIĞI TAKIP EDILIR VE BU YOLLA BIR PAKETIN UÇ NOKTAYA HANGI YOLLARI IZLEYEREK GITTIĞI GÖRÜLÜR ) HADI HEP BERABER GÖRELIM ASLINDA TWITTERA GIRERKEN BILGILERIMIZ NERELERE KAYDOLUYOR.', 'Şaşırmaya hazır mısınız', 'vayy Gezintimiz kendi yerel hostlarımızdan başlıyor sonra Ulus Incesu Türk Telekom’a gidiyor verilerimiz.', 'Orada çay ikram ediyorlar.', 'sonra onların bir yedeği alınıyor', 'Ordan hoop Frankurta biraz orda geziniyoruz', 'SONRA TWITTERIN SUNUCULARINA YÖNLENEBILIYOR VERILERIMIZ', 'EEE BUNUN NE ZARARI VAR DIYECEKSINIZ?', 'Twittera yaptırıma gücü yetmeyen AKP hükümeti bu şekilde bir DNS Hijacking olayı ile IPlerimizi öğrenme fırsatını bu şekilde elde etmeye çalışıyor', 'Kişisel güvenliğiniz her şeyden önemlidir Not Bilişim uzmanlarının da uyarılarını dikkate alarak siz siz olun VPNi kullanmaya devam edin']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': 'Ağustos ayı içerisinde duyurulan ve tüm dünyada satışa çıkarılan Samsung Galaxy Note 7 modelinde şok etkisi yaratan sorun sebebiyle cihazlar geri toplatılmıştı 2 EYLÜL TARIHINDE SAMSUNG KÜRESEL OLARAK CIHAZLARI GERI ÇAĞIRDIĞINI DUYURMUŞTU Yaklaşık 25 milyon Galaxy Note 7 Samsung satış noktalarında toplandı ve para iadesi yapıldı para iadesi almayan kullanıcılar ise değişim cihazları gelene kadar beklemek zorunda Peki cihaz tekrar ne zaman satışa çıkacak? Samsung o tarihi açıkladı. GÜNEY KORE MERKEZLI HABERE GÖRE GALAXY NOTE 7 MODELI 28 EYLÜL TARIHINDE ILK ETAPTA GÜNEY KORE’DE OLMAK ÜZERE TEKRAR SATIŞA ÇIKACAK. CIHAZ ILK OLARAK GÜNEY KORE’DE TOPLATILMIŞTI. DOLAYISIYLA TEKRAR ORADA SATIŞINA BAŞLANMASI PEK SÜRPRIZ SAYILMAZ. Ancak ülkemizde tekrar ne zaman satışa çıkacağı konusundaki belirsizlik sürüyor. ÇÜNKÜ SAMSUNG TÜRKIYE KONU HAKKINDA SESSIZLIĞINI KORUYOR BEKLENTILER EKIM AYINI IŞARET EDIYOR Batarya hücreleri sağlam olan yeni cihazların Ekim ayı içerisinde küresel olarak tekrar satışa çıkarılması bekleniyor Yapılan açıklama gönüllü geri çağırma olduğu için cihazlarını götürmeyen kullanıcılar için ek bir düzenleme beklenmiyor. YANI HALEN GALAXY NOTE 7 KULLANIYORSANIZ CIHAZINIZI GÖTÜRMEDIĞINIZ SÜRECE KULLANMAYA DEVAM EDEBILIRSINIZ Ancak bunu kesinlikle önermiyoruz. Yetkililer tarafından yapılan açıklamaya göre Galaxy Note 7 kullanıcılarının cihazlarını \"kapalı\" şekilde kullanmadan muhafaza etmesi gerekiyor.', 'sentences': ['Ağustos ayı içerisinde duyurulan ve tüm dünyada satışa çıkarılan Samsung Galaxy Note 7 modelinde şok etkisi yaratan sorun sebebiyle cihazlar geri toplatılmıştı', '2 EYLÜL TARIHINDE SAMSUNG KÜRESEL OLARAK CIHAZLARI GERI ÇAĞIRDIĞINI DUYURMUŞTU', 'Yaklaşık 25 milyon Galaxy Note 7 Samsung satış noktalarında toplandı ve para iadesi yapıldı', 'para iadesi almayan kullanıcılar ise değişim cihazları gelene kadar beklemek zorunda', 'Peki cihaz tekrar ne zaman satışa çıkacak?', 'Samsung o tarihi açıkladı.', 'GÜNEY KORE MERKEZLI HABERE GÖRE GALAXY NOTE 7 MODELI 28 EYLÜL TARIHINDE ILK ETAPTA GÜNEY KORE’DE OLMAK ÜZERE TEKRAR SATIŞA ÇIKACAK.', 'CIHAZ ILK OLARAK GÜNEY KORE’DE TOPLATILMIŞTI.', 'DOLAYISIYLA TEKRAR ORADA SATIŞINA BAŞLANMASI PEK SÜRPRIZ SAYILMAZ.', 'Ancak ülkemizde tekrar ne zaman satışa çıkacağı konusundaki belirsizlik sürüyor.', 'ÇÜNKÜ SAMSUNG TÜRKIYE KONU HAKKINDA SESSIZLIĞINI KORUYOR', 'BEKLENTILER EKIM AYINI IŞARET EDIYOR', 'Batarya hücreleri sağlam olan yeni cihazların Ekim ayı içerisinde küresel olarak tekrar satışa çıkarılması bekleniyor', 'Yapılan açıklama gönüllü geri çağırma olduğu için cihazlarını götürmeyen kullanıcılar için ek bir düzenleme beklenmiyor.', 'YANI HALEN GALAXY NOTE 7 KULLANIYORSANIZ CIHAZINIZI GÖTÜRMEDIĞINIZ SÜRECE KULLANMAYA DEVAM EDEBILIRSINIZ', 'Ancak bunu kesinlikle önermiyoruz.', 'Yetkililer tarafından yapılan açıklamaya göre Galaxy Note 7 kullanıcılarının cihazlarını \"kapalı\" şekilde kullanmadan muhafaza etmesi gerekiyor.']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': '@user Doğum Gününüz Kutlu Olsun Başkanım iyi ki varsiniz Allah yolunuzu açık etsin. Ankara icin yaptiklariniz inanılmaz    BirlikteBasaracagiz Saygilarimla 💐 some.url', 'sentences': ['@user Doğum Gününüz Kutlu Olsun Başkanım', 'iyi ki varsiniz', 'Allah yolunuzu açık etsin.', 'Ankara icin yaptiklariniz inanılmaz    BirlikteBasaracagiz Saygilarimla', '💐 some.url']} True \n",
      "\n",
      "\n",
      "\n",
      "{'text': '@USER BAŞKANIM RTE TALIMATI VERMIŞTIR \" TM ITIRAZLARI REDDEDIN \" DIYE. YSK AKPNIN MAŞASI DURUMUNDADIR ŞU AN AHİME GIDER BU HIRSIZLIK', 'sentences': ['@USER BAŞKANIM RTE TALIMATI VERMIŞTIR \" TM ITIRAZLARI REDDEDIN \" DIYE.', 'YSK AKPNIN MAŞASI DURUMUNDADIR ŞU AN AHİME GIDER BU HIRSIZLIK']} True \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [corrupted_train_abstracts,\n",
    "corrupted_test_abstracts,\n",
    "corrupted_train_news,\n",
    "corrupted_test_news,\n",
    "corrupted_train_tweets,\n",
    "corrupted_test_tweets]:\n",
    "    print(i[0], ['text', 'sentences'] == [j for j in i[0].keys()], \"\\n\" *3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_all = corrupted_train_abstracts + corrupted_train_news + corrupted_train_tweets\n",
    "corrupted_test_all = corrupted_test_abstracts + corrupted_test_news + corrupted_test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/\"\n",
    "with open(os.path.join(path, \"train_tweets_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_tweets:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_tweets_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_tweets:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "with open(os.path.join(path, \"train_abstracts_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_abstracts:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_abstracts_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_abstracts:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "with open(os.path.join(path, \"train_news_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_news:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_news_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_news:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "with open(os.path.join(path, \"train_all_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_train_all:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "with open(os.path.join(path, \"test_all_corrupted.jsonl\"), \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        data.write(json.dumps(i, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Abbreviation: [0.3373] yuk\n",
      "  Abbreviation: [7.3343] m.s\n",
      "  Abbreviation: [0.3373] dür\n",
      "  Abbreviation: [1.0118] t.a.b\n",
      "  Abbreviation: [0.3373] fak\n",
      "  Abbreviation: [5.5007] m.ö\n",
      "  Abbreviation: [0.3373] ünv\n",
      "  Abbreviation: [0.3373] i̇b\n",
      "  Abbreviation: [0.3373] mey\n",
      "  Abbreviation: [1.3491] a.-v\n",
      "  Abbreviation: [0.3373] sat\n",
      "  Abbreviation: [0.3373] kek\n",
      "  Abbreviation: [1.8336] z.r\n",
      "  Abbreviation: [1.8336] c.y\n",
      "  Abbreviation: [1.8336] n.a\n",
      "  Abbreviation: [0.4104] lev\n",
      "  Abbreviation: [0.3373] reh\n",
      "  Abbreviation: [0.9168] mv\n",
      "  Abbreviation: [1.8336] b.b\n",
      "  Abbreviation: [3.6671] r.e\n",
      "  Abbreviation: [2.3609] //t\n",
      "  Abbreviation: [1.8336] o.o\n",
      "  Abbreviation: [0.6745] abs\n",
      "  Abbreviation: [22.9341] i̇.ö\n",
      "  Abbreviation: [1.8336] a.b\n",
      "  Abbreviation: [0.3373] ins\n",
      "  Abbreviation: [3.0692] i̇.s\n",
      "  Abbreviation: [0.9168] tı\n",
      "  Abbreviation: [0.3373] 'no\n",
      "  Abbreviation: [1.8336] y.y\n",
      "  Abbreviation: [0.3722] i̇.s.4\n",
      "  Abbreviation: [0.3722] a.h.i̇\n",
      "  Abbreviation: [1.8336] hz\n",
      "  Abbreviation: [1.8336] ır\n",
      "  Abbreviation: [0.3373] 'ti\n",
      "  Abbreviation: [1.8336] ö.y\n",
      "  Abbreviation: [0.9168] 'z\n",
      "  Abbreviation: [0.6745] i̇.b\n",
      "  Abbreviation: [0.3373] blm\n",
      "  Abbreviation: [1.0118] m.a.b\n",
      "  Abbreviation: [0.9168] uz\n",
      "  Abbreviation: [0.9168] mk\n",
      "  Abbreviation: [3.6671] s.k\n",
      "  Abbreviation: [0.4963] 'tur\n",
      "  Abbreviation: [0.4963] 'tir\n",
      "  Abbreviation: [0.9168] aw\n",
      "  Abbreviation: [1.8336] m.a\n",
      "  Rare Abbrev: farklidir.\n",
      "  Rare Abbrev: bieb.\n",
      "  Rare Abbrev: staterdir.\n",
      "  Rare Abbrev: şeklindedir.\n",
      "  Rare Abbrev: geliştirmişlerdir.\n",
      "  Rare Abbrev: kaynaklanmamıştır.\n",
      "  Rare Abbrev: belirtildi.\n",
      "  Rare Abbrev: gidecektim.\n",
      "  Rare Abbrev: başvuruldu.\n",
      "  Rare Abbrev: planlanıyor.\n",
      "  Rare Abbrev: açıklanmıştı.\n",
      "  Rare Abbrev: beklenmektedir.\n",
      "  Rare Abbrev: çoğalır.\n",
      "  Rare Abbrev: baksın.\n",
      "  Rare Abbrev: birakarak.\n",
      "  Rare Abbrev: ispatladiniz.\n",
      "  Rare Abbrev: alınmaktadır.\n",
      "  Sent Starter: [567.4642] 'bu'\n",
      "  Sent Starter: [34.8684] 'mansur'\n",
      "  Sent Starter: [160.8511] 'ancak'\n",
      "  Sent Starter: [33.1297] 'her'\n",
      "  Sent Starter: [31.5753] 'someurl'\n",
      "  Sent Starter: [74.6623] 'biz'\n",
      "  Sent Starter: [39.6084] 'ben'\n",
      "  Sent Starter: [59.4712] 'şimdi'\n",
      "  Sent Starter: [73.4246] 'ayrıca'\n",
      "  Sent Starter: [45.6276] 'çünkü'\n",
      "  Sent Starter: [35.7020] 'bunun'\n",
      "  Sent Starter: [36.5159] 'özellikle'\n",
      "  Sent Starter: [43.7293] 'buna'\n",
      "  Sent Starter: [35.0146] 'i̇lk'\n",
      "  Sent Starter: [39.8072] 'böylece'\n",
      "  Sent Starter: [39.6401] 'bununla'\n",
      "  Sent Starter: [32.8833] 'i̇kinci'\n",
      "  Collocation: [380.6117] '##number##'+'yy'\n",
      "  Collocation: [24.1358] '##number##'+'yüzyılın'\n",
      "  Collocation: [75.6774] 'a'+'ş'\n",
      "  Collocation: [21.2402] '##number##'+'yüzyıla'\n",
      "  Collocation: [19.6953] '##number##'+'yüzyıl'\n",
      "  Collocation: [30.8311] '##number##'+'yüzyilda'\n",
      "  Collocation: [65.6572] 'm'+'calpurnius'\n",
      "  Collocation: [22.1258] '##number##'+'yüzyılda'\n",
      "  Collocation: [23.7427] '##number##'+'yüzyıldan'\n",
      "  Collocation: [45.3152] 'i̇'+'s'\n",
      "  Collocation: [14.0753] 'ö'+'##number##'\n",
      "  Collocation: [9.8330] '##number##'+'dakikada'\n",
      "  Collocation: [31.7398] 'm'+'boyutlarında'\n",
      "  Collocation: [15.6493] '##number##'+'satırlar'\n",
      "  Collocation: [10.1849] '##number##'+'satırda'\n",
      "  Collocation: [27.9214] 'm'+'kalınlığındaki'\n",
      "  Collocation: [27.5416] 'i'+'alâeddin'\n",
      "  Collocation: [21.7374] 'm'+'çapında'\n",
      "  Collocation: [31.4145] 'g'+'heteropoda'\n",
      "  Collocation: [35.2332] 'g'+'antari'\n",
      "  Collocation: [35.2332] 'g'+'muralis'\n",
      "  Collocation: [35.2332] 'g'+'tubulosa'\n",
      "  Collocation: [35.2332] 'g'+'confertifolia'\n",
      "  Collocation: [35.2332] 'g'+'pilosa'\n",
      "  Collocation: [11.8706] '##number##'+'yüzyillarda'\n",
      "  Collocation: [38.1941] 'w'+'ball'\n",
      "  Collocation: [11.8706] '##number##'+'yüzyıllar'\n",
      "  Collocation: [31.7398] 'm'+'ölçülerinde'\n",
      "  Collocation: [15.6493] '##number##'+'maddesine'\n",
      "  Collocation: [9.0403] '##number##'+'http'\n",
      "  Collocation: [19.4339] 'i̇'+'melih'\n",
      "  Collocation: [15.7848] 'i'+'melih'\n",
      "  Collocation: [19.4284] 'c'+'başkani'\n",
      "  Collocation: [16.8151] 's'+'s'\n",
      "  Collocation: [41.0025] 'f'+'tö'\n"
     ]
    }
   ],
   "source": [
    "trainer = PunktTrainer()\n",
    "text = do_it([i[\"text\"] for i in corrupted_train_all])\n",
    "trainer.train(text, verbose=True)\n",
    "trainer.finalize_training()\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "pickle.dump(tokenizer,open(\"mukayese_punkt_corrupted.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7408662712455765,\n",
       " 'token_p': 0.5096983611748872,\n",
       " 'token_r': 0.31885493536724285,\n",
       " 'token_f': 0.39229784900371706}"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7234152119058653,\n",
       " 'token_p': 0.48947156111335216,\n",
       " 'token_r': 0.3004804120647813,\n",
       " 'token_f': 0.3723685018105935}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, dummy_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_punkt = pickle.load(open(\"mukayese_punkt_clean.pkl\", \"rb\"))\n",
    "corrupted_punkt = pickle.load(open(\"mukayese_punkt_corrupted.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9733960939328531,\n",
       " 'token_p': 0.8885398723274901,\n",
       " 'token_r': 0.8639408866995074,\n",
       " 'token_f': 0.8760677356511314}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, clean_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.739462063428342,\n",
       " 'token_p': 0.5210987261146497,\n",
       " 'token_r': 0.3241543261849339,\n",
       " 'token_f': 0.3996824524442002}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, clean_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9726473629782834,\n",
       " 'token_p': 0.8620464039458453,\n",
       " 'token_r': 0.843743842364532,\n",
       " 'token_f': 0.8527969329582513}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, corrupted_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7408662712455765,\n",
       " 'token_p': 0.5096983611748872,\n",
       " 'token_r': 0.31885493536724285,\n",
       " 'token_f': 0.39229784900371706}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, corrupted_punkt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.7234152119058653,\n",
       " 'token_p': 0.48947156111335216,\n",
       " 'token_r': 0.3004804120647813,\n",
       " 'token_f': 0.3723685018105935}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(corrupted_test_all, dummy_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 0.9477446994296941,\n",
       " 'token_p': 0.7646387635496085,\n",
       " 'token_r': 0.726256157635468,\n",
       " 'token_f': 0.7449533867259543}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_metrics(test_all, dummy_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\", \"w\") as data:\n",
    "    for i in train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"İnternet doğduğu günden bu yana İngilizcenin hakim olduğu bir dünyadır. Son yıllarda diğer dillerinde katılımıyla sanal dünya çok dillilikle tanışmıştır. Makalede sanal dünyada dillerin kullanılma oranını tespit edilmeye çalışılmıştır. Örneğin kaç milyon kişi interneti Türkçe olarak ya da Fransızca olarak kullanmaktadır. Günümüzde büyük bir Pazar haline gelen internette tüketiciye ulaşmanın önündeki en büyük engellerden birisi de dil engelidir. İnternet ortamında kullanılan dillerin oranı e-ticaret ve internet reklamcılığı sektöründe çalışanlar için büyük önem taşımaktadır. İnternet kullanıcılarının dil profili için Global Reach, Internet World Stats'ın 1996 yılından beri yapmış oldukları ölçümler esas alınmıştır. Global Reach ise verilerini Uluslararası Telekominikasyon birliğinin (UIT) her ülkedeki kullanıcılara göre verdiği rakamlara dayandırmaktadır. Elde edilen veriler dil atlasları dikkate alınarak rakamlara dönüştürülmektedir Bu çalışmada internet kullanıcılarının dil profili ile Arama motorlarında indekslenen 15 milyarın üzerindeki web sayfasının içerikleri ile ilgili sayısal rakamlar elde edilmeye çalışılmıştır. Bunun için dilbilimsel bazı ölçütlerden yararlanılmıştır. Web sitelerinin dil profili ise arama motorlarından elde edilen rakamlardan yola çıkılarak varsayımlar üretilmiştir. Her bir dilde kullanılan frekansı yüksek kelime, hece ve ekler tespit edilerek buna göre arama motorundan İngilizce, Fransızca vb. dillerdeki sayfalar görüntülenmiştir.\",\n",
       " 'sentences': ['İnternet doğduğu günden bu yana İngilizcenin hakim olduğu bir dünyadır.',\n",
       "  'Son yıllarda diğer dillerinde katılımıyla sanal dünya çok dillilikle tanışmıştır.',\n",
       "  'Makalede sanal dünyada dillerin kullanılma oranını tespit edilmeye çalışılmıştır.',\n",
       "  'Örneğin kaç milyon kişi interneti Türkçe olarak ya da Fransızca olarak kullanmaktadır.',\n",
       "  'Günümüzde büyük bir Pazar haline gelen internette tüketiciye ulaşmanın önündeki en büyük engellerden birisi de dil engelidir.',\n",
       "  'İnternet ortamında kullanılan dillerin oranı e-ticaret ve internet reklamcılığı sektöründe çalışanlar için büyük önem taşımaktadır.',\n",
       "  \"İnternet kullanıcılarının dil profili için Global Reach, Internet World Stats'ın 1996 yılından beri yapmış oldukları ölçümler esas alınmıştır.\",\n",
       "  'Global Reach ise verilerini Uluslararası Telekominikasyon birliğinin (UIT) her ülkedeki kullanıcılara göre verdiği rakamlara dayandırmaktadır.',\n",
       "  'Elde edilen veriler dil atlasları dikkate alınarak rakamlara dönüştürülmektedir Bu çalışmada internet kullanıcılarının dil profili ile Arama motorlarında indekslenen 15 milyarın üzerindeki web sayfasının içerikleri ile ilgili sayısal rakamlar elde edilmeye çalışılmıştır.',\n",
       "  'Bunun için dilbilimsel bazı ölçütlerden yararlanılmıştır.',\n",
       "  'Web sitelerinin dil profili ise arama motorlarından elde edilen rakamlardan yola çıkılarak varsayımlar üretilmiştir.',\n",
       "  'Her bir dilde kullanılan frekansı yüksek kelime, hece ve ekler tespit edilerek buna göre arama motorundan İngilizce, Fransızca vb. dillerdeki sayfalar görüntülenmiştir.']}"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\", \"w\") as data:\n",
    "    for i in test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_train.txt\", \"w\") as data:\n",
    "    for i in corrupted_train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")\n",
    "            \n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_test.txt\", \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20702"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20300"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\").read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237636"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split(\" \")) for i in open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\").read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221305"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split(\" \")) for i in open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\").read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3408 61247\n",
      "3068 51712\n",
      "3408 54884\n",
      "3411 48503\n",
      "13886 121505\n",
      "13821 121090\n"
     ]
    }
   ],
   "source": [
    "for dataset in [train_abstracts,\n",
    "test_abstracts,\n",
    "train_news,\n",
    "test_news,\n",
    "train_tweets_,\n",
    "final_test_tweets,]:\n",
    "    print(sum([len(i[\"sentences\"]) for i in dataset]), sum([ sum([len(j.split(\" \")) for j in i[\"sentences\"]]) for i in dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/scratch/users/user/mukayese/ersatz_training/test_corrupted_plain.txt\",\"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        data.write(i[\"text\"] + \"\\n\")\n",
    "\n",
    "    \n",
    "with open (\"/scratch/users/user/mukayese/ersatz_training/test_plain.txt\",\"w\") as data:\n",
    "    for i in test_all:\n",
    "        data.write(i[\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = open(\"/scratch/users/user/mukayese/ersatz_training/outs/corrupted_clean_500.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = Doc(nlp.vocab, words=q, spaces=np.ones(len(q), dtype=bool))\n",
    "\"\"\"y = []\n",
    "for line in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\"):\n",
    "    for j in json.loads(line)[\"sentences\"]:\n",
    "        y.append(j)\"\"\"\n",
    "references = predicted #Doc(nlp.vocab, words=y, spaces=np.ones(len(y), dtype=bool))\n",
    "scorer = Scorer()\n",
    "Scorer.score_tokenization([Example(references,predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = Doc(nlp.vocab, words=q, spaces=np.ones(len(q), dtype=bool))\n",
    "y = []\n",
    "for line in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\"):\n",
    "    for j in json.loads(line)[\"sentences\"]:\n",
    "        y.append(j)\n",
    "references = Doc(nlp.vocab, words=y, spaces=np.ones(len(y), dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\", \"w\") as data:\n",
    "    for i in test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_train_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/train_all_corrupted.jsonl\")]\n",
    "corrupted_test_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/test_all_corrupted.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/train_all_clean.jsonl\")]\n",
    "test_all = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/clean/test_all_clean.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/users/user/mukayese/ersatz_training/train.txt\", \"w\") as data:\n",
    "    for i in train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n",
    "\n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/test.txt\", \"w\") as data:\n",
    "    for i in test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n",
    "\n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_train.txt\", \"w\") as data:\n",
    "    for i in corrupted_train_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n",
    "\n",
    "with open(\"/scratch/users/user/mukayese/ersatz_training/corrupted_test.txt\", \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        for j in i[\"sentences\"]:\n",
    "            data.write(j + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.906037735849057"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i[\"sentences\"]) for i in train_all   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.830188679245283"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i[\"sentences\"]) for i in test_all   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.86811320754717"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(i[\"sentences\"]) for i in test_all  + train_all ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsegmented_clean_test.txt\" , \"w\") as data:\n",
    "    for i in test_all:\n",
    "        data.write(i[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unsegmented_corrupted_test.txt\" , \"w\") as data:\n",
    "    for i in corrupted_test_all:\n",
    "        data.write(i[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [json.loads(i) for i in open(\"/scratch/users/user/nlp_datasets/trseg-41/corrupted/test_all_corrupted.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/mukayese/ersatz_training/test/\"\n",
    "for i, example in enumerate(corrupted_test_all):\n",
    "    with open(os.path.join(path, str(i) +  \".txt\"), \"w\") as data:\n",
    "        data.write(example[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5300"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"/scratch/users/user/mukayese/ersatz_training/test/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_4(x):\n",
    "    pad = 4 - len(x)\n",
    "    return \"0\" * pad + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0243'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_4(\"243\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1170'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder.split(\"/\")[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/scratch/users/user/mukayese/ersatz_training/test/\"\n",
    "for j,folder in enumerate(glob.glob(\"/scratch/users/user/mukayese/ersatz_training/test/*\")):\n",
    "    a = folder.split(\"/\")[-1][:-4]\n",
    "    q = os.path.join(path, pad_4(a) + \".txt\")\n",
    "    os.rename(folder,  q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = [i[\"sentences\"] for i in corrupted_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_250/\"\n",
    "\n",
    "({'token_acc': 0.7684405764017691,\n",
    "  'token_p': 0.5152147049737411,\n",
    "  'token_r': 0.3351419241396634,\n",
    "  'token_f': 0.40611207500076096},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_500/\"\n",
    "({'token_acc': 0.7681262777540055,\n",
    "  'token_p': 0.5117715167888846,\n",
    "  'token_r': 0.33308214016578747,\n",
    "  'token_f': 0.4035301278149726},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_1000/\"\n",
    "\n",
    "({'token_acc': 0.7667143538388174,\n",
    "  'token_p': 0.5107880287680767,\n",
    "  'token_r': 0.3318261743280583,\n",
    "  'token_f': 0.40230235107808504},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_250/\"\n",
    "\n",
    "({'token_acc': 0.7588465298142718,\n",
    "  'token_p': 0.5088603607151295,\n",
    "  'token_r': 0.32449399829240116,\n",
    "  'token_f': 0.396283120706575},\n",
    " 24)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_500/\"\n",
    "({'token_acc': 0.7674673008323425,\n",
    "  'token_p': 0.5047464690900672,\n",
    "  'token_r': 0.3285606631499623,\n",
    "  'token_f': 0.39802811758261825},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_1000/\"\n",
    "({'token_acc': 0.765642884395269,\n",
    "  'token_p': 0.5028204930067228,\n",
    "  'token_r': 0.32690278824415975,\n",
    "  'token_f': 0.39621262863057904},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_250/\"\n",
    "({'token_acc': 0.7642960812772134,\n",
    "  'token_p': 0.5243911988098034,\n",
    "  'token_r': 0.33639742816958007,\n",
    "  'token_f': 0.40986566296398297},\n",
    " 25)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_500/\"\n",
    "({'token_acc': 0.762773545538357,\n",
    "  'token_p': 0.520903600282375,\n",
    "  'token_r': 0.33363476513438833,\n",
    "  'token_f': 0.40674955595026646},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_1000/\"\n",
    "({'token_acc': 0.7612426467013467,\n",
    "  'token_p': 0.517425431711146,\n",
    "  'token_r': 0.33112316656620455,\n",
    "  'token_f': 0.4038225925018378},\n",
    " 25)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_250/\"\n",
    "({'token_acc': 0.7372365339578455,\n",
    "  'token_p': 0.49835146719419715,\n",
    "  'token_r': 0.3036207502636469,\n",
    "  'token_f': 0.37734435949446093},\n",
    " 23)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_500/\"\n",
    "({'token_acc': 0.7370276294645173,\n",
    "  'token_p': 0.4856345427680184,\n",
    "  'token_r': 0.2972117558402411,\n",
    "  'token_f': 0.36874746782185935},\n",
    " 26)\n",
    "\n",
    "\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_1000/\"\n",
    "({'token_acc': 0.7357313650067574,\n",
    "  'token_p': 0.48178603733245623,\n",
    "  'token_r': 0.2943185814035264,\n",
    "  'token_f': 0.3654110016215542},\n",
    " 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5300it [00:13, 400.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'token_acc': 0.7357313650067574,\n",
       "  'token_p': 0.48178603733245623,\n",
       "  'token_r': 0.2943185814035264,\n",
       "  'token_f': 0.3654110016215542},\n",
       " 25)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_1000/\"\n",
    "examples = []\n",
    "\n",
    "counter = 0\n",
    "for j,file in tqdm(enumerate(zip(sorted(glob.glob(path + \"*.txt\")), test_sentences))):\n",
    "    truth_file = file[1]\n",
    "    prediction_file = open(file[0] + \".out\").read().splitlines()\n",
    "    pred = Doc(nlp.vocab, words=prediction_file, spaces=np.ones(len(prediction_file), dtype=bool))\n",
    "    truth = Doc(nlp.vocab, words=truth_file, spaces=np.ones(len(truth_file), dtype=bool))\n",
    "    e = Example(pred, truth)\n",
    "    try:\n",
    "        Scorer.score_tokenization([e])\n",
    "        examples.append(e)\n",
    "    except:\n",
    "        counter += 1\n",
    "\n",
    "Scorer.score_tokenization(examples), counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E949] Unable to align tokens for the predicted and reference docs. It is only possible to align the docs when both texts are the same except for whitespace and capitalization. The predicted tokens start with: [\"side müzesi'nde mermerden yapılmış sigma (at nalı) formlu kırık parçaların bir araya getirilmesiyle büyük bölümü tamamlanan bir tabla sergilenmektedir.\", 'Alt bölümü kırık tablanın dış kenarları formuna uygun figürlü ve bitkisel bezemeli bir çerçeveye sahiptir bu çerçeve içinde kazıma tekniğinde deniz canlılarına ve bitkilere yer verildiği görülür.', 'Tablanın ortasında yine kazıma tekniğinde yapılmış bir haç haçın sağında ise sadece üst kısmı görülebilen çift kulplu bir kap bulunmaktadır Ortada yaba ile iki yanında ona yönelmiş yunus balıklarından oluşan antitetik düzenleme tablanın ortasındaki haçın kollarına denk gelen bölümlerde tekrarlanmıştır bu düzenleme dışındaki alana farklı deniz hayvanları ve bitkileri yerleştirilmiştir.', \"Tablanın ortasındaki Yunanca yazıt'Kuyumcuların Ortaklığı, Yoldaşlığı ya da Dostluğu'gibi anlamları içermektedir.\", \"deniz ve içinde yaşayanlara yer veren konu ve sahnelerin sorunsuz neşeli keyifli huzurlu bir hayatı ifade ettikleri kabul edilir İÖ 4 yüzyıldan başlayarak şans sembolü olarak kullanılan deniz ve yaşayanlarına ait sahneler Roma Sanatında lahitlerde özellikle resmî ve özel hamamların taban döşemelerinde yer alır aynı konulu geleneksel ikonografi, geç antik-bizans dönemi'nde bazı taban mozaikleri, el sanatları, gümüş eserler, amforalar ve hıristiyan lahitleri üzerinde de devam ettirilmiştir.\", 'Bu sahneler; mutluluğu, şansı, keyifli bir yaşamı; rahat, huzurlu bir yaşamın vaat edildiği, hayal edilen cenneti sembolize ettikleri için Hıristiyanlar tarafından da kabul edilmiş görünmektedir.', 'Bu tür sahnelere yer veren geç örneklerden biri de masa tablalarıdır.', 'konu seçimi açısından side müzesindeki eser denizde bulunan hayvan ve bitkileri konu olarak seçmiş diğer tablalarda ise gerçekte olmayan canlılar ile tanrı ve tanrıçaların yaşadığı hayali bir deniz dünyası oluşturulmuştur tablanın çerçevesindeki belli aralarla tekrarlanan, ortada yaba iki yanında ona yönelmiş yunus balıklarından oluşan antitetik düzenlemenin ikonografisi ilginçtir.', 'HIRISTIYAN INANCINDA YUNUS, BALIKLARIN KRALIDIR VE DENIZDE YAŞAYANLARI SEMBOLIZE EDER.', \"Yunusun koruyucu sıfatı ile İsayı simgelediği kabul edilir hıristiyan tasvir sanatı'nda yunus balığına yer verilişinin nedenini, antik dünyada olduğu gibi yunus balığının denizi, sevgiyi, güveni, şansı sembolize etmesi ya da diğer dünyaya yolculukta ruhların refakatçisi olması şeklinde açıklayanlar olduğu gibi, bunun mitolojik bir form olarak sadece dekoratif amaçla yapıldığını kabul edenlerde vardır.\"]. The reference tokens start with: [\"AMAÇ: ÇALIŞMANIN AMACI, THERMOCOCCUS KODAKARAENSIS'DEN BIR NADH OKSIDAZIN KLONLANMASI VE ESCHERICHIA COLI DE EKSPRESYONU SONRASI GEN ÜRÜNÜNÜN SAFLAŞTIRILMASI VE ÜRÜN ENZIMIN KARAKTERIZE EDILMESIDIR.\", 'Metotlar: NADH oksidaz geni klonlamış ve E. coli ekspresyon sisteminde eksprese edilmiştir.', 'REKOMBINANT PROTEIN ISI MUAMELESI VE IYONDEĞIŞTIRICI KOLON KROMATOGRAFISI VASITASIYLA SAFLAŞTIRILMIŞ VE KARAKTERIZE EDILMIŞTIR', 'BULGULAR: NIKOTINAMIT ADENINE DINÜKLEOTIT OKSIDAZ HOMOLOGLARI, HIPERTERMOFILIK ARKEA GENOMLARI IÇINDE BULUNMAKTADIR.', 'Bu genomlar TK0304 TK 1299 ve TK 1392 şeklinde tasarlanmış üç sekans halinde Thermococcus kodakaraensis KOD 1 içinde bulunmaktadırlar', 'Biz bu çalışmada, TK 1392 genini ve ürününü karakterize etmiş bulunmaktayız.', 'TK 1392 GENI 1239 NÜKLEOTITDEN MEYDANA GELMEKTE VE BU SEKANS, 413 AMINO ASIT ZINCIRINDEN OLUŞAN, MOLEKÜL AĞIRLIĞI 45, 244 DA OLAN BIR POLIPEPTIT ZINCIRINI OLUŞTURMAKTADIR.', 'Proteinin izoelektrik noktası 8.73 bulunmuştur.', 'TK 1392 proteininin amino asit sekansı, diğer mikroorganizmalarla ve oluşturulmuş bir filogenetik ağaç içindeki homologlarıyla karşılaştırılmıştır.', 'NADH oksidazın moleküler özelliklerinin araştırılması için TK 1392 geni klonlanmış ve gen ürünü eksprese edilmiştir'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-58b8604966a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruth_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mScorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/scorer.py\u001b[0m in \u001b[0;36mscore_tokenization\u001b[0;34m(examples, **cfg)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgold_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_unknown_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0malign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mgold_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpred_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.Example.alignment.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/training/alignment.py\u001b[0m in \u001b[0;36mfrom_strings\u001b[0;34m(cls, A, B)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_strings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Alignment\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx2y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_alignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAlignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx2y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my2x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp/lib/python3.8/site-packages/spacy/training/align.pyx\u001b[0m in \u001b[0;36mspacy.training.align.get_alignments\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E949] Unable to align tokens for the predicted and reference docs. It is only possible to align the docs when both texts are the same except for whitespace and capitalization. The predicted tokens start with: [\"side müzesi'nde mermerden yapılmış sigma (at nalı) formlu kırık parçaların bir araya getirilmesiyle büyük bölümü tamamlanan bir tabla sergilenmektedir.\", 'Alt bölümü kırık tablanın dış kenarları formuna uygun figürlü ve bitkisel bezemeli bir çerçeveye sahiptir bu çerçeve içinde kazıma tekniğinde deniz canlılarına ve bitkilere yer verildiği görülür.', 'Tablanın ortasında yine kazıma tekniğinde yapılmış bir haç haçın sağında ise sadece üst kısmı görülebilen çift kulplu bir kap bulunmaktadır Ortada yaba ile iki yanında ona yönelmiş yunus balıklarından oluşan antitetik düzenleme tablanın ortasındaki haçın kollarına denk gelen bölümlerde tekrarlanmıştır bu düzenleme dışındaki alana farklı deniz hayvanları ve bitkileri yerleştirilmiştir.', \"Tablanın ortasındaki Yunanca yazıt'Kuyumcuların Ortaklığı, Yoldaşlığı ya da Dostluğu'gibi anlamları içermektedir.\", \"deniz ve içinde yaşayanlara yer veren konu ve sahnelerin sorunsuz neşeli keyifli huzurlu bir hayatı ifade ettikleri kabul edilir İÖ 4 yüzyıldan başlayarak şans sembolü olarak kullanılan deniz ve yaşayanlarına ait sahneler Roma Sanatında lahitlerde özellikle resmî ve özel hamamların taban döşemelerinde yer alır aynı konulu geleneksel ikonografi, geç antik-bizans dönemi'nde bazı taban mozaikleri, el sanatları, gümüş eserler, amforalar ve hıristiyan lahitleri üzerinde de devam ettirilmiştir.\", 'Bu sahneler; mutluluğu, şansı, keyifli bir yaşamı; rahat, huzurlu bir yaşamın vaat edildiği, hayal edilen cenneti sembolize ettikleri için Hıristiyanlar tarafından da kabul edilmiş görünmektedir.', 'Bu tür sahnelere yer veren geç örneklerden biri de masa tablalarıdır.', 'konu seçimi açısından side müzesindeki eser denizde bulunan hayvan ve bitkileri konu olarak seçmiş diğer tablalarda ise gerçekte olmayan canlılar ile tanrı ve tanrıçaların yaşadığı hayali bir deniz dünyası oluşturulmuştur tablanın çerçevesindeki belli aralarla tekrarlanan, ortada yaba iki yanında ona yönelmiş yunus balıklarından oluşan antitetik düzenlemenin ikonografisi ilginçtir.', 'HIRISTIYAN INANCINDA YUNUS, BALIKLARIN KRALIDIR VE DENIZDE YAŞAYANLARI SEMBOLIZE EDER.', \"Yunusun koruyucu sıfatı ile İsayı simgelediği kabul edilir hıristiyan tasvir sanatı'nda yunus balığına yer verilişinin nedenini, antik dünyada olduğu gibi yunus balığının denizi, sevgiyi, güveni, şansı sembolize etmesi ya da diğer dünyaya yolculukta ruhların refakatçisi olması şeklinde açıklayanlar olduğu gibi, bunun mitolojik bir form olarak sadece dekoratif amaçla yapıldığını kabul edenlerde vardır.\"]. The reference tokens start with: [\"AMAÇ: ÇALIŞMANIN AMACI, THERMOCOCCUS KODAKARAENSIS'DEN BIR NADH OKSIDAZIN KLONLANMASI VE ESCHERICHIA COLI DE EKSPRESYONU SONRASI GEN ÜRÜNÜNÜN SAFLAŞTIRILMASI VE ÜRÜN ENZIMIN KARAKTERIZE EDILMESIDIR.\", 'Metotlar: NADH oksidaz geni klonlamış ve E. coli ekspresyon sisteminde eksprese edilmiştir.', 'REKOMBINANT PROTEIN ISI MUAMELESI VE IYONDEĞIŞTIRICI KOLON KROMATOGRAFISI VASITASIYLA SAFLAŞTIRILMIŞ VE KARAKTERIZE EDILMIŞTIR', 'BULGULAR: NIKOTINAMIT ADENINE DINÜKLEOTIT OKSIDAZ HOMOLOGLARI, HIPERTERMOFILIK ARKEA GENOMLARI IÇINDE BULUNMAKTADIR.', 'Bu genomlar TK0304 TK 1299 ve TK 1392 şeklinde tasarlanmış üç sekans halinde Thermococcus kodakaraensis KOD 1 içinde bulunmaktadırlar', 'Biz bu çalışmada, TK 1392 genini ve ürününü karakterize etmiş bulunmaktayız.', 'TK 1392 GENI 1239 NÜKLEOTITDEN MEYDANA GELMEKTE VE BU SEKANS, 413 AMINO ASIT ZINCIRINDEN OLUŞAN, MOLEKÜL AĞIRLIĞI 45, 244 DA OLAN BIR POLIPEPTIT ZINCIRINI OLUŞTURMAKTADIR.', 'Proteinin izoelektrik noktası 8.73 bulunmuştur.', 'TK 1392 proteininin amino asit sekansı, diğer mikroorganizmalarla ve oluşturulmuş bir filogenetik ağaç içindeki homologlarıyla karşılaştırılmıştır.', 'NADH oksidazın moleküler özelliklerinin araştırılması için TK 1392 geni klonlanmış ve gen ürünü eksprese edilmiştir']."
     ]
    }
   ],
   "source": [
    "pred = Doc(nlp.vocab, words=prediction_file, spaces=np.ones(len(prediction_file), dtype=bool))\n",
    "truth = Doc(nlp.vocab, words=truth_file, spaces=np.ones(len(truth_file), dtype=bool))\n",
    "e = Example(pred, truth)\n",
    "Scorer.score_tokenization([e])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [i[\"sentences\"] for i in corrupted_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [i[\"text\"] for i in corrupted_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_4(x):\n",
    "    pad = 4 - len(x)\n",
    "    return \"0\" * pad + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5300it [03:37, 24.32it/s]\n"
     ]
    }
   ],
   "source": [
    "folders = [\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_1000/\",\n",
    "           \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/clean_500/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_1000/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_200/corrupted_500/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_1000/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/clean_500/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_1000/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_250/\",\n",
    "        \"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos_1/corrupted_500/\",]\n",
    "\n",
    "for j,i in tqdm(enumerate(test_text)):\n",
    "    for folder in folders: \n",
    "        with open(os.path.join(folder, pad_4(str(j)) + \".txt\"), \"w\") as data:\n",
    "            data.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avrupa Birliği (AB) dört temel özgürlük üzerine kurulmuştur.',\n",
       " 'BUNLAR MALLARIN KIŞILERIN IŞÇILERIN HIZMETLERIN SERMAYENIN SERBEST DOLAŞIMIDIR',\n",
       " 'Bir başka özgürlük ise yerleşme hakkıdır, emeğin ve hizmetlerin serbest dolaşımı için olmazsa olmaz (sine qua non) nitelik taşımaktadır.',\n",
       " 'AB bir ekonomik bütünleşme modelidir',\n",
       " 'SERBEST DOLAŞIM ÖZGÜRLÜĞÜ OLMADAN EKONOMIK BÜTÜNLEŞMEDEN SÖZ EDILEMEZ.',\n",
       " 'üye ülkeler arasında 0 (sıfır) gümrük, 3. ülkelere karşı ortak gümrük tarifesi (ogt) uygulaması ve miktar kısıtlamalarının (kotaların) kaldırılmasıyla malların serbest dolaşımı sağlanmıştır.',\n",
       " 'ANCAK IÇ PAZARIN GERÇEKLEŞMESI IÇIN MALLAR YANINDA DIĞER ÜRETIM FAKTÖRLERININ (SERMAYE, IŞÇI, HIZMETLI) SERBEST DOLAŞIMININ DA SAĞLANMASI GEREKIYORDU.',\n",
       " 'Avrupa Topluluğunu kuran Anlaşmanın Topluluk Politikaları başlıklı 3 kısmı II',\n",
       " 'başlığında 23-31 maddeler malların serbest dolaşımına, III.',\n",
       " 'BAŞLIK ISE KIŞILERIN HIZMETLERIN VE SERMAYENIN SERBEST DOLAŞIMINA AYRILMIŞTIR',\n",
       " 'Bu başlık altında 1. bölümde işçiler (m.',\n",
       " '3942 2 BÖLÜMDE YERLEŞME HAKKI M',\n",
       " '4348 3 bölümde hizmetler m',\n",
       " '49-55) 4. BÖLÜMDE SERMAYE VE ÖDEMELER (M.',\n",
       " '5660 yer almıştır',\n",
       " 'Ayrıca ATyi kuran Anlaşmaya Maastrich Anlaşmasıyla giren Avrupa vatandaşlığı kavramının bir sonucu olarak Avrupa vatandaşlığı başlığını taşıyan ikinci kısımda yer alan Nice Anlaşmasıyla değişik şekliyle 181',\n",
       " 'maddesi eski 8 a Avrupa Birliği vatandaşlarının üye ülkelerde serbestçe dolaşım ve yerleşme hakkından söz etmektedir',\n",
       " 'topluluk politikaları başlıklı 3. kısmın kişilerin serbest dolaşımı ile ilgili 4. bölümü vize, sığınma, göç ve diğer politikalarla ilgilidir.',\n",
       " 'DAHA ÖNCE AVRUPA BIRLIĞINI KURAN ANLAŞMANIN GETIRDIĞI 3 SÜTUNDAADALET VE İÇIŞLERINDE İŞBIRLIĞIBAŞLIĞI ALTINDA YER ALAN BU KONU AMSTERDAM ANLAŞMASI ILE ATYI KURAN ANLAŞMANIN IÇINE ALINMIŞTIR',\n",
       " \"Bunun sonucu olarak AB Anlaşması'nın 3. sütununun başlığı'Polis ve Adli İşlerde İşbirliği'olarak değiştirilmiştir.\",\n",
       " \"Hizmetler AT'yi kuran Anlaşma'nın 50. maddesinde malların, sermayenin, ve işçilerin serbest dolaşımını düzenleyen hükümler dışındaki faaliyetler olarak belirlenmiştir.\",\n",
       " 'BUNLAR SINAI TICARI FAALIYETLER EL SANATLARI VE SERBEST MESLEK FAALIYETLERIDIR',\n",
       " \"Avukatların AB'de serbest dolaşımı; doktorlar, mimarlar ve diğer hizmet sektörlerinde olduğu gibi bir ücret karşılığı yapılmayan serbest meslek faaliyetleri içinde değerlendirilmektedir.\",\n",
       " 'AB kurumları önce bu serbest mesleklere mensup kişilerin diplomalarının tanınması konusunda ikincil mevzuat çıkarmış, daha sonra da her bir meslek mensubunun hizmet edimini serbestçe diğer üye ülkelerde icra edebilmesi için gerekli kuralları belirlemiştir.',\n",
       " 'Avukatlara ilişkin direktifler de bu amacı sağlamaya yöneliktir',\n",
       " 'ARAŞTIRMAMIZDA HIZMET EDINIMI SERBESTISINE ILIŞKIN GENEL KURALLARI VE DIPLOMALARIN KARŞILIK TANINMASI KONUSUNDAKI MEVZUATI INCELEDIKTEN SONRA AVUKATLARA ILIŞKIN AB MEVZUATI VE ÜYE ÜLKELERDEKI UYGULAMALARI ELE ALACAK VE TÜRKIYENIN ABYE TAM ÜYE OLMASI HAINDE TÜRK MEVZUATINDA YAPILMASI GEREKLI DEĞIŞIKLIKLERE YER VERECEĞIZ']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Avrupa Birliği (AB) dört temel özgürlük üzerine kurulmuştur.',\n",
       " 'BUNLAR MALLARIN KIŞILERIN IŞÇILERIN HIZMETLERIN SERMAYENIN SERBEST DOLAŞIMIDIR Bir başka özgürlük ise yerleşme hakkıdır, emeğin ve hizmetlerin serbest dolaşımı için olmazsa olmaz (sine qua non) nitelik taşımaktadır.',\n",
       " 'AB bir ekonomik bütünleşme modelidir SERBEST DOLAŞIM ÖZGÜRLÜĞÜ OLMADAN EKONOMIK BÜTÜNLEŞMEDEN SÖZ EDILEMEZ.',\n",
       " 'üye ülkeler arasında 0 (sıfır) gümrük, 3. ülkelere karşı ortak gümrük tarifesi (ogt) uygulaması ve miktar kısıtlamalarının (kotaların) kaldırılmasıyla malların serbest dolaşımı sağlanmıştır.',\n",
       " 'ANCAK IÇ PAZARIN GERÇEKLEŞMESI IÇIN MALLAR YANINDA DIĞER ÜRETIM FAKTÖRLERININ (SERMAYE, IŞÇI, HIZMETLI) SERBEST DOLAŞIMININ DA SAĞLANMASI GEREKIYORDU.',\n",
       " 'Avrupa Topluluğunu kuran Anlaşmanın Topluluk Politikaları başlıklı 3 kısmı II başlığında 23-31 maddeler malların serbest dolaşımına, III.',\n",
       " 'BAŞLIK ISE KIŞILERIN HIZMETLERIN VE SERMAYENIN SERBEST DOLAŞIMINA AYRILMIŞTIR Bu başlık altında 1.',\n",
       " 'bölümde işçiler (m. 3942 2 BÖLÜMDE YERLEŞME HAKKI M 4348 3 bölümde hizmetler m 49-55) 4. BÖLÜMDE SERMAYE VE ÖDEMELER (M. 5660 yer almıştır Ayrıca ATyi kuran Anlaşmaya Maastrich Anlaşmasıyla giren Avrupa vatandaşlığı kavramının bir sonucu olarak Avrupa vatandaşlığı başlığını taşıyan ikinci kısımda yer alan Nice Anlaşmasıyla değişik şekliyle 181 maddesi eski 8 a Avrupa Birliği vatandaşlarının üye ülkelerde serbestçe dolaşım ve yerleşme hakkından söz etmektedir topluluk politikaları başlıklı 3. kısmın kişilerin serbest dolaşımı ile ilgili 4. bölümü vize, sığınma, göç ve diğer politikalarla ilgilidir.',\n",
       " \"DAHA ÖNCE AVRUPA BIRLIĞINI KURAN ANLAŞMANIN GETIRDIĞI 3 SÜTUNDAADALET VE İÇIŞLERINDE İŞBIRLIĞIBAŞLIĞI ALTINDA YER ALAN BU KONU AMSTERDAM ANLAŞMASI ILE ATYI KURAN ANLAŞMANIN IÇINE ALINMIŞTIR Bunun sonucu olarak AB Anlaşması'nın 3. sütununun başlığı'Polis ve Adli İşlerde İşbirliği'olarak değiştirilmiştir.\",\n",
       " \"Hizmetler AT'yi kuran Anlaşma'nın 50. maddesinde malların, sermayenin, ve işçilerin serbest dolaşımını düzenleyen hükümler dışındaki faaliyetler olarak belirlenmiştir.\",\n",
       " \"BUNLAR SINAI TICARI FAALIYETLER EL SANATLARI VE SERBEST MESLEK FAALIYETLERIDIR Avukatların AB'de serbest dolaşımı; doktorlar, mimarlar ve diğer hizmet sektörlerinde olduğu gibi bir ücret karşılığı yapılmayan serbest meslek faaliyetleri içinde değerlendirilmektedir.\",\n",
       " 'AB kurumları önce bu serbest mesleklere mensup kişilerin diplomalarının tanınması konusunda ikincil mevzuat çıkarmış, daha sonra da her bir meslek mensubunun hizmet edimini serbestçe diğer üye ülkelerde icra edebilmesi için gerekli kuralları belirlemiştir.',\n",
       " 'Avukatlara ilişkin direktifler de bu amacı sağlamaya yöneliktir ARAŞTIRMAMIZDA HIZMET EDINIMI SERBESTISINE ILIŞKIN GENEL KURALLARI VE DIPLOMALARIN KARŞILIK TANINMASI KONUSUNDAKI MEVZUATI INCELEDIKTEN SONRA AVUKATLARA ILIŞKIN AB MEVZUATI VE ÜYE ÜLKELERDEKI UYGULAMALARI ELE ALACAK VE TÜRKIYENIN ABYE TAM ÜYE OLMASI HAINDE TÜRK MEVZUATINDA YAPILMASI GEREKLI DEĞIŞIKLIKLERE YER VERECEĞIZ']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"/scratch/users/user/mukayese/ersatz_training/initial_corrupted_eos200/clean_250/1.txt.out\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "mustc_test = open(\"/userfiles/user/mukayese_machine_translation/en-tr/data/dev/txt/dev.en\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26510, 1304)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split(\" \")) for i in mustc_test]), len(mustc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4_628_089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
