{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from dh2loop import mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glove import Glove\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "from keras.models import load_model\n",
    "from scipy import interpolate\n",
    "from itertools import product\n",
    "import gdal\n",
    "import ogr\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original litho classes: 27\n",
      "number of CET_Litho classes : 21\n",
      "unclassified in CET_Litho: 0\n",
      "number of original comment classes: 27\n",
      "number of CET_Comment classes : 21\n",
      "unclassified in CET_Comment: 0\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "DF = mlp.litho_Dataframe('../data/mlp/test3.csv', '../data/mlp/dh2loop_export' )\n",
    "\n",
    "# loading word embeddings model\n",
    "model_embedding = mlp.load_geovec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 1s - loss: 2.6808 - accuracy: 0.6526\n",
      "Epoch 2/30\n",
      " - 0s - loss: 1.6082 - accuracy: 0.7728\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.8763 - accuracy: 0.7781\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.5205 - accuracy: 0.9036\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.3233 - accuracy: 0.9287\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.2213 - accuracy: 0.9538\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.1666 - accuracy: 0.9551\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.1245 - accuracy: 0.9749\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0952 - accuracy: 0.9762\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0752 - accuracy: 0.9841\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0558 - accuracy: 0.9921\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0421 - accuracy: 0.9974\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0309 - accuracy: 0.9974\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0014 - accuracy: 1.0000\n",
      "f1 score:  1.0 accuracy:  1.0 balanced_accuracy: 1.0\n",
      "Predictions saved as: ../data/mlp/YSGBpredictions\n"
     ]
    }
   ],
   "source": [
    "# getting the mean embeddings of descriptions\n",
    "DF = mlp.mean_embeddings('../data/mlp/dh2loop_export.pkl', DF['Company_Litho'], DF['CET_Litho'], model_embedding)\n",
    "\n",
    "# subseting dataset for training classifier\n",
    "X, Y, X_test, Y_test, X_train, Y_train, X_validation, Y_validation = mlp.split_stratified_dataset(DF, 0.1, 0.1)# encoding lithological classes\n",
    "\n",
    "# train, save, assess model and save predictions\n",
    "mlp.generate_model (DF, 'CET_Litho', X, Y, X_train, Y_train, X_validation, Y_validation, '../data/mlp/mlp_prob_model.h5', '../data/mlp/YSGBpredictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = mlp.resampling('../data/mlp/YSGBpredictions.pkl', '../data/mlp/YSGBpredictions_resampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subset_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-be858427a6fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'points_YSGB_Ln.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo2D_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrecake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo2D_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepthMask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m175\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxMask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m772500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myMask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6740000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'YSGBRecakeLn.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subset_data' is not defined"
     ]
    }
   ],
   "source": [
    "# DEM and shapefile must be in the same projection that the dataframe (in this\n",
    "# case all were projected using crs = epsg:37255)\n",
    "\n",
    "model = load_model('../data/mlp/mlp_prob_model.h5')\n",
    "dataPath = '../data/mlp/YSGBpredictions_resampled.pkl'\n",
    "shapefilePath = '../data/dem/CLIP.shp'\n",
    "demPath = '../data/dem/mu_SRTM_Elevation.tif'\n",
    "\n",
    "geo2D_data = mlp.get_2D(dataPath, shapefilePath, demPath, model, 100, 1)\n",
    "np.save('points_YSGB_Ln.npy', geo2D_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 144471\n",
      "1 141180\n"
     ]
    }
   ],
   "source": [
    "recake = mlp.get_3D(geo2D_data, dataPath, 100, depthMask=175, xMask=772500, yMask=6740000)\n",
    "np.save('YSGBRecakeLn.npy', recake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo2D_path = 'points_YSGB_Ln.npy'\n",
    "#geo3D_path = 'YSGBRecakeLn.npy'\n",
    "\n",
    "data1 = np.load(geo2D_path)\n",
    "processed3D = preprocess_3Ddata(geo3D_path)\n",
    "processed2D = preprocess_2Ddata(geo2D_path)\n",
    "patches = [mpatches.Patch(color=litho_colors[n], label=litho_classes[n]) for n in np.unique(processed2D[:, 3])]\n",
    "facecolors = np.array([litho_colors.get(i, -1) for i in range(processed3D.min(), processed3D.max() + 1)])\n",
    "facecolors = facecolors[(processed3D - processed3D.min())]\n",
    "facecolors = explode(facecolors)\n",
    "filled = facecolors[:, :, :, -1] != 0\n",
    "z, y, x = expand_coordinates(np.indices(np.array(filled.shape) + 1))\n",
    "\n",
    "figure = plot3D(x, y, z, data1, processed2D, filled, facecolors, 'Lin')\n",
    "figure.savefig('test.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = 'NSWpredictions.pkl'\n",
    "\n",
    "ents, quants = get_entropies(embeddings_path, model)\n",
    "figure = classification_entropies(ents, quants, cols, patches, classes)\n",
    "\n",
    "Moree = get_subset(dataPath, shapefilePath)\n",
    "uncsss = uncertainties2D(Moree, 10, model, demPath)\n",
    "Confusion2D = CI_points(uncsss, True)\n",
    "Entropy2D = Ent_points(uncsss, True)\n",
    "Ent3D = get_3D(Entropy2D, Moree, 100,\n",
    "               depthMask=175, xMask=772500, yMask=6740000)\n",
    "print(np.nanmax(Ent3D), np.nanmin(Ent3D), np.nanmean(Ent3D))\n",
    "\n",
    "CI3D = get_3D(Confusion2D, Moree, 100,\n",
    "              depthMask=175, xMask=772500, yMask=6740000)\n",
    "print(np.nanmax(CI3D), np.nanmin(CI3D), np.nanmean(CI3D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2D(resampledDF_path, shapefile_path, dem_path, scale, depth_interval=1):\n",
    "    data = pd.read_pickle(resampledDF_path)\n",
    "    one_enc = OneHotEncoder()\n",
    "    encodes = one_enc.fit_transform(data['class'].values.reshape(-1,\n",
    "                                                                 1)).toarray()\n",
    "    file = ogr.Open(shapefile_path)\n",
    "    feature = file.GetLayer(0).GetFeature(0)\n",
    "    bounds = np.array(json.loads(feature.ExportToJson())['geometry']['coordinates'][0])\n",
    "    xbounds, ybounds = bounds[:, 0], bounds[:, 1]\n",
    "    AOI_Ymax, AOI_Ymin = np.max(ybounds), np.min(ybounds)\n",
    "    AOI_Xmax, AOI_Xmin = np.max(xbounds), np.min(xbounds)\n",
    "    subset = data[(data.xmt >= AOI_Xmin) & (data.xmt <= AOI_Xmax) &\n",
    "                  (data.ymt >= AOI_Ymin) & (data.ymt <= AOI_Ymax)]\n",
    "\n",
    "    columns=len(data['pred'].unique().tolist())\n",
    "    \n",
    "    src = gdal.Open(dem_path)\n",
    "    geo = src.GetGeoTransform()\n",
    "    rb = src.GetRasterBand(1)\n",
    "    test, training, validation = mlp.split_dataset(subset)\n",
    "    x_min, x_max = subset.xmt.min(), subset.xmt.max()\n",
    "    y_min, y_max = subset.ymt.min(), subset.ymt.max()\n",
    "    z_min, z_max = subset.zmt.min(), subset.zmt.max()\n",
    "    points_int_x, points_int_y = np.arange(x_min, x_max, scale), np.arange(y_min, y_max, scale)\n",
    "    # y rows, x columns\n",
    "    xys_toint = list(product(points_int_y, points_int_x))\n",
    "    data1 = []\n",
    "    for n in range(60):\n",
    "        de = depth_interval\n",
    "        z = np.arange(z_min+n-0.5, z_min+n+0.5, de)\n",
    "        points_train = training[(training.zmt > z[0]*de) & (training.zmt < (z[0]+1)*de)]\n",
    "        Ln = interpolate.LinearNDInterpolator(points_train[['ymt', 'xmt']].values,\n",
    "                                              np.array(points_train['mean'].tolist()))\n",
    "        Ln_int = Ln(xys_toint)\n",
    "        Lns = model.predict(Ln_int, verbose=0)\n",
    "        \n",
    "        onehot_Ln = np.zeros((Lns.shape[0], columns))\n",
    "        onehot_Ln[np.arange(len(Lns)), Lns.argmax(axis=1)] = 1\n",
    "        codes_Ln = one_enc.inverse_transform(onehot_Ln)\n",
    "        elev = []\n",
    "        for m in xys_toint[:]:\n",
    "            mx, my = m[1], m[0]\n",
    "            px = int((mx - geo[0])/geo[1])\n",
    "            py = int((my - geo[3])/geo[5])\n",
    "            intval = rb.ReadAsArray(px, py, 1, 1)\n",
    "            elev.append(intval[0][0])\n",
    "\n",
    "        elev = [e - n - 0.5 for e in elev]\n",
    "        yxs = np.array([[n[0], n[1]] for n in xys_toint])\n",
    "        yxzs = np.hstack((yxs,\n",
    "                          np.array(elev).reshape(-1, 1),\n",
    "                          codes_Ln.reshape(-1, 1)))\n",
    "        data1.append(yxzs)\n",
    "    data2 = np.vstack(data1)\n",
    "    return data2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
