{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping for real run. \n",
    "\n",
    "Todo: \n",
    "* Define `max_source_length` and `max_target_length` for the model (otherwise truncated).\n",
    "padding token should be replaced with -100, which is the 'ignore_index' of `CrossEntorpyLoss` in PT and TF. For Flax, use `decoder_attention_mask`. \n",
    "Attention_mask. ensures madding tokens of inputs are ignored. \n",
    "\n",
    "* Install apex. \"model will automatically use apex.normalization.FusedRMSNorm instead of T5LayerNorm.\" The former uses an optimized fused kernel which is several times faster than the latter.\n",
    "\n",
    "A note on model sizes: \n",
    "T5-11B (original, not v1.1) weights in float32 are 45.2GB. \n",
    "See this post for using huggingface endpoints on SINGLE GPU for cheap inference: https://www.philschmid.de/deploy-t5-11b\n",
    "Uses mixed precision and sharding, and LLM.int8(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "import accelerate\n",
    "import wandb\n",
    "import math\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "# !wandb login  -- reactivate later\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Starting to encode these text-captions: 69\n",
      "Attention!\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored, cprint\n",
    "print(colored(f\"ðŸ‘‰ Starting to encode these text-captions: 69\", \"cyan\", attrs=[\"reverse\", \"bold\"]))\n",
    "\n",
    "cprint(\"Attention!\", \"red\", attrs=[\"bold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-small-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" int8 -- my GPU doesn't support it.. \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' MODEL QUANTIZATION: fp16 '''\n",
    "\n",
    "# XL worked on my server (tested both CPU and GPU).\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# t5 = AutoModelWithLMHead.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "\n",
    "''' int8 -- my GPU doesn't support it.. '''\n",
    "# t5 = AutoModelWithLMHead.from_pretrained(MODEL_NAME, load_in_8bit=True, device_map='auto', low_cpu_mem_usage=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[1, 512, 512] f16 n=262144 xâˆˆ[5.901e-06, 1.000] Î¼=0.499 Ïƒ=inf cuda:0\n",
      "tensor[1, 512, 512] f16 n=262144 xâˆˆ[1.252e-06, 1.000] Î¼=0.500 Ïƒ=inf cuda:0\n",
      "tensor[1, 512, 512] f64 n=262144 xâˆˆ[1.000, 1.000] Î¼=1.000 Ïƒ=0. cuda:0\n",
      "tensor[1, 7] i64 xâˆˆ[1, 5295] Î¼=1.130e+03 Ïƒ=1.940e+03 cuda:0 [[37, 5295, 1782, 410, 8, 378, 1]]\n"
     ]
    }
   ],
   "source": [
    "''' PREP EMBEDDING INPUTS '''\n",
    "# shape = (batch_size, 'words', embedding_dim) -- here 'words' == each of our embeddings, like clip and language.\n",
    "# one_input_shape = [6, 512, 512]\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "\n",
    "decoder_input_embeds_arr = np.random.rand( *one_input_shape ).astype(np.float16) # need fp32\n",
    "decoder_input_embeds_arr = torch.from_numpy(decoder_input_embeds_arr).to(device)\n",
    "input_embeds_arr = np.random.rand( *one_input_shape ).astype(np.float16)\n",
    "input_embeds_arr = torch.from_numpy(input_embeds_arr).to(device)\n",
    "attn_mask_arr = np.ones( att_mask_shape )\n",
    "attn_mask_arr = torch.from_numpy(attn_mask_arr).to(device)\n",
    "\n",
    "print(decoder_input_embeds_arr)\n",
    "print(input_embeds_arr)\n",
    "print(attn_mask_arr)\n",
    "\n",
    "''' Decoder gets the tokenized caption. Shape is (batch_size, max_caption_length). Use padding to make it fit. '''\n",
    "# WORKING example, but easier with numpy.\n",
    "# import torch.nn.functional as F\n",
    "# decoder_input_ids = tokenizer(\"This is the target output sentence, aka the video caption. I like tacos because they are so delicious.\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# decoder_input_ids = F.pad(decoder_input_ids, (0, (512-decoder_input_ids.shape[1])), value=tokenizer.pad_token_id)\n",
    "# print(decoder_input_ids.shape)\n",
    "# decoder_input_ids\n",
    "\n",
    "labels = tokenizer(\"The cute dog did the things\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# labels = torch.from_numpy(np.random.randint(1, 10_000, size=(one_input_shape[0], one_input_shape[2]))).to(device)\n",
    "print(labels)\n",
    "# labels = torch.cat((labels, torch.ones((1, 512-7), dtype=int).to(device)), dim=1)\n",
    "# print(labels)\n",
    "\n",
    "# labels = torch.from_numpy(np.random.randint(1, 10_000, size=(one_input_shape[0], one_input_shape[2]))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some shapes\n",
    "# In transformer stack layer: \n",
    "# input embeds:  torch.Size([1, 512, 512])\n",
    "# batch size, seq length:  1 512\n",
    "# mask seq length:  512\n",
    "# attnetion mask:  tensor[1, 512] xâˆˆ[1.000, 1.000] Î¼=1.000 Ïƒ=0. cuda:0\n",
    "# past key values:  [None, None, None, None, None, None, None, None]\n",
    "# extended attention mask:  tensor[1, 1, 1, 512] f16 \u001b[38;2;127;127;127mall_zeros\u001b[0m cuda:0\n",
    "# head_mask:  [None, None, None, None, None, None, None, None]\n",
    "# cross_attn_head_mask:  [None, None, None, None, None, None, None, None]\n",
    "# present key value states:  None\n",
    "# all hidden states:  None\n",
    "# all attentions:  None\n",
    "# all cross attentions:  None\n",
    "# hidden states:  tensor[1, 512, 512] f16 n=262144 xâˆˆ[0., 1.111] Î¼=0.499 Ïƒ=inf cuda:0\n",
    "# final hidden states:  tensor[1, 512, 512] f16 n=262144 xâˆˆ[-0.878, 0.658] Î¼=3.207e-05 Ïƒ=0.147 grad NativeDropoutBackward0 cuda:0\n",
    "# encoder outputs:  BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor[1, 512, 512] f16 n=262144 xâˆˆ[-0.878, 0.658] Î¼=3.207e-05 Ïƒ=0.147 grad NativeDropoutBackward0 cuda:0, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
    "# hidden_states:  tensor[1, 512, 512] f16 n=262144 xâˆˆ[-0.878, 0.658] Î¼=3.207e-05 Ïƒ=0.147 grad NativeDropoutBackward0 cuda:0\n",
    "\n",
    "\n",
    "# In transformer stack layer: \n",
    "# input embeds:  torch.Size([1, 7, 512])\n",
    "# batch size, seq length:  1 7\n",
    "# mask seq length:  7\n",
    "# attnetion mask:  tensor[1, 7] xâˆˆ[1.000, 1.000] Î¼=1.000 Ïƒ=0. cuda:0 [[1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000]]\n",
    "# encoder_seq_length:  512\n",
    "# encoder attention mask:  tensor[1, 512] i64 xâˆˆ[1, 1] Î¼=1.000 Ïƒ=0. cuda:0\n",
    "# past key values:  [None, None, None, None, None, None, None, None]\n",
    "# extended attention mask:  tensor[1, 1, 7, 7] f16 n=49 xâˆˆ[-6.550e+04, 0.] Î¼=-2.808e+04 Ïƒ=inf cuda:0\n",
    "# poggers\n",
    "# head_mask:  [None, None, None, None, None, None, None, None]\n",
    "# cross_attn_head_mask:  [None, None, None, None, None, None, None, None]\n",
    "# present key value states:  ()\n",
    "# all hidden states:  None\n",
    "# all attentions:  None\n",
    "# all cross attentions:  None\n",
    "# hidden states:  tensor[1, 7, 512] f16 n=3584 xâˆˆ[-160.000, 80.562] Î¼=-0.429 Ïƒ=inf grad NativeDropoutBackward0 cuda:0\n",
    "# final hidden states:  tensor[1, 7, 512] f16 n=3584 xâˆˆ[-28.641, 17.469] Î¼=0.020 Ïƒ=0.911 grad NativeDropoutBackward0 cuda:0\n",
    "\n",
    "# sequence outputs:  tensor[1, 7, 512] f16 n=3584 xâˆˆ[-28.641, 17.469] Î¼=0.020 Ïƒ=0.911 grad NativeDropoutBackward0 cuda:0\n",
    "# lm logits:  tensor[1, 7, 32128] f16 n=224896 xâˆˆ[-37.438, 6.746] Î¼=-13.016 Ïƒ=inf grad UnsafeViewBackward0 cuda:0\n",
    "# tensor f16 grad NllLossBackward0 cuda:0 7.281  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function\n",
    "\n",
    "T5 forward() docs: https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5ForConditionalGeneration.forward\n",
    "\n",
    "Todo: investigate difference between decoder `decoder_input_ids` and `lm_labels`.\n",
    "For example: \n",
    "```\n",
    "outputs = t5(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "```\n",
    "\n",
    "I think `loss.sum()` is for multi-iteration loss. I was inadverdently using it 6 batches.\n",
    "https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152 \n",
    "loss.backward() # T5 RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "\n",
    "Variables and their purpose:\n",
    "\n",
    "`extended_attention_mask` - just the original attention mask provided, broadcast to a new shape (decoder uses causal mask, encoder uses regular mask)\n",
    "\n",
    "`attention_mask` - attention mask used in the regular attention layer \n",
    "\n",
    "`encoder_attention_mask` - attention mask used in the cross-attention layer\n",
    "\n",
    "`position_bias` - always None, position bias used in attention block\n",
    "\n",
    "`encoder_decoder_position_bias` - always None, position bias used in the cross-attention layer\n",
    "\n",
    "`layer_head_mask` - always None, masks certain heads in the attention block (T5Attention)\n",
    "\n",
    "`cross_attn_layer_head_mask` - always None, masks certain heads in the attention block (T5Attention) and is used specifically for the cross-attention module in the decoder\n",
    "\n",
    "`past_key_value` - always None, if specified then uses the previous key value states from the previous attention blocks in the query, key, value projection layers\n",
    "\n",
    "`output_attentions` - True/False, and outputs the attention weights in addition to the rest of the outputs in the result tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "In transformer stack layer: \n",
      "input embeds:  torch.Size([1, 512, 512])\n",
      "batch size, seq length:  1 512\n",
      "mask seq length:  512\n",
      "past key values:  [None, None, None, None, None, None, None, None]\n",
      "extended attention mask:  tensor[1, 1, 512, 512] f16 \u001b[38;2;127;127;127mall_zeros\u001b[0m cuda:0\n",
      "head_mask:  [None, None, None, None, None, None, None, None]\n",
      "cross_attn_head_mask:  [None, None, None, None, None, None, None, None]\n",
      "present key value states:  None\n",
      "all hidden states:  None\n",
      "all attentions:  None\n",
      "all cross attentions:  None\n",
      "hidden states:  tensor[1, 512, 512] f16 n=262144 xâˆˆ[0., 1.111] Î¼=0.500 Ïƒ=inf cuda:0\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 6)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "final hidden states:  tensor[1, 512, 512] f16 n=262144 xâˆˆ[-0.857, 0.752] Î¼=0.000 Ïƒ=0.142 grad NativeDropoutBackward0 cuda:0\n",
      "encoder outputs:  BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor[1, 512, 512] f16 n=262144 xâˆˆ[-0.857, 0.752] Î¼=0.000 Ïƒ=0.142 grad NativeDropoutBackward0 cuda:0, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "hidden_states:  tensor[1, 512, 512] f16 n=262144 xâˆˆ[-0.857, 0.752] Î¼=0.000 Ïƒ=0.142 grad NativeDropoutBackward0 cuda:0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "In transformer stack layer: \n",
      "input embeds:  torch.Size([1, 7, 512])\n",
      "batch size, seq length:  1 7\n",
      "mask seq length:  7\n",
      "attnetion mask:  tensor[1, 7] xâˆˆ[1.000, 1.000] Î¼=1.000 Ïƒ=0. cuda:0 [[1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000]]\n",
      "past key values:  [None, None, None, None, None, None, None, None]\n",
      "extended attention mask:  tensor[1, 1, 7, 7] f16 n=49 xâˆˆ[-6.550e+04, 0.] Î¼=-2.808e+04 Ïƒ=inf cuda:0\n",
      "poggers\n",
      "head_mask:  [None, None, None, None, None, None, None, None]\n",
      "cross_attn_head_mask:  [None, None, None, None, None, None, None, None]\n",
      "present key value states:  ()\n",
      "all hidden states:  None\n",
      "all attentions:  None\n",
      "all cross attentions:  None\n",
      "hidden states:  tensor[1, 7, 512] f16 n=3584 xâˆˆ[-160.000, 80.562] Î¼=-0.336 Ïƒ=inf grad NativeDropoutBackward0 cuda:0\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "        (relative_attention_bias): Embedding(32, 6)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n",
      "layer moduel:  T5Block(\n",
      "  (layer): ModuleList(\n",
      "    (0): T5LayerSelfAttention(\n",
      "      (SelfAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): T5LayerCrossAttention(\n",
      "      (EncDecAttention): T5Attention(\n",
      "        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): T5LayerFF(\n",
      "      (DenseReluDense): T5DenseGatedActDense(\n",
      "        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (act): NewGELUActivation()\n",
      "      )\n",
      "      (layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "past key value:  None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (512) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m t5\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m t5\u001b[39m.\u001b[39;49mforward(inputs_embeds\u001b[39m=\u001b[39;49minput_embeds_arr, attention_mask\u001b[39m=\u001b[39;49mattn_mask_arr, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:1696\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m decoder_input_ids\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1695\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1697\u001b[0m \u001b[39mif\u001b[39;00m decoder_attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1698\u001b[0m     decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:1078\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1064\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1065\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[1;32m   1076\u001b[0m     \u001b[39m#print(\"poggers 3\")\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1078\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1079\u001b[0m         hidden_states,\n\u001b[1;32m   1080\u001b[0m         attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1081\u001b[0m         position_bias\u001b[39m=\u001b[39mposition_bias,\n\u001b[1;32m   1082\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1084\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1085\u001b[0m         layer_head_mask\u001b[39m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1086\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1087\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m   1088\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1089\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1090\u001b[0m     )\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# #print(\"layer outputs: \", layer_outputs)\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \n\u001b[1;32m   1094\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:701\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     query_length \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m cross_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m1\u001b[39;49m](\n\u001b[1;32m    702\u001b[0m     hidden_states,\n\u001b[1;32m    703\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    704\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    705\u001b[0m     position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m    706\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    707\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    708\u001b[0m     query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    709\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    710\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    712\u001b[0m hidden_states \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    714\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:615\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    603\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    604\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    613\u001b[0m ):\n\u001b[1;32m    614\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 615\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncDecAttention(\n\u001b[1;32m    616\u001b[0m         normed_hidden_states,\n\u001b[1;32m    617\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    618\u001b[0m         key_value_states\u001b[39m=\u001b[39;49mkey_value_states,\n\u001b[1;32m    619\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    620\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    621\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    622\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    623\u001b[0m         query_length\u001b[39m=\u001b[39;49mquery_length,\n\u001b[1;32m    624\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    626\u001b[0m     layer_output \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    627\u001b[0m     outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/custom_huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/thesis/video-pretrained-transformer/model/transformers-VPT/src/transformers/models/t5/modeling_t5.py:531\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m         position_bias \u001b[39m=\u001b[39m position_bias[:, :, \u001b[39m-\u001b[39mhidden_states\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) :, :]\n\u001b[1;32m    530\u001b[0m     \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m         position_bias \u001b[39m=\u001b[39m position_bias \u001b[39m+\u001b[39;49m mask  \u001b[39m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpruned_heads:\n\u001b[1;32m    534\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(position_bias\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (512) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "t5.train()\n",
    "\n",
    "\n",
    "# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "# outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "loss = outputs[0]\n",
    "loss.shape\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successful training iteration\n"
     ]
    }
   ],
   "source": [
    "''' backwards pass '''\n",
    "optimizer = torch.optim.Adam(params =  t5.parameters(), lr=1e-4)\n",
    "optimizer.zero_grad()\n",
    "loss.sum().backward()\n",
    "optimizer.step()\n",
    "print(\"âœ… Successful training iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Objectives\n",
    "\n",
    "Incorporate loss objectives: MLM, caption-image matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[1, 11] i64 xâˆˆ[1, 13959] Î¼=1.894e+03 Ïƒ=4.120e+03\n",
      "tensor[1, 6] i64 xâˆˆ[1, 19250] Î¼=4.121e+03 Ïƒ=7.619e+03 [[644, 4598, 229, 19250, 5, 1]]\n",
      "1566\n",
      "tensor[1, 11] i64 xâˆˆ[1, 1] Î¼=1.000 Ïƒ=0.\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(input_ids[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(tokenizer(\u001b[39m\"\u001b[39m\u001b[39mtranslate English to German: The house is wonderful.\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mattention_mask)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# the forward function automatically creates the correct decoder_input_ids\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B100.107.136.74/home/kastan/thesis/video-pretrained-transformer/model/custom_T5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m t5(input_ids\u001b[39m=\u001b[39minput_ids, labels\u001b[39m=\u001b[39mlabels)\u001b[39m.\u001b[39mloss\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# t5.train()\n",
    "\n",
    "# input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# loss = t5(input_ids=input_ids, labels=labels).loss\n",
    "# loss.item()\n",
    "\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids.to(device)\n",
    "# labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"Das Haus ist wunderbar.\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(input_ids)\n",
    "print(labels)\n",
    "\n",
    "print(input_ids[0][1].item())\n",
    "\n",
    "print(tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").attention_mask)\n",
    "\n",
    "\n",
    "\n",
    "# the forward function automatically creates the correct decoder_input_ids\n",
    "loss = t5(input_ids=input_ids, labels=labels).loss\n",
    "loss.item()\n",
    "\n",
    "# outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "# # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels)\n",
    "# loss = outputs[0]\n",
    "# loss.shape\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' minimal train '''\n",
    "t5.train()\n",
    "\n",
    "loss = outputs[0]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=1e-4)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
    "# The model is put into train mode and then we enumerate over the training loader and passed to the defined network \n",
    "\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10 == 0:\n",
    "            wandb.log({\"Training Loss\": loss.item()})\n",
    "\n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # xm.optimizer_step(optimizer)  # FOR TPU\n",
    "        # xm.mark_step()                # FOR TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # WandB â€“ Initialize a new run\n",
    "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
    "\n",
    "    # WandB â€“ Config is a variable that holds and saves hyperparameters and inputs\n",
    "    # Defining some key variables that will be used later on in the training  \n",
    "    config = wandb.config          # Initialize config\n",
    "    config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
    "    config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
    "    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
    "    config.VAL_EPOCHS = 1 \n",
    "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "    config.SEED = 42               # random seed (default: 42)\n",
    "    config.MAX_LEN = 512\n",
    "    config.SUMMARY_LEN = 150 \n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(config.SEED) # pytorch random seed\n",
    "    np.random.seed(config.SEED) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    \n",
    "\n",
    "    # Importing and Pre-Processing the domain data\n",
    "    # Selecting the needed columns only. \n",
    "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
    "    df = pd.read_csv('./data/news_summary.csv',encoding='latin-1')\n",
    "    df = df[['text','ctext']]\n",
    "    df.ctext = 'summarize: ' + df.ctext\n",
    "    print(df.head())\n",
    "\n",
    "    \n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
    "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    val_params = {\n",
    "        'batch_size': config.VALID_BATCH_SIZE,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "        }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    \n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # Log metrics with wandb\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    # Training loop\n",
    "    print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "    for epoch in range(config.TRAIN_EPOCHS):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "\n",
    "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
    "    # Saving the dataframe as predictions.csv\n",
    "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "    for epoch in range(config.VAL_EPOCHS):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv('./models/predictions.csv')\n",
    "        print('Output Files generated for review')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_prob = 0.4\n",
    "shuffle_offset = 16\n",
    "num_chunks_in_group = 4\n",
    "batch_size = 1024\n",
    "num_chunks = 16\n",
    "B = batch_size * (num_chunks // num_chunks_in_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xa shape:  (6, 23, 768)\n",
      "xb shape:  (6, 23, 768)\n",
      "xa none shape:  (6, 23, 1, 768)\n",
      "xb none shape:  (6, 1, 23, 768)\n",
      "xa tile shape:  (6, 23, 23, 768)\n",
      "xa tile shape 2:  (6, 529, 768)\n",
      "xb tile shape:  (6, 23, 23, 768)\n",
      "xb tile shape 2:  (6, 529, 768)\n",
      "h_joint shape:  (6, 529, 1536)\n",
      "h_joint shape 2:  (3174, 1536)\n",
      "h0 shape:  (3174, 768)\n",
      "logits shape:  (3174, 4)\n",
      "video src ids:  (6, 23)\n",
      "(6, 1, 23)\n",
      "(6, 23, 1)\n",
      "is_same_video:  (6, 23, 23)\n",
      "one-hot labels:  (3174, 4)\n",
      "temporal loss:  tf.Tensor(1.6404991, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_140958/3368077830.py:74: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  h0 = tf.compat.v1.layers.dense(\n",
      "/tmp/ipykernel_140958/3368077830.py:85: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.compat.v1.layers.dense(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.6404991>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def calculate_temporal_loss(shuffled_idx_img, B, batch_size=6, num_chunks_in_group=23, hidden_size = 768):\n",
    "  def gelu(input_tensor):\n",
    "      \"\"\"Gaussian Error Linear Unit.\n",
    "      This is a smoother version of the RELU.\n",
    "      Original paper: https://arxiv.org/abs/1606.08415\n",
    "      Args:\n",
    "        input_tensor: float Tensor to perform activation.\n",
    "      Returns:\n",
    "        `input_tensor` with the GELU activation applied.\n",
    "      \"\"\"\n",
    "      # math.sqrt needed for bfloat16 compatibility\n",
    "      cdf = 0.5 * (1.0 + tf.compat.v1.erf(input_tensor / math.sqrt(2.0)))\n",
    "      return input_tensor * cdf\n",
    "\n",
    "\n",
    "  def layer_norm(input_tensor, name=None, epsilon=1e-5):\n",
    "    \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
    "    name2use = f'LayerNorm_{name}' if name is not None else name\n",
    "    with tf.compat.v1.variable_scope(name2use, default_name='LayerNorm'):\n",
    "        dim = input_tensor.shape[-1]\n",
    "        gamma = tf.compat.v1.get_variable('gamma', [dim], initializer=tf.constant_initializer(1))\n",
    "        beta = tf.compat.v1.get_variable('beta', [dim], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        cast_up_to_float32 = input_tensor.dtype == tf.bfloat16\n",
    "        if cast_up_to_float32:\n",
    "            input_tensor = tf.cast(input_tensor, dtype=tf.float32)\n",
    "\n",
    "        mean, variance = tf.compat.v1.nn.moments(input_tensor, -1, keep_dims=True)\n",
    "        scale_factor = tf.compat.v1.rsqrt(variance + epsilon) * gamma\n",
    "        input_tensor = input_tensor * scale_factor - mean * scale_factor + beta\n",
    "        if cast_up_to_float32:\n",
    "            input_tensor = tf.cast(input_tensor, dtype=tf.bfloat16)\n",
    "    return input_tensor\n",
    "  \n",
    "  is_easy_viz = tf.reshape(tf.less(shuffled_idx_img, 64), [B, num_chunks_in_group])\n",
    "\n",
    "  is_easy = tf.logical_and(is_easy_viz[:, :, None], is_easy_viz[:, None])\n",
    "  label_w = tf.cast(tf.logical_not(is_easy), dtype=tf.float32) * 0.99 + 0.01\n",
    "\n",
    "  label_w = tf.reshape(label_w, [-1])\n",
    "\n",
    "  xa = tf.random.uniform(shape=[batch_size, num_chunks_in_group, hidden_size])\n",
    "\n",
    "  print(\"xa shape: \", xa.shape)\n",
    "\n",
    "  xb = tf.random.uniform(shape=[batch_size, num_chunks_in_group, hidden_size])\n",
    "\n",
    "  print(\"xb shape: \", xb.shape)\n",
    "\n",
    "  print(\"xa none shape: \", xa[:, :, None].shape)\n",
    "  print(\"xb none shape: \", xb[:, None].shape)\n",
    "\n",
    "  xa_tile = tf.tile(xa[:, :, None], [1, 1, num_chunks_in_group, 1])\n",
    "\n",
    "  print(\"xa tile shape: \", xa_tile.shape)\n",
    "\n",
    "  xa_tile = tf.reshape(xa_tile, [batch_size, num_chunks_in_group ** 2, hidden_size])\n",
    "\n",
    "  print(\"xa tile shape 2: \", xa_tile.shape)\n",
    "\n",
    "\n",
    "  xb_tile = tf.tile(xb[:, None], [1, num_chunks_in_group, 1, 1])\n",
    "\n",
    "  print(\"xb tile shape: \", xb_tile.shape)\n",
    "\n",
    "  xb_tile = tf.reshape(xb_tile, [batch_size, num_chunks_in_group ** 2, hidden_size])\n",
    "\n",
    "  print(\"xb tile shape 2: \", xb_tile.shape)\n",
    "\n",
    "  h_joint = tf.concat([xa_tile, xb_tile], 2)\n",
    "\n",
    "  print(\"h_joint shape: \", h_joint.shape)\n",
    "\n",
    "  h_joint = tf.reshape(h_joint, [batch_size * (num_chunks_in_group ** 2), hidden_size * 2])\n",
    "\n",
    "  print(\"h_joint shape 2: \", h_joint.shape)\n",
    "\n",
    "\n",
    "  # Now do the MLP\n",
    "  h0 = tf.compat.v1.layers.dense(\n",
    "      h_joint,\n",
    "      hidden_size,\n",
    "      kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "      name='intermediate',\n",
    "      activation=gelu,\n",
    "  )\n",
    "\n",
    "  print(\"h0 shape: \", h0.shape)\n",
    "\n",
    "  h0_ln = layer_norm(h0, 'ln0')\n",
    "  logits = tf.compat.v1.layers.dense(\n",
    "      h0_ln,\n",
    "      4,\n",
    "      kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "      name='logits',\n",
    "  )\n",
    "\n",
    "  print(\"logits shape: \", logits.shape)\n",
    "\n",
    "\n",
    "\n",
    "  num_chunks_in_group = 23\n",
    "  video_src_ids = tf.random.uniform(shape=[batch_size, num_chunks_in_group])\n",
    "\n",
    "  xa_idx = tf.tile(tf.range(num_chunks_in_group)[:, None], [1, num_chunks_in_group])\n",
    "  xb_idx = tf.tile(tf.range(num_chunks_in_group)[None], [num_chunks_in_group, 1])\n",
    "\n",
    "  # print(\"xa_idx: \", xa_idx)\n",
    "  # print(\"xb_idx: \", xb_idx)\n",
    "\n",
    "  # 1 if identical\n",
    "  is_identical = tf.cast(tf.equal(xa_idx, xb_idx), dtype=tf.int32)\n",
    "  # 2 if less\n",
    "  is_less = 2 * tf.cast(tf.less(xa_idx, xb_idx), dtype=tf.int32)\n",
    "  # 3 if greater\n",
    "  is_greater = 3 * tf.cast(tf.greater(xa_idx, xb_idx), dtype=tf.int32)\n",
    "\n",
    "\n",
    "  # print(\"is_identical: \", is_identical)\n",
    "  # print(\"is_less: \", is_less)\n",
    "  # print(\"is_greater: \", is_greater)\n",
    "\n",
    "  # print(\"sum: \", is_identical + is_less + is_greater)\n",
    "\n",
    "  video_src_ids = tf.reshape(video_src_ids, [batch_size, num_chunks_in_group])\n",
    "\n",
    "  print(\"video src ids: \", video_src_ids.shape)\n",
    "  print(video_src_ids[:, None].shape)\n",
    "  print(video_src_ids[:, :, None].shape)\n",
    "\n",
    "  is_same_video = tf.equal(video_src_ids[:, None], video_src_ids[:, :, None])\n",
    "\n",
    "  print(\"is_same_video: \", is_same_video.shape)\n",
    "\n",
    "  # # 0 if not the same video\n",
    "  labels = tf.compat.v1.where_v2(\n",
    "    is_same_video,\n",
    "    is_identical + is_less + is_greater,\n",
    "    tf.zeros_like(is_identical),\n",
    "  )\n",
    "  labels = tf.reshape(labels, [(batch_size*num_chunks_in_group ** 2)])\n",
    "  # print(\"labels: \", labels)\n",
    "\n",
    "\n",
    "  num_classes = logits.shape[-1]\n",
    "\n",
    "  one_hot_labels = tf.one_hot(labels, depth=num_classes, dtype=logits.dtype)\n",
    "  print(\"one-hot labels: \", one_hot_labels.shape)\n",
    "\n",
    "  cls_logprobs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "  raw_loss = -tf.reduce_sum(cls_logprobs * one_hot_labels, axis=-1) *label_w\n",
    "  temporal_loss = tf.reduce_mean(raw_loss)\n",
    "  print(\"temporal loss: \", temporal_loss)\n",
    "\n",
    "  return temporal_loss\n",
    "\n",
    "calculate_temporal_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num shuffle per group:  [0.6, 1e-06, 0.13333333333333333, 0.13333333333333333, 0.13333333333333333]\n",
      "ev:  1.2000009999999999\n",
      "nspg log prob:  tf.Tensor([[ -0.5108256 -13.815511   -2.014903   -2.014903   -2.014903 ]], shape=(1, 5), dtype=float32)\n",
      "num_shuffle_img:  tf.Tensor([0 0 0 ... 0 3 0], shape=(4096,), dtype=int32)\n",
      "do_shuffle_img:  tf.Tensor(\n",
      "[[False False False False]\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " ...\n",
      " [False False False False]\n",
      " [False  True  True  True]\n",
      " [False False False False]], shape=(4096, 4), dtype=bool)\n",
      "shuffled_idx_img:  tf.Tensor(\n",
      "[[ 0  1  2  3]\n",
      " [ 0  1  2  3]\n",
      " [ 0  1  2  3]\n",
      " ...\n",
      " [ 0  1  2  3]\n",
      " [ 0 19 18 17]\n",
      " [ 0  1  2  3]], shape=(4096, 4), dtype=int32)\n",
      "features_shuffle_idx_img:  (16384,)\n",
      "tf.Tensor([0 1 2 ... 1 2 3], shape=(16384,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "shuffle_prob = 0.4\n",
    "shuffle_offset = 16\n",
    "num_chunks_in_group = 4\n",
    "batch_size = 1024\n",
    "num_chunks = 16\n",
    "B = batch_size * (num_chunks // num_chunks_in_group)\n",
    "\n",
    "def generate_shuffled_idx_img(B, shuffle_prob=0.4, shuffle_offset=16, num_chunks_in_group=4):\n",
    "    # first index in num shuffle per group corresponds to 60% of the time, no shuffling done\n",
    "    num_shuffle_per_group_probs = [1.0 - shuffle_prob, 1e-6] + [\n",
    "        shuffle_prob / (num_chunks_in_group - 1) for i in range(num_chunks_in_group - 1)] \n",
    "\n",
    "    print(\"num shuffle per group: \", num_shuffle_per_group_probs)\n",
    "\n",
    "    ev = sum([i * p for i, p in enumerate(num_shuffle_per_group_probs)])\n",
    "\n",
    "    print(\"ev: \", ev)\n",
    "    # tf.logging.info(\n",
    "    #     \"probs: {}\\nExpected # of {}s out of place: {:.3f}\".format(num_shuffle_per_group_probs, k, ev))\n",
    "    nspg_logprob = tf.math.log(num_shuffle_per_group_probs)[None]\n",
    "\n",
    "    print(\"nspg log prob: \", nspg_logprob)\n",
    "\n",
    "    # scramble i frames at random\n",
    "    num_shuffle_img = tf.squeeze(tf.random.categorical(nspg_logprob, dtype=tf.int32, num_samples=B), 0)\n",
    "    print(\"num_shuffle_img: \", num_shuffle_img)\n",
    "\n",
    "    # boolean tensor of shape [B, num_chunks_in_group] where each elt in B is of the form [True, False, True, False]\n",
    "    # describing the frames that should be scrambled\n",
    "    do_shuffle_img = tf.less(tf.argsort(tf.compat.v1.random_uniform([B, num_chunks_in_group]), 1),\n",
    "                            num_shuffle_img[:, None])\n",
    "\n",
    "    print(\"do_shuffle_img: \", do_shuffle_img)\n",
    "\n",
    "    shuffled_idx_img = tf.where(\n",
    "        do_shuffle_img,\n",
    "        shuffle_offset + tf.argsort(tf.compat.v1.random_uniform([B, num_chunks_in_group]), 1),\n",
    "        tf.tile(tf.range(num_chunks_in_group)[None], [B, 1]),\n",
    "    )\n",
    "\n",
    "    print(\"shuffled_idx_img: \", shuffled_idx_img)\n",
    "\n",
    "    features_shuffled_idx_img = tf.reshape(shuffled_idx_img, [-1])\n",
    "    print(\"features_shuffle_idx_img: \", features_shuffled_idx_img.shape)\n",
    "    print(features_shuffled_idx_img)\n",
    "\n",
    "    return features_shuffled_idx_img\n",
    "\n",
    "features_shuffled_idx_img= generate_shuffled_idx_img(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-04 18:18:30.168002: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 12935233536 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
    "    Args:\n",
    "      tensor: A tf.Tensor object to find the shape of.\n",
    "      expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
    "        specified and the `tensor` has a different rank, and exception will be\n",
    "        thrown.\n",
    "      name: Optional name of the tensor for the error message.\n",
    "    Returns:\n",
    "      A list of dimensions of the shape of tensor. All static dimensions will\n",
    "      be returned as python integers, and dynamic dimensions will be returned\n",
    "      as tf.Tensor scalars.\n",
    "    \"\"\"\n",
    "    if name is None and not tf.executing_eagerly():\n",
    "        name = tensor.name\n",
    "\n",
    "    # if expected_rank is not None:\n",
    "    #     assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape\n",
    "\n",
    "    \n",
    "def one_hot_gather(x, idx):\n",
    "    \"\"\"\n",
    "    Does a one-hot gather on a single axis, 0\n",
    "    :param x: [N, H] tensor with a float dtype\n",
    "    :param idx: 1 dimensional int32 with indices 0...N\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N, H = get_shape_list(x, 2)\n",
    "    get_shape_list(idx, 1)\n",
    "    idx_oh = tf.one_hot(idx, depth=N, dtype=tf.bfloat16 if x.dtype == tf.bfloat16 else tf.float32)\n",
    "    return tf.matmul(idx_oh, x)\n",
    "\n",
    "def position_embedder2d(num_h, num_w, embedding_size, name='pos_embs', num_img=1, max_position_embeddings=64,\n",
    "                        max_nimg=4, num_cls_emb=1, initializer_range=0.02):\n",
    "    \"\"\"\n",
    "    This is the same as a 2D pos emb BUT easier to change?\n",
    "    :param num_h:\n",
    "    :param num_w:\n",
    "    :param embedding_size:\n",
    "    :param name:\n",
    "    :param num_img:\n",
    "    :param max_position_embeddings:\n",
    "    :param max_nimg:\n",
    "    :param initializer_range:\n",
    "    :return: [num_img * (1 + num_h * num_w), embedding_size] pos emb\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        pos_embs_3d = tf.compat.v1.get_variable(\n",
    "            name='pos_embs',\n",
    "            shape=[max_nimg, max_position_embeddings, max_position_embeddings, embedding_size],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=initializer_range),\n",
    "        )\n",
    "        cls_embs = tf.compat.v1.get_variable(\n",
    "            name='cls_emb',\n",
    "            shape=[max_nimg, num_cls_emb, embedding_size],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=initializer_range),\n",
    "        ) if num_cls_emb > 0 else None\n",
    "\n",
    "        full_pe = tf.reshape(pos_embs_3d[:num_img, :num_h, :num_w], [num_img, num_h * num_w, embedding_size])\n",
    "        if num_cls_emb > 0:\n",
    "            full_pe = tf.concat([cls_embs[:num_img], full_pe], 1)\n",
    "        return tf.reshape(full_pe, [num_img * (num_cls_emb + num_h * num_w), embedding_size])\n",
    "\n",
    "\n",
    "def position_embedder(seq_length, name, max_position_embeddings, embedding_size, offset=0,\n",
    "                      initializer_range=0.02):\n",
    "    \"\"\"\n",
    "    :param seq_length: Length of the sequence to position embed. Must be less than max_position_embeddings.\n",
    "    :param name: Name of the embedding\n",
    "    :param max_position_embeddings: Highest it'll go\n",
    "    :param embedding_size: dimension to map to\n",
    "    :param offset: Currently this isn't supported but it's so you can deal with caching. In that case\n",
    "                   we don't want to run all the old sequences through the transformer\n",
    "    :param initializer_range: for truncated normal initializer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Do something special for position embeddings\n",
    "    assert_op = tf.compat.v1.assert_less_equal(seq_length, max_position_embeddings)\n",
    "    with tf.control_dependencies([assert_op]):\n",
    "        full_position_embeddings = tf.compat.v1.get_variable(\n",
    "            name=name,\n",
    "            shape=[max_position_embeddings, embedding_size],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=initializer_range),\n",
    "        )\n",
    "\n",
    "        # Since the position embedding table is a learned variable, we create it\n",
    "        # using a (long) sequence length `max_position_embeddings`. The actual\n",
    "        # sequence length might be shorter than this, for faster training of\n",
    "        # tasks that do not have long sequences.\n",
    "        #\n",
    "        # So `full_position_embeddings` is effectively an embedding table\n",
    "        # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
    "        # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
    "        # perform a slice.\n",
    "        if offset == 0:\n",
    "            position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])[None]\n",
    "        else:\n",
    "            # Tensorflow is too stupid to allow slicing\n",
    "            flat_pos_ids = (tf.range(seq_length, dtype=tf.int32) + offset)\n",
    "            one_hot_pos_ids = tf.one_hot(flat_pos_ids, depth=max_position_embeddings)\n",
    "\n",
    "            # [seq_length, full_position_embeddings], [full_position_embeddings, dim]\n",
    "            seq_embeds = tf.matmul(one_hot_pos_ids, full_position_embeddings)\n",
    "            position_embeddings = seq_embeds[None]\n",
    "\n",
    "    return position_embeddings, full_position_embeddings\n",
    "\n",
    "\n",
    "def vision_pos_emb(shuffled_idx_img=None, B=4096, num_chunks_in_group=4, hidden_size=768, num_h=16, num_w=16, viz_chunk_length=257, P=16, num_imgs=1, max_vision_pos_embeddings=32):\n",
    "    \"\"\"\n",
    "    Add only image level position embeddings\n",
    "    :param shuffled_idx_img: If not None, then we will do shuffle the input\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # num_chunks_in_group = 4\n",
    "    # hidden_size = 768\n",
    "    my_pe, img_pe_table = position_embedder(\n",
    "        num_chunks_in_group * num_imgs,\n",
    "        name='img_idx_pe',\n",
    "        max_position_embeddings=max_vision_pos_embeddings,\n",
    "        embedding_size=hidden_size,\n",
    "        initializer_range=0.02,\n",
    "    )\n",
    "    if shuffled_idx_img is None:\n",
    "        # tf.logging.info(\"NOT shuffling the vision input! this is probably what you want for downstream\")\n",
    "        my_pe = tf.tile(my_pe[:, :, None], [1, 1, viz_chunk_length, 1])\n",
    "        my_pe = tf.reshape(my_pe, [1, P * num_imgs, hidden_size])\n",
    "    else:\n",
    "        # tf.logging.info(\"!!!shuffling the vision input!!!!\")\n",
    "        # Idk how to handle these things\n",
    "        assert num_imgs == 1\n",
    "        # assert self.num_texts == 1\n",
    "        my_pe = one_hot_gather(img_pe_table, tf.reshape(shuffled_idx_img, [-1]))\n",
    "        my_pe = tf.tile(my_pe[:, None], [1, viz_chunk_length, 1])\n",
    "        my_pe = tf.reshape(my_pe, [B, P, hidden_size])\n",
    "\n",
    "    # add extra position embeddings, since even though the vision transformer had position\n",
    "    # embeddings we did an avgpool so they might have gotten washed out\n",
    "    image_pe2d = position_embedder2d(num_h=num_h,\n",
    "                                        num_w=num_w,\n",
    "                                        embedding_size=hidden_size,\n",
    "                                        num_img=1,\n",
    "                                        num_cls_emb=1,\n",
    "                                        max_nimg=1,\n",
    "                                        initializer_range=0.02,\n",
    "                                        name='final_pe',\n",
    "                                        )\n",
    "    my_pe += tf.tile(image_pe2d, [num_chunks_in_group * num_imgs, 1])[None]\n",
    "    return my_pe\n",
    "\n",
    "\n",
    "P = 16\n",
    "h0 = 256\n",
    "w0 = 256\n",
    "h1 = h0 // P\n",
    "w1 = w0 // P\n",
    "vision_pos_emb(shuffled_idx_img=features_shuffled_idx_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08d608f700504c6b03e88c868d0cc9b143978899209b13a888b26c423352d24d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
