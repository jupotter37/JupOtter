{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "torch==2.0.1\n",
        "torchvision==0.15.2\n",
        "transformers==4.35.0\n",
        "tokenizers>=0.14,<0.15\n",
        "sentencepiece==0.1.99\n",
        "shortuuid\n",
        "accelerate==0.21.0\n",
        "peft==0.4.0\n",
        "bitsandbytes==0.41.0\n",
        "pydantic<2,>=1\n",
        "markdown2[all]\n",
        "numpy\n",
        "scikit-learn==1.2.2\n",
        "gradio==3.35.2\n",
        "gradio_client==0.2.9\n",
        "requests\n",
        "httpx==0.24.0\n",
        "uvicorn\n",
        "fastapi\n",
        "einops==0.6.1\n",
        "einops-exts==0.0.4\n",
        "timm==0.6.13\n",
        "ipywidgets\n",
        "diffusers\n",
        "ipykernel\n",
        "protobuf==3.20.1\n",
        "```"
      ],
      "metadata": {
        "id": "vPAb2LGGF4Ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. Load up LLM model in 4-bit mode"
      ],
      "metadata": {
        "id": "6TAGROZ9F9-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTClzbRyFwrY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer, BitsAndBytesConfig\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "model_name = \"/path/to/ai_models/zephyr-7b-beta\"\n",
        "\n",
        "m = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name\n",
        "    , trust_remote_code = True\n",
        "    , quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit                = True,\n",
        "        bnb_4bit_compute_dtype      = torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant   = True,\n",
        "        bnb_4bit_quant_type         = 'nf4'\n",
        "    )\n",
        "    , torch_dtype   = torch.bfloat16\n",
        "    , device_map    = \"auto\"#{\"\": 0}\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_name\n",
        ")\n",
        "tokenizer.bos_token_id = 1\n",
        "print(f\"Successfully loaded the model {model_name} into memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Initialise parameters and functions"
      ],
      "metadata": {
        "id": "ugxS44ANGB0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_id in stop_token_ids:\n",
        "            if input_ids[0][-1] == stop_id:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "\n",
        "import transformers\n",
        "model = m\n",
        "device = \"cuda:0\"\n",
        "pipe = transformers.pipeline(\n",
        "    model               = model,\n",
        "    tokenizer           = tokenizer,\n",
        "    return_full_text    = True,  # langchain expects the full text\n",
        "    task                = 'text-generation',\n",
        "    #device=device,\n",
        "    device_map          = \"auto\",\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria   = stopping_criteria,    # without this model will ramble\n",
        "    temperature         = 0.15,                 # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    top_p               = 0.15,                 # select from top tokens whose probability add up to 15%\n",
        "    top_k               = 0,                    # select from top 0 tokens (because zero, relies on top_p)\n",
        "    max_new_tokens      = 768*4,                # max number of tokens to generate in the output\n",
        "    repetition_penalty  = 1.1                   # without this output begins repeating\n",
        ")\n",
        "\n",
        "def gen_text(system_prompt:str = None ,input_text:str = \"hello\"):\n",
        "    if system_prompt is None:\n",
        "        system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\":system_prompt\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": input_text},\n",
        "    ]\n",
        "    prompt = pipe.tokenizer.apply_chat_template(\n",
        "        messages\n",
        "        , tokenize              = False\n",
        "        , add_generation_prompt = True\n",
        "    )\n",
        "    outputs = pipe(\n",
        "        prompt\n",
        "        , max_new_tokens=1024\n",
        "        , do_sample=True\n",
        "        , temperature=0.2\n",
        "        , top_k=50\n",
        "        , top_p=0.95\n",
        "    )\n",
        "    return outputs[0][\"generated_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "UZgJS3oqGEmI",
        "outputId": "c5a35533-6e50-42ac-ee69-a51a866e9188"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4aeedcb0cb7d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_token_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# define custom stopping criteria object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStopOnTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStoppingCriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Run it up"
      ],
      "metadata": {
        "id": "pCk83wf7GUgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"You are smart, you can solve math arithmetic problems, you can doing reasoning and logical inference, the answer is critical to me\"\n",
        "input = '''\n",
        "Jane is faster than joe, Joe is faster than Sam. Is Sam faster than Jane?\n",
        "'''\n",
        "r = gen_text(system_prompt=system, input_text=input)\n",
        "r"
      ],
      "metadata": {
        "id": "LKR4L-QZGX-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "OxLwgPUFGI-T"
      }
    }
  ]
}