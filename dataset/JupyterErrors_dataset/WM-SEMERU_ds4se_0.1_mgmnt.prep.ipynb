{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Preprocessing\n",
    "\n",
    "> This module comprises preprocessing techniques applied to software artifacts (TODO:cite here the papers employed for this preprocessings):\n",
    ">\n",
    ">This is an adapted version of Daniel McCrystal Nov 2019\n",
    ">\n",
    ">This version also includes BPE preprocesing and NLTK. It's the main class to execute conventional pipelines. \n",
    "\n",
    ">Author: @danaderp March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install dit\n",
    "#! pip install nltk\n",
    "#! pip install tokenizers\n",
    "#! pip install tensorflow_datasets\n",
    "! pip install -U tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List, Set, Callable, Tuple, Dict, Optional\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "from string import punctuation\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import sentencepiece as spm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq cisco/CSB-CICDPipelineEdition-master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-1d55101459cd>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-1d55101459cd>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    'model_prefix':path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def libest_params():\n",
    "    return {\n",
    "        'system': 'libest',\n",
    "        #'path_zip': Path(\"cisco/sacp-python-common.zip\"),\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/libest_data',\n",
    "        'language': 'english',\n",
    "        'dataset' : path_data + ''\n",
    "        #'model_prefix': path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k' #For BPE Analysis\n",
    "        #'model_prefix': path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k'\n",
    "        'model_prefix':path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = {\n",
    "    'bpe8k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k',\n",
    "    'bpe32k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k',\n",
    "    'bpe128k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = default_params()\n",
    "params = libest_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConventionalPreprocessing():\n",
    "    '''NLTK libraries for Conventional Preprocessing'''\n",
    "    def __init__(self, params, bpe = False):\n",
    "        self.params = params\n",
    "        \n",
    "        #If BPE provided, then preprocessing with BPE is allowed on CONV\n",
    "        if bpe:\n",
    "            self.sp_bpe = spm.SentencePieceProcessor()\n",
    "            self.sp_bpe.load(params['model_prefix']+'.model')\n",
    "        else:\n",
    "            self.sp_bpe = None\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def bpe_pieces_pipeline(self, doc_list):\n",
    "        '''Computes BPE preprocessing according to params'''\n",
    "        encoded_str = ''\n",
    "        if self.sp_bpe is None:\n",
    "            logging.info('Provide a BPE Model!')\n",
    "        else:\n",
    "            encoded_str = [self.sp_bpe.encode_as_pieces(doc) for doc in doc_list]  \n",
    "        return encoded_str\n",
    "    \n",
    "    #ToDo Transforme it into a For-Comprenhension\n",
    "    def clean_punctuation(self, token): \n",
    "        #remove terms !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789\n",
    "        return re.sub(r'[^a-zA-Z\\s]', ' ', token, re.I|re.A)\n",
    "\n",
    "    def split_camel_case_token(self, token):\n",
    "        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token)\n",
    "\n",
    "    def remove_terms(self, filtered_tokens):\n",
    "        remove_terms = punctuation + '0123456789'\n",
    "        return [token for token in filtered_tokens if token not in remove_terms and len(token)>2 and len(token)<21]\n",
    "\n",
    "    def stemmer(self, filtered_tokens):\n",
    "        return [englishStemmer.stem(token) for token in filtered_tokens ]\n",
    "\n",
    "    def stop_words(self, filtered_tokens):\n",
    "        stop_words = nltk.corpus.stopwords.words(self.params['language'])\n",
    "        return [token for token in filtered_tokens if token not in stop_words]\n",
    "    \n",
    "    def basic_pipeline(self, dict_filenames):\n",
    "        '''@dict_filenames: {filename: code}'''\n",
    "        pre_process = [( key.replace('.txt', '-pre.txt') , self.clean_punctuation(dict_filenames[key][0])  ) for key in dict_filenames]\n",
    "        pre_process = [( doc[0] , self.split_camel_case_token(doc[1])  ) for doc in pre_process]\n",
    "        pre_process = [( doc[0] , doc[1].lower()  ) for doc in pre_process]\n",
    "        pre_process = [( doc[0] , doc[1].strip()) for doc in pre_process] # Leading whitepsace are removed\n",
    "        pre_process_tokens = [(doc[0] , nltk.WordPunctTokenizer().tokenize(doc[1])) for doc in pre_process]\n",
    "        filtered_tokens = [(doc[0], self.stop_words(doc[1]) ) for doc in pre_process_tokens] #Stop Words\n",
    "        filtered_tokens = [(doc[0], self.stemmer(doc[1]) ) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        filtered_tokens = [(doc[0], self.remove_terms(doc[1])) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        pre_process = [(doc[0], ' '.join(doc[1])) for doc in filtered_tokens]\n",
    "        return pre_process\n",
    "    \n",
    "    def fromdocs_pipeline(self, docs):\n",
    "        #TODO\n",
    "        \"\"\"@tokenized_file: a list of tokens that represents a document/code\"\"\"\n",
    "        pre_process = [ self.clean_punctuation(doc) for doc in docs]\n",
    "        logging.info('fromtokens_pipeline: clean punctuation')\n",
    "        pre_process = [ self.split_camel_case_token(doc) for doc in pre_process]\n",
    "        logging.info('fromtokens_pipeline: camel case')\n",
    "        pre_process = [ doc.lower() for doc in pre_process] \n",
    "        logging.info('fromtokens_pipeline: lowe case')\n",
    "        pre_process = [ doc.strip() for doc in pre_process] # Leading whitepsace are removed\n",
    "        logging.info('fromtokens_pipeline: white space removed')\n",
    "        pre_process_tokens = [ nltk.WordPunctTokenizer().tokenize(doc) for doc in pre_process]\n",
    "        logging.info('fromtokens_pipeline: WordPunctTokenizer')\n",
    "        filtered_tokens = [ self.stop_words(doc) for doc in pre_process_tokens] #Stop Words\n",
    "        logging.info('fromtokens_pipeline: Stop words')\n",
    "        filtered_tokens = [ self.stemmer(doc) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        logging.info('fromtokens_pipeline: Stemmings')\n",
    "        filtered_tokens = [ self.remove_terms(doc) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        logging.info('fromtokens_pipeline: Removed Special Terns')\n",
    "        pre_process = [ ' '.join(doc) for doc in filtered_tokens]\n",
    "        logging.info('fromtokens_pipeline END')\n",
    "        return pre_process\n",
    "    \n",
    "    def frombatch_pipeline(self, batch):\n",
    "        #TODO\n",
    "        \"\"\"@batch: a TensorFlow Dataset Batch\"\"\"\n",
    "        pre_process = [ self.clean_punctuation( doc.decode(\"utf-8\") ) for doc in batch]\n",
    "        logging.info('frombatch_pipeline: clean punctuation')\n",
    "        pre_process = [ self.split_camel_case_token(doc) for doc in pre_process]\n",
    "        logging.info('frombatch_pipeline: camel case')\n",
    "        pre_process = [ doc.lower() for doc in pre_process] \n",
    "        logging.info('frombatch_pipeline: lowe case')\n",
    "        pre_process = [ doc.strip() for doc in pre_process] # Leading whitepsace are removed\n",
    "        logging.info('frombatch_pipeline: white space removed')\n",
    "        pre_process_tokens = [ nltk.WordPunctTokenizer().tokenize(doc) for doc in pre_process]\n",
    "        logging.info('frombatch_pipeline: WordPunctTokenizer')\n",
    "        filtered_tokens = [ self.stop_words(doc) for doc in pre_process_tokens] #Stop Words\n",
    "        logging.info('frombatch_pipeline: Stop words')\n",
    "        filtered_tokens = [ self.stemmer(doc) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        logging.info('frombatch_pipeline: Stemmings')\n",
    "        filtered_tokens = [ self.remove_terms(doc) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        logging.info('frombatch_pipeline: Removed Special Terns')\n",
    "        #pre_process = [ ' '.join(doc) for doc in filtered_tokens]\n",
    "        logging.info('frombatch_pipeline [END]')\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def fromtensor_pipeline(self, ts_x):\n",
    "        \"\"\"@ts_x: es un elemento del tensor\"\"\"\n",
    "        #TODO\n",
    "        pre_process = self.clean_punctuation(ts_x)\n",
    "        pre_process = self.split_camel_case_token(pre_process)\n",
    "        pre_process = pre_process.lower()\n",
    "        pre_process = pre_process.strip()\n",
    "        pre_process = nltk.WordPunctTokenizer().tokenize(pre_process)\n",
    "        filtered_tokens = self.stop_words(pre_process)\n",
    "        filtered_tokens = self.stemmer(filtered_tokens)\n",
    "        filtered_tokens = self.remove_terms(filtered_tokens)\n",
    "        pre_process = ' '.join(filtered_tokens)\n",
    "        logging.info('fromtokens_pipeline END')\n",
    "        return pre_process\n",
    "    \n",
    "    def SaveCorpus(self, df, language='js', sep=',', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "\n",
    "        df.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)     \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def LoadCorpus(self, timestamp, language='js', sep=',', mode='a'):\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "        return pd.read_csv(path_to_link, header=0, index_col=0, sep=sep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def open_file(f, encoding='utf-8'):\n",
    "    try:\n",
    "        #return open(filename, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        return open(f, 'r', encoding = encoding).read()\n",
    "    except:\n",
    "        print(\"Exception: \", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_files(system, ends):\n",
    "    path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "    names = [entry for entry in path.glob('**/*' +ends)]\n",
    "    filenames = [(filename, os.path.basename(filename), open_file(filename) ) for filename in names]\n",
    "    return pd.DataFrame( filenames ,columns = ['names','filenames','content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Utils \n",
    "> From @Nathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f,\n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#This if for SearchNet Dataset\n",
    "def get_dfs(path):\n",
    "    \"\"\"\n",
    "        Grabs the different data splits and converts them into dataframes.\n",
    "        Expects format from Code Search Net Challenge.\n",
    "        SearchNetDataset\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path/split).glob(\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\"])\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "    \n",
    "    with open(output/'text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(merged_df)))\n",
    "    return output/'text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_df(df, output, model_name, cols = None):\n",
    "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output / model_name} --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_glob(path, glob, model_name):\n",
    "    fns = list(path.glob(glob))\n",
    "    fns = \",\".join(map(str, fns))\n",
    "    sp.SentencePieceTrainer.train(f'--input={fns} --model_prefix={path / model_name} --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_hugface_model(df, output, tokenizer = ByteLevelBPETokenizer(), vocab_sz = 30_000, min_freq = 3, cols = None):\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    tokenizer.train(files = [str(fname)], vocab_size = vocab_sz, min_frequency = min_freq, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize_fns(fns, tokenizer, exts, output, data_type):\n",
    "    docs = []\n",
    "    for fn in fns:\n",
    "        system = fn.parent.name\n",
    "        output_path = output/system/data_type\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        files = []\n",
    "        for ext in exts:\n",
    "            files.extend(fn.glob(f'**/*.{ext}'))\n",
    "        for file in files:\n",
    "            if 'README' not in file.name:\n",
    "                with open(file, encoding='ISO-8859-1') as f:\n",
    "                    docs.append(tokenizer.EncodeAsPieces(f.read()))\n",
    "                with open((output_path/file.name).with_suffix('.bpe'), 'w') as f:\n",
    "                    f.write(' '.join(docs[-1]))\n",
    "            \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_bpe_files(path):\n",
    "    bpe_files = []\n",
    "    for file in path.glob('**/*.bpe'):\n",
    "        with open(file) as f:\n",
    "            bpe_files.append(f.read().split(' '))\n",
    "    \n",
    "    return bpe_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "#This implementation was oriented to traceability datasets\n",
    "def split_lines_to_files(lines, fn_pattern, output_path, tokenizer):\n",
    "    for line in lines:\n",
    "        fn, content = line.split(fn_pattern)\n",
    "        fn = fn.replace('\"', '')\n",
    "        fn = fn.replace(' Test ', '')\n",
    "        content = tokenizer.EncodeAsPieces(content)\n",
    "        with open((output_path/fn).with_suffix('.bpe'), 'w') as f:\n",
    "                    f.write(' '.join(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path_data = Path('../dvc-ds4se/') #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_params():\n",
    "    return {\n",
    "        'system': 'searchnet',\n",
    "        'path_data': path_data / 'code/searchnet/java/final/jsonl',\n",
    "        'path_test_out': path_data /'nbs_experiments'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path('/tf/data/')\n",
    "params = utils_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protected final void bindIndexed(Configuration...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public void setServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public void setServletNames(Collection&lt;String&gt;...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void addServletNames(String... servletN...</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  protected final void bindIndexed(Configuration...   \n",
       "1  public void setServletRegistrationBeans(\\n\\t\\t...   \n",
       "2  public void addServletRegistrationBeans(\\n\\t\\t...   \n",
       "3  public void setServletNames(Collection<String>...   \n",
       "4  public void addServletNames(String... servletN...   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn, df_val, df_tst = get_dfs( params['path_data'] )\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../dvc-ds4se/nbs_experiments/trn.csv')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['path_test_out'] / 'trn.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling some data\n",
    "df_trn.sample(frac = 0.01).to_json(params['path_test_out'] /'trn.jsonl', index = False)\n",
    "df_val.sample(frac = 0.01).to_csv(params['path_test_out'] /'val.csv', index = False)\n",
    "df_tst.sample(frac = 0.01).to_csv(params['path_test_out'] /'tst.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame index must be unique for orient='columns'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-91345d731e54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_trn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path_test_out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m'trn.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent)\u001b[0m\n\u001b[1;32m   2362\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m             \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m         )\n\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index, indent)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mdefault_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     ).write()\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, orient, date_format, double_precision, ensure_ascii, date_unit, index, default_handler, indent)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_format_axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             raise ValueError(\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0;34mf\"DataFrame index must be unique for orient='{self.orient}'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             )\n\u001b[1;32m    214\u001b[0m         if not self.obj.columns.is_unique and self.orient in (\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame index must be unique for orient='columns'."
     ]
    }
   ],
   "source": [
    "df_trn.sample(frac = 0.01).to_json(params['path_test_out'] /'trn.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>public static DiscountCurveInterface createDis...</td>\n",
       "      <td>Create a discount curve from forwards given by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public static MaskFormat maskFormat(final Stri...</td>\n",
       "      <td>&lt;p&gt;\\nReturns a {@link MaskFormat} instance for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void init(RecordOwnerParent parent, Rec...</td>\n",
       "      <td>Initialize the RecordOwner.\\n@param parentSess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public Content getLink(LinkInfo linkInfo) {\\n ...</td>\n",
       "      <td>Constructs a link from the given link informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public static String separatorsToSystem(String...</td>\n",
       "      <td>Converts all separators to the system separato...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  public static DiscountCurveInterface createDis...   \n",
       "1  public static MaskFormat maskFormat(final Stri...   \n",
       "2  public void init(RecordOwnerParent parent, Rec...   \n",
       "3  public Content getLink(LinkInfo linkInfo) {\\n ...   \n",
       "4  public static String separatorsToSystem(String...   \n",
       "\n",
       "                                           docstring  \n",
       "0  Create a discount curve from forwards given by...  \n",
       "1  <p>\\nReturns a {@link MaskFormat} instance for...  \n",
       "2  Initialize the RecordOwner.\\n@param parentSess...  \n",
       "3  Constructs a link from the given link informat...  \n",
       "4  Converts all separators to the system separato...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(params['path_test_out'] / 'trn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tst tokenizer hugface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gen_hugface_model(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'public', 'Ġstatic', 'Ġvoid', 'Ġmain', '(', 'String', '[]', 'Ġargs', ')', 'Ġ{', 'Ġget', 'Dir', 'From', 'Lib', '();', 'Ġ}', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"public static void main(String[] args) { getDirFromLib(); }\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'java_tokenizer' to PyBool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-6ebcddef9a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path_test_out'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"java_tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, pretty)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0mTokenizer\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'java_tokenizer' to PyBool"
     ]
    }
   ],
   "source": [
    "tokenizer.save(str( params['path_test_out'] ), \"java_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "        'first': ['1', '2', '6', '7', '8'],\n",
    "        'second': ['K', 'M', 'O', 'Q', 'S'],\n",
    "        'third': ['L', 'N', 'P', 'R', 'T']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>K</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Q</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first second third\n",
       "0     1      K     L\n",
       "1     2      M     N\n",
       "2     6      O     P\n",
       "3     7      Q     R\n",
       "4     8      S     T"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dummy_data); df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
