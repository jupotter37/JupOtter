{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import fastText\n",
    "\n",
    "from validation import compute_f1\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12152\n",
      "2867\n",
      "3005\n"
     ]
    }
   ],
   "source": [
    "# trainSentences = utils.get_sentences_germeval('../data/GermEVAL/NER-de-train.tsv')\n",
    "# devSentences = utils.get_sentences_germeval('../data/GermEVAL/NER-de-dev.tsv')\n",
    "# testSentences = utils.get_sentences_germeval('../data/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "trainSentences = utils.get_sentences_conll('../data/CONLL/deu/deu_utf.train.bio')\n",
    "devSentences = utils.get_sentences_conll('../data/CONLL/deu/deu_utf.testa.bio')\n",
    "testSentences = utils.get_sentences_conll('../data/CONLL/deu/deu_utf.testb.bio')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Die', 'O'], ['sechs', 'O'], ['Abgeordneten', 'O'], ['der', 'O'], ['Grünen', 'B-ORG'], [',', 'O'], ['die', 'O'], ['seit', 'O'], ['Bildung', 'O'], ['einer', 'O'], ['Großen', 'O'], ['Koalition', 'O'], ['von', 'O'], ['Christ-', 'O'], ['und', 'O'], ['Sozialdemokraten', 'O'], ['die', 'O'], ['Opposition', 'O'], ['darstellen', 'O'], [',', 'O'], ['hatten', 'O'], ['sich', 'O'], ['beim', 'O'], ['Vorsteher', 'O'], ['Rainer', 'B-PER'], ['Bergert', 'I-PER'], ['für', 'O'], ['ihr', 'O'], ['Fehlen', 'O'], ['entschuldigen', 'O'], ['lassen', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "labelSet = set()\n",
    "characters= set()\n",
    "models.max_sequence_length = 0\n",
    "n_gt_100 = 0\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            for char in word:\n",
    "                characters.add(char)\n",
    "            labelSet.add(label)\n",
    "        if len(sentence) > models.max_sequence_length:\n",
    "            models.max_sequence_length = len(sentence)\n",
    "        if len(sentence) > 100:\n",
    "            n_gt_100 += 1\n",
    "print(n_gt_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(labelSet))\n",
    "print(models.max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "models.label2Idx = {\"PADDING_TOKEN\":0}\n",
    "for label in labelSet:\n",
    "    models.label2Idx[label] = len(models.label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 1, 'I-PER': 5, 'B-MISC': 2, 'B-LOC': 8, 'B-ORG': 6, 'PADDING_TOKEN': 0, 'I-MISC': 7, 'O': 3, 'B-PER': 4, 'I-ORG': 9}\n"
     ]
    }
   ],
   "source": [
    "print(models.label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "models.case2Idx = {'PADDING_TOKEN':0, 'numeric': 1, 'allLower':2, 'allUpper':3, 'initialUpper':4, 'other':5, 'mainly_numeric':6, 'contains_digit': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allUpper': 3, 'mainly_numeric': 6, 'other': 5, 'PADDING_TOKEN': 0, 'contains_digit': 7, 'initialUpper': 4, 'numeric': 1, 'allLower': 2}\n"
     ]
    }
   ],
   "source": [
    "print(models.case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ereignis', 'O'], ['und', 'O'], ['Erzählung', 'O'], ['oder', 'O'], [':', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ü': 52, 'F': 40, 'C': 53, '§': 54, 'E': 85, 'g': 6, \"'\": 22, 'w': 7, 'M': 56, ',': 57, '*': 9, '?': 43, 'T': 10, 'Z': 59, 'Y': 62, 'G': 11, 'z': 12, '+': 63, '<W>': 3, 'U': 13, 'p': 14, '6': 15, 'e': 16, '\"': 17, 'I': 55, 'Ä': 64, '1': 46, 'i': 18, 'b': 65, ';': 19, 'O': 66, '2': 67, 'ô': 28, '9': 20, 'í': 21, 'K': 68, '-': 23, '4': 8, 'S': 25, 'u': 26, 'è': 27, 'A': 70, '&': 29, 'ê': 90, '(': 73, '8': 72, 'ó': 31, 'ñ': 74, 'ò': 75, 'Ö': 60, 'k': 71, 'UNKNOWN': 103, 'c': 76, 'ì': 30, 'P': 32, 'á': 77, 'B': 78, 'j': 34, '%': 80, '7': 24, 'ä': 101, '</S>': 2, 'y': 79, 'h': 35, 'PADDING_TOKEN': 0, 'ç': 81, 'q': 82, 'd': 83, '<S>': 1, 'é': 84, '.': 58, ')': 69, '/': 61, 'n': 86, '0': 87, 'o': 36, 'H': 88, 'Q': 89, 'v': 37, 'l': 91, 'R': 100, 'ü': 92, 'ß': 38, 's': 39, 'J': 41, '=': 42, 't': 93, '!': 94, ':': 95, '</W>': 4, 'x': 96, 'V': 33, 'ö': 97, 'D': 44, 'f': 98, 'm': 45, '3': 99, 'L': 5, 'W': 47, 'N': 50, 'X': 102, '5': 48, 'a': 49, 'r': 51}\n"
     ]
    }
   ],
   "source": [
    "models.char2Idx={\"PADDING_TOKEN\":0, \"<S>\":1, \"</S>\":2, \"<W>\":3, \"</W>\":4}\n",
    "for char in characters:\n",
    "    models.char2Idx[char] = len(models.char2Idx)\n",
    "models.char2Idx['UNKNOWN'] = len(models.char2Idx)\n",
    "print(models.char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ereignis', 'O'], ['und', 'O'], ['Erzählung', 'O'], ['oder', 'O'], [':', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.ft = fastText.load_model(\"../embeddings/wiki.de.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(models.nb_embedding_dims)\n",
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "models.idx2Label = {v: k for k, v in models.label2Idx.items()}\n",
    "print(len(models.label2Idx))\n",
    "print(len(models.idx2Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model_filename = 'model_lstm2_conll.h5'\n",
    "# checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "history = utils.F1History(tmp_model_filename, devSet = devSentences)\n",
    "model = models.get_model_2lstm_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    utils.NerSequence(trainSentences[:1000], shuffle_data=True, batch_size=256), \n",
    "    validation_data = utils.NerSequence(devSentences[:1000], batch_size=256), \n",
    "    epochs = 3, callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.acc)\n",
    "print(history.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    utils.NerSequence(trainSentences, shuffle_data=True, batch_size=2048), \n",
    "    validation_data = utils.NerSequence(devSentences, batch_size=256), \n",
    "    epochs = 5, callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = utils.predict_sequences(model, testSentences[:1000])\n",
    "print(compute_f1(pred_labels, true_labels, models.idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 3328        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, None, 52, 100 33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, None, 100)    60400       time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_39 (Bidirectional (None, None, 400)    974400      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, None, 10)     4010        bidirectional_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, None, 10)     230         time_distributed_39[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,075,632\n",
      "Trainable params: 1,075,568\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9890\n",
      "New maximum F1 score: 0.7432939541348159 (before: 0) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 366s 962ms/step - loss: 0.0451 - acc: 0.9890 - val_loss: 0.0225 - val_acc: 0.9937\n",
      "Epoch 2/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9956\n",
      "New maximum F1 score: 0.8152319656744437 (before: 0.7432939541348159) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 336s 885ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0148 - val_acc: 0.9951\n",
      "Epoch 3/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9964\n",
      "New maximum F1 score: 0.8254357298474946 (before: 0.8152319656744437) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 341s 897ms/step - loss: 0.0083 - acc: 0.9964 - val_loss: 0.0107 - val_acc: 0.9954\n",
      "Epoch 4/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9969\n",
      "New maximum F1 score: 0.8309308904755489 (before: 0.8254357298474946) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 340s 894ms/step - loss: 0.0034 - acc: 0.9969 - val_loss: 0.0069 - val_acc: 0.9956\n",
      "Epoch 5/10\n",
      "380/380 [==============================] - 342s 900ms/step - loss: -0.0012 - acc: 0.9973 - val_loss: 0.0022 - val_acc: 0.9956\n",
      "Epoch 6/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: -0.0063 - acc: 0.9976\n",
      "New maximum F1 score: 0.8365079365079364 (before: 0.8309308904755489) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 341s 897ms/step - loss: -0.0063 - acc: 0.9975 - val_loss: -0.0024 - val_acc: 0.9957\n",
      "Epoch 7/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: -0.0118 - acc: 0.9979\n",
      "New maximum F1 score: 0.8435104414485858 (before: 0.8365079365079364) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 354s 931ms/step - loss: -0.0119 - acc: 0.9979 - val_loss: -0.0078 - val_acc: 0.9958\n",
      "Epoch 8/10\n",
      "380/380 [==============================] - 343s 903ms/step - loss: -0.0178 - acc: 0.9980 - val_loss: -0.0139 - val_acc: 0.9958\n",
      "Epoch 9/10\n",
      "379/380 [============================>.] - ETA: 0s - loss: -0.0241 - acc: 0.9982\n",
      "New maximum F1 score: 0.8437293946986681 (before: 0.8435104414485858) Saving to model_lstm2_conll.9.h5\n",
      "380/380 [==============================] - 350s 921ms/step - loss: -0.0241 - acc: 0.9982 - val_loss: -0.0195 - val_acc: 0.9958\n",
      "Epoch 10/10\n",
      "380/380 [==============================] - 352s 927ms/step - loss: -0.0308 - acc: 0.9984 - val_loss: -0.0251 - val_acc: 0.9958\n",
      "Epoch 1/10\n",
      "14/64 [=====>........................] - ETA: 1:39 - loss: -0.0278 - acc: 0.9985"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[52,19200,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2 = ReverseV2[T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_12/Nadam/gradients/time_distributed_38/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3, loss_12/crf_loss/Const_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2', defined at:\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-eb536cb0c91f>\", line 13, in <module>\n    epochs = 10, callbacks = [history]\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\", line 2026, in fit_generator\n    self._make_train_function()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\", line 970, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/optimizers.py\", line 599, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2512, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 385, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py\", line 639, in _ReverseV2Grad\n    return array_ops.reverse_v2(grad, axis), None\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6566, in reverse_v2\n    \"ReverseV2\", tensor=tensor, axis=axis, name=name)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'time_distributed_38/ReverseV2', defined at:\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 22 identical lines from previous traceback]\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-eb536cb0c91f>\", line 9, in <module>\n    model = models.get_model_2lstm_v2()\n  File \"/home/gwiedemann/microNER/scripts/models.py\", line 54, in get_model_2lstm_v2\n    char_lstm = TimeDistributed(Bidirectional(LSTM(50, name = 'char_lstm2')))(char_lstm)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/wrappers.py\", line 203, in call\n    y = self.layer.call(inputs, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/wrappers.py\", line 316, in call\n    y_rev = self.backward_layer.call(inputs, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/recurrent.py\", line 2032, in call\n    initial_state=initial_state)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/recurrent.py\", line 595, in call\n    input_length=timesteps)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2674, in rnn\n    inputs = reverse(inputs, 0)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2292, in reverse\n    return tf.reverse(x, axes)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2688, in reverse\n    return gen_array_ops.reverse_v2(tensor, axis, name)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6566, in reverse_v2\n    \"ReverseV2\", tensor=tensor, axis=axis, name=name)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,19200,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2 = ReverseV2[T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_12/Nadam/gradients/time_distributed_38/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3, loss_12/crf_loss/Const_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[52,19200,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2 = ReverseV2[T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_12/Nadam/gradients/time_distributed_38/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3, loss_12/crf_loss/Const_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-eb536cb0c91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNerSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNerSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevSentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[52,19200,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2 = ReverseV2[T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_12/Nadam/gradients/time_distributed_38/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3, loss_12/crf_loss/Const_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2', defined at:\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-eb536cb0c91f>\", line 13, in <module>\n    epochs = 10, callbacks = [history]\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\", line 2026, in fit_generator\n    self._make_train_function()\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/training.py\", line 970, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/optimizers.py\", line 599, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/optimizers.py\", line 78, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2512, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 385, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 636, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py\", line 639, in _ReverseV2Grad\n    return array_ops.reverse_v2(grad, axis), None\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6566, in reverse_v2\n    \"ReverseV2\", tensor=tensor, axis=axis, name=name)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'time_distributed_38/ReverseV2', defined at:\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 22 identical lines from previous traceback]\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-eb536cb0c91f>\", line 9, in <module>\n    model = models.get_model_2lstm_v2()\n  File \"/home/gwiedemann/microNER/scripts/models.py\", line 54, in get_model_2lstm_v2\n    char_lstm = TimeDistributed(Bidirectional(LSTM(50, name = 'char_lstm2')))(char_lstm)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/wrappers.py\", line 203, in call\n    y = self.layer.call(inputs, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/wrappers.py\", line 316, in call\n    y_rev = self.backward_layer.call(inputs, **kwargs)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/recurrent.py\", line 2032, in call\n    initial_state=initial_state)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/layers/recurrent.py\", line 595, in call\n    input_length=timesteps)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2674, in rnn\n    inputs = reverse(inputs, 0)\n  File \"/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2292, in reverse\n    return tf.reverse(x, axes)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2688, in reverse\n    return gen_array_ops.reverse_v2(tensor, axis, name)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6566, in reverse_v2\n    \"ReverseV2\", tensor=tensor, axis=axis, name=name)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/gwiedemann/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[52,19200,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: training_12/Nadam/gradients/time_distributed_38/ReverseV2_grad/ReverseV2 = ReverseV2[T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_12/Nadam/gradients/time_distributed_38/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3, loss_12/crf_loss/Const_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "f = open('results_lstm2_conll.txt', 'a')\n",
    "for run_i in range(9,10):\n",
    "    print(\"Run \" + str(run_i))\n",
    "    \n",
    "    tmp_model_filename = 'model_lstm2_conll.' + str(run_i) + '.h5'\n",
    "\n",
    "    history = utils.F1History(tmp_model_filename, devSet=devSentences)\n",
    "\n",
    "    model = models.get_model_2lstm_v2()\n",
    "    model.fit_generator(\n",
    "        utils.NerSequence(trainSentences, shuffle_data=True, batch_size=32), \n",
    "        validation_data = utils.NerSequence(devSentences, batch_size=512), \n",
    "        epochs = 10, callbacks = [history]\n",
    "    )\n",
    "    \n",
    "    model.load_weights(tmp_model_filename)\n",
    "    \n",
    "    model.fit_generator(\n",
    "        utils.NerSequence(trainSentences, shuffle_data=True, batch_size=192), \n",
    "        validation_data = utils.NerSequence(devSentences, batch_size=512), \n",
    "        epochs = 10, callbacks = [history]\n",
    "    )\n",
    "    \n",
    "    model.load_weights(tmp_model_filename)\n",
    "    \n",
    "    true_labels, pred_labels = utils.predict_sequences(model, testSentences)\n",
    "    \n",
    "    pre, rec, f1 = compute_f1(pred_labels, true_labels, models.idx2Label)\n",
    "    f.write(str(run_i) + \"\\t\" + str(pre) + \"\\t\" + str(rec) +  \"\\t\" + str(f1))\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/76 [============================>.] - ETA: 1s - loss: -0.0288 - acc: 0.9987\n",
      "New maximum F1 score: 0.8511701288456482 (before: 0.8437293946986681) Saving to model_lstm2_conll.9.h5\n",
      "76/76 [==============================] - 138s 2s/step - loss: -0.0288 - acc: 0.9987 - val_loss: -0.0208 - val_acc: 0.9961\n",
      "Epoch 2/10\n",
      "76/76 [==============================] - 130s 2s/step - loss: -0.0305 - acc: 0.9988 - val_loss: -0.0221 - val_acc: 0.9960\n",
      "Epoch 3/10\n",
      "75/76 [============================>.] - ETA: 1s - loss: -0.0320 - acc: 0.9989\n",
      "New maximum F1 score: 0.8528252299605782 (before: 0.8511701288456482) Saving to model_lstm2_conll.9.h5\n",
      "76/76 [==============================] - 130s 2s/step - loss: -0.0321 - acc: 0.9989 - val_loss: -0.0233 - val_acc: 0.9961\n",
      "Epoch 4/10\n",
      "76/76 [==============================] - 129s 2s/step - loss: -0.0335 - acc: 0.9990 - val_loss: -0.0245 - val_acc: 0.9961\n",
      "Epoch 5/10\n",
      "76/76 [==============================] - 131s 2s/step - loss: -0.0351 - acc: 0.9990 - val_loss: -0.0258 - val_acc: 0.9960\n",
      "Epoch 6/10\n",
      "76/76 [==============================] - 130s 2s/step - loss: -0.0365 - acc: 0.9990 - val_loss: -0.0271 - val_acc: 0.9961\n",
      "Epoch 7/10\n",
      "75/76 [============================>.] - ETA: 1s - loss: -0.0381 - acc: 0.9991\n",
      "New maximum F1 score: 0.8530644316396019 (before: 0.8528252299605782) Saving to model_lstm2_conll.9.h5\n",
      "76/76 [==============================] - 129s 2s/step - loss: -0.0381 - acc: 0.9991 - val_loss: -0.0284 - val_acc: 0.9961\n",
      "Epoch 8/10\n",
      "75/76 [============================>.] - ETA: 1s - loss: -0.0396 - acc: 0.9992\n",
      "New maximum F1 score: 0.8538289178858574 (before: 0.8530644316396019) Saving to model_lstm2_conll.9.h5\n",
      "76/76 [==============================] - 130s 2s/step - loss: -0.0396 - acc: 0.9992 - val_loss: -0.0292 - val_acc: 0.9961\n",
      "Epoch 9/10\n",
      "76/76 [==============================] - 130s 2s/step - loss: -0.0411 - acc: 0.9992 - val_loss: -0.0308 - val_acc: 0.9961\n",
      "Epoch 10/10\n",
      "75/76 [============================>.] - ETA: 1s - loss: -0.0426 - acc: 0.9992\n",
      "New maximum F1 score: 0.8566152222661918 (before: 0.8538289178858574) Saving to model_lstm2_conll.9.h5\n",
      "76/76 [==============================] - 129s 2s/step - loss: -0.0426 - acc: 0.9992 - val_loss: -0.0320 - val_acc: 0.9962\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(tmp_model_filename)\n",
    "\n",
    "model.fit_generator(\n",
    "    utils.NerSequence(trainSentences, shuffle_data=True, batch_size=160), \n",
    "    validation_data = utils.NerSequence(devSentences, batch_size=512), \n",
    "    epochs = 10, callbacks = [history]\n",
    ")\n",
    "\n",
    "model.load_weights(tmp_model_filename)\n",
    "\n",
    "true_labels, pred_labels = utils.predict_sequences(model, testSentences)\n",
    "\n",
    "pre, rec, f1 = compute_f1(pred_labels, true_labels, models.idx2Label)\n",
    "f.write(str(run_i) + \"\\t\" + str(pre) + \"\\t\" + str(rec) +  \"\\t\" + str(f1))\n",
    "f.write(\"\\n\")\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(run_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, json\n",
    "# copy file for best run\n",
    "shutil.copyfile('model_lstm_conll.9.h5', '../models/final_model_conll.h5')\n",
    "shutil.copyfile('model_lstm_conll.9.h5.indexes', '../models/final_model_conll.indexes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "def create_custom_objects():\n",
    "    instanceHolder = {\"instance\": None}\n",
    "    class ClassWrapper(CRF):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            instanceHolder[\"instance\"] = self\n",
    "            super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "    def loss(*args):\n",
    "        method = getattr(instanceHolder[\"instance\"], \"loss_function\")\n",
    "        return method(*args)\n",
    "    def accuracy(*args):\n",
    "        method = getattr(instanceHolder[\"instance\"], \"accuracy\")\n",
    "        return method(*args)\n",
    "    return {\"ClassWrapper\": ClassWrapper ,\"CRF\": ClassWrapper, \"loss\": loss, \"accuracy\":accuracy}\n",
    "\n",
    "finalmodel = load_model('../models/final_model_conll.h5', custom_objects=create_custom_objects())\n",
    "true_labels, pred_labels = utils.predict_sequences(finalmodel, testSentences)\n",
    "print(compute_f1(pred_labels, true_labels, models.idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('conll_output.tsv', 'w', encoding='UTF-8')\n",
    "for i_sent, sent in enumerate(testSentences):\n",
    "    for i_tok, tok in enumerate(sent):\n",
    "        if tok[0] == 'PADDING_TOKEN':\n",
    "            break\n",
    "        correctlabel = models.idx2Label[true_labels[i_sent][i_tok]]\n",
    "        guessedlabel = models.idx2Label[pred_labels[i_sent][i_tok]]\n",
    "        line = \" \".join([tok[0], correctlabel, guessedlabel])\n",
    "        f.write(line + '\\n')\n",
    "    f.write('\\n')\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
