{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!export LANGCHAIN_TRACING_V2=true\n",
    "!export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "!export LANGCHAIN_API_KEY=\"ls__991755ac40ff4af1b05a7ec2e89d978a\"\n",
    "!export LANGCHAIN_PROJECT=\"CSDC-LLM-Bot\"\n",
    "\n",
    "!echo $LANGCHAIN_TRACING_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langsmith.utils' has no attribute 'get_tracer_project'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/llm-bot/source/panel/benchmark.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsd-console-us-east-1/home/ubuntu/llm-bot/source/panel/benchmark.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msk-Onwcs62Seh7F1srW43B0E12c1e2847729bF24aE29aA422B9\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsd-console-us-east-1/home/ubuntu/llm-bot/source/panel/benchmark.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(openai_api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msk-mFY2sr69JlW83pBTMc2fT3BlbkFJ1gNIR8iiebrBlFIGoT5s\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsd-console-us-east-1/home/ubuntu/llm-bot/source/panel/benchmark.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m llm\u001b[39m.\u001b[39;49minvoke(\u001b[39m\"\u001b[39;49m\u001b[39mHello, world!\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py:142\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    139\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    141\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    143\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    144\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    145\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    146\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    147\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    148\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    149\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    150\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    151\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/langchain/chat_models/base.py:319\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_invocation_params(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    317\u001b[0m options \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop}\n\u001b[0;32m--> 319\u001b[0m callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39;49mconfigure(\n\u001b[1;32m    320\u001b[0m     callbacks,\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks,\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    323\u001b[0m     tags,\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtags,\n\u001b[1;32m    325\u001b[0m     metadata,\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata,\n\u001b[1;32m    327\u001b[0m )\n\u001b[1;32m    328\u001b[0m run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    329\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    330\u001b[0m     messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    334\u001b[0m )\n\u001b[1;32m    335\u001b[0m results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/langchain/callbacks/manager.py:1480\u001b[0m, in \u001b[0;36mCallbackManager.configure\u001b[0;34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1449\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfigure\u001b[39m(\n\u001b[1;32m   1451\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     local_metadata: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1459\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CallbackManager:\n\u001b[1;32m   1460\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Configure the callback manager.\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \n\u001b[1;32m   1462\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[39m        CallbackManager: The configured callback manager.\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m _configure(\n\u001b[1;32m   1481\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1482\u001b[0m         inheritable_callbacks,\n\u001b[1;32m   1483\u001b[0m         local_callbacks,\n\u001b[1;32m   1484\u001b[0m         verbose,\n\u001b[1;32m   1485\u001b[0m         inheritable_tags,\n\u001b[1;32m   1486\u001b[0m         local_tags,\n\u001b[1;32m   1487\u001b[0m         inheritable_metadata,\n\u001b[1;32m   1488\u001b[0m         local_metadata,\n\u001b[1;32m   1489\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/langchain/callbacks/manager.py:2025\u001b[0m, in \u001b[0;36m_configure\u001b[0;34m(callback_manager_cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   2023\u001b[0m tracer_v2 \u001b[39m=\u001b[39m tracing_v2_callback_var\u001b[39m.\u001b[39mget()\n\u001b[1;32m   2024\u001b[0m tracing_v2_enabled_ \u001b[39m=\u001b[39m _tracing_v2_is_enabled()\n\u001b[0;32m-> 2025\u001b[0m tracer_project \u001b[39m=\u001b[39m _get_tracer_project()\n\u001b[1;32m   2026\u001b[0m run_collector_ \u001b[39m=\u001b[39m run_collector_var\u001b[39m.\u001b[39mget()\n\u001b[1;32m   2027\u001b[0m debug \u001b[39m=\u001b[39m _get_debug()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/langchain/callbacks/manager.py:1941\u001b[0m, in \u001b[0;36m_get_tracer_project\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_tracer_project\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m   1927\u001b[0m     run_tree \u001b[39m=\u001b[39m get_run_tree_context()\n\u001b[1;32m   1928\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m   1929\u001b[0m         run_tree,\n\u001b[1;32m   1930\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msession_name\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1931\u001b[0m         \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m   1932\u001b[0m             \u001b[39m# Note, if people are trying to nest @traceable functions and the\u001b[39;00m\n\u001b[1;32m   1933\u001b[0m             \u001b[39m# tracing_v2_enabled context manager, this will likely mess up the\u001b[39;00m\n\u001b[1;32m   1934\u001b[0m             \u001b[39m# tree structure.\u001b[39;00m\n\u001b[1;32m   1935\u001b[0m             tracing_v2_callback_var\u001b[39m.\u001b[39mget(),\n\u001b[1;32m   1936\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1937\u001b[0m             \u001b[39m# Have to set this to a string even though it always will return\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m             \u001b[39m# a string because `get_tracer_project` technically can return\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m             \u001b[39m# None, but only when a specific argument is supplied.\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m             \u001b[39m# Therefore, this just tricks the mypy type checker\u001b[39;00m\n\u001b[0;32m-> 1941\u001b[0m             \u001b[39mstr\u001b[39m(ls_utils\u001b[39m.\u001b[39;49mget_tracer_project()),\n\u001b[1;32m   1942\u001b[0m         ),\n\u001b[1;32m   1943\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langsmith.utils' has no attribute 'get_tracer_project'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-Onwcs62Seh7F1srW43B0E12c1e2847729bF24aE29aA422B9\"\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"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\")\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_inputs = [\n",
    "  (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),\n",
    "  (\"What are reptiles known for?\", \"Having scales\"),\n",
    "  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Elementary Animal Questions\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",\n",
    ")\n",
    "for input_prompt, output_answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs={\"answer\": output_answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
