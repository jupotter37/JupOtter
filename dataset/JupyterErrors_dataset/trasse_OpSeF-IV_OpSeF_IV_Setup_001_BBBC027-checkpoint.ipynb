{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General description of OpSeF\n",
    "\n",
    "The analysis pipeline consists of four principle sets of functions to import and reshape the data, to pre-process it, to segment it and to analyze and classify results.\n",
    "\n",
    "<img src=\"./Figures_Demo/Fig_M1.jpg\" alt = \"Paper Fig M1\" style = \"width: 500px;\"/>\n",
    "\n",
    "OpSeF is designed such that parameter for pre-processing and selection of the ideal model for segmentation are one functional unit. \n",
    "\n",
    "We recommend a iterative optimization, that starts with a large number of model, and relatively few, but conceptually distinct preprocessing pipelines, to the lower then the number of model to be explored while fine-tuning the most pre-processing pipeline, e.g. by optimizing filter kernel or the way how histograms are equalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the parameter tuning process for the BBBC027 dataset\n",
    "\n",
    "The parameter tuning was performed as described below:\n",
    "\n",
    "Substacts of three z-planes were sum projected: Input 000, 001,006,007,008,009.\n",
    "Images were either not (000,001) subjected to filtering, or weak filtering (median 3: 006,007) \n",
    "or strong filtering (008,009) was applied. Input 001,007 and 009 were additionally subjected \n",
    "to histogram equalization.\n",
    "\n",
    "Substacts of three z-planes were maximum projected: Input 002,003,004,005.\n",
    "Images were subjected to weak filtering (median 3: 002,003) or strong filtering (004,005) was applied.\n",
    "Input 003,005 were additionally subjected to histogram equalization.\n",
    "\n",
    "Substacts of three z-planes were medium projected: Input 010,011\n",
    "Input 011 was additionally subjected to histogram equalization.\n",
    "\n",
    "All available model were tested, the Cellpose size range 0.15,0.25,0.4,0.6 were explored.\n",
    "\n",
    "### Run 1:\n",
    "\n",
    "The quality of the input is rather low, thus, many sementations are also for human experts hard to perform. Within this limitation the cellpose nuclei model with size = 0.4 & input 11 and the StarDist model with the same input produced without further optimzation resonable results. Only few larger complex areas are not segmented\n",
    "\n",
    "<img src=\"./Figures_Demo/Fig_R3_CD.jpg\" alt = \"Paper Fig R3 CD\" style = \"width: 600px;\"/>\n",
    "\n",
    "\n",
    "#### => check how well both model generalize with more (Run2 or all cells)\n",
    "\n",
    "### Run 2 & 3:\n",
    "\n",
    "Both model generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Core-Settings that shall not be changed\n",
    "Please use OpSef_Configure_XXX to change these global settings.\n",
    "Changes in this file only necessary to intergrate new model,\n",
    "or to change the aut-ogenerated folderstructure.\n",
    "Changes to the folderstructure might cause errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processing pipeline from ./my_runs/main_settings.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./my_runs/main_settings.pkl\"\n",
    "infile = open(file_path,'rb')\n",
    "parameter = pickle.load(infile)\n",
    "print(\"Loading processing pipeline from\",file_path)\n",
    "infile.close()\n",
    "model_dic,folder_structure = parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define General Parameter\n",
    "most parameter define the overall processing and likeley do not change between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables that determine the processing pipeline and (generally) do not change between runs\n",
    "\n",
    "pc = {}\n",
    "\n",
    "#################\n",
    "## Basic ########\n",
    "#################\n",
    "\n",
    "pc[\"sub_f\"] = folder_structure # these folder will be auto-generated\n",
    "pc[\"batch_size\"] = 2 # the number of images to be quantified must be a multiple of batch size (for segmentation)\n",
    "                     # extract the properties (below) from region_props\n",
    "pc[\"get_property\"] = [\"label\",\"area\",\"centroid\", \"eccentricity\", \n",
    "                      \"equivalent_diameter\",\"mean_intensity\",\"max_intensity\",\n",
    "                      \"min_intensity\",\"perimeter\"]\n",
    "pc[\"naming_scheme\"] = \"Simple\"        # or \"Simple\" Export_ZSplit\" to create substacks\n",
    "pc[\"toFiji\"] = False   # Shall images be prepared for import in Fiji \n",
    "\n",
    "###############################\n",
    "# Define use of second channel\n",
    "###############################\n",
    "\n",
    "pc[\"export_another_channel\"] = False  # export other channel (to create a mask or for quantification) ?\n",
    "if [\"export_another_channel\"]:\n",
    "    pc[\"create_filter_mask_from_channel\"] = False # use second channel to make a mask?\n",
    "    pc[\"Quantify_2ndCh\"] = False      # shall this channel be quantified?\n",
    "    if pc[\"Quantify_2ndCh\"]:\n",
    "        pc[\"merge_results\"] = True    # shall the results of the two intensity quantification be merged \n",
    "                                     # (needed for advanced plotting)\n",
    "        pc[\"plot_merged\"] = True     # plot head of dataframe in notebook ?\n",
    "\n",
    "################################\n",
    "# Define Analysis & Plotting ###\n",
    "################################\n",
    "\n",
    "pc[\"Export_to_CSV\"] = False # shall results be exported to CSV (usually only true for the final run)\n",
    "if pc[\"Export_to_CSV\"]:\n",
    "    pc[\"Intensity_Ch\"] = 999 # put 999 if data contains only one channel\n",
    "    pc[\"Plot_Results\"] = True # Do you want to plot results ?\n",
    "    pc[\"Plot_xy\"] = [[\"area\",\"mean_intensity\"],[\"area\",\"circularity\"]] # Define what you want to plot (x/y)\n",
    "    pc[\"plot_head_main\"] = True # plot head of dataframe in notebook ?\n",
    "    pc[\"Do_ClusterAnalysis\"] = False # shall cluster analysis be performed? \n",
    "    if pc[\"Do_ClusterAnalysis\"]: # Define (below) which values will be included in the TSNE:\n",
    "        pc[\"include_in_tsne\"] = [\"area\",\"eccentricity\",\"equivalent_diameter\",\n",
    "                                 \"mean_intensity\",\"max_intensity\",\"min_intensity\",\"perimeter\"]\n",
    "        pc[\"cluster_expected\"] = 4 # How many groups/classes do you expected?\n",
    "        pc[\"tSNE_learning_rate\"] = 100 # Define learning rate\n",
    "        pc[\"link_method\"] = \"ward\" # or \"average\", or \"complete\", or \"single\" details see below\n",
    "else:\n",
    "    pc[\"Plot_Results\"] = False\n",
    "    pc[\"Do_ClusterAnalysis\"] = False \n",
    "    \n",
    "pc[\"toFiji\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here input & basic processing that (generally) does not change between runs\n",
    "\n",
    "input_def = {}\n",
    "input_def[\"root\"] = \"/home/trasse/OpSefDemo/BBBC027_1024\" # define folder where images are located\n",
    "input_def[\"dataset\"] = \"BBBC027\" # give the dataset a common name\n",
    "input_def[\"mydtype\"] = np.uint8 # bit depth of input images\n",
    "\n",
    "input_def[\"input_type\"] = \".tif\" # or .tif\n",
    "if input_def[\"input_type\"] == \".tif\":\n",
    "    input_def[\"is3D\"] = False # is the data 3D or 2D ???\n",
    "elif input_def[\"input_type\"] == \".lif\":\n",
    "    input_def[\"rigth_size\"] = (2048,2048) \n",
    "    input_def[\"export_single_ch\"] = 99   # which channel to extract from the lif file (if only one)\n",
    "\n",
    "input_def[\"split_z\"] = True # chose here to split z-stack into multiple substacts to avoid that cells fuse after projection\n",
    "if input_def[\"split_z\"]:# choosen, define:\n",
    "    input_def[\"z_step\"] = 3 # size of substacks\n",
    "    if input_def[\"input_type\"] == \".lif\":\n",
    "        input_def[\"export_multiple_ch\"] = [0,1] # channels to be exported\n",
    "    \n",
    "\n",
    "#########################################################################\n",
    "## the following options are only implemented with Tiff files as input ##\n",
    "#########################################################################\n",
    "\n",
    "input_def[\"toTiles\"] = False\n",
    "if input_def[\"toTiles\"]:\n",
    "    input_def[\"patch_size\"] = (15,512,512)\n",
    "\n",
    "input_def[\"bin\"] = False \n",
    "if input_def[\"bin\"]:\n",
    "    input_def[\"bin_factor\"] = 2 # same for x/y\n",
    "\n",
    "# coming soon...\n",
    "input_def[\"n2v\"] = False\n",
    "input_def[\"CARE\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter for export (if needed)\n",
    "\n",
    "if pc[\"export_another_channel\"]:\n",
    "    input_def[\"post_export_single_ch\"] = 0                   # which channel to extract from the lif file\n",
    "    input_def[\"post_subset\"] = [\"09_Ch_000_CS985\"]          # analyse these intensity images\n",
    "    if pc[\"Quantify_2ndCh\"]:\n",
    "        pc[\"Intensity_2ndCh\"] = input_def[\"post_export_single_ch\"]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "# in this dictionary all settings for the model are stored\n",
    "initModelSettings = {}\n",
    "# Variables U-Net Cellprofiler\n",
    "initModelSettings[\"UNet_model_file_CP01\"] = \"./model_Unet/UNet_CP001.h5\"\n",
    "initModelSettings[\"UNetShape\"] = (1024,1024)\n",
    "initModelSettings[\"UNetSettings\"] = [{\"activation\": \"relu\", \"padding\": \"same\"},{ \"momentum\": 0.9}]\n",
    "# Variables StarDist\n",
    "initModelSettings[\"basedir_StarDist\"] = \"./model_stardist\"\n",
    "# Variables Cellpose\n",
    "initModelSettings[\"Cell_Channels\"] = [[0,0],[0,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Runs\n",
    "\n",
    "These parameter listed below are likely change between runs.\n",
    "\n",
    "Preprocessing is mainly based on scikit-image.\n",
    "\n",
    "Segmentation in cooperates the pre-trained U-Net implementation used in Cellprofiler 3.0, the StarDist 2D model and Cellpose.\n",
    "\n",
    "Importantly, OpSeF is designed such that parameter for pre-processing and selection of the ideal model for segmentation are one functional unit.\n",
    "\n",
    "<img src=\"./Figures_Demo/Fig_M4.jpg\" alt = \"Paper Fig M4\" style = \"width: 500px;\"/>\n",
    "\n",
    "The above show Figure illustrates this concept with a processing pipeline, in three different models are applied to four different pre-processing pipelines each. Next, the resulting images are classified into results that are largely correct or suffer from failure to detect objects, under- or over-segmentation. In the given example, pre-processing pipeline three and model two seem to give overall the best result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define variable that might change in each run\n",
    "\n",
    "run_def = {}\n",
    "run_def[\"display_base\"] = \"000\" # defines the image used as basis for the overlay. See documemtation for details.\n",
    "run_def[\"run_ID\"] = \"003\" #give each run a new ID (unless you want to overwrite the old data)\n",
    "run_def[\"clahe_prm\"] = [(18,18),3] # Parameter for CLAHE\n",
    "\n",
    "# Run 1\n",
    "input_def[\"subset\"] = [\"009_Train\"] # filter by name\n",
    "# Run 2\n",
    "input_def[\"subset\"] = [\"All\"] # filter by name\n",
    "\n",
    "#########################\n",
    "# Define preprocessing ##\n",
    "#########################\n",
    "\n",
    "\n",
    "# Run1\n",
    "run_def[\"pre_list\"] = [[\"None\",0,120,\"Sum\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"None\",0,120,\"Sum\",True,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Median\",3,70,\"Max\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Median\",3,70,\"Max\",True,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Mean\",7,70,\"Max\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Mean\",7,70,\"Max\",True,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Median\",3,120,\"Sum\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Median\",3,120,\"Sum\",True,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Mean\",7,120,\"Sum\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Mean\",7,120,\"Sum\",True,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"None\",0,70,\"Median\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"None\",0,70,\"Median\",True,run_def[\"clahe_prm\"],\"no\",False]]\n",
    "\n",
    "# Run2\n",
    "run_def[\"pre_list\"] = [[\"None\",0,70,\"Median\",True,run_def[\"clahe_prm\"],\"no\",False]]\n",
    "\n",
    "# For cellpose\n",
    "run_def[\"rescale_list\"] = [0.15,0.25,0.4,0.6] # run1\n",
    "run_def[\"rescale_list\"] = [0.4] # run2,3\n",
    "\n",
    "# Define model\n",
    "run_def[\"ModelType\"] = [\"CP_nuclei\",\"CP_cyto\",\"SD_2D_dsb2018\",\"UNet_CP001\"] # run1\n",
    "run_def[\"ModelType\"] = [\"CP_nuclei\",\"SD_2D_dsb2018\"] # run2,3\n",
    "\n",
    "############################################################\n",
    "# Define postprocessing & filtering                       #\n",
    "# keep only the objects within the defined ranges         #\n",
    "###########################################################\n",
    "\n",
    "# (same for all runs) \n",
    "run_def[\"filter_para\"] = {}\n",
    "run_def[\"filter_para\"][\"area\"] = [0,10000]\n",
    "run_def[\"filter_para\"][\"perimeter\"] = [0,99999999]\n",
    "run_def[\"filter_para\"][\"circularity\"] = [0,1] # (equivalent_diameter * math.pi) / perimeter\n",
    "run_def[\"filter_para\"][\"mean_intensity\"] = [0,65535]\n",
    "run_def[\"filter_para\"][\"sum_intensity\"] = [0,100000000000000]\n",
    "run_def[\"filter_para\"][\"eccentricity\"] = [0,10]\n",
    "\n",
    "############################################################################\n",
    "# settings that are only needed if condition below is met which means you  #\n",
    "# plan to use a mask from a second channel to filter results               #\n",
    "############################################################################\n",
    "\n",
    "if pc[\"create_filter_mask_from_channel\"]:\n",
    "    run_def[\"binary_filter_mp\"] = [[\"open\",5,2,1,\"Morphology\"],[\"close\",5,3,1,\"Morphology\"],[\"erode\",5,1,1,\"Morphology\"]]\n",
    "    run_def[\"para_mp\"] = [[\"Mean\",5],[0.6],run_def[\"binary_filter_mp\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reproduce these results?\n",
    "\n",
    "The notebook is set up to reproduce the results of the last run (Run 4)\n",
    "\n",
    "The execute previous runs please delete or commented out settings used for Run 4 \n",
    "\n",
    "Settings are saved in a .pkl file.\n",
    "\n",
    "The next cell prints the filepath & name of this file.\n",
    "\n",
    "OpSef_Run_XXX loads the settings specified above and processed all images.\n",
    "The only change you have to make within OpSef_Run_XXX  is specifying the location\n",
    "of this setting file.\n",
    "\n",
    "Rename tiff_to_split_train to tiff_to_split to reproduce Run1\n",
    "Rename tiff_to_split_all to tiff_to_split to reproduce Run2\n",
    "\n",
    "## Save Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_def' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90e7b67852eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  auto-create parameter set from input above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_now_list\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ModelType\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitModelSettings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_def' is not defined"
     ]
    }
   ],
   "source": [
    "#  auto-create parameter set from input above\n",
    "run_def[\"run_now_list\"] = [model_dic[x] for x in run_def[\"ModelType\"]]\n",
    "parameter = [pc,input_def,run_def,initModelSettings]\n",
    "\n",
    "# save it\n",
    "file_name = \"./my_runs/Parameter_{}_Run_{}.pkl\".format(input_def[\"dataset\"],run_def[\"run_ID\"])\n",
    "file_name_load = \"./Demo_Notebooks/my_runs/Parameter_{}_Run_{}.pkl\".format(input_def[\"dataset\"],run_def[\"run_ID\"])\n",
    "print(\"Please execute this file with OPsef_Run_XXX\",file_name_load)\n",
    "outfile = open(file_name,'wb')\n",
    "pickle.dump(parameter,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## Folderstructure    ####\n",
    "\n",
    "input_def[\"root\"] = \"/home/trasse/OpSefDemo/leaves\" # defines the main folder \n",
    "\n",
    "# Put files in these subfolder\n",
    "# .lif\n",
    "# root/myimage_container.lif\n",
    "# root/tiff/myimage1.tif (in case this folder is the direct input to the pre-processing pipeline)\n",
    "#          /myimage2.tif ...\n",
    "# or\n",
    "# root/tiff_raw_2D/myimage1.tif (if you want to make patches in 2D)\n",
    "# root/tiff_to_split/myimage1.tif (if you want ONLY create substacts, bt not BIN or patch before)\n",
    "# root/tiff_raw/myimage1.tif (for all pipelines that start with patching or binning and use stacks)\n",
    "\n",
    "\n",
    "######################################\n",
    "### What is a display base image ????\n",
    "######################################\n",
    "\n",
    "run_def[\"display_base\"]\n",
    "''' \n",
    "display base is ideally set to \"same\".\n",
    "in this case the visualiation of segmentation border will be\n",
    "drawn on top of the input image to the segmentation\n",
    "If this behavior is not desired a three digit number\n",
    "that refers to the position in the run_def[\"pre_list\"] might to be entered.\n",
    "example:\n",
    "run_def[\"pre_list\"] = [[\"Median\",3,8,\"Sum\",True,clahe_prm],[\"Mean\",5,3,\"Max\",True,clahe_prm]]\n",
    "& the image resulting from:\n",
    "[\"Mean\",5,3,\"Max\",True,clahe_prm]\n",
    "shall be used as basis for display:\n",
    "then set: \n",
    "run_def[\"display_base\"] = \"001\"\n",
    "'''\n",
    "\n",
    "##########################\n",
    "# Parameter for Cellpose:\n",
    "##########################\n",
    "\n",
    "# Define: \n",
    "# initModelSettings[\"Cell_Channels\"] \n",
    "# to run segementation on grayscale=0, R=1, G=2, B=3\n",
    "# initModelSettings[\"Cell_Channels\"] = [cytoplasm, nucleus]\n",
    "# if NUCLEUS channel does not exist, set the second channel to 0\n",
    "initModelSettings[\"Cell_Channels\"] = [[0,0],[0,0]]\n",
    "\n",
    "# IF ALL YOUR IMAGES ARE THE SAME TYPE, you can give a list with 2 elements\n",
    "initModelSettings[\"Cell_Channels\"] = [0,0] # IF YOU HAVE GRAYSCALE\n",
    "initModelSettings[\"Cell_Channels\"] = [2,3] # IF YOU HAVE G=cytoplasm and B=nucleus\n",
    "initModelSettings[\"Cell_Channels\"] = [2,1] # IF YOU HAVE G=cytoplasm and R=nucleus\n",
    "\n",
    "# if rescale is set to None, the size of the cells is estimated on a per image basis\n",
    "# if you want to set the size yourself, set it to 30. / average_cell_diameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is mainly based on scikit-image. It consist of a linear pipeline:\n",
    "\n",
    "<img src=\"./Figures_Demo/Fig_M3.jpg\" alt = \"Paper Fig M3\" style = \"width: =800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Variables for Preprocessing\n",
    "#####################################\n",
    "\n",
    "## The list \n",
    "\n",
    "run_def[\"pre_list\"]\n",
    "\n",
    "#is organized as such:\n",
    "\n",
    "# (1) Filter type\n",
    "# (2) Kernel\n",
    "# (3) substract fixed value\n",
    "# (4) projection type\n",
    "# (5) calhe enhance (Yes/No) as defined above\n",
    "# (6) Calhe parameter\n",
    "# (7) enhance edges (and how) (no, roberts, sobel)\n",
    "# (8) invert image\n",
    "\n",
    "# It is a list of lists, each entry defines one pre-processing pipeline:\n",
    "\n",
    "# e.g.\n",
    "# Run1\n",
    "run_def[\"pre_list\"] = [[\"Median\",3,50,\"Max\",False,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Median\",3,50,\"Max\",True,run_def[\"clahe_prm\"],\"no\",False],\n",
    "                       [\"Median\",3,50,\"Max\",False,run_def[\"clahe_prm\"],\"sobel\",False],\n",
    "                       [\"Median\",3,50,\"Max\",False,run_def[\"clahe_prm\"],\"no\",True]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link analysis (Settings from t-SNE)\n",
    "\n",
    "from https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "\n",
    "linkage{“ward”, “complete”, “average”, “single”}\n",
    "\n",
    "Which linkage criterion to use:\n",
    "\n",
    "The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "\n",
    "ward minimizes the variance of the clusters being merged. average uses the average of the distances of each observation of the two sets. complete or maximum linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "single uses the minimum of the distances between all observations of the two sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
