{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merlot Reserve Pretraining loss \n",
    "\n",
    "\n",
    "```text\n",
    "L_text  = Text corruption matching.  -- via span prediction..? \n",
    "L_audio = Audio corruption matching. -- span prediction\n",
    "L_frame = Img to text matching.      -- matching \n",
    "```\n",
    "\n",
    "What are these? \"Sources\"? \n",
    "['text2audio', 'audio2text', 'random_text']:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_970952/3700627984.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mloss_fn_given_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m# def loss_fn(params, batch, rng):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#     preds = model.apply(params, batch, rng=rng, mutable=['cache'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_970952/3700627984.py\u001b[0m in \u001b[0;36mloss_fn_given_preds\u001b[0;34m(preds)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtext_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_preds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jax' is not defined"
     ]
    }
   ],
   "source": [
    "#### LOSS is here\n",
    "# Yes, but a little hard to read. They clearly compute 3 losses. text2audio, audio2text, and random_text.\n",
    "\n",
    "# jnp == import jax.numpy as jnp\n",
    "\n",
    "preds = {'text_preds': {'logits': [1,2,3]},\n",
    "        'audio_preds': 'audio_preds', \n",
    "        'audio_preds2': 'audio_preds2'\n",
    "        }\n",
    "\n",
    "def loss_fn_given_preds(preds):\n",
    "    loss_info = {}\n",
    "\n",
    "    if 'text_preds' in preds:\n",
    "        # Special-case of mask LM loss\n",
    "        text_preds = preds.pop('text_preds')\n",
    "        logits = text_preds['logits']\n",
    "        \n",
    "        # get predictions from logits over vocab(?) or what's num_classes?\n",
    "        labels = jax.nn.one_hot(text_preds['labels'], num_classes=logits.shape[1], dtype=logits.dtype)\n",
    "        logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "        mask = (text_preds['labels'] != 0).astype(logits.dtype)\n",
    "\n",
    "        # Kastan -- what is mask.sum() doing?\n",
    "        loss_info['audio2text'] = -(jnp.sum(logprobs * labels, axis=-1) * mask).sum() / mask.sum()\n",
    "\n",
    "    for c_type, c_dict in preds.items():\n",
    "        numer_logits = (c_dict['x'] * c_dict['y']).sum(-1)\n",
    "        loss_info[c_type] = 0.0\n",
    "\n",
    "        if '_sources' in c_dict:\n",
    "            for k in ['text2audio', 'audio2text', 'random_text']:\n",
    "                loss_info[f'_{c_type}_from_{k}'] = 0.0\n",
    "        # For both directions (average the result)\n",
    "        for k1, k2 in ['xy', 'yx']:\n",
    "            x = c_dict[k1]\n",
    "            y = c_dict[k2]\n",
    "\n",
    "            # Add in extra things that are only valid as targets\n",
    "            if f'{k2}_extra' in c_dict:\n",
    "                y = jnp.concatenate([y, c_dict[f'{k2}_extra']])\n",
    "            y_allgather = jax.lax.all_gather(y, 'batch').reshape(-1, x.shape[-1])\n",
    "\n",
    "            print(f\"{c_type} {k1}->{k2} dot product sim:  shaped [{x.shape}] -> [{y_allgather.shape}\", flush=True)\n",
    "            denom_logits = jnp.einsum('lh,vh->lv', x, y_allgather)\n",
    "            denom_lse = jax.nn.logsumexp(denom_logits.astype(jnp.float32), axis=-1)\n",
    "            loss_info[c_type] += (denom_lse - numer_logits).mean() / 2.0\n",
    "            if '_sources' in c_dict:\n",
    "                for i, type_i in enumerate(['text2audio', 'audio2text', 'random_text']):\n",
    "                    does_match = (c_dict['_sources'] == i).astype(jnp.float32)\n",
    "                    loss_match = ((denom_lse - numer_logits) * does_match).sum() / (does_match.sum() + 1e-5)\n",
    "                    loss_info[f'_{c_type}_from_{type_i}'] += loss_match / 2.0\n",
    "\n",
    "    loss = sum([v for k, v in loss_info.items() if not k.startswith('_')])\n",
    "    return loss, loss_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state: train_state.TrainState, batch, use_bfloat16_grads=True):\n",
    "    \"\"\"\n",
    "    Note: we'll compile this with pmap so no need to jit\n",
    "    :param state:\n",
    "    :param batch:\n",
    "    :param use_bfloat16_grads: Whether to use bfloat16 for storing grads. I think it is probably OK considering\n",
    "                               momentum is bfloat16 anyways. i'm just going to cast down (rounding down here rather\n",
    "                               than to nearest or anything)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def _loss_fn(params):\n",
    "        return loss_fn_given_preds(state.apply_fn({'params': params}, batch))\n",
    "\n",
    "    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "\n",
    "    params = state.params\n",
    "    if use_bfloat16_grads:\n",
    "        params = f32_to_bf16(state.params)\n",
    "\n",
    "    (loss, loss_info), grads = grad_fn(params)\n",
    "\n",
    "    grads = jax.tree_map(lambda x: jnp.nan_to_num(x, copy=False), grads)\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "\n",
    "    # Cast up grads here (after the pmean) which reduces bandwidth maybe\n",
    "    if use_bfloat16_grads:\n",
    "        grads = bf16_to_f32(grads)\n",
    "\n",
    "    # Average metrics over all replicas (maybe this isn't a great idea, idk)\n",
    "    loss_info = jax.lax.pmean(loss_info, axis_name='batch')\n",
    "    loss_info = bf16_to_f32(loss_info)\n",
    "\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openpsg_custom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a608ccb501c69e230c11b99670cd7bccb59a69a266e9d5ef687d45497635fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
