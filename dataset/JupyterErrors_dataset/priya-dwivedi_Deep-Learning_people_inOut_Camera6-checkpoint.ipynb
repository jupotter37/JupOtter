{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/priya/miniconda3/envs/tfpy3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division  \n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import timeit\n",
    "import cv2\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = '/home/priya/Documents/AI_Apps/damien_counter/model' + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Rectangle = namedtuple('Rectangle', 'xmin ymin xmax ymax')\n",
    "\n",
    "# ra = Rectangle(3., 3., 5., 5.)\n",
    "# rb = Rectangle(1., 1., 4., 3.5)\n",
    "# # intersection here is (3, 3, 4, 3.5), or an area of 1*.5=.5\n",
    "\n",
    "def area(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n",
    "    dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n",
    "    area_a = abs(a.xmax - a.xmin)* abs(a.ymax - a.ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy/area_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'camera6_out' + \".avi\"\n",
    "out = cv2.VideoWriter(out_filename,cv2.VideoWriter_fourcc('M','J','P','G'), 10, (704,576))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bcaff13efbc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0morig_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0morigW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#       print(origW, origH)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "filename = '/home/priya/Documents/AI_Apps/damien_counter/camera6_small.mp4'\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "length = int(cap.get(7))\n",
    "fps = cap.get(5)\n",
    "duration = length/fps\n",
    "\n",
    "# Running the tensorflow session\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "\n",
    "   counter = 0\n",
    "   fac = 1\n",
    " \n",
    "   while True:\n",
    "#       print(\"Current Vals are:\", person_timer, no_person_timer)\n",
    "      ret, image_np = cap.read()\n",
    "      orig_image = image_np\n",
    "      origW, origH, _ = orig_image.shape\n",
    "#       print(origW, origH)\n",
    "      if not ret:\n",
    "        break\n",
    "\n",
    "      counter += 1\n",
    "\n",
    "\n",
    "      if counter % fac == 0:          \n",
    "#           cv2.rectangle(orig_image,(100,200),(600,500),(0,0,255),3)\n",
    "#           crop[startY:endY, startX:endX]\n",
    "#           image_np = image_np[300:450,0:500,:]\n",
    "          image_np = image_np\n",
    "          w, h, _ = image_np.shape\n",
    "        \n",
    "          ops = tf.get_default_graph().get_operations()\n",
    "\n",
    "\n",
    "          all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "\n",
    "          tensor_dict = {}\n",
    "\n",
    "          for key in ['num_detections', 'detection_boxes', 'detection_scores',\n",
    "                            'detection_classes', 'detection_masks']:\n",
    "\n",
    "                tensor_name = key + ':0'\n",
    "\n",
    "                if tensor_name in all_tensor_names:\n",
    "\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "\n",
    "          if 'detection_masks' in tensor_dict:\n",
    "\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "\n",
    "\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "\n",
    "\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image_np.shape[0], image_np.shape[1])\n",
    "\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.4), tf.uint8)\n",
    "\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "\n",
    "\n",
    "          image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "          # Run inference\n",
    "          output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image_np, 0)})\n",
    "\n",
    "          # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "          output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "\n",
    "\n",
    "          output_dict['detection_classes'] = output_dict[\n",
    "                                        'detection_classes'][0][0:5].astype(np.uint8)\n",
    "\n",
    "          output_dict['detection_boxes'] = output_dict['detection_boxes'][0][0:5]\n",
    "            \n",
    "          for i, bbox in enumerate(output_dict['detection_boxes']):\n",
    "                \n",
    "              output_dict['detection_boxes'][i] = [int(bbox[0] * h), int(bbox[1] * w), int(bbox[2] * h), int(bbox[3] * w)]\n",
    "            \n",
    "\n",
    "          output_dict['detection_scores'] = output_dict['detection_scores'][0][0:5]\n",
    "\n",
    "\n",
    "          if 'detection_masks' in output_dict:\n",
    "\n",
    "            output_dict['detection_masks'] = output_dict['detection_masks'][0][0:5]\n",
    "\n",
    "\n",
    "          vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "              orig_image,\n",
    "              output_dict['detection_boxes'],\n",
    "              output_dict['detection_classes'],\n",
    "              output_dict['detection_scores'],\n",
    "              category_index,\n",
    "              instance_masks=output_dict.get('detection_masks'),\n",
    "#               use_normalized_coordinates=True,\n",
    "              line_thickness=4,\n",
    "              min_score_thresh = 0.60)\n",
    "          \n",
    "            ## Draw a rectangle in front of the counter\n",
    "#           uncomment line below to see the box on the frame\n",
    "          #top left and bottom right\n",
    "          cv2.rectangle(orig_image,(90,450),(420,550),(0,255,0),3)\n",
    "    \n",
    "#           for n, object in enumerate(output_dict['detection_classes']):\n",
    "                \n",
    "#             if output_dict['detection_classes'][0]== 1 and output_dict['detection_scores'][0]> 0.70 and counter > 0:\n",
    "\n",
    "#                 ## Is atleast some portion of the human in this box\n",
    "#                 ymin = int(output_dict['detection_boxes'][0][0])\n",
    "#                 xmin = int(output_dict['detection_boxes'][0][1])\n",
    "#                 ymax = int(output_dict['detection_boxes'][0][2])\n",
    "#                 xmax = int(output_dict['detection_boxes'][0][3])\n",
    "\n",
    "#     #                 print(\"Human coords are: \", xmin, ymin, xmax, ymax)\n",
    "\n",
    "#                 # calculate overlap b/w 2 rectanges\n",
    "#                 rBox = Rectangle(120,120,210,210)\n",
    "#                 rHuman = Rectangle(xmin,ymin, xmax,ymax)\n",
    "#                 print(\"Overlap is: \", area(rBox, rHuman))\n",
    "#                 overlap = area(rBox, rHuman)\n",
    "#                 if overlap == None:\n",
    "#                     overlap = 0\n",
    "\n",
    "#                 if overlap > 0.4:\n",
    "#                     ## Human is present. Let them be there for long enough time \n",
    "\n",
    "#                     person_timer += fac\n",
    "#                     no_person_timer = 0\n",
    "\n",
    "\n",
    "#                     if person_timer > fac*13 and already_counted == False:\n",
    "#                         # register as human \n",
    "#                         already_counted = True\n",
    "#                         total_humans += 1\n",
    "\n",
    "\n",
    "\n",
    "#                 else:\n",
    "#                     #reset our counters\n",
    "#                     no_person_timer += fac\n",
    " \n",
    "\n",
    "#                     if no_person_timer > 20*fac:\n",
    "#                         person_timer = 0\n",
    "#                         already_counted = False\n",
    "\n",
    "#             # if no human detected reset my counters\n",
    "#             else:\n",
    "\n",
    "#                 no_person_timer += fac\n",
    "\n",
    "#                 if no_person_timer > 5*fac:\n",
    "#                     person_timer = 0\n",
    "#                     already_counted = False\n",
    "\n",
    "\n",
    "#           print('Total humans: ', total_humans)\n",
    "#           print('person_timer: ',person_timer)\n",
    "#           print('no_person_timer: ',no_person_timer)\n",
    "#           print(\"already counted:\", already_counted )\n",
    "\n",
    "            \n",
    "     \n",
    "#           text_pos = \"No. of persons: \" + str(total_humans)\n",
    "#           cv2.putText(orig_image, text_pos, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1) # Text in red\n",
    "\n",
    "#           timestamp = \"Time: {} secs\".format(timedelta(seconds=(counter / fps)))\n",
    "#           cv2.putText(orig_image, timestamp, (10, 40), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 0, 0), 1) # Text in blue\n",
    "\n",
    "\n",
    "      out.write(orig_image)\n",
    "#       cv2.imshow('image', orig_image)\n",
    "#       key = cv2.waitKey(20) & 0xFF\n",
    "\n",
    "#         # if the `q` key is pressed, break from the lop\n",
    "#       if key == ord(\"q\"):\n",
    "#            break\n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#intializing the web camera device\n",
    "\n",
    "filename = '/home/priya/Documents/AI_Apps/damien_counter/camera6_1.avi'\n",
    "cap = cv2.VideoCapture(filename)\n",
    "\n",
    "length = int(cap.get(7))\n",
    "fps = cap.get(5)\n",
    "duration = length/fps\n",
    "\n",
    "# Running the tensorflow session\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "\n",
    "   counter = 0\n",
    "   fac = 3\n",
    "   counter = 0\n",
    "   total_humans = 0\n",
    "   person_timer = 0\n",
    "   no_person_timer = 0\n",
    "   already_counted = False\n",
    " \n",
    "   while True:\n",
    "#       print(\"Current Vals are:\", person_timer, no_person_timer)\n",
    "      ret, image_np = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "    \n",
    "      orig_image = image_np\n",
    "      origW, origH, _ = orig_image.shape\n",
    "#       print(\"Orig dimensions are: \", origW, origH)\n",
    "      counter += 1\n",
    "\n",
    "\n",
    "      if counter % fac == 0:          \n",
    "          ## Take a fraction of image in front of the counter\n",
    "          image_np = image_np[30:300, 50:370,:]\n",
    "          w, h, _ = image_np.shape\n",
    "#           print(\"Image dimensions are: \", w, h)\n",
    "        \n",
    "          ops = tf.get_default_graph().get_operations()\n",
    "\n",
    "\n",
    "          all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "\n",
    "          tensor_dict = {}\n",
    "\n",
    "          for key in ['num_detections', 'detection_boxes', 'detection_scores',\n",
    "                            'detection_classes', 'detection_masks']:\n",
    "\n",
    "                tensor_name = key + ':0'\n",
    "\n",
    "                if tensor_name in all_tensor_names:\n",
    "\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "\n",
    "          if 'detection_masks' in tensor_dict:\n",
    "\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "\n",
    "\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "\n",
    "\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image_np.shape[0], image_np.shape[1])\n",
    "\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.4), tf.uint8)\n",
    "\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "\n",
    "\n",
    "          image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "          # Run inference\n",
    "          output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image_np, 0)})\n",
    "\n",
    "          # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "          output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "\n",
    "\n",
    "          output_dict['detection_classes'] = output_dict[\n",
    "                                        'detection_classes'][0][0:5].astype(np.uint8)\n",
    "\n",
    "          output_dict['detection_boxes'] = output_dict['detection_boxes'][0][0:5]\n",
    "            \n",
    "          for i, bbox in enumerate(output_dict['detection_boxes']):\n",
    "                \n",
    "              output_dict['detection_boxes'][i] = [int(bbox[0] * h), int(bbox[1] * w), int(bbox[2] * h), int(bbox[3] * w)]\n",
    "            \n",
    "\n",
    "          output_dict['detection_scores'] = output_dict['detection_scores'][0][0:5]\n",
    "\n",
    "\n",
    "          if 'detection_masks' in output_dict:\n",
    "\n",
    "            output_dict['detection_masks'] = output_dict['detection_masks'][0][0:5]\n",
    "\n",
    "\n",
    "          vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "              orig_image,\n",
    "              output_dict['detection_boxes'],\n",
    "              output_dict['detection_classes'],\n",
    "              output_dict['detection_scores'],\n",
    "              category_index,\n",
    "              instance_masks=output_dict.get('detection_masks'),\n",
    "#               use_normalized_coordinates=True,\n",
    "              line_thickness=4,\n",
    "              min_score_thresh = 0.70)\n",
    "          \n",
    "            ## Draw a rectangle in front of the counter\n",
    "#           uncomment line below to see the box on the frame\n",
    "#           cv2.rectangle(orig_image,(120,120),(210,210),(0,255,0),3)\n",
    "    \n",
    "          for n, object in enumerate(output_dict['detection_classes']):\n",
    "                \n",
    "            if output_dict['detection_classes'][0]== 1 and output_dict['detection_scores'][0]> 0.70 and counter > 0:\n",
    "\n",
    "                ## Is atleast some portion of the human in this box\n",
    "                ymin = int(output_dict['detection_boxes'][0][0])\n",
    "                xmin = int(output_dict['detection_boxes'][0][1])\n",
    "                ymax = int(output_dict['detection_boxes'][0][2])\n",
    "                xmax = int(output_dict['detection_boxes'][0][3])\n",
    "\n",
    "    #                 print(\"Human coords are: \", xmin, ymin, xmax, ymax)\n",
    "\n",
    "                # calculate overlap b/w 2 rectanges\n",
    "                rBox = Rectangle(120,120,210,210)\n",
    "                rHuman = Rectangle(xmin,ymin, xmax,ymax)\n",
    "                print(\"Overlap is: \", area(rBox, rHuman))\n",
    "                overlap = area(rBox, rHuman)\n",
    "                if overlap == None:\n",
    "                    overlap = 0\n",
    "\n",
    "                if overlap > 0.4:\n",
    "                    ## Human is present. Let them be there for long enough time \n",
    "\n",
    "                    person_timer += fac\n",
    "                    no_person_timer = 0\n",
    "\n",
    "\n",
    "                    if person_timer > fac*13 and already_counted == False:\n",
    "                        # register as human \n",
    "                        already_counted = True\n",
    "                        total_humans += 1\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    #reset our counters\n",
    "                    no_person_timer += fac\n",
    " \n",
    "\n",
    "                    if no_person_timer > 20*fac:\n",
    "                        person_timer = 0\n",
    "                        already_counted = False\n",
    "\n",
    "            # if no human detected reset my counters\n",
    "            else:\n",
    "\n",
    "                no_person_timer += fac\n",
    "\n",
    "                if no_person_timer > 5*fac:\n",
    "                    person_timer = 0\n",
    "                    already_counted = False\n",
    "\n",
    "\n",
    "          print('Total humans: ', total_humans)\n",
    "          print('person_timer: ',person_timer)\n",
    "          print('no_person_timer: ',no_person_timer)\n",
    "          print(\"already counted:\", already_counted )\n",
    "\n",
    "            \n",
    "     \n",
    "          text_pos = \"No. of persons: \" + str(total_humans)\n",
    "          cv2.putText(orig_image, text_pos, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 255), 1) # Text in red\n",
    "\n",
    "          timestamp = \"Time: {} secs\".format(timedelta(seconds=(counter / fps)))\n",
    "          cv2.putText(orig_image, timestamp, (10, 40), cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 0, 0), 1) # Text in blue\n",
    "\n",
    "\n",
    "      out.write(orig_image)\n",
    "#       cv2.imshow('image', orig_image)\n",
    "#       key = cv2.waitKey(20) & 0xFF\n",
    "\n",
    "#         # if the `q` key is pressed, break from the lop\n",
    "#       if key == ord(\"q\"):\n",
    "#            break\n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfpy3]",
   "language": "python",
   "name": "conda-env-tfpy3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
