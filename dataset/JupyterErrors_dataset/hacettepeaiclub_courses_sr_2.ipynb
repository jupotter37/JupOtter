{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "realistic-lafayette",
   "metadata": {},
   "source": [
    "# Konuşma Tanıma II: Ses Sınıflandırma\n",
    "<hr>\n",
    "\n",
    "### İçerik \n",
    "- Öznitelik dersi özet geçilmesi\n",
    "- Kullanacağımız veri kümesinin incelenmesi\n",
    "- Sınıflandırma kodunun yazılması\n",
    "\n",
    "### Veri Kümeleri\n",
    "- Librispeech https://www.openslr.org/12\n",
    "- **Common Voice** https://commonvoice.mozilla.org/en\n",
    "\n",
    "### Derin Öğrenme Mimarisi\n",
    "\n",
    "<img src=\"figures/model.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-70fd762832c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-netherlands",
   "metadata": {},
   "source": [
    "### Veri kümesinin oluşturulması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "billion-genesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_table(csv_path)\n",
    "        self.df = df.dropna(subset=['gender'])\n",
    "        \n",
    "        self.wav_folder = \"clips\"\n",
    "        self.classes = {'female': 0, 'male': 1}\n",
    "        \n",
    "        self.mel_limit = 256\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav_name = self.df.iloc[idx].path\n",
    "        wav_path = os.path.join(self.wav_folder, wav_name)\n",
    "        \n",
    "        label = self.df.iloc[idx].gender\n",
    "        label = self.classes[label]\n",
    "        \n",
    "        feature = self.load_sound(wav_path)\n",
    "        \n",
    "        return {'data': feature, 'label': label}\n",
    "    \n",
    "    def load_sound(self, path):\n",
    "        signal, sampling_rate = librosa.load(path, sr=None)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(signal, sampling_rate)\n",
    "        mel_spectrogram = torch.from_numpy(mel_spectrogram)\n",
    "        \n",
    "        # padding & cutting\n",
    "        length = mel_spectrogram.shape[1]\n",
    "        n_mels = mel_spectrogram.shape[0]\n",
    "        \n",
    "        if length < self.mel_limit:\n",
    "            pad_tensor = torch.zeros((n_mels, self.mel_limit-length))\n",
    "            mel_spectrogram = torch.cat((mel_spectrogram, pad_tensor), 1)\n",
    "        mel_spectrogram = mel_spectrogram[:, :self.mel_limit]\n",
    "\n",
    "        return mel_spectrogram.unsqueeze(0)\n",
    "\n",
    "\n",
    "train_dataset = VoiceDataset(\"~/Datasets/common_voice/tr/cv-corpus-5.1-2020-06-22/tr/train.tsv\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = VoiceDataset(\"~/Datasets/common_voice/tr/cv-corpus-5.1-2020-06-22/tr/test.tsv\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "dataloaders= {'train': train_dataloader,\n",
    "             'val': val_dataloader}\n",
    "data_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-fields",
   "metadata": {},
   "source": [
    "### Veri kümesinin incelenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial-spray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train sample: 1483\n",
      "number of val sample: 536\n"
     ]
    }
   ],
   "source": [
    "print(\"number of train sample: {}\".format(len(train_dataset)))\n",
    "print(\"number of val sample: {}\".format(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "empirical-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#durations = []\n",
    "#for path in tqdm(train_dataset.df.path):\n",
    "#    y, sr = librosa.load(os.path.join(train_dataset.wav_folder, path))\n",
    "#    dur = librosa.get_duration(y, sr)\n",
    "#    durations.append(dur)\n",
    "#\n",
    "#pd.DataFrame(durations).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "experienced-disposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_dataset.df.gender.hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-session",
   "metadata": {},
   "source": [
    "### Eğitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "technical-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "thrown-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoiceModel(\n",
      "  (feature): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=59520, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VoiceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VoiceModel, self).__init__()\n",
    "        \n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(59520, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.feature(inputs)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = VoiceModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "random-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-alert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [09:15<00:00, 23.14s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.6314890483546245 | accuracy: 0.7835469245910645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:03<00:00, 20.44s/it]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3680933642743239 | accuracy: 0.9123134016990662\n",
      "Epoch : 1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [08:22<00:00, 20.92s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2577472819280399 | accuracy: 0.8853675127029419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [03:02<00:00, 20.33s/it]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2567859256445472 | accuracy: 0.9029850363731384\n",
      "Epoch : 2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13/24 [04:41<04:03, 22.16s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    print(f\"Epoch : {epoch}\")\n",
    "    print(\"-\"*10)\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for batch in tqdm(dataloaders[phase]):\n",
    "            inputs = batch['data'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "        epoch_loss = running_loss / data_sizes[phase]\n",
    "        epoch_acc = running_corrects / data_sizes[phase]\n",
    "        \n",
    "        print('loss: {} | accuracy: {}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-passage",
   "metadata": {},
   "source": [
    "### Eğitimin İyileştirilmesi\n",
    "- Learning rate scheduler\n",
    "- SGD\n",
    "- Hyper parameter optimization\n",
    "- Veri artırma (specAugment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-vault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-bailey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-explosion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-express",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-adelaide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
