{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T03:31:01.012300Z",
     "start_time": "2020-02-27T03:30:58.757745Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Dropbox/Ranee_Joshi_PhD_Local/04_PythonCodes/dh2loop/notebooks\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, sys\n",
    "\n",
    "print(os.getcwd())\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ranee/anaconda3/envs/dh2loop/lib/python36.zip',\n",
       " '/home/ranee/anaconda3/envs/dh2loop/lib/python3.6',\n",
       " '/home/ranee/anaconda3/envs/dh2loop/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home/ranee/.local/lib/python3.6/site-packages',\n",
       " '/home/ranee/.local/lib/python3.6/site-packages/map2loop-0.0.50-py3.6.egg',\n",
       " '/home/ranee/.local/lib/python3.6/site-packages/LoopStructural-0.0.0-py3.6-linux-x86_64.egg',\n",
       " '/home/ranee/.local/lib/python3.6/site-packages/dh2loop-0.0.1-py3.6.egg',\n",
       " '/home/ranee/anaconda3/envs/dh2loop/lib/python3.6/site-packages',\n",
       " '/home/ranee/anaconda3/envs/dh2loop/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/tmp/tmpqef8adso',\n",
       " '/mnt/d/Dropbox/Ranee_Joshi_PhD_Local/04_PythonCodes/dh2loop/notebooks',\n",
       " '/mnt/d/Dropbox/Ranee_Joshi_PhD_Local/04_PythonCodes/dh2loop/notebooks']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dh2loop import dh2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parameters loaded from dh2l_config.py:\n",
      "#Database Files\n",
      "data_path='../data/'\n",
      "wamex_path='../data/wamex/'\n",
      "collar_file=wamex_path+'collar.csv'\n",
      "collarattr_file=wamex_path+'collarattr.csv'\n",
      "dhsurvey_file=wamex_path+'dhsurvey.csv'\n",
      "dhsurveyattr_file=wamex_path+'dhsurveyattr.csv'\n",
      "dhgeology_file=wamex_path+'dhgeology.csv'\n",
      "dhgeologyattr_file=wamex_path+'dhgeologyattr.csv'\n",
      "\n",
      "#Thesauri\n",
      "rl_maxdepth_dic_file= wamex_path+'rl_maxdepth_dic.csv'\n",
      "survey_dic_file=wamex_path+'survey_dic.csv'\n",
      "dic_attr_col_lithology_file=wamex_path+'dic_att_col_lithology.csv'\n",
      "dic_attr_val_lithology_file=wamex_path+'dic_attr_val_lithology_filter.csv'\n",
      "cleanup_lithology_file=wamex_path+'cleanup_lithology.csv'\n",
      "litho_dic_file=wamex_path+'litho_dic_1.csv'\n",
      "CET_hierarchy_dico_file=wamex_path+'hierarchy_dico.csv'\n",
      "\n",
      "#ExportFiles\n",
      "export_path='../data/export/'\n",
      "DB_Collar_Export=export_path+'DB_Collar_Export.csv'\n",
      "DB_Survey_Export=export_path+'DB_Survey_Export.csv'\n",
      "DB_Survey_Export_Calc=export_path+'DB_Survey_Export_Calc.csv'\n",
      "CET_Litho=export_path+'CET_Litho.csv'\n",
      "DB_Lithology_Export=export_path+'DB_Lithology_Export.csv'\n",
      "DB_Lithology_Export_Backup=export_path+'DB_Lithology_Export_Backup.csv'\n",
      "DB_Lithology_Upscaled=export_path+'DB_Lithology_Upscaled.csv'\n",
      "DB_Lithology_Export_Calc=export_path+'DB_Lithology_Export_Calc.csv'\n",
      "DB_Lithology_Export_VTK=export_path+'DB_Lithology_Export.vtp'\n",
      "\n",
      "\n",
      "#Shapefiles\n",
      "shapefile_path='../data/shapefile/'\n",
      "geology=shapefile_path+'500K_interpgeol16_yalgoo_singleton.shp'\n",
      "\n",
      "#Projections\n",
      "src_crs = {'init': 'EPSG:4326'}  # coordinate reference system for imported dtms (geodetic lat/long WGS84)\n",
      "dst_crs = {'init': 'EPSG:28350'} # coordinate system for data\n",
      "\n",
      "print('Default parameters loaded from dh2l_config.py:')\n",
      "with open('../notebooks/dh2l_config.py', 'r') as myfile:\n",
      "  data = myfile.read()\n",
      "  print(data)\n",
      "  myfile.close()\n",
      "print('\\nModify these parameters in the cell below')\n",
      "\n",
      "Modify these parameters in the cell below\n"
     ]
    }
   ],
   "source": [
    "%run -i \"dh2l_config.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ranee/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glove import Glove\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geovec(path):\n",
    "    instance = Glove()\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        v = np.zeros(f['vectors'].shape, f['vectors'].dtype)\n",
    "        f['vectors'].read_direct(v)\n",
    "        dct = f['dct'][()].tostring().decode('utf-8')\n",
    "        dct = json.loads(dct)\n",
    "    instance.word_vectors = v\n",
    "    instance.no_components = v.shape[1]\n",
    "    instance.word_biases = np.zeros(v.shape[0])\n",
    "    instance.add_dictionary(dct)\n",
    "    return instance\n",
    "\n",
    "new_model = load_geovec('../glove/geovec_300d_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-e1fdfa1c66ad>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-e1fdfa1c66ad>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    tokens = [word.lower() for lambda text: nltk.sent_tokenize(text)\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Stopwords\n",
    "extra_stopwords = [\n",
    "    'also',\n",
    "]\n",
    "stop = stopwords.words('english') + extra_stopwords\n",
    "\n",
    "\n",
    "def tokenize(text, min_len=1):\n",
    "    '''Function that tokenize a set of strings\n",
    "    Input:\n",
    "        -text: set of strings\n",
    "        -min_len: tokens length\n",
    "    Output:\n",
    "        -list containing set of tokens'''\n",
    "\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text)\n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and len(token) >= min_len:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return [x.lower() for x in filtered_tokens if x not in stop]\n",
    "\n",
    "\n",
    "def tokenize_and_lemma(text, min_len=0):\n",
    "    '''Function that retrieves lemmatised tokens\n",
    "    Inputs:\n",
    "        -text: set of strings\n",
    "        -min_len: length of text\n",
    "    Outputs:\n",
    "        -list containing lemmatised tokens'''\n",
    "    filtered_tokens = tokenize(text, min_len=min_len)\n",
    "\n",
    "    lemmas = [lemma.lemmatize(t) for t in filtered_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test= pd.read_csv('../data/testdata.csv',encoding = \"ISO-8859-1\", dtype='object')\n",
    "test=test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-88172762a16c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d1fc5fb9318c>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, min_len)\u001b[0m\n\u001b[1;32m     14\u001b[0m         -list containing set of tokens'''\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     tokens = [word.lower() for sent in nltk.sent_tokenize(text)\n\u001b[0m\u001b[1;32m     17\u001b[0m               for word in nltk.word_tokenize(sent)]\n\u001b[1;32m     18\u001b[0m     \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \"\"\"\n\u001b[0;32m-> 1331\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[1;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dh2loop/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokenize=tokenize(test, min_len=1)\n",
    "tokenize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word, model, return_zero=False):\n",
    "    '''Function that retrieves word embeddings (vector)\n",
    "    Inputs:\n",
    "        -word: token (string)\n",
    "        -model: trained MLP model\n",
    "        -return_zero: boolean variable\n",
    "    Outputs:\n",
    "        -wv: numpy array (vector)'''\n",
    "    epsilon = 1.e-10\n",
    "\n",
    "    unk_idx = model.dictionary['unk']\n",
    "    idx = model.dictionary.get(word, unk_idx)\n",
    "    wv = model.word_vectors[idx].copy()\n",
    "\n",
    "    if return_zero and word not in model.dictionary:\n",
    "        n_comp = model.word_vectors.shape[1]\n",
    "        wv = np.zeros(n_comp) + epsilon\n",
    "\n",
    "    return wv\n",
    "\n",
    "\n",
    "def mean_embeddings(dataframe_file, model):\n",
    "    '''Function to retrieve sentence embeddings from dataframe with\n",
    "    lithological descriptions.\n",
    "    Inputs:\n",
    "        -dataframe_file: pandas dataframe containing lithological descriptions\n",
    "                         and reclassified lithologies\n",
    "        -model: word embeddings model generated using GloVe\n",
    "    Outputs:\n",
    "        -DF: pandas dataframe including sentence embeddings'''\n",
    "    DF = pd.read_pickle(dataframe_file)\n",
    "    DF = DF.drop_duplicates(subset=['x', 'y', 'z'])\n",
    "    DF['tokens'] = DF['Description'].apply(lambda x: tokenize_and_lemma(x))\n",
    "    DF['length'] = DF['tokens'].apply(lambda x: len(x))\n",
    "    DF = DF[DF.length > 0]\n",
    "    DF['vectors'] = DF['tokens'].apply(lambda x: np.asarray([get_vector(n, model) for n in x]))\n",
    "    DF['mean'] = DF['vectors'].apply(lambda x: np.mean(x[~np.all(x == 1.e-10, axis=1)], axis=0))\n",
    "    DF['reclass'] = pd.Categorical(DF.reclass)\n",
    "    DF['code'] = DF.reclass.cat.codes\n",
    "    DF['drop'] = DF['mean'].apply(lambda x: (~np.isnan(x).any()))\n",
    "    DF = DF[DF['drop']]\n",
    "    return DF\n",
    "\n",
    "\n",
    "def split_stratified_dataset(Dataframe, test_size, validation_size):\n",
    "    '''Function that split dataset into test, training and validation subsets\n",
    "    Inputs:\n",
    "        -Dataframe: pandas dataframe with sentence mean_embeddings\n",
    "        -test_size: decimal number to generate the test subset\n",
    "        -validation_size: decimal number to generate the validation subset\n",
    "    Outputs:\n",
    "        -X: numpy array with embeddings\n",
    "        -Y: numpy array with lithological classes\n",
    "        -X_test: numpy array with embeddings for test subset\n",
    "        -Y_test: numpy array with lithological classes for test subset\n",
    "        -Xt: numpy array with embeddings for training subset\n",
    "        -yt: numpy array with lithological classes for training subset\n",
    "        -Xv: numpy array with embeddings for validation subset\n",
    "        -yv: numpy array with lithological classes for validation subset\n",
    "        '''\n",
    "    X = np.vstack(Dataframe['mean'].values)\n",
    "    Y = Dataframe.code.values.reshape(len(Dataframe.code), 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        Y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify=Y,\n",
    "                                                        random_state=42)\n",
    "    Xt, Xv, yt, yv = train_test_split(X_train,\n",
    "                                      y_train,\n",
    "                                      test_size=validation_size,\n",
    "                                      stratify=None,\n",
    "                                      random_state=1)\n",
    "    return X, Y, X_test, y_test, Xt, yt, Xv, yv\n",
    "\n",
    "\n",
    "def retrieve_predictions(classifier, x):\n",
    "    '''Function that retrieves lithological classes using the trained classifier\n",
    "    Inputs:\n",
    "        -classifier: trained MLP classifier\n",
    "        -x: numpy array containing embbedings\n",
    "    Outputs:\n",
    "        -codes_pred: numpy array containing lithological classes predicted'''\n",
    "    preds = classifier.predict(x, verbose=0)\n",
    "    new_onehot = np.zeros((x.shape[0], 18))\n",
    "    new_onehot[np.arange(len(preds)), preds.argmax(axis=1)] = 1\n",
    "    codes_pred = one_enc.inverse_transform(new_onehot)\n",
    "    return codes_pred\n",
    "\n",
    "\n",
    "def classifier_assess(classifier, x, y):\n",
    "    '''Function that prints the performance of the classifier\n",
    "    Inputs:\n",
    "        -classifier: trained MLP classifier\n",
    "        -x: numpy array with embeddings\n",
    "        -y: numpy array with lithological classes predicted'''\n",
    "    Y2 = retrieve_predictions(classifier, x)\n",
    "    print('f1 score: ', metrics.f1_score(y, Y2, average='macro'),\n",
    "          'accuracy: ', metrics.accuracy_score(y, Y2),\n",
    "          'balanced_accuracy:', metrics.balanced_accuracy_score(y, Y2))\n",
    "\n",
    "\n",
    "def save_predictions(Dataframe, classifier, x, name):\n",
    "    '''Function that saves dataframe predictions as a pickle file\n",
    "    Inputs:\n",
    "        -Dataframe: pandas dataframe with mean_embeddings\n",
    "        -classifier: trained MLP model,\n",
    "        -x: numpy array with embeddings,\n",
    "        -name: string name to save dataframe\n",
    "    Outputs:\n",
    "        -save dataframe'''\n",
    "    preds = classifier.predict(x, verbose=0)\n",
    "    Dataframe['predicted_probabilities'] = preds.tolist()\n",
    "    Dataframe['pred'] = retrieve_predictions(classifier, x).astype(np.int32)\n",
    "    Dataframe[['x', 'y', 'FromDepth', 'ToDepth', 'TopElev', 'BottomElev',\n",
    "               'mean', 'predicted_probabilities', 'pred', 'reclass', 'code']].to_pickle('{}.pkl'.format(name))\n",
    "\n",
    "\n",
    "# loading word embeddings model\n",
    "# (This can be obtained from https://github.com/spadarian/GeoVec )\n",
    "modelEmb = Glove.load('/home/ignacio/Documents/chapter2/best_glove_300_317413_w10_lemma.pkl')\n",
    "\n",
    "# getting the mean embeddings of descriptions\n",
    "DF = mean_embeddings('manualTest.pkl', modelEmb)\n",
    "\n",
    "# subseting dataset for training classifier\n",
    "X, Y, X_test, Y_test, X_train, Y_train, X_validation, Y_validation = split_stratified_dataset(DF, 0.1, 0.1)\n",
    "\n",
    "# encoding lithological classes\n",
    "encodes = one_enc.fit_transform(Y_train).toarray()\n",
    "\n",
    "# MLP model generation\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=300, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(units=len(DF.code.unique()), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training MLP model\n",
    "model.fit(X_train, encodes, epochs=30, batch_size=100, verbose=2)\n",
    "\n",
    "# saving MLP model\n",
    "model.save('mlp_prob_model.h5')\n",
    "\n",
    "# assessment of model performance\n",
    "classifier_assess(model, X_validation, Y_validation)\n",
    "\n",
    "# save lithological prediction likelihoods dataframe\n",
    "save_predictions(DF, model, X, 'NSWpredictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Collar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T03:31:01.074765Z",
     "start_time": "2020-02-27T03:31:01.015229Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    CollarID                    HoleId Longitude  Latitude       RL MaxDepth  \\\n",
      "0    1146528  71288Minjar NorthSSDD008   116.947  -28.9021  356.255    546.4   \n",
      "1    1146530  71288Minjar NorthSSDD010   116.949  -28.9083  356.871    405.0   \n",
      "2    1146531  71288Minjar NorthSSDD011   116.948  -28.9002  360.214    249.0   \n",
      "3    1146650  71288Minjar NorthSSDD005   116.948  -28.9002  357.153    468.0   \n",
      "4    1233095        82655KararaBHN1001   116.818  -29.1504  357.156      150   \n",
      "..       ...                       ...       ...       ...      ...      ...   \n",
      "127   931216         79530KararaLSC016   117.148   -29.426  315.535      124   \n",
      "128   931217         79530KararaLSC017   117.144  -29.4045  361.603      158   \n",
      "129   931218         79530KararaLSC018   117.151  -29.4335  323.052      136   \n",
      "130   931219         79530KararaLSD006   117.153  -29.4336  365.334      248   \n",
      "131   931220         79530KararaLSD007   117.151  -29.4405  368.845    260.7   \n",
      "\n",
      "                 X             Y  \n",
      "0    494832.946487  6.802860e+06  \n",
      "1    495028.225204  6.802173e+06  \n",
      "2    494930.345746  6.803070e+06  \n",
      "3    494930.345746  6.803070e+06  \n",
      "4    482298.918542  6.775337e+06  \n",
      "..             ...           ...  \n",
      "127  514355.695351  6.744807e+06  \n",
      "128  513970.643608  6.747189e+06  \n",
      "129  514645.613436  6.743975e+06  \n",
      "130  514839.580770  6.743964e+06  \n",
      "131  514644.608861  6.743200e+06  \n",
      "\n",
      "[132 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "collar= pd.read_csv(collar_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "collarattr= pd.read_csv(collarattr_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "rl_maxdepth_dic= pd.read_csv(rl_maxdepth_dic_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dh2l.collar_attr_col_dic(rl_maxdepth_dic_file)\n",
    "dh2l.collar_collarattr_final(collar_file, collarattr_file, rl_maxdepth_dic_file, DB_Collar_Export)\n",
    "DB_Collar_Export_table=pd.read_csv(DB_Collar_Export,encoding = \"ISO-8859-1\", dtype='object')\n",
    "DB_Collar_Export_table= DB_Collar_Export_table.loc[:, ~DB_Collar_Export_table.columns.str.contains('^Unnamed')]\n",
    "DB_Collar_Export_table[['X','Y']] = DB_Collar_Export_table.apply(dh2l.convert_coords,axis=1)\n",
    "DB_Collar_Export_table.to_csv(DB_Collar_Export)\n",
    "print(DB_Collar_Export_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T03:31:03.696297Z",
     "start_time": "2020-02-27T03:31:03.663114Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CollarID       Depth     Azimuth         Dip Index              X  \\\n",
      "0      153802    0.000000   90.000000  -50.000000     2  495028.106029   \n",
      "1      153802    0.000000   90.000000  -50.000000     2  494917.618977   \n",
      "2      153803    0.000000   90.000000  -55.000000     4  495028.106029   \n",
      "3      153803    0.000000   90.000000  -55.000000     4  494906.041567   \n",
      "4      153804    0.000000   90.000000  -60.000000     6  495028.106029   \n",
      "...       ...         ...         ...         ...   ...            ...   \n",
      "1345  1233228  140.000000  137.400000  -90.000000  1339  482677.460027   \n",
      "1346  1233228  160.000000  137.800000  -90.000000  1339  482663.948285   \n",
      "1347  1233228  180.000000  137.200000  -90.000000  1339  482650.372272   \n",
      "1348  1233228  200.000000  137.300000  -90.000000  1339  482636.821912   \n",
      "1349  1233228  220.000000  137.400000  -90.000000  1339  482630.053152   \n",
      "\n",
      "                   Y           Z  \n",
      "0     6802449.793954  390.166000  \n",
      "1     6802449.793954  349.952002  \n",
      "2     6802449.793954  359.251000  \n",
      "3     6802449.793954  320.764223  \n",
      "4     6802449.793954  359.132000  \n",
      "...              ...         ...  \n",
      "1345  6775654.582516  364.772524  \n",
      "1346  6775669.327996  364.772524  \n",
      "1347  6775684.014444  364.772524  \n",
      "1348  6775698.724565  364.772524  \n",
      "1349  6775706.085535  364.772524  \n",
      "\n",
      "[1350 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "dhsurvey= pd.read_csv(dhsurvey_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dhsurveyattr= pd.read_csv(dhsurveyattr_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "survey_dic= pd.read_csv(survey_dic_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dh2l.survey_attr_col_dic(survey_dic_file)\n",
    "dh2l.survey_final(dhsurvey_file,dhsurveyattr_file, DB_Survey_Export)\n",
    "dhsurvey= pd.read_csv(DB_Survey_Export,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dhsurvey= dhsurvey.loc[:, ~dhsurvey.columns.str.contains('^Unnamed')]\n",
    "dh2l.convert_survey(DB_Collar_Export,DB_Survey_Export, DB_Survey_Export_Calc)\n",
    "dhsurvey_calc= pd.read_csv(DB_Survey_Export_Calc,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dhsurvey_calc.dropna()\n",
    "dhsurvey_calc= dhsurvey_calc.loc[:, ~dhsurvey_calc.columns.str.contains('^Unnamed')]\n",
    "print(dhsurvey_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Lithology Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhgeology= pd.read_csv(dhgeology_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dhgeologyattr= pd.read_csv(dhgeologyattr_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dh2l.litho_attr_val_dic(dic_attr_val_lithology_file)\n",
    "dh2l.litho_dico (litho_dic_file)\n",
    "dh2l.clean_up (cleanup_lithology_file)\n",
    "dh2l.litho_attr_val_with_fuzzy (CET_Litho)\n",
    "dh2l.litho_final (collar_file, dhgeology_file, dhgeologyattr_file, dic_attr_col_lithology_file, CET_Litho, DB_Lithology_Export)\n",
    "dic_att_col_lithology=pd.read_csv(dic_attr_col_lithology_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "dic_attr_val_lithology=pd.read_csv(dic_attr_val_lithology_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "cleanup_lithology=pd.read_csv(cleanup_lithology_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "lithology_thesaurus=pd.read_csv(litho_dic_file,encoding = \"ISO-8859-1\", dtype='object')\n",
    "DB_Lithology_Export_View=pd.read_csv(DB_Lithology_Export,encoding = \"ISO-8859-1\", dtype='object')\n",
    "print(DB_Lithology_Export_View)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upscaling Lithology Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to link and upscale the drillhole information, a <b> hierarchical thesaurus (Thesaurus 4) </b> was also built. The dictionary involved three levels that would upscale a list of 757 rock names to more general rock groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CET_hierarchy_dico = pd.read_csv(CET_hierarchy_dico_file,encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dh2l.upscale_litho (DB_Lithology_Export, CET_hierarchy_dico_file, DB_Lithology_Upscaled)\n",
    "Upscaled_Litho= pd.read_csv(DB_Lithology_Upscaled,encoding = \"ISO-8859-1\")\n",
    "Upscaled_Litho.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Upscaled_Litho[Upscaled_Litho['Level_1'] == \"igneous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upscaled_Litho[Upscaled_Litho['Level_2'] == \"ore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Upscaled_Litho[Upscaled_Litho['CET_Litho'] == \"granodiorite\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate for the X,Y,Z coordinates for FromDepth, ToDepth and its midpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dh2l.convert_lithology(DB_Collar_Export, DB_Survey_Export, DB_Lithology_Upscaled, DB_Lithology_Export_Calc)\n",
    "DB_Lithology_Export_Calculated=pd.read_csv(DB_Lithology_Export_Calc)\n",
    "print(DB_Lithology_Export_Calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to VTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dh2l.intervals2vtk(DB_Lithology_Export_Calc, DB_Lithology_Export_VTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./graphics/vtk.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Paraview to view the VTK files and have some fun!<br>\n",
    "The VTK file output is at \\dh2loop\\data\\export\\DB_Lithology_Export.vtp"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
