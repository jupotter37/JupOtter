{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, n_layers, \n",
    "            size, activation=torch.tanh, output_activation=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        self.size = size\n",
    "        self.n_layers = n_layers\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        \n",
    "        layers_size = [self.input_size] + ([self.size]*self.n_layers) + [self.output_size]\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers_size[i], layers_size[i+1]) \n",
    "                                    for i in range(len(layers_size)-1)])\n",
    "        for layer in self.layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = self.activation(layer(out))\n",
    "\n",
    "        if self.output_activation is not None:\n",
    "            out = self.output_activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_dim = 4\n",
    "obs_dim = 10\n",
    "n_layers = 2\n",
    "size = 64\n",
    "learning_rate = 5e-3\n",
    "criterion = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = torch.randn(4, obs_dim)\n",
    "actions = torch.randn(4, ac_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "          0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959,\n",
      "          0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530, -0.2159, -0.7425,  0.5627,  0.2596,\n",
      "         -0.1740, -0.6787],\n",
      "        [ 0.9383,  0.4889,  1.2032,  0.0845, -1.2001, -0.0048, -0.5181, -0.3067,\n",
      "         -1.5810,  1.7066]]) tensor([[ 0.2055, -0.4503, -0.5731, -0.5554],\n",
      "        [ 0.5943,  1.5419,  0.5073, -0.5910],\n",
      "        [-1.3253,  0.1886, -0.0691, -0.4949],\n",
      "        [-1.4959, -0.1938,  0.4455,  1.3253]])\n"
     ]
    }
   ],
   "source": [
    "print(observation, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = MLP(obs_dim, output_size=ac_dim, n_layers=n_layers, size=size)\n",
    "logstd = torch.zeros(ac_dim, requires_grad=True) # BUG\n",
    "optimizer = optim.Adam([logstd] + list(mean.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0924,  0.0440, -0.2718,  0.1034, -0.0782,  0.2838,  0.2779,  0.0983,\n",
      "          0.2245,  0.1028],\n",
      "        [ 0.2694, -0.0263,  0.0704, -0.0470, -0.0050, -0.0859, -0.1127,  0.1647,\n",
      "          0.1222, -0.1257],\n",
      "        [-0.2509,  0.1392, -0.2619, -0.0126, -0.2450,  0.2384,  0.2319, -0.2340,\n",
      "         -0.2647, -0.1684],\n",
      "        [ 0.0786, -0.1424, -0.0678, -0.0722,  0.0522, -0.1347,  0.1295, -0.2038,\n",
      "         -0.0253,  0.2387],\n",
      "        [-0.1572,  0.2185,  0.1482, -0.0920,  0.0987, -0.1924,  0.0229,  0.0807,\n",
      "          0.2463, -0.1593],\n",
      "        [-0.1619, -0.1421, -0.2774,  0.0576, -0.2357, -0.0764, -0.1270,  0.0159,\n",
      "          0.0864, -0.2069],\n",
      "        [-0.0841, -0.0274, -0.1646,  0.0658,  0.1054, -0.2029,  0.2239,  0.2023,\n",
      "         -0.0855,  0.0424],\n",
      "        [ 0.0347, -0.0393, -0.1911,  0.2346,  0.1401, -0.2380, -0.2207,  0.2708,\n",
      "          0.0454, -0.0654],\n",
      "        [ 0.1930,  0.0818,  0.1022,  0.2259, -0.0222,  0.2713, -0.2000, -0.1733,\n",
      "          0.0482, -0.0346],\n",
      "        [ 0.1625, -0.0774,  0.0448, -0.0592,  0.2138, -0.0187,  0.0044,  0.2086,\n",
      "          0.2768,  0.2725],\n",
      "        [-0.2784, -0.1609, -0.0618, -0.0588,  0.1647,  0.1286, -0.0234,  0.0067,\n",
      "          0.0610,  0.2455],\n",
      "        [ 0.1476, -0.1436, -0.1459,  0.0901, -0.0288, -0.1827, -0.2756,  0.0408,\n",
      "         -0.0832, -0.1657],\n",
      "        [ 0.0536,  0.2773,  0.2365,  0.1624, -0.2148,  0.1625,  0.1473,  0.0950,\n",
      "         -0.0709, -0.0050],\n",
      "        [-0.1544, -0.0641, -0.0469, -0.0403, -0.2834,  0.1005, -0.2330, -0.2129,\n",
      "          0.1157, -0.2331],\n",
      "        [ 0.2716,  0.2727, -0.0905, -0.0491, -0.2178,  0.0176,  0.0521, -0.1562,\n",
      "          0.0893, -0.0110],\n",
      "        [-0.0839, -0.0010, -0.2760, -0.2006,  0.0377,  0.2750, -0.1688,  0.1825,\n",
      "          0.0261, -0.1225],\n",
      "        [ 0.0003,  0.1365,  0.0339,  0.2521, -0.0539, -0.1901,  0.2573, -0.0813,\n",
      "          0.1807,  0.1878],\n",
      "        [ 0.0504, -0.2337, -0.2795,  0.1352,  0.0696,  0.1657, -0.2510, -0.0392,\n",
      "          0.2460,  0.1496],\n",
      "        [ 0.2366,  0.2236, -0.0699, -0.1939, -0.1698,  0.0967, -0.1130,  0.0011,\n",
      "          0.0454,  0.1768],\n",
      "        [ 0.1588,  0.1625,  0.0116,  0.1608, -0.1333, -0.1140, -0.0451, -0.1654,\n",
      "          0.2540, -0.0436],\n",
      "        [ 0.0828, -0.0089, -0.0039, -0.1518, -0.0209, -0.2547, -0.1362,  0.2191,\n",
      "         -0.2671,  0.0469],\n",
      "        [ 0.0454, -0.2725,  0.0556,  0.1322,  0.2062,  0.0533,  0.2226,  0.0107,\n",
      "          0.1085,  0.0841],\n",
      "        [-0.2100, -0.0625, -0.2005,  0.2212,  0.1358, -0.1208, -0.0521,  0.0760,\n",
      "         -0.2483,  0.0080],\n",
      "        [ 0.1358,  0.0357, -0.0262,  0.1831,  0.1239, -0.2531,  0.1676, -0.2246,\n",
      "          0.1600,  0.2398],\n",
      "        [ 0.0262, -0.0962, -0.1471, -0.1264, -0.0525, -0.1181,  0.2827, -0.1228,\n",
      "          0.1182, -0.2641],\n",
      "        [ 0.1443,  0.2423, -0.2041, -0.0727,  0.0679, -0.2726, -0.1802,  0.0990,\n",
      "          0.2681, -0.0851],\n",
      "        [-0.0026, -0.2749, -0.2733,  0.2476,  0.2416, -0.0868, -0.1126, -0.0184,\n",
      "          0.1321, -0.2752],\n",
      "        [-0.1577, -0.0939, -0.1239, -0.0075,  0.2077,  0.0622, -0.2577,  0.2634,\n",
      "          0.2293,  0.0897],\n",
      "        [ 0.1394,  0.1064,  0.2334, -0.2739, -0.1773, -0.0362, -0.0542, -0.1891,\n",
      "         -0.2731, -0.2175],\n",
      "        [-0.0442,  0.2417,  0.1888,  0.0821, -0.1749, -0.1146,  0.0274, -0.2639,\n",
      "         -0.0130,  0.0579],\n",
      "        [ 0.2574, -0.0747,  0.2795,  0.0215, -0.1976,  0.2691,  0.2469, -0.2250,\n",
      "         -0.1652,  0.1362],\n",
      "        [-0.1752,  0.1936, -0.1359, -0.0884, -0.0817,  0.2717, -0.0963, -0.0554,\n",
      "          0.1723, -0.1948],\n",
      "        [-0.0014, -0.2199, -0.2628, -0.0650,  0.0315,  0.2428, -0.2731, -0.0819,\n",
      "          0.0749,  0.1075],\n",
      "        [ 0.1664, -0.2290, -0.0696,  0.0658, -0.2283,  0.1435, -0.2717,  0.1777,\n",
      "          0.0926, -0.0809],\n",
      "        [ 0.0924,  0.2412,  0.2095,  0.1727, -0.1658,  0.0818,  0.2137,  0.0502,\n",
      "          0.1885, -0.1766],\n",
      "        [ 0.2001, -0.0094, -0.2555, -0.0738,  0.2517, -0.0995, -0.0493, -0.1139,\n",
      "         -0.1548,  0.0493],\n",
      "        [ 0.1727,  0.0337, -0.0176,  0.0165, -0.2135, -0.1500,  0.0142, -0.2276,\n",
      "          0.1410,  0.0434],\n",
      "        [ 0.0030, -0.0183,  0.2405,  0.2599,  0.1390,  0.1414, -0.1381,  0.1164,\n",
      "         -0.1926,  0.1231],\n",
      "        [-0.2022, -0.0016,  0.2064,  0.1891,  0.2518,  0.0971,  0.1105,  0.0764,\n",
      "          0.2079,  0.1494],\n",
      "        [ 0.0840, -0.0994, -0.1378, -0.0704,  0.1359, -0.1550,  0.0778,  0.2491,\n",
      "          0.2411, -0.2096],\n",
      "        [ 0.0812,  0.1862, -0.2586,  0.1820, -0.1774, -0.2059, -0.1207, -0.1071,\n",
      "         -0.1177,  0.0820],\n",
      "        [ 0.1103,  0.1107,  0.1966, -0.1802, -0.2704, -0.2284, -0.1565, -0.1302,\n",
      "          0.1251, -0.2061],\n",
      "        [-0.2082,  0.1525, -0.1812,  0.0534, -0.2171,  0.1086,  0.1779, -0.2395,\n",
      "          0.0969, -0.1493],\n",
      "        [ 0.1807,  0.2561, -0.2402,  0.1065,  0.1923,  0.2589,  0.2151,  0.0560,\n",
      "          0.0166,  0.0587],\n",
      "        [-0.1698, -0.0558, -0.0044, -0.1802, -0.0126, -0.0378, -0.0997, -0.0875,\n",
      "         -0.2836,  0.2714],\n",
      "        [ 0.1404,  0.2059, -0.2424, -0.2788,  0.2118, -0.0500,  0.0444,  0.1428,\n",
      "          0.0182, -0.0094],\n",
      "        [ 0.1692,  0.2275, -0.0794, -0.2431, -0.1561,  0.2035,  0.0248,  0.2438,\n",
      "         -0.0996, -0.2698],\n",
      "        [ 0.0884, -0.1213, -0.1702,  0.1775,  0.0198, -0.0243,  0.2773,  0.2481,\n",
      "         -0.2326, -0.2618],\n",
      "        [-0.1408, -0.2211, -0.1497,  0.1921, -0.1895,  0.2612,  0.0888,  0.0124,\n",
      "          0.0487,  0.0601],\n",
      "        [ 0.0916, -0.2316, -0.0050,  0.1889,  0.2829, -0.0151, -0.2034,  0.1309,\n",
      "         -0.2605,  0.2538],\n",
      "        [ 0.1750,  0.1283,  0.0389,  0.2070, -0.1150, -0.2375, -0.1574,  0.2119,\n",
      "         -0.1423, -0.2270],\n",
      "        [ 0.0624, -0.1769, -0.0121, -0.1807,  0.0602, -0.0465,  0.0376,  0.1316,\n",
      "         -0.1833, -0.0971],\n",
      "        [-0.1899,  0.2371, -0.1020,  0.1072,  0.0657, -0.2809, -0.2021, -0.0732,\n",
      "         -0.2183,  0.1510],\n",
      "        [ 0.1790, -0.0005,  0.0349,  0.0750,  0.0947, -0.0551, -0.1120, -0.1761,\n",
      "          0.0113,  0.2252],\n",
      "        [-0.0354,  0.2450,  0.0996, -0.1465,  0.1106,  0.1141, -0.1267, -0.0499,\n",
      "         -0.0296,  0.1005],\n",
      "        [ 0.2222, -0.0626,  0.1852,  0.1535, -0.2467,  0.2082, -0.0115,  0.2198,\n",
      "          0.2218, -0.2757],\n",
      "        [-0.2371, -0.0324,  0.0915, -0.0453, -0.2741,  0.1549, -0.1628,  0.1293,\n",
      "          0.2011, -0.0960],\n",
      "        [ 0.1715,  0.0379, -0.0894,  0.0185,  0.1024,  0.1458, -0.1634,  0.2128,\n",
      "         -0.1347, -0.2453],\n",
      "        [-0.1231, -0.2521,  0.0797,  0.0192, -0.0139, -0.1927,  0.1501,  0.0466,\n",
      "         -0.0715,  0.0504],\n",
      "        [ 0.1004, -0.1711, -0.0820, -0.1293,  0.2520,  0.0704, -0.2149,  0.2011,\n",
      "         -0.0452,  0.0196],\n",
      "        [ 0.0368, -0.0333,  0.2696,  0.1233,  0.0613, -0.1035,  0.0655, -0.2073,\n",
      "         -0.0984,  0.1444],\n",
      "        [-0.2802, -0.0072, -0.1049,  0.1455, -0.1779,  0.0493, -0.0821,  0.2389,\n",
      "         -0.0425, -0.2079],\n",
      "        [-0.0885,  0.2007,  0.1090,  0.0610,  0.1776,  0.1100, -0.0365, -0.1508,\n",
      "         -0.2725,  0.1369],\n",
      "        [ 0.2221, -0.1088, -0.1280,  0.0491, -0.0263,  0.2186, -0.0049, -0.0086,\n",
      "         -0.2647,  0.0869]])\n",
      "tensor([-2.7537e-01, -2.9014e-01,  3.0883e-01, -7.8867e-02,  1.5795e-02,\n",
      "         8.5733e-02,  2.1495e-01,  2.6989e-01,  2.5647e-01, -2.3429e-01,\n",
      "        -5.0665e-02, -1.8712e-01, -1.8068e-01,  7.5011e-02,  2.9683e-01,\n",
      "        -2.5333e-01,  1.9138e-01, -1.6395e-01, -6.1593e-02,  2.5101e-01,\n",
      "        -7.1521e-02,  2.8794e-02, -2.2105e-01,  2.6919e-01, -4.0879e-02,\n",
      "        -2.3129e-01,  9.2595e-02, -2.2484e-01, -2.5093e-01,  1.9264e-02,\n",
      "         2.5072e-01, -8.9487e-02,  1.4884e-01,  2.7174e-01,  2.0974e-01,\n",
      "        -1.6588e-01, -3.4665e-02, -9.9512e-02, -2.5429e-01,  1.3065e-04,\n",
      "         2.3794e-01,  2.6643e-01,  2.9504e-02,  7.1823e-02, -1.3690e-01,\n",
      "         2.3870e-01, -1.3159e-01, -2.1967e-01,  4.8697e-02,  1.8954e-01,\n",
      "        -2.8511e-01,  2.8586e-01,  1.1376e-01, -2.2155e-01, -6.8118e-02,\n",
      "         2.7435e-01, -2.4262e-01, -9.2426e-02,  1.0372e-01, -2.7704e-01,\n",
      "         1.7336e-01,  1.6461e-01,  1.9613e-01, -2.0161e-01])\n",
      "tensor([[ 0.0008,  0.0511,  0.1324,  ..., -0.1262, -0.1690,  0.0577],\n",
      "        [-0.0025, -0.0028,  0.1360,  ..., -0.0243, -0.0493,  0.1066],\n",
      "        [-0.0719, -0.1322, -0.0614,  ...,  0.0313,  0.1920,  0.0918],\n",
      "        ...,\n",
      "        [-0.0425, -0.0646,  0.0485,  ...,  0.1962, -0.1036, -0.0428],\n",
      "        [-0.1110,  0.0442, -0.0780,  ..., -0.0082,  0.0698,  0.1426],\n",
      "        [-0.1903, -0.1028,  0.2123,  ...,  0.0762,  0.0297,  0.0668]])\n",
      "tensor([ 0.0855,  0.0746,  0.0699,  0.0338,  0.0645, -0.0858, -0.0809,  0.0336,\n",
      "         0.0195, -0.1099, -0.0911, -0.0995,  0.0993,  0.0817, -0.0658,  0.0189,\n",
      "        -0.0288,  0.1087, -0.1132, -0.0912, -0.1041,  0.0727, -0.0625,  0.0745,\n",
      "         0.0423, -0.0611, -0.0337, -0.0506,  0.0551, -0.0501,  0.0424,  0.0089,\n",
      "        -0.0998, -0.0786, -0.0867,  0.0655,  0.0271, -0.0031, -0.0744, -0.1239,\n",
      "        -0.0442, -0.0957, -0.1243, -0.1192, -0.0816, -0.1215,  0.0390,  0.0468,\n",
      "         0.0504, -0.1244,  0.0773,  0.0244,  0.0639, -0.0307,  0.0600, -0.1186,\n",
      "         0.0426,  0.0991,  0.0525, -0.1178, -0.0351, -0.0155,  0.0655, -0.0473])\n",
      "tensor([[-0.1631,  0.0632, -0.1215,  0.0826,  0.2773, -0.2489,  0.1139, -0.1916,\n",
      "          0.0910, -0.0648,  0.1212, -0.2217,  0.2089,  0.1678, -0.2042, -0.2096,\n",
      "          0.1623, -0.2331,  0.1160, -0.1468, -0.2662,  0.0922, -0.1248, -0.1960,\n",
      "          0.0884,  0.1373,  0.2401,  0.2905,  0.1028, -0.0858, -0.0510,  0.2242,\n",
      "         -0.2309,  0.2134,  0.1371,  0.1141,  0.0919, -0.2466,  0.1188, -0.0179,\n",
      "          0.0039, -0.1380,  0.2147, -0.0496,  0.0024, -0.2537,  0.2281, -0.2742,\n",
      "          0.1839,  0.1302,  0.0452, -0.2876, -0.1370, -0.0197, -0.2200, -0.0961,\n",
      "         -0.0672, -0.0928,  0.2067,  0.2716, -0.1330,  0.1534,  0.0187,  0.2174],\n",
      "        [-0.2591, -0.1472, -0.0595,  0.2853,  0.2351, -0.0804, -0.1831, -0.1483,\n",
      "          0.0365,  0.0672, -0.1503,  0.0837, -0.0343,  0.2211, -0.1132, -0.1668,\n",
      "          0.0457,  0.0153,  0.1677,  0.2900, -0.1196,  0.2762,  0.1395,  0.2529,\n",
      "          0.0915, -0.0444,  0.0408,  0.0078, -0.2194, -0.0989,  0.2250,  0.0582,\n",
      "          0.0789,  0.0287,  0.0641, -0.2068, -0.0559, -0.0643, -0.1015, -0.1877,\n",
      "         -0.0310, -0.0071, -0.0971, -0.0726, -0.0912, -0.0474, -0.2371,  0.2421,\n",
      "         -0.1587,  0.1547, -0.2615, -0.0136, -0.0274, -0.2526,  0.0154, -0.2217,\n",
      "          0.1051,  0.1859, -0.0459,  0.0492, -0.1487,  0.1075, -0.2804,  0.2959],\n",
      "        [-0.0167,  0.0616,  0.2517, -0.2460, -0.1547,  0.0533, -0.2964,  0.0676,\n",
      "         -0.0252, -0.2786, -0.0739, -0.2174,  0.0287,  0.2189,  0.2827, -0.2777,\n",
      "          0.1059, -0.2286,  0.1056,  0.1635, -0.2830, -0.2144, -0.2664, -0.0279,\n",
      "          0.1089, -0.2179, -0.0193, -0.1034,  0.2695, -0.2723, -0.1649, -0.0028,\n",
      "         -0.2315,  0.2286, -0.2729, -0.0958, -0.1242, -0.0027, -0.1411,  0.2133,\n",
      "         -0.2557, -0.1567, -0.1360, -0.2839, -0.2946,  0.1048,  0.2798, -0.0454,\n",
      "         -0.1152,  0.1299,  0.1750, -0.2666, -0.1418,  0.2511,  0.1322, -0.2758,\n",
      "          0.0056, -0.2249, -0.2298, -0.1475,  0.2948, -0.1713,  0.2042, -0.2696],\n",
      "        [-0.0204,  0.2686, -0.0667, -0.1024, -0.1313, -0.2147,  0.0953, -0.2165,\n",
      "         -0.1889,  0.2580,  0.2210, -0.1553,  0.0063, -0.0798,  0.2261, -0.2667,\n",
      "         -0.2669,  0.1454,  0.0142, -0.0869,  0.1521,  0.1220, -0.2021,  0.1697,\n",
      "          0.1725, -0.0420, -0.2570,  0.1247,  0.1770, -0.2951, -0.2748, -0.0359,\n",
      "         -0.2605, -0.1484,  0.2318,  0.0543,  0.1775,  0.0700,  0.2904,  0.1229,\n",
      "          0.0875,  0.2249, -0.1180,  0.2546,  0.0283,  0.2719, -0.1771, -0.2018,\n",
      "          0.2161, -0.1711,  0.0574,  0.2814,  0.0555, -0.2110, -0.1809, -0.1639,\n",
      "         -0.2015,  0.1593,  0.0431, -0.0268,  0.0827,  0.1737,  0.0262,  0.1928]])\n",
      "tensor([ 0.0954, -0.1007,  0.0366, -0.1018])\n"
     ]
    }
   ],
   "source": [
    "for param in list(mean.parameters()):\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    logits_out = mean(observation)\n",
    "    sample_ac = logits_out + torch.exp(logstd) * torch.randn(logits_out.size()) # BUG\n",
    "    loss = criterion(actions, sample_ac)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3758, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0118, -0.0124, -0.0125, -0.0145], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.005, -0.005, -0.005, -0.005], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logstd.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = MLP(obs_dim, output_size=ac_dim, n_layers=n_layers, size=size)\n",
    "logstd = torch.zeros(ac_dim, requires_grad=True) # BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_out = mean(observation)\n",
    "sample_ac = logits_out + torch.exp(logstd) * torch.randn(logits_out.size()) # BUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(actions, sample_ac)\n",
    "optimizer = optim.Adam([logstd] + list(mean.parameters()), lr=learning_rate)\n",
    "\n",
    "# train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 4]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-37eaac4d55d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs285_env/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs285_env/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 4]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward(retain_graph=True)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = torch.distributions.Normal(torch.tensor([1., -1]), scale=torch.tensor([1, 2.]))\n",
    "diag = torch.distributions.Independent(normal, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.tensor([1., 3.], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.5310245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n",
      "/home/oriea/anaconda3/envs/cs285_env/lib/python3.5/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=[1., -1],\n",
    "        scale_diag=[1, 2.])\n",
    "\n",
    "    print(mvn.log_prob([1, 3]).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d27187122e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cs285_env/lib/python3.5/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "x = torch.distributions.Categorical(probs=torch.tensor([0.2, 0.5, 0.3]))\n",
    "x.log_prob(torch.tensor(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(torch.multinomial(torch.tensor([.2, .3]), num_samples=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([2,3])\n",
    "x.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  9.,  6.],\n",
       "        [ 2., 12.,  8.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[2, 3, 6], [1, 4, 8]]) * torch.Tensor([2,3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'temp.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'logstd': logstd,\n",
    "            'mean_preds': mean.state_dict(),\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = checkpoint['logstd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _is_method(func):\n",
    "    spec = inspect.signature(func)\n",
    "    return 'self' in spec.parameters\n",
    "\n",
    "def convert_args_to_tensor(positional_args_list=None, keyword_args_list=None):\n",
    "    \"\"\"A decorator which converts args in positional_args_list to torch.Tensor\n",
    "\n",
    "    Args:\n",
    "        positional_args_list ([list]): [arguments to be converted to torch.Tensor. If None, \n",
    "        it will convert all positional arguments to Tensor]\n",
    "        keyword_args_list ([list]): [arguments to be converted to torch.Tensor. If None, \n",
    "        it will convert all keyword arguments to Tensor]\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \n",
    "            \n",
    "            \n",
    "            _keyword_args_list = keyword_args_list\n",
    "            _positional_args_list = positional_args_list\n",
    "            if keyword_args_list is None:\n",
    "                _keyword_args_list = list(kwargs.keys())\n",
    "\n",
    "            if positional_args_list is None:\n",
    "                _positional_args_list = list(range(len(args)))\n",
    "                if _is_method(func):\n",
    "                    _positional_args_list = _positional_args_list[1:]\n",
    "            \n",
    "            args = list(args)\n",
    "            for i, arg in enumerate(args):\n",
    "                if i in _positional_args_list:\n",
    "                    if type(arg) == np.ndarray:\n",
    "                        args[i] = torch.from_numpy(arg).type(torch.FloatTensor)\n",
    "                    elif type(arg) == torch.Tensor:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError('Arguments should be Numpy arrays, but argument in position {} is not'.format(str(i)))\n",
    "            \n",
    "            for key, arg in kwargs.items():\n",
    "                if key in _keyword_args_list:\n",
    "                    if type(arg) == np.ndarray:\n",
    "                        kwargs[key] = torch.from_numpy(arg).type(torch.FloatTensor)\n",
    "                    elif type(arg) == torch.Tensor:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError('Arguments should be Numpy arrays, but argument {} is not'.format(str(key)))\n",
    "            \n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "class X:\n",
    "    @convert_args_to_tensor()\n",
    "    def function(self, a, b):\n",
    "        print('a, b: ', a, b)\n",
    "        return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def is_method(func):\n",
    "    spec = inspect.signature(func)\n",
    "    print(spec.parameters)\n",
    "    return 'self' in spec.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', <Parameter \"a\">), ('b', <Parameter \"b\">)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X()\n",
    "is_method(x.function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, b:  tensor([2.]) tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "a, _ = x.function(np.array([2]),np.array([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "type(np.array([2.,3.])) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "for key, d in a.items():\n",
    "    print(key, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.tensor([2, 4, 5]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
