{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07f1c8a-342f-4828-8269-ddd920b20ec2",
   "metadata": {},
   "source": [
    "![ga4](https://www.google-analytics.com/collect?v=2&tid=G-6VDTYWLKX6&cid=1&en=page_view&sid=1&dl=statmike%2Fvertex-ai-mlops%2FDev%2Fnew&dt=Python+Asynchronous+API+Calls.ipynb)\n",
    "\n",
    "# Python Asynchronous API Calls\n",
    "\n",
    "Methods for making asynchronous API calls.  Additionally, managing concurrent request and handling errors.\n",
    "\n",
    "To illustrate the concepts, the Vertex AI SDK will be used to make sychronous and asynchronous request for generative AI APIs for Gemini and PaLM.  The concept and solutions for managing concurrency and errors apply to any API with an asynchronous client.\n",
    "\n",
    "The example used below starts with requesting a list of vocabulary words.  This is a good synchronous task because it is really just a single request.  This is followed with the tasks of requesting definitions for each word.  Using a synchronous approach to this would be time consuming.  Switching to an asynchronous approach allows requesting many words at the same time.  However, this introduces the need to manage concurrency - how many simountaneous requests are being made.  As more requests are made the chances of hitting qouta limits increase, especially in a shared environment with multiple application make calls.  The concept of concurrency is also extended to include error handling and retries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220187f-f83b-4a65-a32d-cbf486d19409",
   "metadata": {
    "id": "od_UkDpvRmgD",
    "tags": []
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "To run this notebook in Colab click [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Getting%20Started%20-%20Vertex%20AI%20GenAI%20Python%20Client.ipynb) and run the cells in this section.  Otherwise, skip this section.\n",
    "\n",
    "This cell will authenticate to GCP (follow prompts in the popup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf9e789-f8d9-455f-9d93-da2f385e8179",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b61850-8266-4f16-a882-86f47fa6a7d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a83d89-4188-4b70-a892-16d384dd1b43",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf908f99-58a6-4c4b-931e-fe413ecdaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83729efc-2809-4d62-9ed4-d0a83eb0451c",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc874b0c-c47e-40fa-8042-214a4bee8939",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0855d86-5380-403d-ae48-941ce776c98b",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4acaa-da79-44a1-961d-c8874930c223",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed30be3-9541-49f7-b6a9-7e854cffc9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6192e238-4e5a-47d1-87f0-c436dfdb4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'tips'\n",
    "EXPERIMENT = 'async-api'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086341c-3baa-49e3-bac5-1143794f8bf2",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b67a11b2-2728-4d51-b3df-2428504a7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import asyncio\n",
    "\n",
    "import vertexai.language_models\n",
    "import vertexai.preview.generative_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509e363-d1ec-4c57-9b42-71364e6b4e01",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17802a1f-8df3-46df-9d1f-40ada1a473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16869614-5a02-47a1-8211-91325e3cd648",
   "metadata": {},
   "source": [
    "---\n",
    "## Synchronous Use Of APIs - Using Vertex AI Generative AI Models\n",
    "\n",
    "To get started, the [Vertex AI SDK for Python](https://cloud.google.com/python/docs/reference/aiplatform/latest) will be used to make requests using the generative AI APIs for PaLM and Gemini.\n",
    "- [Vertex AI SDK for Python](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "- [Gemini Class Overview](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/sdk-for-gemini/gemini-sdk-overview-reference)\n",
    "- [PaLM Text Model Classes](https://cloud.google.com/vertex-ai/docs/generative-ai/sdk-for-llm/sdk-use-text-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbef2b-2cc8-4c45-8f54-1204fc6c08e1",
   "metadata": {},
   "source": [
    "### Generate A List of Vocabulary Words - With Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6d0ee-32f4-4c39-82dc-7f0b681bd23d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Connect to the Gemini Model API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "21bb631d-701f-44c7-b00c-2a1b883f1669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemini_model = vertexai.preview.generative_models.GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d1440-d3ac-45f3-8511-8d1d37a632ab",
   "metadata": {},
   "source": [
    "Request a list of vocabulary words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2a37752e-fc0f-4338-b93a-376afd1ff229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_words = gemini_model.generate_content(\n",
    "    [\n",
    "        \"I need a long list of vocabulary words to study for the GMAT.\",\n",
    "        \"Respond with only a comma separated list of words.\"\n",
    "    ],\n",
    "    generation_config = dict(max_output_tokens = 8000, temperature = 0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "792e5718-f095-45db-a7cc-26452ccf54c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abrogate, acquiesce, adjudicate, aggregate, alleviate, allocate, ambivalent, ameliorate, amicable, amortize, anachronistic, analogous, anomalous, antagonistic, antiquated, apathetic, apprehensive, arbitrary, articulate, ascertain, ascetic, assiduous, audacious, auspicious, autonomous, avarice, aversion, benevolent, begrudging, bellicose, benignant, capricious, candid, cessation, circumspect, clandestine, coalesce, cognizant, commensurate, conciliatory, conducive, conundrum, copious, corroborate, covetous, credulous, dearth, debunk, decimate, decorous, deferential, definitive, deleterious, delineate, demagogue, deprecate, derogatory, descry, desolate, desuetude, didactic, diffident, diligent, disabuse, discerning, disincentive, disparage, disparate, dispassionate, disparate, disquisition, dogmatic, duplicity, eclectic, efface, efficacious, egregious, elegiac, elicit, elucidate, emaciated, empirical, enigmatic, endemic, ephemeral, equivocal, eradicate, errant, erstwhile, esoteric, ethereal, etymology, exacerbate, exigency, exonerate, expiate, expunge, facetious, fastidious, feasible, felicitous, fortuitous, fractious, furtive, gainsay, gratuitous, gregarious, guile, heterogeneous, iconoclast, idiosyncratic, ignoble, illicit, immutable, imperative, impervious, imperturbable, impetuous, implacable, implicit, impervious, importune, impregnable, improvident, inadvertent, incendiary, incessant, incipient, inconclusive, incorrigible, incredulous, indemnity, indigenous, indigent, indolent, indomitable, ineffable, ingenuous, inherent, iniquitous, inscrutable, insipid, insolent, insidious, intangible, integral, interminable, interrogatory, intrepid, inviolable, irascible, irreconcilable, irreproachable, labyrinthine, laconic, languid, lapidary, lascivious, latent, laudatory, lexicon, licentious, limpid, loquacious, magnanimous, malleable, maudlin, mellifluous, mendacious, meticulous, milieu, myriad, nebulous, neophyte, nonplussed, obsequious, obdurate, obloquy, obsolete, obviate, odious, officious, ominous, opprobrium, ostentatious, paradigm, paradox, pariah, parsimonious, pedestrian, penitent, perfidious, perfunctory, perspicacious, pertinacious, peruse, philology, phlegmatic, poignant, polemic, portend, precarious, precipitate, precipitous, predilection, presage, probity, prodigious, profligate, propitious, proselyte, protean, provident, puerile, pusillanimous, qualm, quandary, quiescent, quixotic, rabid, raconteur, recalcitrant, recidivism, refractory, relegate, remonstrate, renegade, renitent, repudiate, resilient, resolution, reticent, salacious, sanctimonious, sardonic, saturnine, scapegoat, sedulous, sententious, serendipity, sibylline, sobriquet, solipsism, specious, sporadic, stolid, strident, sublimate, subterfuge, succinct, supercilious, superfluous, surreptitious, sycophant, taciturn, tangential, temerity, tenuous, tirade, tractable, transient, trenchant, trepidation, ubiquitous, unctuous, unfathomable, unmitigated, uxorious, vacillate, venerate, veracity, verbose, vestige, vexatious, viable, vicissitude, vindictive, visage, volatile, voracious, waver, whimsical, zealous'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520df8a-58c8-4a4a-b4ed-9b6311047dfb",
   "metadata": {},
   "source": [
    "Reformat the list of words as a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d3c4248-f58f-4c90-bff8-9fd1d5ce573b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_words = [word.strip() for word in vocab_words.text.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9b3b5f1e-f9f5-4082-99a8-662418c35a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abrogate',\n",
       " 'acquiesce',\n",
       " 'adjudicate',\n",
       " 'aggregate',\n",
       " 'alleviate',\n",
       " 'allocate',\n",
       " 'ambivalent',\n",
       " 'ameliorate',\n",
       " 'amicable',\n",
       " 'amortize',\n",
       " '... (263 more words)',\n",
       " 'vexatious',\n",
       " 'viable',\n",
       " 'vicissitude',\n",
       " 'vindictive',\n",
       " 'visage',\n",
       " 'volatile',\n",
       " 'voracious',\n",
       " 'waver',\n",
       " 'whimsical',\n",
       " 'zealous']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words[0:10] + [f'... ({len(vocab_words) - 20} more words)'] + vocab_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e7f78-ace6-4c98-9f84-49305e5d8c6a",
   "metadata": {},
   "source": [
    "### Get Word Definitions - With PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c049779-b93e-421a-bed0-55bc254a24da",
   "metadata": {},
   "source": [
    "Connect to the Palm Model API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c7aad09-fdf0-4283-bf6c-47a0deaef032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "palm_model = vertexai.language_models.TextGenerationModel.from_pretrained(\"text-bison@002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f4132-48de-480a-b44b-52283345bb66",
   "metadata": {},
   "source": [
    "Request a definition for the first vocabulary word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6ac167ac-8428-43cf-97d6-820ab8b89586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " **Mnemonic:** \n",
       "Imagine a person trying to \"abrogate\" or \"ab-rogate\" a rug from under someone's feet. The rug is suddenly pulled out, causing the person to fall.\n",
       "\n",
       "**Definition:**\n",
       "To repeal or annul a law, treaty, or agreement."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_model.predict(prompt = f'Describe the word {vocab_words[0]} in a way that will make it easy to remember.  Then, provide a definition of the word.', max_output_tokens = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8473d35-98c1-4b62-aafc-b494f1ffa16f",
   "metadata": {},
   "source": [
    "### Definitions For Many Words\n",
    "\n",
    "What if the tasks changes to needing to make multiple calls, like requesting the definition for many words.  If the list is short or timing is not important then doing synchronous, one at a time, calls may work.  In the following example the `predict_streaming()` method is so that results appear as they are generated by the API.\n",
    "- [Streaming text generation](https://cloud.google.com/vertex-ai/docs/generative-ai/sdk-for-llm/sdk-use-text-models#stream-text-generation-sdk)\n",
    "- [`.predict_streaming()` method](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel#vertexai_language_models_TextGenerationModel_predict_streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0386cc0c-2d41-459b-b687-794c1cef027f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for abrogate:\n",
      " **Mnemonic:** \n",
      "Imagine a person trying to \"abrogate\" or \"break away\" from\n",
      " a rope that is tying them down.\n",
      "\n",
      "**Definition:**\n",
      "To repeal or annul a law,\n",
      " treaty, or agreement.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for acquiesce:\n",
      " **Mnemonic:** \n",
      "*Acqui*esce sounds like \"I *acqu*ire peace\n",
      ".\"\n",
      "\n",
      "**Definition:** \n",
      "To comply without protest; to agree or consent, usually reluctantly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for adjudicate:\n",
      " **Adjudicate** can be remembered as \"**Ad**vocate **Jud**ge **\n",
      "Cat**e**.\"\n",
      "\n",
      "**Definition**: To make an official decision about who is right in a\n",
      " dispute or competition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for aggregate:\n",
      " **Aggregate**\n",
      "\n",
      "Imagine a large pile of sand. Each grain of sand is an individual particle,\n",
      " but when you look at the pile as a whole, you see an aggregate of sand.\n",
      "\n",
      "**\n",
      "Definition**\n",
      "\n",
      "Aggregate means a mass or collection of things brought together or accumulated.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for alleviate:\n",
      " **Alleviate** can be remembered by thinking of the word \"elevator.\" Just as an elevator alle\n",
      "viates the need to climb stairs, the word alleviate means to make something easier or less difficult.\n",
      "\n",
      "\n",
      "**Definition**: To make something easier or less difficult; to relieve or lessen.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for word in vocab_words[0:5]:\n",
    "    print(f'Results for {word}:')\n",
    "    for r in palm_model.predict_streaming(\n",
    "        prompt = f'Describe the word {word} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "        max_output_tokens = 500\n",
    "    ):\n",
    "        print(r)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d0a3c-5dd2-49a9-ae76-afda540c62e9",
   "metadata": {},
   "source": [
    "## Asynchronous Use of APIs - Using Vertex AI Generative AI Models\n",
    "\n",
    "To request the definition for all words in the vocabularly list it will be beneficial to make request asynchronously - at the same time.  Some APIs have separate clients for asynchronous requests.  In the case of the PaLM model APIs there is actually a helpful asynchronous method provided `.predict_async()`.\n",
    "- [`.predict_async()` method](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel#vertexai_language_models_TextGenerationModel_predict_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ac9af-d273-47ed-bb5a-9e7fc3139681",
   "metadata": {},
   "source": [
    "### What Exactly is Async?\n",
    "\n",
    "If we make a request with the async method the response is a [coroutine](https://docs.python.org/3/glossary.html#term-coroutine) object.  This means the method is already implemented with an `async def` statement which makes it [awaitable](https://docs.python.org/3/library/asyncio-task.html#awaitables).\n",
    "\n",
    "The following cells show using the method with, and without, an await expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f94da73b-c7c6-45c3-968e-e6af6a0c103f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object _TextGenerationModel.predict_async at 0x7fc844694f20>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_model.predict_async(\n",
    "    prompt = f'Describe the word {vocab_words[0]} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "    max_output_tokens = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9e59fb58-f2cc-417d-9e60-35f86a4b8a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " **Mnemonic:** \n",
       "Imagine a person trying to \"abrogate\" or \"break away\" from a rope that is tying them down.\n",
       "\n",
       "**Definition:**\n",
       "To repeal or annul a law, treaty, or agreement."
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await palm_model.predict_async(\n",
    "    prompt = f'Describe the word {vocab_words[0]} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "    max_output_tokens = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc99c4-9b71-4551-bac8-eaf9803d2f86",
   "metadata": {},
   "source": [
    "### How To Use Async Concurrently\n",
    "\n",
    "The previous section showed that the `predict_async()` method returns a coroutine, which is an awaitable object.  When multiple coroutines are grouped together they can be awaited together - concurrently.\n",
    "\n",
    "To group the coroutines together use [asyncio.gather()](https://docs.python.org/3/library/asyncio-task.html#running-tasks-concurrently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5579c38b-75b1-4552-bc88-6279c9c7a750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = asyncio.gather(*[\n",
    "    palm_model.predict_async(\n",
    "        prompt = f'Describe the word {word} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "        max_output_tokens = 500\n",
    "    ) for word in vocab_words[0:5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "949d99f0-5b8c-41e3-bbe2-8447c1393ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asyncio.tasks._GatheringFuture"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5b7ef-ec7a-48db-947d-423d632af8c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "To make the requests concurrent, `await` the coroutine grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fda8e0f5-f1e1-48db-8493-abb306c10420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = await asyncio.gather(*[\n",
    "    palm_model.predict_async(\n",
    "        prompt = f'Describe the word {word} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "        max_output_tokens = 500\n",
    "    ) for word in vocab_words[0:5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5b7c44b3-a500-44b9-a183-e00539ff044e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 5)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "87a66b1a-0125-4154-af49-c0dc6b49da89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **Mnemonic:** \n",
      "Imagine a person trying to \"abrogate\" or \"break away\" from a rope that is tying them down.\n",
      "\n",
      "**Definition:**\n",
      "To repeal or annul a law, treaty, or agreement.\n",
      "----------------------------------------------------------------------------------------------------\n",
      " **Mnemonic:** \n",
      "*Acqui*esce sounds like \"I'm *acquiring* peace.\"\n",
      "\n",
      "**Definition:** \n",
      "To comply without protest; to agree or consent, usually reluctantly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      " **Adjudicate** can be remembered as \"**Ad**vocate **Jud**ge **Cat**e**.\"\n",
      "\n",
      "**Definition**: To make an official decision about who is right in a dispute or competition.\n",
      "----------------------------------------------------------------------------------------------------\n",
      " **Aggregate**\n",
      "\n",
      "Imagine a large pile of sand. Each grain of sand is an individual particle, but when you look at the pile as a whole, you see an aggregate of sand.\n",
      "\n",
      "**Definition**\n",
      "\n",
      "Aggregate means a mass or collection of things brought together or accumulated.\n",
      "----------------------------------------------------------------------------------------------------\n",
      " **Alleviate** can be remembered by thinking of the word \"elevator.\" Just as an elevator alleviates the need to climb stairs, the word alleviate means to make something easier or less difficult.\n",
      "\n",
      "**Definition**: To make something easier or less difficult; to relieve or lessen.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for response in responses:\n",
    "    print(response.text)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6104f7-3d52-4574-a911-6c8a25230fbb",
   "metadata": {},
   "source": [
    "## Managing Concurrency\n",
    "\n",
    "In some cases, doing all the tasks concurrently can work. Usually, there are limitations though. Waiting on a API to respond does not put a burden on the local compute so managing lots of requests may not be an issue on the client side.  It can still be helpful to limits to concurrency for managing the requests.  A first step to limiting concurrency is using a tool like [asyncio.Semaphore](https://docs.python.org/3/library/asyncio-sync.html#semaphore) to managed a counter of current concurrent requests.\n",
    "\n",
    "The following builds a function that managed the full list of request and uses a semaphore to control the concurrency.  Think of this as the currency buffer limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a44af4bd-4885-4dc3-bc0e-29ef1767970a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def study_notes(instances, limit_concur_requests = 10):\n",
    "    limit = asyncio.Semaphore(limit_concur_requests)\n",
    "    results = [None] * len(instances)\n",
    "    \n",
    "    # make requests\n",
    "    async def make_request(p):\n",
    "        async with limit:\n",
    "            if limit.locked():\n",
    "                await asyncio.sleep(.01)\n",
    "            result = await palm_model.predict_async(\n",
    "                                prompt = f'Describe the word {instances[p]} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "                                max_output_tokens = 500\n",
    "                            )\n",
    "        results[p] = (instances[p], result.text)\n",
    "        \n",
    "    # manage tasks\n",
    "    tasks = [asyncio.create_task(make_request(p)) for p in range(len(instances))]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fd1fbbd2-6791-4e55-bbd9-51de77d51658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = await study_notes(vocab_words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fa073246-055a-4d39-ab00-2fb724bd8202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, tuple, 20)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), type(responses[0]), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "794c12e3-2ded-406e-b8e9-de9ec06dbb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascertain\n",
      " **Mnemonic:** \"**A** **S**ail **C**rawls **E**very **R**ock **T**o **A**void **I**njury **N**ow\"\n",
      "\n",
      "**Definition:** To find out or determine with certainty; to establish as a fact.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1][0])\n",
    "print(responses[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a66a70-f3f7-41fd-ac0b-13623c324d9f",
   "metadata": {},
   "source": [
    "## Managing Concurrency - With Limits\n",
    "\n",
    "Just managing the concurrency may not be enough.  In cases where API have limits the total requests need to stay under these limits to prevent errors. In the case of this example, the PaLM model is limited by request per minute.  The default per project is 60 request per minute for the model used here ('text-bison@002').  See [Quotas and limits](https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai).\n",
    "\n",
    "The following modifies the previous function to also incorporate a time based limit for requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fa11dc3c-94b5-4ab4-924e-b50f7c9d1e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def study_notes(instances, limit_concur_requests = 10, limit_per_minute = 60):\n",
    "    limit = asyncio.Semaphore(limit_concur_requests)\n",
    "    results = [None] * len(instances)\n",
    "    \n",
    "    # make requests\n",
    "    async def make_request(p):\n",
    "        \n",
    "        # pause for time based limit\n",
    "        if p >= limit_per_minute:\n",
    "            await asyncio.sleep(60 * (p // limit_per_minute))\n",
    "        \n",
    "        async with limit:\n",
    "            if limit.locked():\n",
    "                await asyncio.sleep(.01)\n",
    "            result = await palm_model.predict_async(\n",
    "                                prompt = f'Describe the word {instances[p]} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "                                max_output_tokens = 500\n",
    "                            )\n",
    "        results[p] = (instances[p], result.text)\n",
    "        \n",
    "    # manage tasks\n",
    "    tasks = [asyncio.create_task(make_request(p)) for p in range(len(instances))]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5cb0c5-4032-4338-b27e-05d4be1fa7e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try the function under the limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6ceec214-b836-4ce1-a2ff-d84360661c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = await study_notes(vocab_words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "48e5c54b-63fa-4ba9-b9de-d2a84c43504b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 20)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4a1d74be-79c6-4d2d-8897-8939f7b3651f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascertain\n",
      " **Mnemonic:** \"**A** **S**ure **C**ertain**\"\n",
      "\n",
      "**Definition:** To find out or determine with certainty; to establish as a fact.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1][0])\n",
    "print(responses[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef42f0-507e-49ff-a90f-5bbe03ff8b52",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try the function just over the limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bd2df1bd-bdb6-4169-8afd-9bc91bbca888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait a minute for the qouta to clear - assumes no other activity in the project\n",
    "await asyncio.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "08559b45-6ee0-45bf-9b31-3f57568fbde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = await study_notes(vocab_words[0:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "22ceb0ca-85c2-4f76-bc36-d069991a323f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 20)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c9e8651a-ded5-4f34-b005-8866b50435ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discerning\n",
      " **Mnemonic:** A discerning person is like a detective, carefully examining evidence and making thoughtful judgments.\n",
      "\n",
      "**Definition:** Having or showing good judgment; able to make careful distinctions.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1][0])\n",
    "print(responses[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0274f-8816-468a-b1bf-ec53cddff3ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try the function at triple the limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0627dee1-4caa-45ab-92b4-cf950a54a6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait a minute for the qouta to clear - assumes no other activity in the project\n",
    "await asyncio.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bdea97d8-3777-4d80-a6fa-f8ce233a743d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = await study_notes(vocab_words[0:180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8b51bbd9-5e4a-4cfd-9bce-92e14c8ab76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 180)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ea3489db-6012-45bc-b4c5-dfbcdc1648ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obviate\n",
      " **Mnemonic:** Obviate is like \"obliterate\" - it means to do away with something completely.\n",
      "\n",
      "**Definition:** To make something unnecessary or no longer needed; to do away with.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1][0])\n",
    "print(responses[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c656a9-7678-4591-b73e-a9e4217a922f",
   "metadata": {},
   "source": [
    "## Managing Concurrency - With Limits And Error Handling\n",
    "\n",
    "Sometimes handling concurrency and limits is still not enough.  For example, in a shared enviornment it may not be possible to know how many other applications are making requesst in the same time frame.  In some cases clients have retry methods built in.  In other cases errors are returned and the calling application has to handle them.\n",
    "\n",
    "The following with futher modify the function to handle error responses by retrying and increasing time increment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e99c3f-04f7-4461-a841-b226aa8feca1",
   "metadata": {},
   "source": [
    "First, force an error by exceeding the limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4cf4ce6f-7f3c-46b4-a323-4935e349edd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAioRpcError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers_async.py:85\u001b[0m, in \u001b[0;36m_WrappedUnaryResponseMixin.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__await__\u001b[39m()\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grpc/aio/_call.py:318\u001b[0m, in \u001b[0;36m_UnaryResponseMixin.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m _create_rpc_error(\n\u001b[1;32m    319\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_call\u001b[38;5;241m.\u001b[39m_initial_metadata,\n\u001b[1;32m    320\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_call\u001b[38;5;241m.\u001b[39m_status,\n\u001b[1;32m    321\u001b[0m         )\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAioRpcError\u001b[0m: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:173.194.192.95:443 {created_time:\"2024-01-03T15:01:24.659775531+00:00\", grpc_status:8, grpc_message:\"Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# setting the limit_per_minute to 80, higher than the actual limit of 60\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m study_notes(vocab_words[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m80\u001b[39m], limit_per_minute \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[186], line 23\u001b[0m, in \u001b[0;36mstudy_notes\u001b[0;34m(instances, limit_concur_requests, limit_per_minute)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# manage tasks\u001b[39;00m\n\u001b[1;32m     22\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [asyncio\u001b[38;5;241m.\u001b[39mcreate_task(make_request(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(instances))]\n\u001b[0;32m---> 23\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[0;32mIn[186], line 15\u001b[0m, in \u001b[0;36mstudy_notes.<locals>.make_request\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m limit\u001b[38;5;241m.\u001b[39mlocked():\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m.01\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m palm_model\u001b[38;5;241m.\u001b[39mpredict_async(\n\u001b[1;32m     16\u001b[0m                         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescribe the word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstances[p]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in a way that will make it easy to remember.  Then, provide a definition of the word.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m                         max_output_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     18\u001b[0m                     )\n\u001b[1;32m     19\u001b[0m results[p] \u001b[38;5;241m=\u001b[39m (instances[p], result\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:1113\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict_async\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences, candidate_count, grounding_source, logprobs, presence_penalty, frequency_penalty, logit_bias)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \n\u001b[1;32m   1065\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m prediction_request \u001b[38;5;241m=\u001b[39m _create_text_generation_prediction_request(\n\u001b[1;32m   1099\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1100\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39mmax_output_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1111\u001b[0m )\n\u001b[0;32m-> 1113\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint\u001b[38;5;241m.\u001b[39mpredict_async(\n\u001b[1;32m   1114\u001b[0m     instances\u001b[38;5;241m=\u001b[39m[prediction_request\u001b[38;5;241m.\u001b[39minstance],\n\u001b[1;32m   1115\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mprediction_request\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m   1116\u001b[0m )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse_text_generation_model_multi_candidate_response(\n\u001b[1;32m   1119\u001b[0m     prediction_response\n\u001b[1;32m   1120\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1638\u001b[0m, in \u001b[0;36mEndpoint.predict_async\u001b[0;34m(self, instances, parameters, timeout)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make an asynchronous prediction against this Endpoint.\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;124;03mExample usage:\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;124;03m        Prediction with returned predictions and Model ID.\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m-> 1638\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prediction_async_client\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m   1639\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1640\u001b[0m     instances\u001b[38;5;241m=\u001b[39minstances,\n\u001b[1;32m   1641\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   1642\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1643\u001b[0m )\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1646\u001b[0m     predictions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   1647\u001b[0m         json_format\u001b[38;5;241m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     model_resource_name\u001b[38;5;241m=\u001b[39mprediction_response\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   1653\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/async_client.py:360\u001b[0m, in \u001b[0;36mPredictionServiceAsyncClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    355\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    356\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mendpoint),)),\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rpc(\n\u001b[1;32m    361\u001b[0m     request,\n\u001b[1;32m    362\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m    363\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    364\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    365\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers_async.py:88\u001b[0m, in \u001b[0;36m_WrappedUnaryResponseMixin.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrpc_error\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Quota exceeded for aiplatform.googleapis.com/online_prediction_requests_per_base_model with base model: text-bison. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/quotas."
     ]
    }
   ],
   "source": [
    "# setting the limit_per_minute to 80, higher than the actual limit of 60\n",
    "responses = await study_notes(vocab_words[0:80], limit_per_minute = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5dbd5-e85b-471b-b674-8acdfc56714c",
   "metadata": {},
   "source": [
    "Now, Modify the function to capture the error and retry with incrementing wait times.  The method used below does two things:\n",
    "- sets a limit on the retries, 20 in this case\n",
    "- increments the wait time for each retry, exponential backoff in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b58bb0d7-cedf-4d1c-8aef-80637acf3ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def study_notes(instances, limit_concur_requests = 10, limit_per_minute = 60):\n",
    "    limit = asyncio.Semaphore(limit_concur_requests)\n",
    "    results = [None] * len(instances)\n",
    "    \n",
    "    # make requests\n",
    "    async def make_request(p):\n",
    "        \n",
    "        # pause for time based limit\n",
    "        if p >= limit_per_minute:\n",
    "            await asyncio.sleep(60 * (p // limit_per_minute))\n",
    "        \n",
    "        async with limit:\n",
    "            if limit.locked():\n",
    "                await asyncio.sleep(.01)\n",
    "            ########## ERROR HANDLING ##################################\n",
    "            fail_count = 0\n",
    "            while fail_count <= 20:\n",
    "                try:\n",
    "                    result = await palm_model.predict_async(\n",
    "                                        prompt = f'Describe the word {instances[p]} in a way that will make it easy to remember.  Then, provide a definition of the word.',\n",
    "                                        max_output_tokens = 500\n",
    "                                    )\n",
    "                    if fail_count > 0:\n",
    "                        print(f'Item {p} succeed after fail count = {fail_count}')\n",
    "                    break\n",
    "                except:\n",
    "                    fail_count += 1\n",
    "                    print(f'Item {p} failed: current fail count = {fail_count}')\n",
    "                    await asyncio.sleep(2^(min(counter, 6) - 1))\n",
    "            ############################################################\n",
    "        results[p] = (instances[p], result.text)\n",
    "        \n",
    "    # manage tasks\n",
    "    tasks = [asyncio.create_task(make_request(p)) for p in range(len(instances))]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f579a5f-a97d-4e63-904b-74316aab9f26",
   "metadata": {},
   "source": [
    "Try 200 words with the correct limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "40394e47-ce7a-4660-955f-9f07caca2998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait a minute for the qouta to clear - assumes no other activity in the project\n",
    "await asyncio.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b4b73286-3617-450a-8bad-104008cdd63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = await study_notes(vocab_words[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e7bc2d06-cc7f-41d7-a64d-b89744f61d73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 200)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f599d83e-9323-4bcc-8286-ac59991f7ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polemic\n",
      " **Polemic:** \n",
      "\n",
      "Imagine a heated debate or argument where people are passionately defending their viewpoints. The word \"polemic\" comes from the Greek word \"polemos,\" which means \"war\" or \"battle.\" Just like in a battle, a polemic is a controversial or provocative statement that often sparks intense debate and discussion.\n",
      "\n",
      "**Definition:** \n",
      "\n",
      "A polemic is a strongly worded or controversial statement that is intended to provoke debate or argument. It is often used in political, religious, or social contexts to express strong opinions or challenge opposing viewpoints. Polemics are typically characterized by their passionate and persuasive tone, and they can be highly influential in shaping public opinion or promoting certain ideologies.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1][0])\n",
    "print(responses[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27781bf-2aba-4a0f-89b8-f7e23c2487b9",
   "metadata": {},
   "source": [
    "Now, try 200 words but force errors by setting the limit higher than the actual (60):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "fd2d2633-c308-4166-900c-84ccc707083e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait a minute for the qouta to clear - assumes no other activity in the project\n",
    "await asyncio.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1a653644-6f90-41d2-8ce2-eaa42df4cdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 75 failed: current fail count = 1\n",
      "Item 76 failed: current fail count = 1\n",
      "Item 77 failed: current fail count = 1\n",
      "Item 79 failed: current fail count = 1\n",
      "Item 75 failed: current fail count = 2\n",
      "Item 76 failed: current fail count = 2\n",
      "Item 77 failed: current fail count = 2\n",
      "Item 79 failed: current fail count = 2\n",
      "Item 75 failed: current fail count = 3\n",
      "Item 76 failed: current fail count = 3\n",
      "Item 77 failed: current fail count = 3\n",
      "Item 79 failed: current fail count = 3\n",
      "Item 75 failed: current fail count = 4\n",
      "Item 76 failed: current fail count = 4\n",
      "Item 77 failed: current fail count = 4\n",
      "Item 79 failed: current fail count = 4\n",
      "Item 75 failed: current fail count = 5\n",
      "Item 76 failed: current fail count = 5\n",
      "Item 77 failed: current fail count = 5\n",
      "Item 79 succeed after fail count = 4\n",
      "Item 76 succeed after fail count = 5\n",
      "Item 75 succeed after fail count = 5\n",
      "Item 77 succeed after fail count = 5\n",
      "Item 126 failed: current fail count = 1\n",
      "Item 127 failed: current fail count = 1\n",
      "Item 139 failed: current fail count = 1\n",
      "Item 140 failed: current fail count = 1\n",
      "Item 141 failed: current fail count = 1\n",
      "Item 142 failed: current fail count = 1\n",
      "Item 143 failed: current fail count = 1\n",
      "Item 144 failed: current fail count = 1\n",
      "Item 145 failed: current fail count = 1\n",
      "Item 146 failed: current fail count = 1\n",
      "Item 126 failed: current fail count = 2\n",
      "Item 127 failed: current fail count = 2\n",
      "Item 139 failed: current fail count = 2\n",
      "Item 140 failed: current fail count = 2\n",
      "Item 141 failed: current fail count = 2\n",
      "Item 142 failed: current fail count = 2\n",
      "Item 143 failed: current fail count = 2\n",
      "Item 144 failed: current fail count = 2\n",
      "Item 145 failed: current fail count = 2\n",
      "Item 146 failed: current fail count = 2\n",
      "Item 126 failed: current fail count = 3\n",
      "Item 127 failed: current fail count = 3\n",
      "Item 139 failed: current fail count = 3\n",
      "Item 140 failed: current fail count = 3\n",
      "Item 141 failed: current fail count = 3\n",
      "Item 142 failed: current fail count = 3\n",
      "Item 143 succeed after fail count = 2\n",
      "Item 145 succeed after fail count = 2\n",
      "Item 144 succeed after fail count = 2\n",
      "Item 146 succeed after fail count = 2\n",
      "Item 126 succeed after fail count = 3\n",
      "Item 127 succeed after fail count = 3\n",
      "Item 139 succeed after fail count = 3\n",
      "Item 140 succeed after fail count = 3\n",
      "Item 142 succeed after fail count = 3\n",
      "Item 141 succeed after fail count = 3\n",
      "Item 190 failed: current fail count = 1\n",
      "Item 191 failed: current fail count = 1\n",
      "Item 192 failed: current fail count = 1\n",
      "Item 191 failed: current fail count = 2\n",
      "Item 192 failed: current fail count = 2\n",
      "Item 190 succeed after fail count = 1\n",
      "Item 191 failed: current fail count = 3\n",
      "Item 192 failed: current fail count = 3\n",
      "Item 191 failed: current fail count = 4\n",
      "Item 192 failed: current fail count = 4\n",
      "Item 192 failed: current fail count = 5\n",
      "Item 191 failed: current fail count = 5\n",
      "Item 191 succeed after fail count = 5\n",
      "Item 192 succeed after fail count = 5\n"
     ]
    }
   ],
   "source": [
    "# setting the limit_per_minute to 80, higher than the actual limit of 60\n",
    "responses = await study_notes(vocab_words[0:200], limit_per_minute = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9b2abe8b-74d9-48b7-96ba-7d31ad2b7162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 200)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(responses), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "914d0f11-b282-470a-8597-29da9afe4568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polemic\n",
      " **Polemic:** \n",
      "\n",
      "Imagine a heated debate or argument where people are passionately defending their viewpoints. The word \"polemic\" comes from the Greek word \"polemos,\" which means \"war\" or \"battle.\" Just like in a battle, a polemic is a controversial or provocative statement that often sparks heated debate and discussion.\n",
      "\n",
      "**Definition:** \n",
      "\n",
      "A polemic is a strongly worded or controversial statement that is intended to provoke debate or argument. It is often used in political, religious, or social contexts to express strong opinions or challenge opposing viewpoints. Polemics are typically characterized by their passionate and persuasive tone, and they often aim to influence or persuade the reader to adopt a particular point of view.\n"
     ]
    }
   ],
   "source": [
    "print(responses[-1][0])\n",
    "print(responses[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb282bfd-3402-4b01-aa37-57f68beb5665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
