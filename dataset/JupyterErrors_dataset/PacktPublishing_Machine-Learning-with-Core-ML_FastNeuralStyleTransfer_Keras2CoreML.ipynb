{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Machine Learning with CoreML](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-core-ml)\n",
    "**By:** Joshua Newnham (Author)  \n",
    "**Publisher:** [Packt Publishing](https://www.packtpub.com/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 - Fast Neural Style Transfer \n",
    "In this notebook we will look at converting our Fast Neural Style Transfer keras model to Core ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: To run locally;** create the environment from the *coreml27_environment.yml* file in this directory. Details of how to do this can be found [here](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers \n",
    "reload(helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the Fast Neural Style Transfer Model passing in the style image used in training the image.  \n",
    "**Note: This may take sometime as it has to download the VGG weights** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Van_Gogh-Starry_Night.jpg\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = helpers.build_model('images/Van_Gogh-Starry_Night.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load our weights; these are the weights that will transform our content image into a stylised version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('data/van-gogh-starry-night_style.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print out the architecture of the model (/network); understanding the details in it's entirety is out of scope for this book but it's please take time to scan through it noting down units of of type Lamdba e.g. *res_crop_1 (Lambda)*,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 320, 320, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 400, 400, 3)   0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 400, 400, 64)  15616       zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 400, 400, 64)  256         conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 400, 400, 64)  0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 200, 200, 64)  36928       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 200, 200, 64)  256         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 200, 200, 64)  0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 100, 100, 64)  36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 100, 100, 64)  256         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 100, 100, 64)  0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 98, 98, 64)    36928       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 98, 98, 64)    256         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 98, 98, 64)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 96, 96, 64)    36928       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 96, 96, 64)    256         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "res_crop_0 (Lambda)              (None, 96, 96, 64)    0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 96, 96, 64)    0           batch_normalization_5[0][0]      \n",
      "                                                                   res_crop_0[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 94, 94, 64)    36928       add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 94, 94, 64)    256         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 94, 94, 64)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 92, 92, 64)    36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 92, 92, 64)    256         conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "res_crop_1 (Lambda)              (None, 92, 92, 64)    0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 92, 92, 64)    0           batch_normalization_7[0][0]      \n",
      "                                                                   res_crop_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 90, 90, 64)    36928       add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 90, 90, 64)    256         conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 90, 90, 64)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 88, 88, 64)    36928       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 88, 88, 64)    256         conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "res_crop_2 (Lambda)              (None, 88, 88, 64)    0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 88, 88, 64)    0           batch_normalization_9[0][0]      \n",
      "                                                                   res_crop_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 86, 86, 64)    36928       add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 86, 86, 64)    256         conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 86, 86, 64)    0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 84, 84, 64)    36928       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 84, 84, 64)    256         conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res_crop_3 (Lambda)              (None, 84, 84, 64)    0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 84, 84, 64)    0           batch_normalization_11[0][0]     \n",
      "                                                                   res_crop_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 82, 82, 64)    36928       add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 82, 82, 64)    256         conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 82, 82, 64)    0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 80, 80, 64)    36928       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 80, 80, 64)    256         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "res_crop_4 (Lambda)              (None, 80, 80, 64)    0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 80, 80, 64)    0           batch_normalization_13[0][0]     \n",
      "                                                                   res_crop_4[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)   (None, 160, 160, 64)  0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 160, 160, 64)  36928       up_sampling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 160, 160, 64)  256         conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 160, 160, 64)  0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)   (None, 320, 320, 64)  0           activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 320, 320, 64)  36928       up_sampling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 320, 320, 64)  256         conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 320, 320, 64)  0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 320, 320, 3)   15555       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "rescale_output (Lambda)          (None, 320, 320, 3)   0           conv2d_16[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 552,003\n",
      "Trainable params: 550,083\n",
      "Non-trainable params: 1,920\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for identifying these layers is that they are **not** supported in the standard set of models available in **CoreML Tools**. We have a couple of options of supporting them, such as extending modifying the CoreML Tools themselves or creating custom layers. In context of this chapter we are interested in the later (custom layers). So let's inspect each of the layers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### res_crop\n",
    "The first one we will look at is res_crop; part of the ResNet block that crops (as the name suggests) the output. The code is simple enough; let's have a look at it to see what it does (*nb; this is the function assigned to the Lamdba for the res_crop layer*). "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def res_crop(x):\n",
    "    return x[:, 2:-2, 2:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially all this is doing is cropping the outputs with a padding of 2 on for the width and height axis. We can further interogate this my inspecting the input and output shapes of the custom layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res_crop_3_layer input shape (None, 88, 88, 64), output shape (None, 84, 84, 64)\n"
     ]
    }
   ],
   "source": [
    "res_crop_3_layer = [layer for layer in model.layers if layer.name == 'res_crop_3'][0] \n",
    "\n",
    "print(\"res_crop_3_layer input shape {}, output shape {}\".format(\n",
    "    res_crop_3_layer.input_shape, res_crop_3_layer.output_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rescale_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next custom layer is used at the end of the network to rescale the outputs from the Convolution 2D layer that passes our data through a tanh activation. We can get a better feel what this layer does by inspecting the implementation. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def rescale_output(x):\n",
    "    return (x+1)*127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method performs an element-wise operation that adds one and scales it by 127.5 (the result of the *tanh* activaiton is that these values would be in a range of -1.0 - 1.0). Similar to above; let's inspect the input and output of this layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescale_output_layer input shape (None, 320, 320, 3), output shape (None, 320, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "rescale_output_layer = [layer for layer in model.layers if layer.name == 'rescale_output'][0]\n",
    "\n",
    "print(\"rescale_output_layer input shape {}, output shape {}\".format(\n",
    "    rescale_output_layer.input_shape, \n",
    "    rescale_output_layer.output_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Trained Models to Core ML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model now create and weights loaded; it's time to convert it to a CoreML model; first we need to ensure that the CoreMLTools are install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: coremltools in /Users/Joshua.Newnham/anaconda/envs/coreml27/lib/python2.7/site-packages (0.8)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /Users/Joshua.Newnham/anaconda/envs/coreml27/lib/python2.7/site-packages (from coremltools) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /Users/Joshua.Newnham/anaconda/envs/coreml27/lib/python2.7/site-packages (from coremltools) (1.14.2)\n",
      "Requirement already satisfied: six==1.10.0 in /Users/Joshua.Newnham/anaconda/envs/coreml27/lib/python2.7/site-packages (from coremltools) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /Users/Joshua.Newnham/anaconda/envs/coreml27/lib/python2.7/site-packages (from protobuf>=3.1.0->coremltools) (39.0.1)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    import coremltools\n",
    "    from coremltools.proto import NeuralNetwork_pb2, FeatureTypes_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we try to convert the model without acknowledging the custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Keras layer '<class 'keras.layers.core.Lambda'>' not supported. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ece391520df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_input_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     output_names=\"output\")\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/coreml27/lib/python2.7/site-packages/coremltools/converters/keras/_keras_converter.pyc\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model, input_names, output_names, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, model_precision, predicted_probabilities_output, add_custom_layers, custom_conversion_functions)\u001b[0m\n\u001b[1;32m    743\u001b[0m                       \u001b[0mpredicted_probabilities_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                       \u001b[0madd_custom_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                       custom_conversion_functions=custom_conversion_functions)\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_MLModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/coreml27/lib/python2.7/site-packages/coremltools/converters/keras/_keras_converter.pyc\u001b[0m in \u001b[0;36mconvertToSpec\u001b[0;34m(model, input_names, output_names, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, model_precision, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, custom_objects)\u001b[0m\n\u001b[1;32m    541\u001b[0m                                            \u001b[0madd_custom_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_custom_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                                            \u001b[0mcustom_conversion_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_conversion_functions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                                            custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    544\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         raise RuntimeError(\n",
      "\u001b[0;32m/anaconda3/envs/coreml27/lib/python2.7/site-packages/coremltools/converters/keras/_keras2_converter.pyc\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(model, input_names, output_names, image_input_names, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, custom_objects)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Check valid versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0m_check_unsupported_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_custom_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# Build network graph to represent Keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/coreml27/lib/python2.7/site-packages/coremltools/converters/keras/_keras2_converter.pyc\u001b[0m in \u001b[0;36m_check_unsupported_layers\u001b[0;34m(model, add_custom_layers)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_KERAS_LAYER_REGISTRY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keras layer '%s' not supported. \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_KERAS_LAYER_REGISTRY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Keras layer '<class 'keras.layers.core.Lambda'>' not supported. "
     ]
    }
   ],
   "source": [
    "coreml_model = coremltools.converters.keras.convert(\n",
    "    model, \n",
    "    input_names=['image'], \n",
    "    image_input_names=['image'], \n",
    "    output_names=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected; the method failed returning the error ValueError: Keras layer '' not supported.; let's now proceed to adding support for custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function that will be passed to the **convert** method that will be used to create the layer parameters for our custom layers. Set set the property *className* to the custom class we will implement in Swift along describing it via the *description* property which will come through when inspecting the CoreML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lambda(layer):\n",
    "    if layer.function.__name__ == 'rescale_output':\n",
    "        params = NeuralNetwork_pb2.CustomLayerParams()\n",
    "\n",
    "        # The name of the Swift or Obj-C class that implements this layer.\n",
    "        params.className = \"RescaleOutputLambda\"\n",
    "\n",
    "        # The desciption is shown in Xcode's mlmodel viewer.\n",
    "        params.description = \"Rescale output to adjust for ReLu ((x+1)*127.5)\"\n",
    "\n",
    "        return params\n",
    "    elif layer.function.__name__ == 'res_crop':\n",
    "        params = NeuralNetwork_pb2.CustomLayerParams()\n",
    "\n",
    "        # The name of the Swift or Obj-C class that implements this layer.\n",
    "        params.className = \"ResCropBlockLambda\"\n",
    "\n",
    "        # The desciption is shown in Xcode's mlmodel viewer.\n",
    "        params.description = \"return x[:, 2:-2, 2:-2]\"\n",
    "\n",
    "        return params\n",
    "    else:\n",
    "        raise Exception('Unknown layer')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to convert our model; we set the parameter *add_custom_layers* to True so that the conversion process delegates unknown method to our *convert_lambda* function we defined above which we set via the parameter *custom_conversion_functions* which takes a dictionary where the key defines the custom layer name along with a reference to the function to handle creating the customer layer parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : input_1, <keras.engine.topology.InputLayer object at 0x111658890>\n",
      "1 : zero_padding2d_1, <keras.layers.convolutional.ZeroPadding2D object at 0x1115a8d90>\n",
      "2 : conv2d_1, <keras.layers.convolutional.Conv2D object at 0x104d42150>\n",
      "3 : batch_normalization_1, <keras.layers.normalization.BatchNormalization object at 0x112a87f10>\n",
      "4 : activation_1, <keras.layers.core.Activation object at 0x112b43890>\n",
      "5 : conv2d_2, <keras.layers.convolutional.Conv2D object at 0x112a71250>\n",
      "6 : batch_normalization_2, <keras.layers.normalization.BatchNormalization object at 0x112ad2050>\n",
      "7 : activation_2, <keras.layers.core.Activation object at 0x1115a8ed0>\n",
      "8 : conv2d_3, <keras.layers.convolutional.Conv2D object at 0x112c75110>\n",
      "9 : batch_normalization_3, <keras.layers.normalization.BatchNormalization object at 0x112bdad90>\n",
      "10 : activation_3, <keras.layers.core.Activation object at 0x112c65890>\n",
      "11 : conv2d_4, <keras.layers.convolutional.Conv2D object at 0x112d9a890>\n",
      "12 : batch_normalization_4, <keras.layers.normalization.BatchNormalization object at 0x112cd1f10>\n",
      "13 : activation_4, <keras.layers.core.Activation object at 0x112daa950>\n",
      "14 : conv2d_5, <keras.layers.convolutional.Conv2D object at 0x112ec2810>\n",
      "15 : batch_normalization_5, <keras.layers.normalization.BatchNormalization object at 0x112edf310>\n",
      "16 : res_crop_0, <keras.layers.core.Lambda object at 0x112eac910>\n",
      "17 : add_1, <keras.layers.merge.Add object at 0x112f2bb10>\n",
      "18 : conv2d_6, <keras.layers.convolutional.Conv2D object at 0x112fa2650>\n",
      "19 : batch_normalization_6, <keras.layers.normalization.BatchNormalization object at 0x112ff1ad0>\n",
      "20 : activation_5, <keras.layers.core.Activation object at 0x11308eb50>\n",
      "21 : conv2d_7, <keras.layers.convolutional.Conv2D object at 0x1130ab390>\n",
      "22 : batch_normalization_7, <keras.layers.normalization.BatchNormalization object at 0x1130c4390>\n",
      "23 : res_crop_1, <keras.layers.core.Lambda object at 0x113147450>\n",
      "24 : add_2, <keras.layers.merge.Add object at 0x1131d1e90>\n",
      "25 : conv2d_8, <keras.layers.convolutional.Conv2D object at 0x1131dd4d0>\n",
      "26 : batch_normalization_8, <keras.layers.normalization.BatchNormalization object at 0x113194410>\n",
      "27 : activation_6, <keras.layers.core.Activation object at 0x1133095d0>\n",
      "28 : conv2d_9, <keras.layers.convolutional.Conv2D object at 0x1132fbf10>\n",
      "29 : batch_normalization_9, <keras.layers.normalization.BatchNormalization object at 0x1132d9790>\n",
      "30 : res_crop_2, <keras.layers.core.Lambda object at 0x11336f650>\n",
      "31 : add_3, <keras.layers.merge.Add object at 0x113495650>\n",
      "32 : conv2d_10, <keras.layers.convolutional.Conv2D object at 0x11345ac10>\n",
      "33 : batch_normalization_10, <keras.layers.normalization.BatchNormalization object at 0x1134e8ed0>\n",
      "34 : activation_7, <keras.layers.core.Activation object at 0x113571c10>\n",
      "35 : conv2d_11, <keras.layers.convolutional.Conv2D object at 0x1135e6210>\n",
      "36 : batch_normalization_11, <keras.layers.normalization.BatchNormalization object at 0x113590c90>\n",
      "37 : res_crop_3, <keras.layers.core.Lambda object at 0x11364da90>\n",
      "38 : add_4, <keras.layers.merge.Add object at 0x1137ae410>\n",
      "39 : conv2d_12, <keras.layers.convolutional.Conv2D object at 0x113697c90>\n",
      "40 : batch_normalization_12, <keras.layers.normalization.BatchNormalization object at 0x1136d5090>\n",
      "41 : activation_8, <keras.layers.core.Activation object at 0x1137a04d0>\n",
      "42 : conv2d_13, <keras.layers.convolutional.Conv2D object at 0x1137ebe10>\n",
      "43 : batch_normalization_13, <keras.layers.normalization.BatchNormalization object at 0x1138288d0>\n",
      "44 : res_crop_4, <keras.layers.core.Lambda object at 0x113943f50>\n",
      "45 : add_5, <keras.layers.merge.Add object at 0x11395fdd0>\n",
      "46 : up_sampling2d_1, <keras.layers.convolutional.UpSampling2D object at 0x112cffb50>\n",
      "47 : conv2d_14, <keras.layers.convolutional.Conv2D object at 0x113a59910>\n",
      "48 : batch_normalization_14, <keras.layers.normalization.BatchNormalization object at 0x113a1a790>\n",
      "49 : activation_9, <keras.layers.core.Activation object at 0x113a66cd0>\n",
      "50 : up_sampling2d_2, <keras.layers.convolutional.UpSampling2D object at 0x113b5c8d0>\n",
      "51 : conv2d_15, <keras.layers.convolutional.Conv2D object at 0x113b3c510>\n",
      "52 : batch_normalization_15, <keras.layers.normalization.BatchNormalization object at 0x113a97290>\n",
      "53 : activation_10, <keras.layers.core.Activation object at 0x113bb9890>\n",
      "54 : conv2d_16, <keras.layers.convolutional.Conv2D object at 0x113ce2e50>\n",
      "55 : conv2d_16__activation__, <keras.layers.core.Activation object at 0x112016390>\n",
      "56 : rescale_output, <keras.layers.core.Lambda object at 0x113c36950>\n"
     ]
    }
   ],
   "source": [
    "coreml_model = coremltools.converters.keras.convert(\n",
    "    model, \n",
    "    input_names=['image'], \n",
    "    image_input_names=['image'], \n",
    "    output_names=\"output\",\n",
    "    add_custom_layers=True,\n",
    "    custom_conversion_functions={ \"Lambda\": convert_lambda })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add metadata to our model to help remind us (and anyone else who might use our model) the details of the mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.author = 'Joshua Newnham'\n",
    "coreml_model.license = 'BSD'\n",
    "coreml_model.short_description = 'FastStyle Transfer with style Van Gogh Starry Night'\n",
    "coreml_model.input_description['image'] = 'Preprocessed content image'\n",
    "coreml_model.output_description['output'] = 'Stylized content image'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the format of the output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stop here but if you were to save the model and import into XCode you will notice that the output is *MultiArrayType* and we would need to manually convert it to an image in Swift; it would be more convieient to have our CoreML model output a image we can simply use. In order to do this we will modify the output of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the spec of our model (the definition of the model that is used by \n",
    "# XCode when importing and serialising the model)\n",
    "spec = coreml_model.get_spec() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we get reference to the output from the specification (this is what we will be modifying)\n",
    "output = [output for output in spec.description.output if output.name == 'output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"output\"\n",
      "shortDescription: \"Stylized content image\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 3\n",
      "    shape: 320\n",
      "    shape: 320\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next check if we need to convert it \n",
    "if output.type.WhichOneof('Type') == 'multiArrayType':  \n",
    "    # Get the shape of the output \n",
    "    array_shape = tuple(output.type.multiArrayType.shape)  \n",
    "    channels, height, width = array_shape   # NB; we don't use channels here but you could generalise \n",
    "                                            # this to handle grayscale images \n",
    "    \n",
    "    # Next we set the imageType properties to indicate that we are expecting a image for this output \n",
    "    \n",
    "    # Update the colorspace of the output     \n",
    "    output.type.imageType.colorSpace = FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('RGB')  \n",
    "    \n",
    "    # Update the image dimensions \n",
    "    output.type.imageType.width = width  \n",
    "    output.type.imageType.height = height\n",
    "    \n",
    "    # Recreate the coreml using the new specs \n",
    "    coreml_model = coremltools.models.MLModel(spec)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's, again, get reference to the output from the specification to check inspect it's type \n",
    "output = [output for output in spec.description.output if output.name == 'output'][0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Core ML model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finall we can save it \n",
    "coreml_model.save('output/FastStyleTransferVanGoghStarryNight.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Converting Trained Models to Core ML](https://developer.apple.com/documentation/coreml/converting_trained_models_to_core_ml)\n",
    "- [CoreML Tools Documentation](https://apple.github.io/coremltools/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreml27",
   "language": "python",
   "name": "coreml27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
