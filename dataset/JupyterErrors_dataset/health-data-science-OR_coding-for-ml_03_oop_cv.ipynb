{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3102a4bd",
   "metadata": {},
   "source": [
    "# Case study: Cross-validation\n",
    "\n",
    "In supervised learning studies we distinguish between the training and test error rates. The test error rate refers to the average performance of a model when predicting a new observation (data that was not used in the training of the model).  A useful model is one that can accurately predict new observations!\n",
    "\n",
    "An issue in real ML studies is that data are expensive to collect and often a health data scientist doesn't have as much validation data as they would ideally like for estimating the test error rate.  Cross-validation is a general term to describe a set of statistical methods for efficiently splitting a dataset in order to estimating the test error of a model.\n",
    "\n",
    "Cross validation methods are a nice example of where python classes can be used to design a framework for your ML. In the case study we will explore creating two classes for cross validation in machine learning: **Leave-One-Out Cross-Validation** and **K-Fold Cross Validation.**\n",
    "\n",
    "> If you are interested to read further on cross validation I recommend the excellent: **An introduction to statistical learning (with applications in R)** by James, Witten, Hastie and Tibshirani. \n",
    "\n",
    "\n",
    ">>  In a real machine learning study in python there is no doubt that you would implement these classes using a `numpy` array.  In practice I would therefore implement these classes slightly differently. We will cover `numpy `in the next chapter.  Another bullet proof option, that requires no implementation, is to use the `sklearn.model_selection` namespace.  I've based the naming on sklearn's interface here.  After you have completed the case study do check out the `sklearn` source code that's [available via its documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut).\n",
    "\n",
    "We'll begin by creating each class independently.  Then we'll work on our OOP design credentials by extracting what can be encapsulated into a common baseclass.  Finally we will take a short look at what is meant by an **Abstract** class and how this is implemented in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8ec8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b166444-c0c3-417b-9f54-7574558c2e53",
   "metadata": {},
   "source": [
    "## Synthetic data\n",
    "\n",
    "The function below generates synthetic data that can be used to test the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c01810-41e1-460b-b8bd-f1d55767556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_classification(n_samples=10, n_features=1, shuffle=False, \n",
    "                             random_seed=None):\n",
    "    '''\n",
    "    Generates a simple random synthetic dataset in a given shape.\n",
    "    Used for testing of generator classes.\n",
    "    \n",
    "    X: each feature is a sequence i to i + n_samples where i is the feature no.\n",
    "    y data is 0 or 1 weighted very roughly 50/50.\n",
    "    \n",
    "    These sequences are randomised if shuffle is set to True.\n",
    "    \n",
    "    No error checking. Assumes all inputs are valid.\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    n_samples: int, optional (default=10)\n",
    "        The number of samples\n",
    "        \n",
    "    n_features: int, optional (default=1)\n",
    "        The number of features in the classification problem\n",
    "        \n",
    "    shuffle: bool, optional (default=False)\n",
    "        If true then sequences are randomly shuffled\n",
    "        \n",
    "    random_seed: int or None, optional (default=None)\n",
    "        If shuffle then controls the ordering of the sequences generated.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X, y\n",
    "    Where X and y are python lists and X will be a list of lists sized \n",
    "    (n_samples, n_features) \n",
    "        \n",
    "    '''\n",
    "    X = [[(col * (n_samples)) + row for col in range(n_features)] \n",
    "                                          for row in range(n_samples)]\n",
    "    y = ([1] * (n_samples // 2)) + ([0] * ((n_samples // 2) + (n_samples % 2)))\n",
    "    \n",
    "    if shuffle: \n",
    "        for lst in [X, y]:\n",
    "            random.seed(random_seed)\n",
    "            random.shuffle(lst)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fa862c-e683-493a-a677-c9b9835c96a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [2]]\n",
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_classification(n_samples=3)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc8705c-e88c-4c2a-9d85-7e6f84cbf396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 6], [1, 7], [2, 8], [3, 9], [4, 10], [5, 11]]\n",
      "[1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_classification(n_samples=6, n_features=2)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16738a7f-3a8e-4131-9d07-f1e2016ff51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 9], [1, 7], [2, 8], [4, 10], [0, 6], [5, 11]]\n",
      "[0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_classification(n_samples=6, n_features=2, shuffle=True, \n",
    "                                random_seed=42)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07717224-d9c3-4244-81b7-5ab2dfe998ef",
   "metadata": {},
   "source": [
    "## Leave one out cross validation\n",
    "\n",
    "As it's name suggests **Leave on out cross validation (LOOCV)** incrementally leaves one data point out of the training of a model.  It then predicts the single holdout sample.  This is repeated for every sample in the dataset.\n",
    "\n",
    "> I've rarely seen this approach used in practice and its not a method I would consider using myself.  The reason is that it requires a lot of training cycles.  Combine LOOCV with parameter optimisation, a moderate sized dataset and a computationally intensive training routine and you'll need to leave your code running overnight!  But its a nice simple example that we can use in our learning.\n",
    "\n",
    "Rather than perform both the generation of the splits **and** cross validation in the same class we will seperate their responsibilities into seperate classes.  We will have one class called `LeaveOneOut` and a function called `cross_validate`.  The function will accept an instance of `LeaveOneOut` and use it to generate the data needed for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c90450e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeaveOneOut:\n",
    "    '''\n",
    "    Leave one out dataset generator for cross validation.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'LeaveOneOut()'\n",
    "\n",
    "    def get_n_splits(self, X):\n",
    "        '''\n",
    "        The number of splits returned by the cross validation\n",
    "        method.\n",
    "        '''\n",
    "        return len(X)\n",
    "    \n",
    "    def split(self, X, y):\n",
    "        '''\n",
    "        Generator method.  Split the dataset\n",
    "        '''\n",
    "        for test_index in range(len(X)):\n",
    "        \n",
    "            # training data indexes\n",
    "            train_X = X[:test_index] + X[test_index + 1:]\n",
    "            train_y = y[:test_index] + y[test_index + 1:]\n",
    "            \n",
    "            # test data\n",
    "            test_X, test_y = X[test_index], y[test_index]\n",
    "            \n",
    "            yield train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977df80f-ba6e-4348-ac66-86804ee1338c",
   "metadata": {},
   "source": [
    "I will use the `synthetic_classification` to generate a test dataset for `LeaveOneOut`.  I will keep the inputs as a sequence so you can easily see it working.\n",
    "\n",
    "\n",
    "> Remember that `LeaveOneOut.split()` uses the **yield** keyword.  This means it is a **generator** method and we need to call it repeatedly in a loop in order to generate new splits for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca580e9-1be9-4dc7-98fd-27b5ef033355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Train:\tX:[[1, 7], [2, 8], [3, 9], [4, 10], [5, 11]], y:[1, 1, 0, 0, 0]\n",
      "Test:\tX:[0, 6], y:1\n",
      "Fold 2:\n",
      "Train:\tX:[[0, 6], [2, 8], [3, 9], [4, 10], [5, 11]], y:[1, 1, 0, 0, 0]\n",
      "Test:\tX:[1, 7], y:1\n",
      "Fold 3:\n",
      "Train:\tX:[[0, 6], [1, 7], [3, 9], [4, 10], [5, 11]], y:[1, 1, 0, 0, 0]\n",
      "Test:\tX:[2, 8], y:1\n",
      "Fold 4:\n",
      "Train:\tX:[[0, 6], [1, 7], [2, 8], [4, 10], [5, 11]], y:[1, 1, 1, 0, 0]\n",
      "Test:\tX:[3, 9], y:0\n",
      "Fold 5:\n",
      "Train:\tX:[[0, 6], [1, 7], [2, 8], [3, 9], [5, 11]], y:[1, 1, 1, 0, 0]\n",
      "Test:\tX:[4, 10], y:0\n",
      "Fold 6:\n",
      "Train:\tX:[[0, 6], [1, 7], [2, 8], [3, 9], [4, 10]], y:[1, 1, 1, 0, 0]\n",
      "Test:\tX:[5, 11], y:0\n"
     ]
    }
   ],
   "source": [
    "# generate test dataset\n",
    "X, y = synthetic_classification(n_samples=6, n_features=2, shuffle=False)\n",
    "\n",
    "# create an instance of LeaveOneOut\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "# basic cross validation loop.\n",
    "# I've zipped together a range and the splits into order to get fold no.\n",
    "for i, split_data in zip(range(cv.get_n_splits(X)), cv.split(X, y)):\n",
    "    train_X, train_y, test_X, test_y = split_data\n",
    "    print(f'Fold {i+1}:\\nTrain:\\tX:{train_X}, y:{train_y}')\n",
    "    print(f'Test:\\tX:{test_X}, y:{test_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46e708-ecd8-4a85-b107-701326944a11",
   "metadata": {},
   "source": [
    "Now let's create a function to perform the cross validation.\n",
    "\n",
    "> This could easily be another class.  But for simplicity I've used a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa2d1f5-9552-4447-a4f0-a55ec946523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, cv):\n",
    "    scores = []\n",
    "    print('split=> ', end='')\n",
    "    for i, split_data in zip(range(cv.get_n_splits(X)), cv.split(X, y)):\n",
    "        print(f'{i+1},', end=' ')\n",
    "        train_X, train_y, test_X, test_y = split_data\n",
    "        # model fitting, prediction and accuracy assesment is\n",
    "        # just simulated here.\n",
    "        scores.append(random.uniform(0.8, 1))\n",
    "    print('end')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2d7d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split=> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, end\n",
      "cv score: 0.90\n"
     ]
    }
   ],
   "source": [
    "X, y = X, y = synthetic_classification(n_samples=160)\n",
    "scores = cross_validate(X, y, LeaveOneOut())\n",
    "\n",
    "# print out fake accuracy!\n",
    "print(f'cv score: {sum(scores) / len(scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a9f29-dfe3-4faf-87ba-66481c6c6bff",
   "metadata": {},
   "source": [
    "## What about if we wanted to use a different cross validation method?\n",
    "\n",
    "The second cross validation method on the menu is **k-fold** cross validation where $k$ is typically set to 5 or 10. K-fold cross validation is therefore less computationally demanding than LOOCV. Here we will shuffle the dataset and then split it into k segments.  In each loop of our cross validation a model is trained on $k-1$ segments while the final one is held out for testing.\n",
    "\n",
    "We will implement `KFold` so that it has the same **interface** as `LeaveOneOut`.  This means it has the a set of method signatures and properties in common with `LeaveOneOut`.  It will have the following method signatures in common:\n",
    "\n",
    "* `get_n_splits(self, X)` - this is a generator \n",
    "* `split(self, X, y)` - returns an int\n",
    "\n",
    "### Implementing a `KFold` class\n",
    "\n",
    "The implementation of `KFold` is slightly more involved than LOOCV, but it is perfectly possible in standard python using either `for` loops or list comprehensions. I've opted for the latter as it allows a proportion of the logic to be expressed in a single (I would argue readable) line of python.\n",
    "\n",
    "First take a look at the class and then I'll spend a bit of time explaining how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547d1fe6-7ea9-4dc7-a21c-2042a1268f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFold:\n",
    "    '''\n",
    "    A generator class for k-fold cross validation.\n",
    "    '''\n",
    "    def __init__(self, k=5, shuffle=False, random_seed=None):\n",
    "        self.k = k\n",
    "        self.random_seed = random_seed\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'KFold({self.k=}, {self.shuffle=}, {self.random_seed=})'\n",
    "    \n",
    "    \n",
    "    def get_n_splits(self, X):\n",
    "        '''\n",
    "        Return an integer representing the number of splits that \n",
    "        will be generated.\n",
    "    \n",
    "        '''\n",
    "        return self.k\n",
    "    \n",
    "    def split(self, X, y):\n",
    "        '''\n",
    "        Generator method.  Returns incremental splits of the dataset\n",
    "        on each call.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        X: list\n",
    "            python list containing X data. For multiple features\n",
    "            shape should be (n_samples, n_features)\n",
    "        \n",
    "        y: list\n",
    "            python list containing y target data. For multiple \n",
    "            targets shape should be (n_samples, n_targets)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        train_X, test_X, train_y, test_y \n",
    "        \n",
    "        Where each is a python list\n",
    "        '''\n",
    "        \n",
    "        # store the indexes of each element \n",
    "        idx = [i for i in range(len(X))]\n",
    "        if self.shuffle:\n",
    "            random.seed(self.random_seed)\n",
    "            random.shuffle(indicies)\n",
    "        \n",
    "        # length of k - 1 splits... final split continues to end.\n",
    "        split_len = int(len(X) / (self.k))\n",
    "\n",
    "        for test_idx in range(0, len(X), split_len):\n",
    "        \n",
    "            # create k - 1 training folds for X \n",
    "            train_X = self._fold_training_data(X, idx, test_idx, split_len)\n",
    "            # X test data for fold\n",
    "            test_X = [X[idx[i]] for i in range(test_idx, test_idx + split_len)]\n",
    "            \n",
    "            # create k - 1 training segments for y\n",
    "            train_y = self._fold_training_data(y, idx, test_idx, split_len)\n",
    "            # y test data fold\n",
    "            test_y = [y[idx[i]] for i in range(test_idx, test_idx + split_len)]\n",
    "            \n",
    "            yield train_X, test_X, train_y, test_y\n",
    "            \n",
    "        \n",
    "    def _fold_training_data(self, data, idx, test_idx, split_len):\n",
    "        '''\n",
    "        create training segments for X or y\n",
    "        '''\n",
    "        train_seg1 = [data[idx[i]] for i in range(test_idx)]\n",
    "        train_seg2 = [data[idx[i]] for i in range((test_idx + split_len), \n",
    "                                                 len(data))]                                \n",
    "        return train_seg1 + train_seg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372df46-da67-43dc-bdb4-47db025aa9ad",
   "metadata": {},
   "source": [
    "### `KFold.split()`\n",
    "\n",
    "In the `split()` method you should note that I do not directly randomise the `X` and `y` list parameters (containing the training data). I do not directly modify these lists because it will also modify the variables pointing to the same data outside of the class (its the same data because of the way python passes a list to a function).\n",
    "\n",
    "Here's a simple function `modify_list` that illustrates what happens when you modify a list passed as a parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51d3395-f0d8-416c-beb7-914c4d9a1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outside function BEFORE call lst=['knights', 'who', 'say', 'ni']\n",
      "inside function to_modify=['knights', 'who', 'say', 'ni', 'shrubbery', 'another shrubbery']\n",
      "Outside function AFTER call lst=['knights', 'who', 'say', 'ni', 'shrubbery', 'another shrubbery']\n"
     ]
    }
   ],
   "source": [
    "def modify_list(to_modify):\n",
    "    to_modify.append('shrubbery')\n",
    "    to_modify.append('another shrubbery')\n",
    "    print(f'inside function {to_modify=}')\n",
    "    \n",
    "lst = ['knights', 'who', 'say', 'ni']\n",
    "print(f'Outside function BEFORE call {lst=}')\n",
    "modify_list(lst)\n",
    "print(f'Outside function AFTER call {lst=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ab395-a16c-4bf1-ab39-3470ada2c9e9",
   "metadata": {},
   "source": [
    "In my implementation you will see that I first create `idx` which is a simple integer list 0 to `len(X)`.  I then shuffle the indexes and use them to create a new list via a comprehension.  For example to create the test data\n",
    "\n",
    "```python\n",
    "test_X = [X[idx[i]] for i in range(test_idx, test_idx + split_len)]\n",
    "```\n",
    "A key bit of code in the comprehension is `X[idx[i]]`.  Let's say that we have \n",
    "\n",
    "```python\n",
    "idx = [5, 3, 0, 2, 1, 4]\n",
    "X = [10, 100, 1_000, 10_000, 100_000, 1_000_000]\n",
    "```\n",
    "\n",
    "If `i = 2` then `idx[2]` would evaluate to `0`.  `X[idx[2]` evaluates to `10`.  You can think of this technique as an indirect lookup of your data.\n",
    "\n",
    "In the above example the reordered data is\n",
    "\n",
    "```python\n",
    "reordered_X = [1_000_000, 10_000, 10, 1_000, 100, 100_000]\n",
    "```\n",
    "\n",
    "> I chose to use a list of indexes, but you could also `import copy` and use `internal_X = copy.copy(X)` to achieve the same results - that might even be clearer to read and understand.  You may also not care about modifying the data your assigned to your external variable.  Personally I think that modifying in place is bad practice and will may lead to unintended **silent** side-effects and bugs in your code at a later date.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58cad1-72ad-421f-bba6-8d6e9ff7c5f8",
   "metadata": {},
   "source": [
    "### Using `KFold`\n",
    "\n",
    "As KFold implements the same interface as `LeaveOneOut` we can reuse `cross_validate`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7bcb107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split=> 1, 2, 3, 4, 5, end\n",
      "cv score: 0.85\n"
     ]
    }
   ],
   "source": [
    "X, y = X, y = synthetic_classification(n_samples=160)\n",
    "scores = cross_validate(X, y, KFold(k=5))\n",
    "\n",
    "# print out fake accuracy!\n",
    "print(f'cv score: {sum(scores) / len(scores):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f5e52-3ae4-41b8-953d-4979156c1e4d",
   "metadata": {},
   "source": [
    "## Abstract classes\n",
    "\n",
    "The two classes we have put together is the start of a small framework for cross validation. Its important to recognise that the moment you develop a framework of classes you hit problems - understanding, maintenance, and extension!  Here's a few scenarios where you might hit problems:\n",
    "\n",
    "* When you have a large number of classes to generate splits for cross validation it becomes difficult to keep these all in your head!  A far simpler concept is the idea of a `SplitGenerator`.\n",
    "* Other data scientists, in your team or elsewhere, might want to add their own custom classes to use in place of yours.  Its likely that you want to control this to a certain extent to avoid crashes and improve the experience of your fellow data scientists. \n",
    "\n",
    "In python, to an extent, you can provide this control via a concept called **Abstract base classes**. These are strange beasts!  Here's one for generating our CV splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda833ff-31da-46d0-8711-58bc0a108cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class AbstractSplitGenerator(ABC):\n",
    "    '''\n",
    "    Abstract base class for generating splits of X, y data\n",
    "    for machine learning cross validation. \n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def get_n_splits(self, X):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def split(X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24facbbd-394a-4c3d-9ab2-e6e2c4ddae16",
   "metadata": {},
   "source": [
    "The first line of code imports `ABC` and `abstractmethod` from the built in module `abc`.  This gives us the ingredients we need for creating the base class.  The first step is to tell python that this is a abstract class by subclassing `ABC`\n",
    "\n",
    "```python\n",
    "class AbstractSplitGenerator(ABC):\n",
    "    pass\n",
    "```\n",
    "\n",
    "It is not possible to create an instance of `AbstractSplitGenerator`.  Try it!  Python won't let you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35d61f6c-ffc0-45b4-9237-62182a783952",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class AbstractSplitGenerator with abstract methods get_n_splits, split",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_433104/2482074630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractSplitGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class AbstractSplitGenerator with abstract methods get_n_splits, split"
     ]
    }
   ],
   "source": [
    "cv = AbstractSplitGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f790449-10a5-4f45-be77-e304ae715f2b",
   "metadata": {},
   "source": [
    "In the class body we have defined two **abstract methods** using the `@abstractmethod` decorator.  This decorator **forces** any subclass of `AbstractSplitGenerator` to implement these methods.  Otherwise we get a run time error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecea4f0f-3303-4a85-a1bb-f08508a58a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class DodgyImplementation with abstract methods get_n_splits, split",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_433104/1287983807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDodgyImplementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class DodgyImplementation with abstract methods get_n_splits, split"
     ]
    }
   ],
   "source": [
    "class DodgyImplementation(AbstractSplitGenerator):\n",
    "    pass\n",
    "\n",
    "cv = DodgyImplementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a6cbe-1d84-4501-b4c5-1cd53d7e5072",
   "metadata": {},
   "source": [
    "The style of Abstract classes I have shown you here is akin to interface based inheritance from other languages.  Its a neat way to use inheritance without introducing logic bugs from higher up the class tree (as there is no logic code).  You can think of it as a weak \"contract\".  If you want to write a class in this framework then you guarantee that you provide the methods that other code expects. \n",
    "\n",
    "I specifically called this a **weak** contract above.  This is because in python almost anything goes!  If you want to enforce your contract you need to modify `cross_validation` to check the type of `cv`.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c286f688-678b-450c-89a3-cbba3260b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, cv):\n",
    "    \n",
    "    # enforce interface\n",
    "    if not isinstance(cv, AbstractSplitGenerator):\n",
    "        raise TypeError(f'Expected cv to be AbstractSplitGenerator, '\n",
    "                        + f'but found {type(cv)}')\n",
    "    \n",
    "    scores = []\n",
    "    print('split=> ', end='')\n",
    "    for i, split_data in zip(range(cv.get_n_splits(X)), cv.split(X, y)):\n",
    "        print(f'{i+1},', end=' ')\n",
    "        train_X, train_y, test_X, test_y = split_data\n",
    "        # model fitting, prediction and accuracy assesment is\n",
    "        # just simulated here.\n",
    "        scores.append(random.uniform(0.8, 1))\n",
    "    print('end')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9b7947-1a35-46a7-a913-f72bd179a7c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected cv to be AbstractSplitGenerator, but found <class '__main__.LeaveOneOut'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_433104/696997810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynthetic_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print out fake accuracy!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'cv score: {sum(scores) / len(scores):.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_433104/2306781539.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(X, y, cv)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# enforce interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAbstractSplitGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         raise TypeError(f'Expected cv to be AbstractSplitGenerator, '\n\u001b[0m\u001b[1;32m      6\u001b[0m                         + f'but found {type(cv)}')\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected cv to be AbstractSplitGenerator, but found <class '__main__.LeaveOneOut'>"
     ]
    }
   ],
   "source": [
    "X, y = X, y = synthetic_classification(n_samples=160)\n",
    "scores = cross_validate(X, y, LeaveOneOut())\n",
    "\n",
    "# print out fake accuracy!\n",
    "print(f'cv score: {sum(scores) / len(scores):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0af002-f826-4770-8219-c7c6cddca14c",
   "metadata": {},
   "source": [
    "You then simply create concrete classes by subclassing AbstractSplitGenerator. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b89990-b21b-4b31-a222-aab00a5178f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeaveOneOut(AbstractSplitGenerator):\n",
    "    '''\n",
    "    Leave one out dataset generator for cross validation.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'LeaveOneOut()'\n",
    "\n",
    "    def get_n_splits(self, X):\n",
    "        '''\n",
    "        The number of splits returned by the cross validation\n",
    "        method.\n",
    "        '''\n",
    "        return len(X)\n",
    "    \n",
    "    def split(self, X, y):\n",
    "        '''\n",
    "        Generator method.  Split the dataset\n",
    "        '''\n",
    "        for test_index in range(len(X)):\n",
    "        \n",
    "            # training data indexes\n",
    "            train_X = X[:test_index] + X[test_index + 1:]\n",
    "            train_y = y[:test_index] + y[test_index + 1:]\n",
    "            \n",
    "            # test data\n",
    "            test_X, test_y = X[test_index], y[test_index]\n",
    "            \n",
    "            yield train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c9cccef-4c95-4ae7-94f8-fbddeac05d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split=> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, end\n",
      "cv score: 0.90\n"
     ]
    }
   ],
   "source": [
    "X, y = X, y = synthetic_classification(n_samples=160)\n",
    "scores = cross_validate(X, y, LeaveOneOut())\n",
    "\n",
    "# print out fake accuracy!\n",
    "print(f'cv score: {sum(scores) / len(scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f7643b-5533-4840-a5b1-01be5455f1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
