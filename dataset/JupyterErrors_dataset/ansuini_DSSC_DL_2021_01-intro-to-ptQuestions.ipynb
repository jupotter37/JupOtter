{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "source": [
    "# Example of *inconsistency* in the behavior between `reshape` and `view`\n",
    "\n",
    "Provided by Andrea Gasparin.\n",
    "\n",
    "We have a 2x3 matrix and we wish to reshape it into a size 6 vector"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-36b3fdeceafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# I wish to reshape the matrix z in a vector of size 6 - this gives an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "z = torch.tensor([[1,2,3],[4,5,6]])\n",
    "y = z.t()\n",
    "y.size()\n",
    "y.view(6) # I wish to reshape the matrix z in a vector of size 6 - this gives an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1, 4, 2, 5, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([[1,2,3],[4,5,6]])\n",
    "y = z.t()\n",
    "y.size()\n",
    "yy = y.reshape(6) # this instead is safe\n",
    "print(yy)"
   ]
  },
  {
   "source": [
    "The reason for the error in the first cell lies in the way PyTorch stores the tensor in memory. More in detail, `view` expects the memory to be contiguous, but the transposition `.t()` caused it to be non-contiguous.\n",
    "This is because `.t()` itself is not modifying the underlying memory.\n",
    "\n",
    "A detailed, yet easy-to-grasp explanation is given [here](https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2) by Piotr Bialecki, one of the top PyTorch developers.\n",
    "\n",
    "Let's go more in-depth in the memory management:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([  1,   4, 100,   5,   3,   6])\ntensor([[ -4, 100],\n        [  2,   5],\n        [  3,   6]])\ntensor([[ -4,   2,   3],\n        [100,   5,   6]])\n"
     ]
    }
   ],
   "source": [
    "# the memory of yy is different from the one of y and z\n",
    "yy[2] = 100\n",
    "print(yy)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  1, 100],\n        [  2,   5],\n        [  3,   6]])\ntensor([[  1,   2,   3],\n        [100,   5,   6]])\n"
     ]
    }
   ],
   "source": [
    "# but the memory of y and z is the same\n",
    "y[0,1] = 100\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ -4,   2,   3],\n        [100,   5,   6]])\ntensor([[ -4, 100],\n        [  2,   5],\n        [  3,   6]])\n"
     ]
    }
   ],
   "source": [
    "# also using .T we get the same result\n",
    "zz = y.T\n",
    "zz[0,0] = -4\n",
    "print(zz)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ -4,   2,   3],\n        [100,  36,   6]])\nthe update we made on zz does not propagate on to y\ntensor([[ -4, 100],\n        [  2,   5],\n        [  3,   6]])\n"
     ]
    }
   ],
   "source": [
    "# if we want a deep copy, we need to call .clone() to copy the tensor (NB: .copy() for ndarrays)\n",
    "zz = y.T.clone()\n",
    "zz[1,1] = 36\n",
    "print(zz)\n",
    "print(\"the update we made on zz does not propagate on to y\")\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "# Concerning the behavior of `torch.manual_seed()`\n",
    "\n",
    "Suppose we have two MLPs with identical structure and we wish to ensure they have the same parameters at initialization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 5)\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)\n",
    "\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 5)\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # fix RNG\n",
    "nn1 = NN1()\n",
    "nn2 = NN2()"
   ]
  },
  {
   "source": [
    "Let us print the weights of the layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.2883,  0.0234],\n        [-0.3512,  0.2667],\n        [-0.6025,  0.5183],\n        [-0.5140, -0.5622],\n        [-0.4468,  0.3202]])\ntensor([[-0.1390, -0.5394],\n        [ 0.4630, -0.1668],\n        [ 0.2270,  0.5000],\n        [ 0.1317,  0.1934],\n        [ 0.6825, -0.3189]])\n"
     ]
    }
   ],
   "source": [
    "print(nn1.state_dict()[\"layer.weight\"])\n",
    "print(nn2.state_dict()[\"layer.weight\"])"
   ]
  },
  {
   "source": [
    "we see that they're different even if we fixed the seed above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "They're different even if we instantiate the same class twice:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # fix RNG\n",
    "nn1 = NN1()\n",
    "nn1_copy = NN1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.2883,  0.0234],\n        [-0.3512,  0.2667],\n        [-0.6025,  0.5183],\n        [-0.5140, -0.5622],\n        [-0.4468,  0.3202]])\ntensor([[-0.1390, -0.5394],\n        [ 0.4630, -0.1668],\n        [ 0.2270,  0.5000],\n        [ 0.1317,  0.1934],\n        [ 0.6825, -0.3189]])\n"
     ]
    }
   ],
   "source": [
    "print(nn1.state_dict()[\"layer.weight\"])\n",
    "print(nn1_copy.state_dict()[\"layer.weight\"])"
   ]
  },
  {
   "source": [
    "If we want to ensure equal initialization between the two classes, we need to fix the seed _inside_ the class constructor:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.3643, -0.3121],\n        [-0.1371,  0.3319],\n        [-0.6657,  0.4241],\n        [-0.1455,  0.3597],\n        [ 0.0983, -0.0866]])\ntensor([[ 0.3643, -0.3121],\n        [-0.1371,  0.3319],\n        [-0.6657,  0.4241],\n        [-0.1455,  0.3597],\n        [ 0.0983, -0.0866]])\n"
     ]
    }
   ],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(1)\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 5)\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)\n",
    "\n",
    "class NN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(1)\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 5)\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)\n",
    "\n",
    "nn1 = NN1()\n",
    "nn2 = NN2()\n",
    "\n",
    "print(nn1.state_dict()[\"layer.weight\"])\n",
    "print(nn2.state_dict()[\"layer.weight\"])"
   ]
  },
  {
   "source": [
    "now they're the same.\n",
    "\n",
    "They are the same even if they have structural differences, but the first set of parameters (weights of the first linear layer) have the same size.\n",
    "\n",
    "For instance, let us create and instantiate a model which is NN1, but its linear layer have no bias:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.3643, -0.3121],\n        [-0.1371,  0.3319],\n        [-0.6657,  0.4241],\n        [-0.1455,  0.3597],\n        [ 0.0983, -0.0866]])\ntensor([[ 0.3643, -0.3121],\n        [-0.1371,  0.3319],\n        [-0.6657,  0.4241],\n        [-0.1455,  0.3597],\n        [ 0.0983, -0.0866]])\n"
     ]
    }
   ],
   "source": [
    "class NN1_nobias(nn.Module):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(1)\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 5, bias=False)\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)\n",
    "\n",
    "nn1 = NN1()\n",
    "nn2 = NN1_nobias()\n",
    "\n",
    "print(nn1.state_dict()[\"layer.weight\"])\n",
    "print(nn2.state_dict()[\"layer.weight\"])"
   ]
  },
  {
   "source": [
    "They're the same again.\n",
    "\n",
    "If we wish to opt for an _elegant_ solution which lets us force deterministic initialization **only when we want it**, we can do something like this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1(nn.Module):\n",
    "    def __init__(self, manual_seed=None):\n",
    "        # if the user passes a manual seed, we set it, otherwise we don't\n",
    "        if manual_seed is not None:\n",
    "            torch.manual_seed(manual_seed)\n",
    "\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(2, 5)\n",
    "    def forward(self, X):\n",
    "        return self.layer(X)"
   ]
  }
 ]
}