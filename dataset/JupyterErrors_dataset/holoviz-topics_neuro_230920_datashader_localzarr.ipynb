{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import hvneuro\n",
    "import os\n",
    "\n",
    "import numcodecs\n",
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "import zarr\n",
    "import dask.array as da\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import warnings\n",
    "import kerchunk\n",
    "import kerchunk.hdf\n",
    "import json\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_download = {\n",
    "    \"http://api.brain-map.org/api/v2/well_known_file_download/1026124481\": \"probe_810755797_lfp.nwb\",\n",
    "    # \"http://api.brain-map.org/api/v2/well_known_file_download/1026124479\": \"probe_810755799_lfp.nwb\",\n",
    "    # \"http://api.brain-map.org/api/v2/well_known_file_download/1026124471\": \"probe_810755801_lfp.nwb\",\n",
    "    # \"http://api.brain-map.org/api/v2/well_known_file_download/1026124473\": \"probe_810755803_lfp.nwb\",\n",
    "    # \"http://api.brain-map.org/api/v2/well_known_file_download/1026124475\": \"probe_810755805_lfp.nwb\",\n",
    "    # \"http://api.brain-map.org/api/v2/well_known_file_download/1026124477\": \"probe_810755807_lfp.nwb\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/droumis/data/allen/probe_810755797_lfp.nwb already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "hvneuro.download_files(files_to_download, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/data/.zarray\n",
      "{'chunks': [41859, 1], 'compressor': {'id': 'zlib', 'level': 9}, 'dtype': 'float32', 'fill_value': 0.0, 'filters': None, 'order': 'C', 'shape': [10715666, 93], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['time', 'channel']}\n",
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/electrodes/.zarray\n",
      "{'chunks': [93], 'compressor': None, 'dtype': 'int64', 'fill_value': 0, 'filters': None, 'order': 'C', 'shape': [93], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['channel']}\n",
      "acquisition/probe_810755797_lfp/probe_810755797_lfp_data/timestamps/.zarray\n",
      "{'chunks': [10465], 'compressor': {'id': 'zlib', 'level': 9}, 'dtype': 'float64', 'fill_value': 0.0, 'filters': None, 'order': 'C', 'shape': [10715666], 'zarr_format': 2, '_ARRAY_DIMENSIONS': ['time']}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     f\u001b[39m.\u001b[39mwrite(ujson\u001b[39m.\u001b[39mdumps(refs)\u001b[39m.\u001b[39mencode())\n\u001b[1;32m     60\u001b[0m fs \u001b[39m=\u001b[39m fsspec\u001b[39m.\u001b[39mfilesystem(\u001b[39m\"\u001b[39m\u001b[39mreference\u001b[39m\u001b[39m\"\u001b[39m, fo\u001b[39m=\u001b[39mnwb_ref_filepath)\n\u001b[0;32m---> 61\u001b[0m ds \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39;49mopen_dataset(\n\u001b[1;32m     62\u001b[0m     fs\u001b[39m.\u001b[39;49mget_mapper(),\n\u001b[1;32m     63\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mzarr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     64\u001b[0m     group\u001b[39m=\u001b[39;49mlfp_group_path, \u001b[39m# i think setting the group is one aspect that Ian's approach was missing.. \u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m     backend_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mconsolidated\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mmask_and_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m}\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m data \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mlfp[:\u001b[39m100\u001b[39m, :\u001b[39m4\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==> data\u001b[39m\u001b[39m\"\u001b[39m, data\u001b[39m.\u001b[39mcompute())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[39m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[39m=\u001b[39mbackend\u001b[39m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[39m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moverwrite_encoded_chunks\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mopen_dataset(\n\u001b[1;32m    571\u001b[0m     filename_or_obj,\n\u001b[1;32m    572\u001b[0m     drop_variables\u001b[39m=\u001b[39;49mdrop_variables,\n\u001b[1;32m    573\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdecoders,\n\u001b[1;32m    574\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    575\u001b[0m )\n\u001b[1;32m    576\u001b[0m ds \u001b[39m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/zarr.py:980\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    978\u001b[0m store_entrypoint \u001b[39m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    979\u001b[0m \u001b[39mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 980\u001b[0m     ds \u001b[39m=\u001b[39m store_entrypoint\u001b[39m.\u001b[39;49mopen_dataset(\n\u001b[1;32m    981\u001b[0m         store,\n\u001b[1;32m    982\u001b[0m         mask_and_scale\u001b[39m=\u001b[39;49mmask_and_scale,\n\u001b[1;32m    983\u001b[0m         decode_times\u001b[39m=\u001b[39;49mdecode_times,\n\u001b[1;32m    984\u001b[0m         concat_characters\u001b[39m=\u001b[39;49mconcat_characters,\n\u001b[1;32m    985\u001b[0m         decode_coords\u001b[39m=\u001b[39;49mdecode_coords,\n\u001b[1;32m    986\u001b[0m         drop_variables\u001b[39m=\u001b[39;49mdrop_variables,\n\u001b[1;32m    987\u001b[0m         use_cftime\u001b[39m=\u001b[39;49muse_cftime,\n\u001b[1;32m    988\u001b[0m         decode_timedelta\u001b[39m=\u001b[39;49mdecode_timedelta,\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/backends/store.py:58\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     44\u001b[0m encoding \u001b[39m=\u001b[39m filename_or_obj\u001b[39m.\u001b[39mget_encoding()\n\u001b[1;32m     46\u001b[0m \u001b[39mvars\u001b[39m, attrs, coord_names \u001b[39m=\u001b[39m conventions\u001b[39m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     47\u001b[0m     \u001b[39mvars\u001b[39m,\n\u001b[1;32m     48\u001b[0m     attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     decode_timedelta\u001b[39m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m ds \u001b[39m=\u001b[39m Dataset(\u001b[39mvars\u001b[39;49m, attrs\u001b[39m=\u001b[39;49mattrs)\n\u001b[1;32m     59\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mset_coords(coord_names\u001b[39m.\u001b[39mintersection(\u001b[39mvars\u001b[39m))\n\u001b[1;32m     60\u001b[0m ds\u001b[39m.\u001b[39mset_close(filename_or_obj\u001b[39m.\u001b[39mclose)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/dataset.py:685\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    683\u001b[0m     coords \u001b[39m=\u001b[39m coords\u001b[39m.\u001b[39m_variables\n\u001b[0;32m--> 685\u001b[0m variables, coord_names, dims, indexes, _ \u001b[39m=\u001b[39m merge_data_and_coords(\n\u001b[1;32m    686\u001b[0m     data_vars, coords\n\u001b[1;32m    687\u001b[0m )\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(attrs) \u001b[39mif\u001b[39;00m attrs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/dataset.py:416\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data_vars, coords)\u001b[0m\n\u001b[1;32m    412\u001b[0m     coords \u001b[39m=\u001b[39m create_coords_with_default_indexes(coords, data_vars)\n\u001b[1;32m    414\u001b[0m \u001b[39m# exclude coords from alignment (all variables in a Coordinates object should\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m# already be aligned together) and use coordinates' indexes to align data_vars\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m merge_core(\n\u001b[1;32m    417\u001b[0m     [data_vars, coords],\n\u001b[1;32m    418\u001b[0m     compat\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbroadcast_equals\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    419\u001b[0m     join\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mouter\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    420\u001b[0m     explicit_coords\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(coords),\n\u001b[1;32m    421\u001b[0m     indexes\u001b[39m=\u001b[39;49mcoords\u001b[39m.\u001b[39;49mxindexes,\n\u001b[1;32m    422\u001b[0m     priority_arg\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    423\u001b[0m     skip_align_args\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    424\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/merge.py:723\u001b[0m, in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value, skip_align_args)\u001b[0m\n\u001b[1;32m    718\u001b[0m prioritized \u001b[39m=\u001b[39m _get_priority_vars_and_indexes(aligned, priority_arg, compat\u001b[39m=\u001b[39mcompat)\n\u001b[1;32m    719\u001b[0m variables, out_indexes \u001b[39m=\u001b[39m merge_collected(\n\u001b[1;32m    720\u001b[0m     collected, prioritized, compat\u001b[39m=\u001b[39mcompat, combine_attrs\u001b[39m=\u001b[39mcombine_attrs\n\u001b[1;32m    721\u001b[0m )\n\u001b[0;32m--> 723\u001b[0m dims \u001b[39m=\u001b[39m calculate_dimensions(variables)\n\u001b[1;32m    725\u001b[0m coord_names, noncoord_names \u001b[39m=\u001b[39m determine_coords(coerced)\n\u001b[1;32m    726\u001b[0m \u001b[39mif\u001b[39;00m explicit_coords \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/neuro-ephys-viewer/lib/python3.9/site-packages/xarray/core/variable.py:3257\u001b[0m, in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m   3255\u001b[0m             last_used[dim] \u001b[39m=\u001b[39m k\n\u001b[1;32m   3256\u001b[0m         \u001b[39melif\u001b[39;00m dims[dim] \u001b[39m!=\u001b[39m size:\n\u001b[0;32m-> 3257\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3258\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconflicting sizes for dimension \u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m!r}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3259\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlength \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m!r}\u001b[39;00m\u001b[39m and length \u001b[39m\u001b[39m{\u001b[39;00mdims[dim]\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mlast_used\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3260\u001b[0m             )\n\u001b[1;32m   3261\u001b[0m \u001b[39mreturn\u001b[39;00m dims\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'phony_dim_0': length 93 on 'electrodes' and length 10715666 on {'phony_dim_0': 'data', 'phony_dim_1': 'data'}"
     ]
    }
   ],
   "source": [
    "probe_id = \"810755797\"\n",
    "data_dir='~/data/allen/'\n",
    "data_dir = os.path.expanduser(data_dir)\n",
    "nwb_filepath = os.path.join(data_dir, f\"probe_{probe_id}_lfp.nwb\")\n",
    "nwb_ref_filepath = os.path.join(data_dir, \"lfp_one_probe_ref.json\")\n",
    "lfp_group_path = f\"acquisition/probe_{probe_id}_lfp/probe_{probe_id}_lfp_data\"\n",
    "lfp_data_path = lfp_group_path + \"/data\"\n",
    "electrodes_data_path = lfp_group_path + \"/electrodes\"\n",
    "time_data_path = lfp_group_path + \"/timestamps\"\n",
    "\n",
    "def get_compressor(compression_type, compression_level):\n",
    "    if compression_type == \"gzip\":\n",
    "        return numcodecs.Zlib(compression_level)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "## Create a reference file to the NWB file \n",
    "def set_metadata(f, refs, data_path, dimensions):\n",
    "    ref_path = data_path + \"/.zarray\"\n",
    "    if ref_path in refs['refs']:\n",
    "        ref_str = refs['refs'][ref_path]\n",
    "        ref_dict = json.loads(ref_str)  # Deserialize the JSON string\n",
    "        print(ref_path)\n",
    "        ref_dict['_ARRAY_DIMENSIONS'] = dimensions\n",
    "        compression_type = f[data_path].compression\n",
    "        compression_level = f[data_path].compression_opts\n",
    "        # compressor_info = get_compressor(compression_type, compression_level)\n",
    "        # Convert compressor_info to a dictionary if it's not None\n",
    "        if compression_type == \"gzip\": \n",
    "            compressor_dict = {\n",
    "                'id': 'zlib',\n",
    "                'level': compression_level,\n",
    "            }\n",
    "        else:\n",
    "            compressor_dict = None\n",
    "\n",
    "\n",
    "        ref_dict['dtype'] = str(f[data_path].dtype)\n",
    "        ref_dict['compressor'] = compressor_dict\n",
    "        ref_str = json.dumps(ref_dict)  # Serialize back to a JSON string\n",
    "        refs['refs'][ref_path] = ref_str  # Replace the reference in refs\n",
    "        print(ref_dict)  # Original reference\n",
    "\n",
    "with fsspec.open(nwb_filepath) as f:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(f, nwb_filepath)\n",
    "        refs = h5chunks.translate()\n",
    "\n",
    "# Open the file with h5py to fetch attributes\n",
    "with h5py.File(nwb_filepath, 'r') as f:\n",
    "    # Adding _ARRAY_DIMENSIONS, compression, dtype\n",
    "    set_metadata(f, refs, lfp_data_path, [\"time\", \"channel\"])\n",
    "    set_metadata(f, refs, electrodes_data_path, [\"channel\"])\n",
    "    set_metadata(f, refs, time_data_path, [\"time\"])\n",
    "\n",
    "with open(nwb_ref_filepath, \"wb\") as f:\n",
    "    f.write(ujson.dumps(refs).encode())\n",
    "\n",
    "fs = fsspec.filesystem(\"reference\", fo=nwb_ref_filepath)\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(),\n",
    "    engine=\"zarr\",\n",
    "    group=lfp_group_path, # i think setting the group is one aspect that Ian's approach was missing.. \n",
    "    backend_kwargs={\"consolidated\": False, \"mask_and_scale\": False}\n",
    ")\n",
    "\n",
    "data = ds.lfp[:100, :4]\n",
    "print(\"==> data\", data.compute())\n",
    "mean = data.mean().compute()\n",
    "print(\"==> mean\", mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-ephys-viewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
